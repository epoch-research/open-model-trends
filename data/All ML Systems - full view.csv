System,Domain,Task,Publication date,Accessibility notes,Model accessibility,Dataset accessibility,Code accessibility,Link,Organization,Training compute (FLOP),Training compute notes,Country (from Organization),Authors,Notability criteria,Notability criteria notes,Reference,Training dataset,Training dataset notes,Training dataset size (datapoints),Dataset size notes,Confidence,Batch size,Batch size notes,Citations,Parameters,Parameters notes,Epochs,Inference compute (FLOP),Inference compute notes,Training time (hours),Training time notes,Training hardware,Approach,Training compute cost (2020 USD),Compute cost notes,Compute sponsor categorization,Abstract,Last modified,Created By,Benchmark data,Exclude,Base model,Finetune compute (FLOP),Finetune compute notes,Hardware quantity,Hardware utilization,Training cost trends,Training cloud compute vendor,Training data center,Archived links,Organization categorization,Foundation model,Training compute lower bound,Training compute upper bound,Training chip-hours,Organization categorization (from Organization),Possibly over 1e23 FLOP,Training cost trends 2,Training cost trends 3
Gemini 1.0 Ultra,"Multimodal,Language,Vision","Language modelling,Visual question answering,Chat,Translation",2023-12-06,,Hosted access (no API),,,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Google DeepMind,5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",Multinational,Gemini Team,SOTA improvement,""" Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.""",Gemini: A Family of Highly Capable Multimodal Models,,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",,,Speculative,,,633.00,,,,,,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,,,,Industry,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",2024-05-17 13:51,Anonymous,,0,,,,55000,,Gemini Ultra,,,,Industry,,,,132000000,Industry,,,
GPT-4,"Multimodal,Language,Vision,Image generation",Language modelling,2023-03-15,,API access,,,https://arxiv.org/abs/2303.08774,OpenAI,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",United States of America,"OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)","Highly cited,SOTA improvement","See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""",GPT-4 Technical Report,,,4900000000000,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",Speculative,,,3280.00,,,2.00,,,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,Industry,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",2024-05-13 09:03,Robi Rahman,,0,,,,25000,0.3400,GPT-4,,,,Industry,checked,,,57000000,Industry,,,
Mistral Large,Language,Chat,2024-02-26,,API access,,,https://mistral.ai/news/mistral-large/,Mistral AI,2.0000000001e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48
https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque
Assuming bf16 or fp16, H100 PCIe performance is 1513 TFLOPS
At 1.9 euro per H100-hour and 33% utilization, spending 20M euro produces 1.9*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+3958+TFLOPS+*+0.33
https://www.scaleway.com/en/h100-pcie-try-it-now/",France,,,,"Mistral Large, our new flagship model",,,,,Speculative,,,,,,,,,2500.0,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,NVIDIA H100 PCIe,,$18500000.00,"In February 2024, 20M EUR = 22M USD
Converting to 2020 USD, this is 18.5M
https://www.in2013dollars.com/us/inflation/2024?endYear=2020&amount=22000000",,,2024-03-20 09:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
MegaScale (Production),Language,Language modelling/generation,2024-02-23,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
The model itself is unreleased.",Unreleased,,,https://arxiv.org/abs/2402.15627,"ByteDance,Peking University",1.2e+25,"Speculative. The model is stated to have trained for ""several weeks"". Assuming 530B parameters and ""several"" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:
2166.3 aggregate PFlops/sec * 530B/175B * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 1.2e25  ","China,China","Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",SOTA improvement,Improves SOTA in FLOP utilization for distributed LLM training by 1.34X.,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",,,,"Speculative. Authors note production system was trained on ""multi-trillions of tokens"". This could refer to training for multiple epochs on the same 300B tokens used to train the 175B and 530B models outlined in more detail in the paper. Alternatively, it could refer to a larger dataset of perhaps 3-9 trillion tokens.",Speculative,,,1.00,530000000000.00,"Production run is stated to have ""hundreds of billions of parameters"". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.",,,,504.0,"Speculative. Authors state ""several weeks"". For analysis, I've assumed this means around 3 weeks.",NVIDIA A100,Self-supervised learning,,,Industry,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",2024-04-11 19:10,Anonymous,,0,,,,12288,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Inflection-2,Language,Language modelling,2023-11-22,"via Pi, no API",Hosted access (no API),,,https://inflection.ai/inflection-2,Inflection AI,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",United States of America,,Significant use,"Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2

Inflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",Inflection-2: The Next Step Up,,,,,Likely,,,,,,,,,,,NVIDIA H100 SXM5,,,,Industry,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",2024-03-27 17:27,Anonymous,,0,,,,5000,,Inflection-2,,,,Industry,checked,,,,Industry,,,
Inflection-2.5,Language,Chat,2024-03-07,,Hosted access (no API),,,https://inflection.ai/inflection-2-5,Inflection AI,1.0001e+25,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.""

This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. But Inflection 2, the previous model, was trained on ~1e25 FLOP per Inflection. Inflection-2.5 also does better on benchmarks than 2. Intuitively Inflection-2.5 would be trained on appreciably more compute. 

1e25 seems like a rough, perhaps conservative guess given all this.",United States of America,,Significant use,one million daily users; six million monthly,Inflection-2.5: meet the world's best personal AI,,,,,Speculative,,,,,,,,,,,NVIDIA H100 SXM5,,,,,"At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi—a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.

Now we are adding IQ to Pi’s exceptional EQ.

We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.",2024-04-22 19:30,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
PaLM 2,Language,Language modelling,2023-05-10,,API access,,,https://arxiv.org/abs/2305.10403,Google,7.34e+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP.",United States of America,"Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",SOTA improvement,,PaLM 2 Technical Report,,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)",2700000000000,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",Likely,,,642.00,340000000000.00,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,,,,Google TPU v4,,,PaLM 2 was trained on TPU v4 according to the model card (pages 91-92),Industry,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",2024-05-01 09:24,Robi Rahman,,0,,,,,,PaLM 2,,,,Industry,checked,,,,Industry,,,
Llama 3-70B,Language,"Chat,Language modelling/generation,Code generation",2024-04-18,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",Open access (restricted use),,Open access (restricted use),"https://ai.meta.com/blog/meta-llama-3/

https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/",Meta AI,6.30000000001e+24,"direct calculation
15000000000000 tokens*70000000000.00 parameters*6=6.300000000000001e+24
GPU calculation
400 TFLOPS per GPU * 6.4 million GPU hours * 3600s=9.216 × 10^24

""Our most efficient implementation achieves a compute utilization of over 400 TFLOPS per GPU when trained on 16K GPUs simultaneously. We performed training runs on two custom-built 24K GPU clusters.""
So the throughput may have been lower if they used more than 16k GPUs.",United States of America,Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,Significant use,"Will almost certainly be very influential and widely used in the open access AI industry, as with the previous Llama generations.",Introducing Meta Llama 3: The most capable openly available LLM to date,,,15000000000000,,Confident,,,,70000000000.00,,,,,,,NVIDIA H100 SXM5,,,,,,2024-05-06 17:19,Anonymous,,0,,,,16000,0.4000,,,,,Industry,,,,6400000,Industry,checked,,
Amazon Titan,"Language,Image generation","Semantic search,Image generation,Language modelling/generation,Code generation,Chat",2023-09-28,,API access,,Unreleased,https://aws.amazon.com/bedrock/titan/,Amazon,4.8e+24,"trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",United States of America,,,,,,,4000000000000,4T tokens of data,Speculative,,,,200000000000.00,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",,,,1152.0,,NVIDIA A100,,,,Industry,,2024-05-20 11:55,Anonymous,,0,,,,13760,,,,,,Industry,,,,,Industry,checked,,
Claude 2,Language,"Language modelling,Chat",2023-07-11,,API access,,,"https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",Anthropic,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,United States of America,,Historical significance,,,,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2’s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""",,,Speculative,,,0.00,,,,,,,,,,,,,,2024-05-23 06:35,Anonymous,,0,,,,,,Claude 2,,,,Industry,checked,,,,Industry,,,
Falcon-180B,Language,Language modelling,2023-09-06,"""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b",Open access (restricted use),,,https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,Technology Innovation Institute,3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",United Arab Emirates,"Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",SOTA improvement,"""It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.""

""This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.""",The Falcon Series of Open Language Models,RefinedWeb,"""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",3500000000000,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,Confident,4194304,"from paper (https://arxiv.org/pdf/2311.16867.pdf):

Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

2048*2048 = 4194304",86.00,180000000000.00,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",1.00,360000000000.00,C_inference = 2 FLOP / token / param * N => 360B FLOP per token,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,,,"From Hugging Face:
""Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO.""
""Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances.""
https://huggingface.co/tiiuae/falcon-180B

Utilization must have been at least 12.5%, and they probably did not use the whole 4096 GPU cluster for 9 months, so it was probably higher. Lower bound estimate:
https://www.wolframalpha.com/input?i=%286+FLOP+*+3.5+trillion+*+180+billion%29+%2F+%284096*312+teraFLOPS+*+9+months%29",,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",2024-05-23 04:44,Anonymous,,0,,,,4096,0.1876,Falcon 180B,Amazon Web Services,,,Government,,,,17694720,Government,checked,,
Grok-1,Language,"Language modelling,Chat",2023-11-04,apache 2.0,Open source,Unreleased,Unreleased,"https://x.ai/model-card/, https://x.ai/blog/grok-os",xAI,2.90000000001e+24,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). 

For optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.
https://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion",United States of America,,SOTA improvement,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1""",Announcing Grok,,,,,Likely,,,,314000000000.00,"""314B parameter Mixture-of-Experts model with 25% of the weights active on a given token"". So effectively 78B parameters

Mixture of 8 experts: https://github.com/xai-org/grok-1",,,,,,,,,,,"Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please don’t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help.",2024-04-09 10:25,Anonymous,,,,,,,,,,,,Industry,checked,2,7,,Industry,checked,,
Minerva (540B),Language,Quantitative reasoning,2022-06-29,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2206.14858,Google,2.7415e+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

",United States of America,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",SOTA improvement,,Solving Quantitative Reasoning Problems with Language Models,,"PaLM, finetuned on arxiv",613875000000,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM",,,,427.00,540350000000.00,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery
et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",,,,696.0,,Google TPU v4,Self-supervised learning,$3267257.75,,Industry,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",2024-05-01 09:22,Robi Rahman,,0,PaLM (540B),2,,1024,,Minerva (540B),,,,Industry,checked,,,712704,Industry,,,
DBRX,Language,"Chat,Code generation",2024-03-27,"license: https://www.databricks.com/legal/open-model-license
conditions based on monthly users",Open access (restricted use),Unreleased,Unreleased,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,Databricks,2.6e+24,"Mixture of Experts (MoE)

36 billion params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",United States of America,Mosaic Research Team,,,Introducing DBRX: A New State-of-the-Art Open LLM,,"12T tokens, text and code

""It was pre-trained on 12T tokens of text and code data...

DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models""

from HF: https://huggingface.co/databricks/dbrx-base

The training mix used for DBRX contains both natural-language and code examples. The vast majority of our training data is in the English language",9000000000000,"12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",Confident,,,,132000000000.00,132B mixture of experts. 36B parameters active per inference,1.00,,,,,NVIDIA H100 SXM5,,,,,"Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.",2024-05-17 10:19,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
GPT-3.5 (text-davinci-003),Language,Language modelling,2022-11-28,,API access,,,https://platform.openai.com/docs/models/gpt-3-5,OpenAI,2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,United States of America,,"Historical significance,Significant use,SOTA improvement",,,,,,,Speculative,,,,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",,,,,,NVIDIA A100 SXM4 40 GB,Reinforcement learning,,,Industry,,2024-03-20 09:06,Anonymous,,0,,,,,,GPT-3.5 (text-davinci-003),,,,Industry,,,,,Industry,,,
U-PaLM (540B),Language,Language generation,2022-10-20,,Unreleased,,,https://arxiv.org/abs/2210.11399,Google,2.53e+24,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

original PaLM was 2.527e+24. adding 0.16% is ~2.53e24",United States of America,"Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani",SOTA improvement,"""We show that U-PaLM 540B outperforms PaLM 540B on 21 out of 26 tasks. Given that PaLM is
the SOTA language model on these tasks, this makes U-PaLM the new state-of-the-art on these tasks.""

performance improvement equivalent to 2x training efficiency: ""Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget """,Transcending Scaling Laws with 0.1% Extra Compute,,"""To keep things consistent, we train this model with the same data mixture as PaLM and do not rely on
additional sources of data (labeled or unlabeled).""",,,Confident,,,45.00,540000000000.00,,,,,120.0,5 days,Google TPU v4,,,,,"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ∼4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",2024-04-01 09:03,Anonymous,,0,PaLM (540B),4,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

PaLM was 2.5e24
0.16% of that is 4e21",512,,U-PaLM (540B),,,,Industry,,,,61440,Industry,,,
PaLM (540B),Language,Language modelling,2022-04-04,,Unreleased,,,https://arxiv.org/abs/2204.02311,Google Research,2.5272e+24,"See Table 20: https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers. "" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains",Multinational,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel","Highly cited,SOTA improvement","Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance",PaLM: Scaling Language Modeling with Pathways,,,585000000000,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",Confident,4000000,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k""",3532.00,540350000000.00,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""",,,,1368.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Self-supervised learning,$3232806.53,"Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",Industry,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",2024-05-20 04:22,Robi Rahman,,0,,,,6144,0.4620,PaLM (540B),,,,Industry,checked,,,8404992,Industry,,,
Flan-PaLM 540B,Language,Language modelling/generation,2022-10-20,,Unreleased,,,https://arxiv.org/abs/2210.11416,Google,2.5e+24,"0.2% greater than Palm 540B, which used 2.5e24",United States of America,"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei","Highly cited,SOTA improvement",">1k cites

""Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU.""",Scaling Instruction-Finetuned Language Models,,"Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0, Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ",,,Confident,,,1615.00,540000000000.00,540B,,,,37.0,"""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""",Google TPU v4,,,,,"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",2024-05-23 04:22,Anonymous,,0,PaLM (540B),5,"5.6e21 per Table 2

""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""

512 * 37 * 3600 * 275 teraflops * 0.3 = 5.6e21 (so 30% utilization was correct)",512,0.3000,,,,,Industry,,,,18944,Industry,,,
Qwen-72B,Language,"Chat,Code generation",2023-11-30,"up to 100m active users:
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Open access (restricted use),Unreleased,Unreleased,https://huggingface.co/Qwen/Qwen-72B,Alibaba,1.3e+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24",China,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",SOTA improvement,"SOTA on several Chinese benchmarks, with highest average rating overall for Chinese benchmarks:

https://opencompass.org.cn/leaderboard-llm",,,"""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",,,Likely,4000000,"Table 1 https://arxiv.org/abs/2309.16609
(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)",,72000000000.00,72B,,,,,,,,,,,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",2024-03-27 14:22,Anonymous,,0,,,,,,Qwen-72B,,,,Industry,,,,,Industry,,,
XVERSE-65B-2,Language,Chat,2023-12-08,Apache 2.0,Open source,,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,XVERSE Technology,1.24800000000001e+24,C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP,China,,,,,,"[2023/12/08] Released the XVERSE-65B-2 base model. This model builds upon its predecessor through Continual Pre-Training, reaching a total training volume of 3.2 trillion tokens.",2720000000000,"Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.

Assume 0.85 words per token on average for the mix of languages.",Likely,,,,65000000000.00,Based on the name. Exact count unknown but may be listed on Hugging Face.,,,,4096.0,November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours.,,,,,,,2024-03-27 13:58,Robi Rahman,,0,,,,,,,,,,Industry,,,,,Industry,,,
Code Llama-70B,Language,Code generation,2024-01-29,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Open access (restricted use),,,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",Meta AI,1.230000000001e+24,Finetune compute for 70B model: 1T tokens of code * ,United States of America,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",,"""In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks""",Code Llama: Open Foundation Models for Code,,"We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens.",3000000000000,Llama 70B training dataset was 2 trillion tokens. Code Llama finetuning dataset was 1 trillion tokens of code.,Confident,4000000,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. 

""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""

Subsequent fine-tuning batch sizes are 500k-1M. 

""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",463.00,70000000000.00,70B,1.00,140000000000.00,"Assume 70B parameters, dense architecture, 2 FLOP/inference -> 140B FLOP/inference",6480.0,Assuming Code Llama 70B training continued on same hardware as Llama 2 70B.,NVIDIA A100 SXM4 80 GB,,,,Industry,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",2024-05-19 12:08,Robi Rahman,,0,Llama 2-70B,4,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",400,0.4350,,,,,Industry,,,,2592000,Industry,,,
Mi:dm 200B,Language,,2023-10-31,"KT said it will open up the foundation model of Mi:dm to other companies, providing a full AI development package, including KT Cloud's hyperscale AI computing service and AI chip startup Rebellions Inc.'s neural processing unit infrastructure, fostering the development of various AI services.",API access,,Unreleased,https://genielabs.ai/midm/about,KT,1.2e+24,=1000000000000*200000000000.00*6=1.2 × 10^24,Korea (Republic of),,,,,,,1000000000000,Mi:dm is the first Korean LLM trained on over 1 trillion tokens.,Speculative,,,,200000000000.00,200B,,,,,,,,,,,"TL;DR:
KT Corp introduces Mi:dm, a massive AI model aimed at diverse sectors.
Mi:dm is the first Korean LLM trained on over 1 trillion tokens.
It offers four models, from basic to large, with up to 200 billion parameters.
KT plans to share Mi:dm’s foundational model with other companies.
Three advanced technologies reduce AI hallucinations by up to 70%.
Collaborations with AI startups, including Upstage, aim to conquer the global generative AI market.",2024-05-16 07:07,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Megatron-Turing NLG 530B,Language,Language modelling,2021-10-11,,Unreleased,,Unreleased,https://arxiv.org/abs/2201.11990,"Microsoft,NVIDIA",1.17e+24,https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx,"United States of America,United States of America","Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",SOTA improvement,"The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings","Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","Common Crawl,The Pile,CC-Stories,Realnews"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",202500000000,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",,3932160,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" 

Final batch size is 1920 * 2048 = 3932160",544.00,530000000000.00,,,,,770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,Self-supervised learning,$3046994.09,,Industry,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",2024-05-01 09:13,Robi Rahman,,0,,,,4480,0.3020,Megatron-Turing NLG 530B,,,,"Industry,Industry",checked,,,3449600,"Industry,Industry",,,
ChatGLM3,"Multimodal,Language,Vision","Chat,Visual question answering",2023-10-27,,,,,https://www.zhipuai.cn/en/news/76,Zhipu AI,1.09200000000001e+24,"Highly speculative.
Assume 1 epoch on 1.4T tokens.
6 FLOP/token/param * 1.4T tokens * 130B params
https://www.wolframalpha.com/input?i=6*130+billion*1.4+trillion",China,,SOTA improvement,"Aiming at GPT-4V, ChatGLM3 has implemented iterative upgrades of several new functions this time, including:

CogVLM with multi-modal understanding capabilities, looks at image semantics, and achieved SOTA on more than 10 international standard image and text evaluation data sets;",Zhipu AI launches third-generation base model,,ChatGLM2 corpus pretraining plus human preference alignment training,1050000000000,"The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.
Sources:
https://chatglm.cn/
https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md
https://www.zhipuai.cn/en/news/76",Speculative,,,,130000000000.00,"Highly speculative. The ChatGLM website https://chatglm.cn/ states that the model has hundreds of billions of parameters, so at least 100e9. It also states that the new model is based on ChatGLM2 and the GLM architectures. There is a previous GLM 130B model, so this may be the most likely size.",,,,,,,,,,Industry,"On October 27, 2023, at the 2023 China Computer Conference (CNCC), Zhipu AI launched the fully self-developed third-generation large base model ChatGLM3 and related series of products.",2024-05-13 08:30,Anonymous,,0,,,,,,ChatGLM3,,,,Industry,checked,,,,Industry,,,
EXAONE 1.0,"Multimodal,Language,Vision",,2021-12-14,,,,,,LG,1.08045e+24,300000000000.00*600250000000*6=1.08045e+24,Korea (Republic of),,,,,,"from

""To create multi-modal AI, LG AI Research Institute learned from 600 billion corpora, the world's largest, and more than 250 million high-resolution images combining language and images. It is also differentiated in that it is a bilingual AI that understands and speaks Korean and English at the level of a native speaker.""

600000000000+250000000=600250000000",600250000000,,Speculative,,,,300000000000.00,,,,,,,,,,,,,2024-05-20 07:31,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
ERNIE 3.0 Titan,Language,,2021-12-23,"The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.

http://research.baidu.com/Blog/index-view?id=165",Hosted access (no API),,Unreleased,https://arxiv.org/abs/2112.12731,"Baidu,Peng Cheng Laboratory",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP","China,China","Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",SOTA improvement,"""Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.""",ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,ERNIE 3.0 Corpus,,668000000000,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words per GB",Likely,1048576,"""The maximum sequence length of context and
the memory length of language generation is 512 and 128, respectively""

In table 1, they use a global batch size of 512 when data parallelism is ""1"" and 2048 when DP is ""4"". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.

2048 * 512 = 1048576.",52.00,260000000000.00,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",,,,,,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB",,,,Industry,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",2024-05-01 09:13,Robi Rahman,,0,,,,1920,,ERNIE 3.0 Titan,,,,"Industry,Academia",checked,,,,"Industry,Academia",,,
TigerBot-70B,Language,"Chat,Language generation",2023-09-06,"Apache 2.0 https://github.com/TigerResearch/TigerBot/blob/main/README_en.md

but it's also a Llama 2 finetune.

They released a 5% sample of training data: "" On this basis, we randomly sampled 100G of data and released it open source.""",Open access (restricted use),,Open source,"https://github.com/TigerResearch/TigerBot/blob/main/README_en.md, https://arxiv.org/abs/2312.08688",Tigerobo,1.02e+24,"~1.02e24

Tigerobo did ~2.1e23 additional pre-training. We estimated Llama 2 was trained on 8.1e23 FLOP.",China,"Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu",,"Outperforms Llama 2:

""We use 10 mainstream benchmark test sets in the industry to evaluate the model's reading comprehension, reasoning, world knowledge, common sense Q&A, mathematics and coding capabilities, including: mmlu, arc, squad_v2, squad, mrqa, web_questions, openbook_qa, commonsense_qa, trivia_qa, wiki_qa. The evaluation results are shown in the figure below. The comprehensive capabilities of Tigerbot-70b are better than Llama-2-70b, and also ahead of large open source models at home and abroad.""

https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81",TigerBot: An Open Multilingual Multitask LLM,,"In addition to Llama 2 pretraining dataset, TigerBot was trained on 500B tokens including Chinese data

""Training data: 500B tokens pre-trained data, knowledge截止 to August 2023. More high-quality data, including: tens of thousands of volumes, arXiv, Chinese textbooks, legal and patent data;""",,,Likely,4000000,"from paper:

""We pretrained TigerBot models using a global batch size (GBS) of 4M tokens, while fine-tuned models with a GBS as small as 100–400k tokens""

It's also based on pretrained Llama 2, which also used a batch size of 4M",,70000000000.00,70B,,,,,,,,,,,"(translated from https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81)

We are pleased to release Tigerbot-70b, which continues to be open source and free for commercial use, including:

Tigerbot-70b-base: Continuing pre-training on the basis of Llama-2-70b, the model's comprehensive capabilities are better than Llama-2-70b in 10 mainstream benchmark tests such as mmlu, reaching SOTA in the industry.

a. Using high-quality multi-lingual data of 300 billion tokens,

b. The algorithm uses GQA, flash-attn, RoPE, holistic-training and other technologies,

c. The training uses tensor/pipeline-partition technology, and the computing efficiency reaches the SOTA reported in the Llama-2 paper.",2024-04-09 09:03,Anonymous,,0,Llama 2-70B,2,"""Training data: 500B tokens pre-trained data, knowledge截止 to August 2023. More high-quality data, including: tens of thousands of volumes, arXiv, Chinese textbooks, legal and patent data;""

70b * 500b * 6 = 2.1e23",,,,,,,Industry,,,,,Industry,,,
Inflection-1,Language,Language modelling,2023-06-23,,Hosted access (no API),,,https://inflection.ai/assets/Inflection-1.pdf,Inflection AI,1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",United States of America,,,"""Inflection-1 outperforms models trained with at most the same amount of compute as PaLM-540B on MMLU and the other benchmarks in Table 1.""",Inflection-1 technical memo,,"""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,Speculative,,,0.00,,,,,,,,NVIDIA H100 SXM5,,,,Industry,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAI’s Chat-GPT and Google’s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) – an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",2024-03-20 09:30,Anonymous,,0,,,,,,,,,,Industry,checked,,,,Industry,,,
MegaScale (530B),Language,Language modelling/generation,2024-02-23,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
Model weights are unreleased",Unreleased,Unreleased,Open source,https://arxiv.org/abs/2402.15627,"ByteDance,Peking University",8.419999999999998e+23,"275B model uses an estimate of 2.78e+23 FLOP.

FLOPs should scale linearly in # parameters, so:
2.78e+23 FLOP * 530B / 175B = 8.42e23","China,China","Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",SOTA improvement,Improves SOTA in FLOP utilization for distributed LLM training by 1.34X,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",,175B and 530B models trained for paper use 300B tokens each.,225000000000,300B tokens * 0.75 words/token = 225B words,Confident,6144,Experiments with both 768 and 6144,1.00,530000000000.00,"Two models are trained for epoch to evaluate the MegaScale training system; one model with 175B and another with 530B parameters. This entry reports the 530B model.

There is a third production model mentioned, with fewer details.",1.00,,,127.2,"175B parameter model is stated to have taken 1.75 days. Assume this scales linearly in parameters:
1.75 days * 24 hours/day * 530B / 175B = 127.2 hours",NVIDIA A100,Self-supervised learning,,,,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",2024-04-11 19:10,Anonymous,,1,,,,11200,0.5430,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Llama 2-70B,Language,Language modelling,2023-07-18,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Open access (restricted use),,,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Meta AI,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",United States of America,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
","Historical significance,Significant use,Highly cited",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,Llama 2: Open Foundation and Fine-Tuned Chat Models,Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",1500000000000,2 trillion tokens ~= 1.5 trillion words,Confident,4000000,,3131.00,70000000000.00,"Llama has been released in 7B, 13B, 34B, and 70B variants.",1.00,140000000000.00,Inference compute usage for the 70B model is 140 billion operations per token of input.,1728.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.",NVIDIA A100 SXM4 80 GB,Supervised,$1620000.00,"A100 cost in 2023: $1.10/hour
Training time: 1720320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023",Industry,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",2024-04-12 11:08,Anonymous,,0,,,,1000,0.4350,Llama 2-70B,,Meta’s Research Super Cluster,,Industry,,,,1720320,Industry,,,
DeepSeek LLM 67B,Language,Chat,2024-01-05,https://github.com/deepseek-ai/deepseek-LLM/blob/main/LICENSE-MODEL,Open access (restricted use),,,"https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",DeepSeek,8.04e+23,67B * 2T * 6 = 8.04e23,China,"Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",,"One of the best open/Chinese models: ""Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.""",DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English.""

""We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a)... We adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump""",2000000000000,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English""",Likely,,,,67000000000.00,"67B
",1.00,,,,,,,,,,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",2024-05-21 03:58,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Llama 3-8B,Language,"Chat,Language modelling/generation,Code generation",2024-04-18,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",Open access (restricted use),,Open access (restricted use),"https://ai.meta.com/blog/meta-llama-3/

https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/",Meta AI,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6=7.2×10^23

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872×10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",United States of America,Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,Significant use,"Will almost certainly be very influential and widely used in the open access AI industry, as with the previous Llama generations.",Introducing Meta Llama 3: The most capable openly available LLM to date,,,15000000000000,,Confident,,,,8000000000.00,,,,,,,NVIDIA H100 SXM5,,,,,,2024-05-21 04:41,Natalia Martemianova,,1,,,,16000,0.4000,,,,,Industry,,,,1300000,Industry,checked,,
Gopher (280B),Language,Language modelling,2021-12-08,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2112.11446,DeepMind,6.31e+23,"Table A26
6.31E+08 Train PFLOPs",United Kingdom of Great Britain and Northern Ireland,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",SOTA improvement,"""These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority""","""Scaling Language Models: Methods, Analysis & Insights from Training Gopher""",MassiveTex,,225000000000,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",Confident,6000000,"Table 1. ""Furthermore, we increase Gopher’s batch size from three to six million tokens per batch during training""",917.00,280000000000.00,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",1.00,,,920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Self-supervised learning,$891638.80,,Industry,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",2024-05-01 09:13,Robi Rahman,Gopher (280B),0,,,,4096,0.3780,Gopher (280B),,,,Industry,,,,3768320,Industry,,,
Reka Flash,"Multimodal,Language,Vision","Chat,Language modelling/generation,Image captioning,Code generation,Code autocompletion",2024-04-15,,API access,,Unreleased,https://publications.reka.ai/reka-core-tech-report.pdf,Reka AI,6.3e+23,"Reka Flash has 21B parameters and was trained on 5 trillion language tokens 

6*21B*5trillion=6.3 × 10^23

Agrees with GPU details: ""Reka Flash and Edge were trained on several hundreds of H100s across a period of several weeks.""

3 weeks * 300 H100s * 7 day/week * 24 hour/day * 3600 s/day * 9.9e14 FLOP/GPU-s = 5.4e23

Not enough info to estimate SFT and RLHF post-training FLOPs.",United States of America,"Aitor Ormazabal Che Zheng Cyprien de Masson d’Autume Dani Yogatama
Deyu Fu Donovan Ong Eric Chen Eugenie Lamprecht Hai Pham Isaac Ong
Kaloyan Aleksiev Lei Li Matthew Henderson Max Bain Mikel Artetxe
Nishant Relan Piotr Padlewski Qi Liu Ren Chen Samuel Phua
Yazheng Yang Yi Tay Yuqi Wang Zhongkai Zhu Zhihui Xie",,,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",Wikipedia,"The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset
knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and
audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively
deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly
defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are
STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to
math. O",5000000000000,,Likely,,,,21000000000.00,,,,,,,"NVIDIA A100,NVIDIA H100 SXM5",,,,,,2024-05-13 07:52,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
Yi-34B,Language,Chat,2023-11-02,"apply for commercial:
https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt",Open access (restricted use),,,https://arxiv.org/abs/2403.04652,01.AI,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",China,"Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",Significant use,"2nd most popular model on HuggingFace: https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-chatgpt

also maybe the best open-source model, does better than Llama 2-70B on several benchmarks",Yi: Open Foundation Models by 01.AI,,Chinese and English dataset,,,Speculative,,,,34000000000.00,34b,,,,,,,,,,,The Yi series models are large language models trained from scratch by developers at 01.AI.,2024-04-02 08:20,Anonymous,,0,,,,,,Yi-34B,,,Yi-34B,Industry,,,,,Industry,,,
xTrimoPGLM -100B,Biology,Proteins,2023-07-06,,Unreleased,,Unreleased,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,"Tsinghua University,BioMap Research",6.0001e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1
trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date,
xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24","China,China","Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",SOTA improvement,"""Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories)""",xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,UniRef50,,,~24M protein sequences,Likely,,,32.00,100000000000.00,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",,,,3912.0,163 days,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",2024-05-13 15:56,Anonymous,,,,,,768,,,,,,"Academia,Industry",,,,3004416,"Academia,Industry",,,
BLOOM-176B,Language,Language modelling,2022-11-08,responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,Open access (restricted use),,,https://arxiv.org/abs/2211.05100,"Hugging Face,BigScience",5.7700000000000996e+23,"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32

384 A100 GPUs * 150 TFLOPS throughput per GPU * 116 days = 5.77e+23 FLOP
https://www.wolframalpha.com/input?i=384+*+150+TFLOPS+*+116+days","Multinational,Multinational","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff","Historical significance,Highly cited","Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.

https://huggingface.co/bigscience/bloom",BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,BigScience ROOTS Corpus,"In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
arXiv:2210.15424

""BLOOM was trained on the ROOTS corpus (Lauren¸con et al., 2022), a composite collection
of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that
span 46 natural languages and 13 programming languages. A high-level overview of this
dataset can be seen in Figure 3, while a detailed itemized list of every language along with
its linguistic genus, family and macroarea is presented in Table 1""",262500000000,350B tokens ~= 262B words,Confident,4194304,Table 3. 2048*2048,1313.00,176247271424.00,"See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom",1.00,,,2808.0,117 days * 24 hours/day,NVIDIA A100 SXM4 80 GB,Self-supervised learning,,,,,2024-04-30 14:38,Robi Rahman,,0,,,,384,0.4808,BLOOM-176B,,,,"Industry,Research collective",checked,,,1078272,"Industry,Research collective",,,
Chinchilla,Language,Language modelling,2022-03-29,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2203.15556,DeepMind,5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3",United Kingdom of Great Britain and Northern Ireland,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",SOTA improvement,"Proposes new scaling law, with good empirical results",Training Compute-Optimal Large Language Models,"MassiveWeb,C4","MassiveWeb, Books, C4, News, Github, Wikipedia (Table A1)",1050000000000,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",Likely,3000000,"Table 1. ""1.5M → 3M""",1043.00,70000000000.00,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",1.00,,,,,"Google TPU v4,Google TPU v3",Self-supervised learning,$753491.58,,Industry,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",2024-05-01 09:22,Robi Rahman,Chinchilla,0,,,,,,Chinchilla,,,,Industry,checked,,,,Industry,,,
BIG-G 137B,Language,Language modelling/generation,2022-06-09,,,,,https://arxiv.org/abs/2206.04615,Google,5.6e+23,"""BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswani
et al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDA
architectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""

Appendix:

""We use a pre-training batch size of 262k tokens for all models...""

2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)
681B * 137B * 6 = 5.6e23",United States of America,,,,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,,"""These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""",,,Likely,,,610.00,137000000000.00,"137B. Table App.1
",,,,,,,,,,Industry,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ""breakthrough"" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
LLaMA-65B,Language,Language modelling,2023-02-24,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",Open access (non-commercial),,Unreleased,https://arxiv.org/abs/2302.13971,Meta AI,5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29",United States of America,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample","Historical significance,Highly cited",Widely-used foundation model that has been adapted for others such as Alpaca.,LLaMA: Open and Efficient Foundation Language Models,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",1400000000000,1.4 trillion tokens * 0.75 words/token = 1.05 trillion words,Likely,4000000,,4640.00,65200000000.00,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",1.09,130000000000.00,per token,500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,Supervised,$1179384.75,"1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ 
According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.
$1391674 / 1.18 = $1179385 in 2020 USD.",Industry,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",2024-05-23 05:09,Anonymous,LLaMA-65B,0,,,,2048,0.4746,LLaMA-65B,,,,Industry,checked,,,1024000,Industry,,,
Code Llama-34B,Language,Code generation,2023-08-14,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Open access (restricted use),,,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",Meta AI,5.3e+23,"1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute
",United States of America,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",,"SOTA for open models: ""Moreover, our largest model,
with its 34B parameters, is significantly larger than previous open-source models – GPT-NeoX-20B (Black
et al., 2022) and StarCoder with 15.5B parameters – which allows it to achieve state-of-the-art performances
on HumanEval, MBPP and MultiPL-E among open-source models""",Code Llama: Open Foundation Models for Code,,"""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of
publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language
questions or answers. To help the model retain natural language understanding skills, we also sample a small
proportion of our batches from a natural language dataset""",,,Likely,4000000,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. 

""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""

Subsequent fine-tuning batch sizes are 500k-1M. 

""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",463.00,34000000000.00,34B,,,,,,NVIDIA A100 SXM4 80 GB,,,,,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",2024-04-09 09:25,Anonymous,,0,Llama 2-34B,1,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",,,,,,,Industry,,,,,Industry,,,
MM1-30B,"Multimodal,Language,Vision","Chat,Image captioning",2024-03-14,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2403.09611,Apple,4.86e+23,"Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.
Then additional multimodal training for 400B tokens, for a total of ~2.7T tokens.

This is the final training recipe: ""We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).""

Compute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23

maybe the size of the visual connector is relevant",United States of America,"Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang",SOTA improvement,""" In particular, the pretrained model MM1 is SOTA, performing better than Emu2 [105], Flamingo [3],
and IDEFICS [47] on captioning and visual question answering (VQA) tasks
in few-shot settings, both in small and large size regimes""

Table 4: outperforms Gemini and GPT-4V on VQA","MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",,"Text, captioned images. See Table 2",1500000000000,at least 2T tokens,Likely,,,11.00,30000000000.00,30B,,,,,,,,,,,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",2024-05-15 16:08,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
PanGu-Σ,Language,"Code generation,Language modelling",2023-03-20,,Unreleased,,,https://arxiv.org/abs/2303.10845,Huawei Noah's Ark Lab,4.669999999999999e+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",China,"Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",SOTA improvement,"""Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks.""",PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,,"""329B tokens in more than 40 natural and programming languages""",246750000000,329B tokens ~= 247B words,Confident,524288,"""We train PanGu-Σ with global batch size of 512 with sequence length of 1024 for each sample""",36.00,1085000000000.00,"""In this work, we present PanGu-Σ , a large language model with sparse architecture containing 1.085 trillion parameters.""",1.84,,,2400.0,"We develop PanGu-Σ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,Self-supervised learning,,,Industry,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",2024-05-01 09:24,Anonymous,,0,,,,512,,PanGu-Σ,,,,Industry,,,,1228800,Industry,,,
OPT-175B,Language,Language modelling,2022-05-02,"non-commercial for weights:
https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,"https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/
https://arxiv.org/abs/2205.01068",Meta AI,4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""",United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer","Significant use,Highly cited",https://ai.meta.com/blog/opt-175b-large-language-model-applications/,OPT: Open Pre-trained Transformer Language Models,"The Pile,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",CC-Stories,Pushshift Reddit","""The pre-training corpus contains a concatenation
of datasets used in RoBERTa (Liu et al., 2019b),
the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021)""
...
""RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021. This CCNews v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).

The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. Other subsets of the Pile were eliminated
...

PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021). To convert the conversational trees
into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.",135000000000,"""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",Confident,2000000,Table 1,1987.00,175000000000.00,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",1.67,,,793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,Self-supervised learning,$1654082.50,,Industry,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",2024-04-30 15:01,Robi Rahman,OPT-175B,0,,,,1024,0.4712,OPT-175B,,,,Industry,,,,812544,Industry,,,
OPT-IML (175B),Language,Language modelling,2022-12-22,"unclear license
https://huggingface.co/facebook/opt-iml-30b",Open access (non-commercial),,Unreleased,https://arxiv.org/abs/2212.12017,Meta AI,4.3e+23,"fine-tuned from OPT-175B (4.3e23) with an estimate 2.1e21 FLOP for fine-tuning. 
""During fine-tuning, our models saw approximately 2 billion tokens, which is only 0.6% of the pre-training budget of OPT""",United States of America,"Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanov",,,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,OPT-IML Bench,"(fine-tuning dataset) ""To this end, we create OPT-IML Bench: a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks""",2000000000,"""During fine-tuning, our models saw approximately 2 billion tokens, which is only 0.6% of the pre-training budget of OPT""",Likely,128,Table 3,175.00,175000000000.00,,,,,72.0,Table 3,NVIDIA A100,,,,,"Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",2024-05-13 10:28,Anonymous,,0,OPT-175B,2,"""We fine-tune all 30B models on 64 40GB A100s, and 175B models on 128 40GB A100s"", no timeframe specified

fine-tuned on 2B tokens. 2B * 175B * 6 = 2.1e21

",128,,,,,,Industry,,,,9216,Industry,,,
BlenderBot 3,Language,Chat,2022-08-10,"weights have a non-commercial license, must go through request form: https://docs.google.com/forms/d/e/1FAIpQLSfRzw8xVzxaxgRyuodTZtkcYADAjzYjN5gcxx6DMa4XaGwwhQ/viewform",Open access (non-commercial),Open source,,"https://arxiv.org/abs/2208.03188, https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md","McGill University,Meta AI,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",4.3e+23,(taken from OPT-175 base),"Canada,United States of America,Canada","Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston",SOTA improvement,"""Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors""",BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,BlenderBot 3 Data,"Fine-tuned from OPT-175B.

""The fine-tuning data for BB3 comprises roughly 4 million source/target examples spread across the various
training modules. This corresponds to around 1.13B training tokens. When fine-tuning the OPT-based
BB3 models, we additionally included 600k examples ( 170m tokens) of pre-training data to help with
training stability. Table 16 and Table 17 enumerate the breakdown by module.""
",,,Likely,262144,"Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.

""The 175B model was trained with a batch size of 2^18""
2^18 = 262144",181.00,175000000000.00,,,,,,,NVIDIA A100 SXM4 40 GB,,,,Industry,"We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",2024-05-01 09:22,Anonymous,,0,OPT-175B,1,"""The 30B and 175B parameter BlenderBot 3 models were each trained for one epoch of the training data
on 64 (30B) or 128 (175B) x 40gb A100 GPUs; we found that the model (especially the 175B version)
overfit significantly when seeing the training data more than once. The 175B model was trained with
a batch size of 2^18 and the 30B model was trained with a batch size of 2^19, resulting in roughly 5600
updates and 2800 updates respectively.""

175b params * 5600 * 2^18 * 6 = 1.5e21
",128,,BlenderBot 3,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Llama 2-34B,Language,,2023-07-18,,Unreleased,,,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Meta AI,4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",United States of America,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom",Highly cited,,Llama 2: Open Foundation and Fine-Tuned Chat Models,Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",,,Confident,4000000,,3131.00,34000000000.00,34B,1.00,,,,,NVIDIA A100 SXM4 80 GB,,,,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.",2024-04-01 09:35,Anonymous,,1,,,,,,,,,,Industry,,,,,Industry,,,
 phi-3-medium 14B,Language,"Chat,Language modelling/generation",2024-04-23,announced to be released later,Unreleased,,,https://arxiv.org/abs/2404.14219,Microsoft,4.032e+23,counting operations: 6×4.8×10^12×14×10^9 ≈ 4.032×10^23 FLOPS,United States of America,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,Unspecified unreleased,"we also trained phi-3-medium, a model with 14B parameters using the same tokenizer and architecture of phi-3-mini, and trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small)",4800000000000,,Likely,,,,14000000000.00,14B,,,,,,,,,,,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",2024-04-30 12:42,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,checked,,
ViT-22B,Vision,"Object detection,Image classification",2023-02-10,,,,,https://arxiv.org/abs/2302.05442v1,Google,4.000100000000001e+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a
14 × 14 patch extracted from 224 × 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k:
approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

256 * 177k * 65k = 3T tokens
6 * 22B * 3T = 3.96e23 ~= 4e23


also, MFU was high:
""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and
backward pass) on TPUv4 (Jouppi et al., 2020). ViT-22B’s model flops utilization (MFU) (Chowdhery et al.,
2022; Dehghani et al., 2021a) is 54.9%, indicating a very efficient use of the hardware.""

as a sanity check, 4e23 / (1024 * 275 teraFLOP/s (TPUv4 FLOP) * 0.55) = 2582644 seconds, or 30 days, which is a plausible length",United States of America,"Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",SOTA improvement,"""The largest
ViT-22B sets the new SOTA on the challenging ObjectNet test set""",Scaling Vision Transformers to 22 Billion Parameters,JFT-4B,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",4000000000,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",Likely,,,285.00,21743000000.00,"21.743B, Table 1",2.90,,,,,Google TPU v4,,,,,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",2024-05-01 09:22,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Parti,Image generation,Text-to-image,2022-06-22,"""For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place""
https://sites.research.google/parti/",Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2206.10789v1,Google Research,3.962895376192635e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""",Multinational,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",SOTA improvement,"""Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO""",Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,"LAION-400M,FIT400M,JFT-4B",,4800000000,,,,,672.00,20000000000.00,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",,,,,,Google TPU v4,Self-supervised learning,$486659.77,,Industry,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",2024-05-01 09:22,Robi Rahman,,0,,,,,,Parti,,,,Industry,checked,,,,Industry,,,
DeepSeek Coder 33B,Language,Code generation,2023-11-02,"code doesn't seem to be training code.

deepseek license:
https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL",Open access (restricted use),Unreleased,Unreleased,https://github.com/deepseek-ai/DeepSeek-Coder,DeepSeek,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",China,,,"SOTA among open-source: ""For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.""",,,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""",2000000000000,,Likely,,,,33000000000.00,33B,,,,,,,,,,,"DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and an extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.",2024-05-21 04:04,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Viking,Language,"Language modelling/generation,Language generation,Translation",2024-04-04,,Open source,,Open source,https://huggingface.co/LumiOpen/Viking-33B,"Silo AI,University of Turku",3.96e+23,6*33B*2trillion=3.96e23,"Finland,Finland",,,,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License.",,,2000000000000,"Viking is being trained on a 2 trillion token mixed dataset of English, Finnish, Swedish, Danish, Norwegian, Icelandic and code.",Unverified,,,,33000000000.00,,,,,,,AMD Instinct MI250X,,,,,,2024-05-16 17:51,Natalia Martemianova,,,,,,1024,,,,LUMI supercomputer (Finland),,"Industry,Academia",,,,,"Industry,Academia",checked,,
StarCoder 2 15B,Language,"Code generation,Code autocompletion",2024-02-29,"commercial use allowed, but various use cases restricted: https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement",Open access (restricted use),,,https://arxiv.org/abs/2402.19173,"Hugging Face,ServiceNow,NVIDIA,BigCode",3.87e+23,estimation is given in Table 6 ,"Multinational,United States of America,United States of America","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",,,StarCoder 2 and The Stack v2: The Next Generation,StarCoder v2,created from repositorites from Github with permissive licences.,4100000000000,"from Table 7, 
4.1T tokens ",Likely,,,,7000000000.00,7B,,,,,,,,,,,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",2024-03-27 13:15,Bartosz Podkanowicz,,0,,,,,,,,,,"Industry,Industry,Industry",,,,,"Industry,Industry,Industry",,,
FunSearch,"Language,Search",Code generation,2023-12-14,"Code to run FunSearch with an LLM of your choice is open source under Apache 2.0 (software) and CC-BY (all else). However, the actual LLM used in the main experiments is unknown and may or may not be one of the Codey models available via API access.",Open source,,,"https://www.nature.com/articles/s41586-023-06924-6
https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",Google DeepMind,3.87e+23,"Appendix A.5: ""Finding the full-sized symmetric admissible set I(15, 10) required the generation and analysis of approximately two million programs... To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days. We estimate that when running on Google Cloud, the price of an experiment is around $800 – $1400, and the energy usage around 250 – 500 kWh; i.e., 0.5% of the energy used for training StarCoder""

15 GPUs * 7.80E+13 FLOP/GPU-sec * 2 days * 24 hours/day * 3600 sec/hour = 2.02e20 FLOP for the GPU servers

We should also add the compute used to train the PaLM2 variant used as the base LLM. Since we don't have any details about this model, I use the compute from StarCoder-15B (used as the open source comparison point): 3.87e+23 FLOP

Unclear how to evaluate the compute from the CPU servers implementing the evolutionary algorithm, but this is very likely dwarfed by the pre-training compute for the LLM.",Multinational,"Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi ","SOTA improvement,Historical significance",Improved SOTA for the cap set problem. Can plausibly claim the first instance of a LLM system making a genuine and novel scientific contribution.,Mathematical discoveries from program search with large language models,,"""The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks""",0,"""The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks""",Speculative,,,67.00,15000000000.00,"From the section called ""Pretrained LLM"": ""We use Codey, an LLM built on top of the PaLM2 model family... Because FunSearch relies on sampling from an LLM extensively, an important performance-defining tradeoff is between the quality of the samples and the inference speed of the LLM. In practice, we have chosen to work with a fast-inference model (rather than slower-inference, higher-quality)""

Unclear which PaLM2 model was used (of Gecko, Otter, Bison, and Unicorn); above quote indicates it was perhaps Otter or Bison, but not Unicorn. Exact parameter counts are not publicly disclosed for any of these models. In comparisons where FunSearch uses StarCoder-15B, Codey is an improvement but not obviously of an entirely different model class.

I report the 15B parameters from StarCoder-15B, used as an open-source comparison",,,,48.0,"Appendix A.5: ""To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days""",,,$929.00,"Appendix A.5: ""We estimate that when running on Google Cloud, the price of an experiment is around $800 – $1400, and the energy usage around 250 – 500 kWh; i.e., 0.5% of the energy used for training StarCoder"" (in reference to a replication done using StarCoder-15B)

Estimate (800+1400)/2 = $1100 at time of publication. CPI conversion to 2020 dollars: $929",Industry,"Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.",2024-04-18 19:49,Anonymous,,0,PaLM 2,0,No finetuning,,,,,,,Industry,,,,,Industry,,,
GLM-130B,Language,,2022-08-04,non commercial: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE,Open access (non-commercial),,,https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,Tsinghua University,3.778e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 30% utilization = 3.778*10^23 FLOPhttps://www.wolframalpha.com/input?i=312+teraflops+*+96+*+8+*+2+months+*+30%25

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""",China,"Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",SOTA improvement,"""GLM-130B achieves an accuracy of 80.2% on zero-shot LAMBADA (En), while 76.2% for GPT-3 175B and 77.9% for the SOTA offered by PaLM 540B.""",GLM-130B: An Open Bilingual Pre-trained Model,"The Pile,WuDao Corpora","""The pre-training data includes 1.2T Pile (train split) (Gao et al., 2020) English, 1.0T Chinese WudaoCorpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and
QA) we crawl from the web, which form a balanced composition of English and Chinese contents""",,,,,,641.00,130000000000.00,Dense model,1.00,,,1440.0,see compute notes,NVIDIA A100 SXM4 40 GB,,,,Industry,,2024-05-09 14:40,Robi Rahman,GLM-130B,0,,,,768,0.4330,GLM-130B,,,,Academia,checked,,,1105920,Academia,,,
GLaM,Language,,2021-12-13,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2112.06905,Google,3.74e+23,"from paper: ""GLaM (64B/64E) training after 600B tokens consumes 456 MWh, about 1/3 of the energy cost of 1287 MWh used by GPT-3. Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we train using 1,024 TPU-v4 chips for 574 hours (with 280B tokens). This consumes 213 MWh or 1/6 of the GPT-3 energy cost""

600/280 is almost exactly 456/213 (2.14) so the later tokens have the same per-token energy cost. 
2.14*574*1024 = 1,257,840 TPU-v4 hours
TPU-v4s are 275 teraFLOP/s. 
Using our usual 0.3 utilization assumption, 275 trillion * 1,257,840 * 3600 * 0.3 = 3.74e23

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.",United States of America,"Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",SOTA improvement,"""As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open-domain setting""",GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,Wikipedia,"""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their quality ranges from professional writing to low-quality comment and forum pages.""",800000000000,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.",Likely,1000000,"""We use a maximum sequence
length of 1024 tokens, and pack each input example to have
up to 1 million tokens per batch.""",440.00,1200000000000.00,1.2 trillion parameters,,180000000000.00,"180 GFLOPs/token, per Table 1",1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,,,,,,2024-05-01 09:13,Anonymous,,0,,,,1024,,GLaM,,,,Industry,checked,,,1398784,Industry,,,
Jurassic-1-Jumbo,Language,,2021-08-11,,API access,,,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,AI21 Labs,3.7e+23,see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,Israel,"Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",,,Jurassic-1: Technical Details and Evaluation,,,225000000000,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,3200000,"""Namely, we used a base learning rate of 1.2 × 10−4 and 0.6 × 10−4 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.""",55.00,178000000000.00,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",,,,,,,Self-supervised learning,$805277.01,,Industry,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",2024-03-27 17:23,Robi Rahman,,0,,,,,,,,,,Industry,,,,,Industry,,,
LaMDA,Language,Language modelling,2022-02-10,,Unreleased,,,https://arxiv.org/abs/2201.08239,Google,3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",United States of America,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",Historical significance,,LaMDA: Language Models for Dialog Applications,Infiniset,"LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1560000000000,"""and are pre-trained on 1.56T words of public dialog data and web text""",Confident,256000,"""All models were trained with 256K tokens per batch""",1082.00,137000000000.00,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",,,,1385.0,57.7 days * 24,Google TPU v3,Self-supervised learning,$484957.20,,Industry,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",2024-04-01 09:28,Robi Rahman,,0,,,,1024,0.5650,LaMDA,,,,Industry,checked,,,1418240,Industry,,,
Yuan 1.0,Language,Language modelling,2021-10-12,https://github.com/Shawn-IEITSystems/Yuan-1.0,API access,,Unreleased,https://arxiv.org/abs/2110.04725,Inspur,3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
",China,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",SOTA improvement,"""The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin""",Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,"Common Crawl,Wikipedia,Sogue News","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""

In order to obtain the high-quality dataset, we develop a Massive Data Filtering System (MDFS) built on Spark to clean and filter the raw data, and train a Bert-based model to select high quality
samples. MDFS is consisted of three parts, data collection, coarse filtering and fine filtering (Fig. 5). The raw data is
collected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data,
we run MDFS system on a high performance cluster with 36 nodes.",1000000000000,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.",Confident,6881280,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",41.00,245730000000.00,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",0.22,,,,,,Self-supervised learning,$606364.75,,Industry,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",2024-05-01 09:13,Robi Rahman,,0,,,,2128,0.4500,Yuan 1.0,,,,Industry,,,,,Industry,,,
AlphaGo Zero,Games,Go,2017-10-18,,,,,https://www.nature.com/articles/nature24270,DeepMind,3.41e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).",United Kingdom of Great Britain and Northern Ireland,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Highly cited,,Mastering the game of Go without human knowledge,,,5800000000,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",,,,8103.00,46400244.00,Quick calculation,,,,480.0,,Google TPU v1,Self-supervised learning,$1544149.42,,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",2024-04-04 12:16,Robi Rahman,,0,,,,,,AlphaGo Zero,,,,Industry,,,,,Industry,,,
CodeFuse-13B,Language,Code generation,2023-10-10,apache: https://github.com/codefuse-ai/codefuse-chatbot?tab=License-1-ov-file#readme,Open source,Unreleased,Unreleased,https://arxiv.org/abs/2310.06266,Ant Group,3.3e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with
a Hardware FLOPs Utilization (HFU) of approximately 60%. The
training process took approximately 40 days to complete""

512 * 312 trillion * 40 * 24 * 3600 * 0.6 = 3.3e23

Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens",China,"Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu",,"""The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes""",CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,"The Stack,GitHub","80% code, 10% English, 10% Chinese: ""The pre-training data for CodeFuse consists of 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of English raw data, totaling 200TB, that are tokenized into 800 billion
tokens of code, 100 billion tokens of Chinese corpus, and 100 billion
tokens of English corpus (see Section 3.1).""",1000000000000,"1T tokens, mostly code but some Chinese/English",Confident,16777216,"4096 batch size, 4096 sequence length",2.00,13000000000.00,,,,,960.0,~40 days,NVIDIA A100 SXM4 80 GB,,,,,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",2024-05-21 10:04,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Galactica,"Language,Biology",Language modelling,2022-11-16,,Open access (non-commercial),,,https://arxiv.org/abs/2211.09085,Meta AI,3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",United States of America,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",SOTA improvement,"""We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH""",Galactica: A Large Language Model for Science,Galactica Corpus,"""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include
academic code to capture computational science""",106000000000,"""Total dataset size = 106 billion tokens""",Likely,2000000,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)",424.00,120000000000.00,"""The largest 120B model we train runs on a single NVIDIA A100 node""",4.00,,,,,NVIDIA A100 SXM4 80 GB,Self-supervised learning,,,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",2024-05-21 03:33,Robi Rahman,,0,,,,128,,Galactica,,,,Industry,checked,,,,Industry,,,
GPT-3 175B (davinci),Language,Text autocompletion,2020-05-28,"https://openai.com/blog/openai-api
",API access,,Unreleased,https://arxiv.org/abs/2005.14165,OpenAI,3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165",United States of America,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",Highly cited,,Language Models are Few-Shot Learners,"Common Crawl,WebText2,Wikipedia,Books1,Books2",Table 2.2 (other datasets also used),374000000000,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",Confident,3200000,"3.2M, per table 2.1",23023.00,175000000000.00,"""we train GPT-3, an autoregressive language model with 175 billion parameters""",0.60,740000000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$1131415.12,,Industry,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",2024-04-25 17:26,Robi Rahman,GPT-3 175B (davinci),0,,,,10000,0.2196,GPT-3 175B (davinci),,,,Industry,checked,,,3552000,Industry,,,
Stable LM 2 12B,Language,Language modelling/generation,2024-04-08,"Requires Stability AI Membership. Free for non-commercial use, $20/month for commercial use if less than $1M in annual revenue, $1M in institutional funding, and 1M monthly active users. ",Open access (restricted use),,,"https://stability.ai/news/introducing-stable-lm-2-12b
https://huggingface.co/stabilityai/stablelm-2-12b",Stability AI,2.91e+23,"2* 12143605760 params * 3* 2T tokens * 2 epochs = 2.91e23. 
Trained on 384 H100s (AWS P5 instances).",Multinational,,,,Introducing Stable LM 2 12B,"Falcon RefinedWeb,RedPajama-Data,The Pile,StarCoder,CulturaX","The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without the Books3 subset, and StarCoder (Li et al., 2023). We further supplement our training with multi-lingual data from CulturaX (Nguyen et al., 2023) and, in particular, from its OSCAR corpora, as well as restructured data in the style of Yuan & Liu (2022).",2000000000000,2T tokens,Confident,,,,12143605760.00,Precise number given in HF model card,2.00,,,,,NVIDIA H100 SXM5,Self-supervised learning,,,Industry,"Introducing the latest additions to our Stable LM 2 language model series: a 12 billion parameter base model and an instruction-tuned variant, trained on 2 trillion tokens in seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. This medium-sized model balances strong performance, efficiency, memory requirements, and speed, following our established Stable LM 2 1.6B framework as detailed in our previously released technical report. With this release, we’re extending our model range, offering a transparent and powerful tool for developers to innovate in AI language technology. Soon, we plan to introduce a long-context variant of these models which will be available on Hugging Face upon release.

From Hugging Face:
Stable LM 2 12B is a 12.1 billion parameter decoder-only language model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.",2024-04-18 14:19,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
ST-MoE,Language,,2022-02-17,,,,,https://arxiv.org/abs/2202.08906v2,Google,2.9000000000000005e+23,"The paper claims ""scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder"". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23",United States of America,"Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",SOTA improvement,"""ST-MoE-32B improves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 → 94.8) and ARC Challenge (81.4 → 86.5).""",ST-MoE: Designing Stable and Transferable Sparse Expert Models,,"""The pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and the dataset introduced in GLaM (Du et al., 2021).""",1500000000000,"""We pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the
dataset from GLaM (Du et al., 2021) summarized in Appendix E""",Likely,,,65.00,269000000000.00,269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,0.84,,"2e12 FLOP per sequence, per Table 11. Earlier, the paper says ""The sequence length for the encoder was 512 and 114 for the decoder""",,,,,,,,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",2024-05-01 09:13,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Luminous-supreme,Language,Language generation,2022-08-15,,,,,https://docs.aleph-alpha.com/docs/introduction/model-card/,Aleph Alpha,2.8e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.

312 trillion * 839000 * 3600 * 0.3 = 2.8e23",Germany,,,,Model Card Luminous,,"""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/",,,Likely,,,,70000000000.00,"""~70B""",,,,,,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",,,,,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model “training” where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
MegaScale (175B),Language,Language modelling/generation,2024-02-23,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
Model weights unreleased",Unreleased,Unreleased,Open source,https://arxiv.org/abs/2402.15627,"ByteDance,Peking University",2.78e+23,"Table 2 gives details for the 175B model. Looking at the largest 1 epoch run with 12288 GPUs:
2166.3 aggregate PFlops/sec * 1.75 days * 24 hours/day * 3600 seconds/hour = 3.275e23

This is consistent with the theoretical computation counting estimate, if they factor MFU rate into their 2166.3 figure:
2 × 175B params × 3 × 300B tokens × 1 epoch = 2.29e23

I use the average of these two:
(3.275e23 + 2.29e23) / 2 = 2.78e23","China,China","Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",SOTA improvement,Improves SOTA in FLOP utilization for distributed LLM training by 1.34X,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",,175B and 530B models trained for paper use 300B tokens each.,225000000000,300B tokens * 0.75 words/token = 225B words,Confident,,,1.00,175000000000.00,"Two models are trained for epoch to evaluate the MegaScale training system; one model with 175B and another with 530B parameters. This entry reports the 175B model.

There is a third production model mentioned, with fewer details.",1.00,,,42.0,,NVIDIA A100,Self-supervised learning,,,,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",2024-04-11 19:08,Anonymous,,1,,,,12288,0.5520,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
LLaMA-33B,Language,Language modelling,2023-02-27,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",Open access (non-commercial),,Unreleased,https://arxiv.org/abs/2302.13971,Meta AI,2.7300000000000996e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,United States of America,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",,,LLaMA: Open and Efficient Foundation Language Models,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",,1400000000000,Table 2,Unverified,4000000,,4640.00,32500000000.00,Table 2 in the paper,1.09,,,,,,,,,,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",2024-05-23 05:13,Robi Rahman,LLaMA-33B,1,,,,,,,,,,Industry,,,,,Industry,,,
Flamingo,"Multimodal,Vision,Language,Video","Visual question answering,Image captioning",2022-04-29,,Unreleased,,,https://arxiv.org/abs/2204.14198,DeepMind,2.7e+23,"1536 TPU v4 chips for 15 days. Assuming 50% utilization:
C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.50 = 2.7*10^23 FLOP

All training and evaluation
was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on
QUSV chips for 15 days and sharded across 16 devices.

All trained parameters and optimizer accumulators are stored
and updated in float32; all activations and gradients are computed in bfloat16 after downcasting
of parameters from float32 to bfloat16",United Kingdom of Great Britain and Northern Ireland,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan","Highly cited,SOTA improvement","""For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.""",Flamingo: a Visual Language Model for Few-Shot Learning,"MultiModal MassiveWeb,LTIP,VTP,ALIGN",,,"Flamingo was trained on a mixture of web-scraped datasets:
43M pages of text with interleaved images (MultiModal MassiveWeb dataset)
312M image-text pairs (LTIP dataset)
27M video-text pairs (VTP dataset)
1.8B image-alt text pairs (ALIGN dataset)

Training dataset size is at least 2.1 billion.",Likely,,,1573.00,80000000000.00,"""We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B""",,,,360.0,1536 TPU v4 chips for 15 days,Google TPU v4,Supervised,,,Industry,"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",2024-05-13 09:16,Anonymous,,0,,,,1536,,Flamingo,,,,Industry,checked,,,552960,Industry,,,
Whisper v3,Speech,Audio speech recognition,2023-11-06,Apache 2.0,Open source,,,https://huggingface.co/openai/whisper-large-v3,OpenAI,2.7e+23,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.

Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23",United States of America,,,"seems not SOTA: ""Our studies show that... accuracy on speech recognition and translation is near the state-of-the-art level""",,,"""The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2""",60000000000,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce

The dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have

200*60*5 million hours = 60,000,000,000 (60B) words",Likely,,,,1550000000.00,,2.00,,,,,,Supervised,,,,,2024-03-27 15:46,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Gemma 7B,Language,"Language modelling/generation,Chat",2024-02-21,"https://ai.google.dev/gemma/terms

no illegal use or abuse",Open access (restricted use),Unreleased,Unreleased,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Google DeepMind,2.52e+23,"6ND aproximation 6*7B*6T = 2.5e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""
We can also try to estimate from: ""We estimate the carbon emissions from pretraining the Gemma models to be ∼ 131 𝑡𝐶𝑂2𝑒𝑞. """,Multinational,"Gemma Team, Google DeepMind",,,Gemma: Open Models Based on Gemini Research and Technology,,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",4500000000000,"assuming 0.75 words per token - so 4500000000000.0 words
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",Likely,,,,7751248896.00,table 2,,14000000000.00,2N,,,Google TPU v5e,,,,Industry,,2024-04-30 09:08,Bartosz Podkanowicz,,,,,,4096,,,,,,Industry,,,,,Industry,,,
Skywork-13B,Language,Language modelling,2023-10-30,"commercial but restrictive license: https://github.com/SkyworkAI/Skywork/blob/main/LICENSE

part of the training data is open, but only 2.5%: ""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",Open access (restricted use),Unreleased,Open access (restricted use),https://arxiv.org/abs/2310.19341,Kunlun Inc.,2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",China,"Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",SOTA improvement,"""We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains""",Skywork: A More Open Bilingual Foundation Model,SkyPile,"""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",3180000000000,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",Confident,16000000,Table 3,36.00,13000000000.00,13B,1.00,,,940.0,39 days,NVIDIA A800,,,,,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",2024-05-20 11:27,Anonymous,,0,,,,512,0.4600,Skywork-13B,,,,Industry,,,,,Industry,,,
Qwen-14B,Language,,2023-09-28,"commercial allowed, can't use to train models
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Open access (restricted use),Unreleased,Unreleased,https://arxiv.org/abs/2309.16609,Alibaba,2.5e+23,"3T tokens per Table 1

14b*3T*6 = 2.5e23",China,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",,,Qwen Technical Report,,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a
significant portion of the data being in English and Chinese.""",,,Likely,4000000,Table 1,169.00,14000000000.00,14B,,,,,,,,,,,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",2024-04-08 18:27,Anonymous,,1,,,,,,,,,,Industry,,,,,Industry,,,
Granite 13B,Language,Chat,2023-11-30,,API access,,,https://www.ibm.com/downloads/cas/X9W4O6BM,IBM,2.44e+23,"Estimate using hardware:

""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.
Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""

Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.

256 * 2208 * 3600 * 120 TFLOPS = 2.44e23

Using 6ND:

""The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.""

""The granite.13b.v1 base model is trained for 300K iterations,
with a batch size of 4M tokens, for a total of 1.25 trillion
5 tokens. The granite.13b.v2 base model continued pre-training
on top of the granite.13b.v1 checkpoint for an additional 300K
iterations and a total of 2.5 trillion tokens.""

2.5T * 13B * 6 = 1.95e23",United States of America,,,"Possible significant use - it is (one of) the foundation models behind IBM's ""watsonx"" product, alongside open models like Llama 2

https://www.ibm.com/products/watsonx-ai",Granite Foundation Models,"Common Crawl,arXiv,many others","""To support the training of large enterprise-grade foundation
models, including granite.13b, IBM curated a massive dataset
of relevant unstructured language data from sources across
academia, the internet, enterprise (e.g., financial, legal), and
code.""

More breakdowns in paper, 20 sources in total",1875000000000,"2.5T tokens, 1.875T words at 0.75 words/token",Likely,,,,13000000000.00,13 billion,1.00,,,2208.0,"""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""",NVIDIA A100,,,,,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that
are ready for enterprise use. We report on the architecture,
capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations,
and usage policies.",2024-03-20 09:28,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Falcon-40B,Language,Language modelling,2023-03-15,apache 2.0,Open source,Unreleased,Unreleased,https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,Technology Innovation Institute,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",United Arab Emirates,,Historical significance,,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,Falcon RefinedWeb,"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",1000000000000,1000B tokens ~= 750B words,Confident,2359296,"Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

https://arxiv.org/pdf/2311.16867.pdf
",0.00,40000000000.00,Model comes in 7B and 40B variants.,,80000000000.00,80B FLOP per token,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,,,,Academia,,2024-05-23 04:49,Anonymous,,0,,,,384,0.3864,Falcon-40B,,,,Government,checked,,,552960,Government,checked,,
Nanbeige-16B,Language,Chat,2023-11-01,Apache 2.0,Open source,Unreleased,Open source,https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,Nanbeige LLM Lab,2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.

16 billion * 2.5 trillion * 6 = 2.4e23",China,,,"a little worse than Qwen on most metrics, and well short of GPT-4, ofc",,,"""The training data includes a large amount of high-quality internet corpus, various books, code, etc""",,,Likely,,,,16000000000.00,16 billion,,,,,,,,,,,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",2024-04-05 12:40,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
LightOn Mini,Language,"Language modelling/generation,Chat",2023-03-21,,Hosted access (no API),,Unreleased,https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19,LightOn,2.4e+23,6ND aproximation: 6*40B*1T = 2.4e23,France,,,,LightOn's Large Language Model of 40 billion parameters: MINI,,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""",1000000000000,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""  assuming 0.75 words per token - 750000000000.0 words",Confident,,,,40000000000.00,"""Boasting an impressive 40 billion parameters, Mini is a formidable addition to the growing array of language models available in the market today.""",,80000000000.00,2N,,,,Self-supervised learning,,,,,2024-05-21 03:34,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,checked,,
BloombergGPT,Language,Language modelling,2023-03-30,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2303.17564,"Bloomberg,Johns Hopkins University",2.3599999999999997e+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)","United States of America,United States of America","Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",SOTA improvement,"""We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks.""",BloombergGPT: A Large Language Model for Finance,,"""To train BloombergGPT, we construct “FinPile”, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""",532000000000,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",Confident,4200000,"""in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.""",332.00,50558868480.00,,0.80,,,1270.0,"""~53 days""",NVIDIA A100,Self-supervised learning,,,,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",2024-05-01 09:24,Anonymous,,0,,,,512,0.3200,BloombergGPT,,,,"Industry,Academia",checked,,,650240,"Industry,Academia",,,
YaLM,Language,Language modelling,2022-06-23,apache 2.0,Open source,Unreleased,,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,Yandex,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""",Russia,,,,Yandex Publishes YaLM 100B. It’s the Largest GPT-Like Neural Network in Open Source,,,300000000000,"1.7TB of data 300B tokens – from github https://github.com/yandex/YaLM-100B
I've assumed that 1 token correspond to 1 word in russian language.",Likely,,,0.00,100000000000.00,100B,,,,1560.0,65 days,NVIDIA A100,Self-supervised learning,,,Industry,,2024-03-28 16:01,Robi Rahman,,0,,,,800,,,,,,Industry,checked,,,1248000,Industry,,,
AlexaTM 20B,Language,Language modelling,2022-08-02,https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls,API access,,,https://arxiv.org/abs/2208.01448,Amazon,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",United States of America,"Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",SOTA improvement,The Abstract reports SOTA improvement on multiple benchmarks.,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,"mC4,Wikipedia",See Table 2 on p.3 of the paper.,,,Confident,2000000,"""We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens""",64.00,19750000000.00,See Table 1 on p.3 of the paper,,,,2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",NVIDIA A100,,,,Industry,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",2024-05-01 09:22,Robi Rahman,,0,,,,128,0.4935,AlexaTM 20B,Amazon Web Services,,,Industry,,,,368640,Industry,,,
Baichuan2-13B,Language,Chat,2023-09-06,"Baichuan community license, restrictive commercial: https://huggingface.co/baichuan-inc/Baichuan2-13B-Base",Open access (restricted use),Unreleased,Unreleased,"https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",Baichuan,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",China,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",,,Baichuan 2: Open Large-scale Language Models,,"2.6 trillion tokens, bilingual.

paper/model card don't give breakdown between English and Chinese",2600000000000,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)

1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words
1.3T English tokens * (0.75 words/token) = 0.975T English words
total: 2.275T, or ~2.3T",Confident,,,,13000000000.00,,1.00,,,,,,,,,,,2024-05-21 03:52,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
AlphaGo Master,Games,Go,2017-01-01,,,,,https://www.nature.com/articles/nature24270,DeepMind,2.0001000000000102e+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 25-30 days (60-75% of the total training time), I estimate the compute to be around 60-75% that of AGZ. I round this to 2e23, and I expect this to only be accurate within an OOM.",United Kingdom of Great Britain and Northern Ireland,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Highly cited,,Mastering the game of Go without human knowledge,,,,,,,,8103.00,,,,,,,,Google TPU v1,,$852748.08,,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",2024-04-23 13:44,Robi Rahman,,0,,,,,,AlphaGo Master,,,,Industry,,,,,Industry,,,
MPT-30B (base),Language,"Language generation,Code generation",2023-06-22,apache 2.0,Open source,,,https://huggingface.co/mosaicml/mpt-30b,MosaicML,1.8900000000001e+23,"According to their blog post,
""MPT-30B FLOPs ~= 6 * 30e9 [params] * 1.05e12 [tokens] = 1.89e23 FLOPs""",United States of America,,,,,"mC4,C4,RedPajama,The Stack",https://www.databricks.com/sites/default/files/inline-images/open-source-foundations-models-1.png,3000000000000,"~4T tokens across sources, or 3T words at 0.75 words/token (ignoring the fact that some of the data is code)",Confident,4096000,"last two batch sizes were 3,456,000 and 4,096,000, but 4,096,000 only used for last 5% of training

""To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long...

The model was trained in three stages using the MosaicML Platform: (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens""


",,30000000000.00,30B,0.25,,,278.4,30B: 512x H100-80gb for 11.6 days,NVIDIA H100 SXM5,,,"https://www.databricks.com/blog/gpt-3-quality-for-500k (post is from September 2022!)
Elsewhere, they train 7B and 30B models for $200k and $714k, respectively.
",,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU—either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",2024-05-14 10:27,Anonymous,,0,,,,512,0.3500,,Databricks,,,Industry,,,,142541,Industry,,,
Nemotron-3-8B,Language,"Chat,Language generation",2023-11-15,"can't use to train other models:

https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf",Open access (restricted use),,,https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/,NVIDIA,1.8e+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23",United States of America,,SOTA improvement,"""The Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in ‌QA. """,NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs,,"""NVIDIA models are trained on a diverse set of public and proprietary datasets. This model was trained on a dataset containing 3.8 Trillion tokens of text. The dataset contains 53 different human languages (including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch) and 37 programming languages. The model also uses the training subsets of downstream academic benchmarks from sources like FLANv2, P3, and NaturalInstructions v2""",,,Likely,,,,8000000000.00,,,,,456.0,19 days,NVIDIA A100,,,,,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.",2024-03-27 15:40,Anonymous,,0,,,,1024,0.3400,Nemotron-3-8B,,,,Industry,,,,,Industry,,,
Konan LLM 41B,"Language,Vision",Language modelling/generation,2023-12-15,,Hosted access (no API),,Unreleased,"https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",Konan Technology,1.722e+23,=41000000000*700000000000*6=1.722 × 10^23,Korea (Republic of),"Yang Seung-hyun, Wiretin, Changmin, Kim Jong-tae",,,Konan LLM: A Korean Large Language Model,,,700000000000,"https://www.konantech.com/pr/press?number=2628&pn=1&stype2=&sfi=subj&sword=

Since 2007, via the real-time AI analysis service pulseK, over 20.5 billion pieces of data have been independently secured.
Among them, only 2 billion high-quality, large-scale data pieces have been used for training.
",Likely,,,,41000000000.00,,,,,,,,,,,,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.",2024-05-15 14:57,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,checked,,
FinGPT-13B,Language,,2023-10-07,"MIT license (though probably subject to Llama 2 license too)
https://github.com/AI4Finance-Foundation/FinGPT/blob/master/LICENSE",Open source,Open source,Open source,https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT,"University of California Los Angeles (UCLA),Columbia University,New York University (NYU)",1.6e+23,From Llama 2-13B,"United States of America,United States of America,United States of America","Neng Wang, Hongyang Yang, Christina Dan Wang",SOTA improvement,SOTA for financial sentiment analysis,FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets,,Financial sentiment data (for fine-tuning): https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train,,,Likely,,,8.00,13000000000.00,"Finetunes using LoRA, so only trains 3.67 million parameters",,,,17.3,https://github.com/AI4Finance-Foundation/FinGPT?tab=readme-ov-file,NVIDIA GeForce RTX 3090,Supervised,,"Finetuning cost for FinGPT v3.3 given as $17.25 at github repo; paper notes cost to train a financial model using their methods are ""typically"" between $100 - $300",,"In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",2024-05-06 19:03,Anonymous,,0,Llama 2-13B,2200000000000000000,"fine-tuned Llama 2 13B

RTX 3090 for 17 hours, at a cost of $17

35.5 trillion flops * 17 * 3600 = 2.2e18",1,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Llama 2-13B,Language,Language modelling,2023-07-18,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Open access (restricted use),,,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",Meta AI,1.6e+23,13 billion * 2 trillion * 6 = 1.6e23,United States of America,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
","Historical significance,Significant use,Highly cited",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,Llama 2: Open Foundation and Fine-Tuned Chat Models,Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",1500000000000,2 trillion tokens ~= 1.5 trillion words,Confident,4000000,,3131.00,13000000000.00,"Llama has been released in 7B, 13B, and 70B variants.",1.00,,,,,NVIDIA A100 SXM4 80 GB,Supervised,,,Industry,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",2024-04-01 09:35,Epoch AI,,1,,,,,,,,Meta’s Research Super Cluster,,Industry,,,,,Industry,,,
SparseOPT-175B,Language,,2023-01-02,"code is Apache 2.0 (but OPT, which you'd need to recreate this model, is non-commercial)

https://github.com/IST-DASLab/sparsegpt",Unreleased,,Open source,https://arxiv.org/abs/2301.00774,"Institute of Science and Technology Austria (ISTA),Neural Magic",1.58e+23,,"Austria,United States of America","Elias Frantar, Dan Alistarh",,,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,,this is a distillation of OPT; see OPT dataset,,,,,,188.00,87500000000.00,,1.67,,,,,,,,,,,2024-04-29 16:32,Robi Rahman,SparseOPT-175B,0,OPT-175B,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
AlphaCode,Language,Code generation,2022-02-02,,,,,https://arxiv.org/abs/2203.07814,DeepMind,1.568160000001e+23,"Figure 7 (a) shows a maximum training compute budget of approx 20000 TPU-days per model.
20000 days * 275 TFLOPS * 0.33 utilization = 1.6e23 FLOP
https://www.wolframalpha.com/input?i=20000+*+275+teraFLOPS+*+1+day+*+0.33",United Kingdom of Great Britain and Northern Ireland,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",SOTA improvement,,Competition-Level Code Generation with AlphaCode,,,,Appendix part A has answers for pretraining.,,4718592,"2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592

trained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup",732.00,41100000000.00,41.1B. Table 3,,,,,,"Google TPU v4,Google TPU v4i",Self-supervised learning,,,Industry,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and
accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete
simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into
code. For example, competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper
reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:
(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the
search space, followed by filtering based on program behavior to a small set of submissions.
",2024-05-01 09:13,Robi Rahman,,0,,,,3750,,AlphaCode,,,,Industry,checked,,,,Industry,,,
StarCoder 2 7B,Language,"Code generation,Code autocompletion",2024-02-29,https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement,Open access (restricted use),,,https://arxiv.org/abs/2402.19173,"Hugging Face,ServiceNow,NVIDIA,BigCode",1.55e+23,estimation is given in Table 6 ,"Multinational,United States of America,United States of America","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",,,StarCoder 2 and The Stack v2: The Next Generation,StarCoder v2,created from repositorites from Github with permissive licences.,3500000000000,"from Table 7, 
3.5T tokens ",Likely,,,,15000000000.00,15B,,,,,"""A cumulative of 145,152 hours of computation was performed on hardware of type H100""",NVIDIA H100 SXM5,,,,,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",2024-03-27 13:14,Bartosz Podkanowicz,,1,,,,,,,,,,"Industry,Industry,Industry",,,,,"Industry,Industry,Industry",,,
Poro34B (700B token checkpoint),Language,"Code generation,Language modelling/generation",2023-12-14,Apache 2.0,Open source,,,https://huggingface.co/LumiOpen/Poro-34B,"High-Performance Language Technologies (HPLT),University of Turku",1.53e+23,"6ND = 6*0.75T*34B= 153000000000000000000000
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",Finland,"Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, Väinö Hatanpää, Peter Sarlin, Sampo Pyysalo",,, Poro 34B Model Card ,mC4,"""The Finnish dataset is a combination of many Finnish resources:

    Finnish Internet Parsebank
    mC4 multilingual colossal, cleaned Common Crawl
    Common Crawl Finnish
    Finnish Wikipedia
    Lönnrot Projekti Lönnrot
    Suomi24 The Suomi 24 Corpus 2001-2020
    Reddit r/Suomi submissions and comments
    STT Finnish News Agency Archive 1992-2018
    Yle Finnish News Archive 2011-2018
    Yle Finnish News Archive 2019-2020
    Yle News Archive Easy-to-read Finnish 2011-2018
    Yle News Archive Easy-to-read Finnish 2019-2020""
""

""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",750000000000,"1T tokens, assuming 0.75 word per token
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",Likely,,,,34000000000.00,"""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,,,,AMD Instinct MI250X,,,,,,2024-04-19 03:07,Anonymous,,,,,,512,,,,,,Academia,,,,,Academia,,,
HyperCLOVA 82B,Language,,2021-09-10,"""We introduce HyperCLOVA Studio, an interactive prompt engineering interface which provides GUI and API interfaces like the OpenAI
playground1""",API access,,Unreleased,https://arxiv.org/abs/2109.04650,"NAVER,Search Solutions",1.476e+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP","Korea (Republic of),Korea (Republic of)","Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, Nako Sung",SOTA improvement,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""",What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,Unspecified unreleased,"Blog corpus: 273.6 billion tokens
Cafe corpus (online community): 83.3 billion tokens
News corpus: 73.8 billion tokens
Comments (crawled from various platforms): 41.1 billion tokens
KiN (Korean QnA website): 27.3 billion tokens
Modu (collection of five datasets): 6.0 billion tokens
WikiEn, WikiJp (Foreign Wikipedia): 5.2 billion tokens
Other unspecified sources: 51.5 billion tokens",300000000000,"""However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

""We introduce HyperCLOVA, a large-scale
Korean in-context learning-based LM with
nearly 100B parameters, by constructing a
large Korean-centric corpus of 560B tokens.""

Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.

This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.

Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",Confident,,"""All models use the mini-batch size of 1,024"". Doesn't state sequence length or number of steps",92.00,82000000000.00,"""We introduce a Korean in-context large-scale LM with 82B parameters, i.e., HyperCLOVA. This is the first discovery on near
100B-scale non-English LM.""

According to media reports, HyperCLOVA has 204B parameters (i.e. a different version than in the paper)
https://m.koreaherald.com/view.php?ud=20210525000824 ",,,,643.2,see compute notes,NVIDIA A100,Self-supervised learning,$103802.31,,Industry,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",2024-05-21 08:06,Robi Rahman,,1,,,,1024,0.2000,HyperCLOVA,,,,"Industry,Industry",checked,,,658637,"Industry,Industry",checked,,
VARCO LLM 2.0 base,Language,"Language modelling/generation,Chat",2023-08-16,,API access,,,"https://ncsoft.github.io/ncresearch/varco-llm-details/
https://aws.amazon.com/marketplace/pp/prodview-d7amr4yxpibew?sr=0-3&ref_=beagle&applicationId=AWSMPContessa",NCSOFT,1.248e+23,=1600000000000*6*13000000000=1.248×10^23,Korea (Republic of),,,,VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.,,"""Our LLM is trained with datasets that are either publicly available for pretraining, collected from the Internet or internally constructed,” Jehee Lee, CRO of NCSOFT, told Engadget via email.",1600000000000,https://ncsoft.github.io/ncresearch/varco-llm-details/,Likely,,,,13000000000.00,,,,,,,,,,,,"VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of various natural language processing-based AI services such as text generation, question answering, chatbots, summarization, and information extraction. NCSOFT's VARCO LLM 2.0 was developed with our own technology, including data construction, pre-training, instruction tuning and alignment tuning. We evaluated VARCO LLM 2.0 on various NLP tasks and its performance has significantly improved compared to VARCO LLM 1.0, and it boasts the highest performance among other Korean LLMs of similar sizes. In particular, it has been trained to be used in high-level natural language processing applications such as creative writing, summarization, question and answering, chatbots and translation, and shows high performance in related quantitative indicators. For inquiries regarding further performance improvement or collaboration for service applications, please contact us by email (varco_llm@ncsoft.com).

Korean Text Generation : VARCO LLM 2.0 is optimized for Korean natural language generation applications. In particular, it provides more natural and creative responses in understanding user instructions and generating text.",2024-05-15 12:20,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,checked,,
UL2,Language,,2022-05-10,Apache 2.0,Open source,Open source,,https://arxiv.org/abs/2205.05131v1,"Google Research,Google Brain",1.2e+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 ","Multinational,United States of America","Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",SOTA improvement,"""by scaling our model up to 20B parameters, we achieve SOTA
performance on 50 well-established supervised NLP tasks""",Unifying Language Learning Paradigms,C4,'The model is trained on a total of 1 trillion tokens on C4 (2 million steps).',750000000000,"1T tokens, assuming 0.75 words per token we have 0.75T words",Likely,65536,"""We pre-train all models for 500K steps with a batch size of 128 and a sequence length of 512 inputs and 512 targets using the C4 corpus. The total approximate tokens seen during pre-training is approximately 32 billion tokens.""

500k*128*512 ~= 32B
128*512=65,536",181.00,20000000000.00,Taken from Directory of LLMs,,,,744.0,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",Google TPU v4,,,,Industry,,2024-05-01 09:22,Robi Rahman,,0,,,,512,0.3180,UL2,,,,"Industry,Industry",checked,,,380928,"Industry,Industry",,,
PLaMo-13B,Language,"Language modelling/generation,Chat",2023-09-28,Apache 2.0 for weights. Open data,Open source,Open source,Unreleased,https://huggingface.co/pfnet/plamo-13b,Preferred Networks Inc,1.17e+23,"6ND = 6*13e9*1.5e12=1.17e+23
from https://huggingface.co/pfnet/plamo-13b#model-details
",Japan,"Preferred Networks, Inc",,, PLaMo-13B,"C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",from https://huggingface.co/pfnet/plamo-13b#training-dataset,1500000000000,"Trained tokens: 1.5T tokens (English: 1.32T tokens, Japanese: 0.18T tokens)
from https://huggingface.co/pfnet/plamo-13b#model-details

0.75*1.32T + 0.18T = 1170000000000
0.75 words per token for English
1 for Japanese ",Confident,,,,13000000000.00,,,26000000000.00,2N = 26000000000.0,,,,Self-supervised learning,,,Industry,,2024-05-23 07:36,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
IDEFICS,"Multimodal,Language,Vision","Language modelling,Image captioning",2023-08-22,Llama license (non commercial),Open access (non-commercial),,,https://huggingface.co/blog/idefics,Hugging Face,1.1593580544e+23,"flops = 512 * 312e12 * 28*24*3600 * 0.3 = 1.159e23
(num gpus) * (peak perforemence) * (time in seconds) * (assumed utilization rate)

""The IDEFICS models were trained on an AWS SageMaker cluster with 8x80GB A100 GPUs nodes and EFA network.
    IDEFICS-80B took ~28 days of training on 64 nodes (512 GPUs).""

https://huggingface.co/HuggingFaceM4/idefics-80b-instruct 

trained on 150B text tokens + images

",Multinational,"Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh",,,Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model,"Wikipedia,Public Multimodal Dataset,LAION,OBELICS","IDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called OBELICS that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.",,150B tokens - images and words,Likely,,,0.00,80000000000.00,IDEFICS... comes in two variants—the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.,,,,,,,,,,Industry,"We are excited to release IDEFICS (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS), an open-access visual language model. IDEFICS is based on Flamingo, a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs.",2024-05-16 09:17,Anonymous,,,,,,,,,,,,Industry,checked,,,,Industry,,,
Meena,Language,Text autocompletion,2020-01-28,,Unreleased,,,https://arxiv.org/abs/2001.09977,Google Brain,1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",United States of America,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA improvement,"""We also propose a human evaluation metric called Sensibleness and
Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and
tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated""",Towards a Human-like Open-Domain Chatbot,,,40000000000,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",,82655,"61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",805.00,2600000000.00,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",,,,720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Self-supervised learning,$263099.94,,Industry,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2024-05-01 09:06,Robi Rahman,,0,,,,1024,0.3439,Meena,,,,Industry,,,,737280,Industry,,,
WizardCoder-15.5B,Language,Code generation,2023-06-14,"commercial, responsible use restrictions: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/MODEL_WEIGHTS_LICENSE

code is apache: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/CODE_LICENSE

data non-commercial: https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/DATA_LICENSE",Open access (restricted use),Open access (non-commercial),Open source,https://arxiv.org/abs/2306.08568,Microsoft,1.12e+23,1.12e23 base compute (StarCoder estimate) + 1.95e19 finetune compute (see below) ~= 1.12e23,United States of America,"Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",SOTA improvement,"""It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic’s Claude and Google’s Bard, on HumanEval and HumanEval+.""",WizardCoder: Empowering Code Large Language Models with Evol-Instruct,,"synthetic code data:

""To construct the training dataset, we initialized it with the 20K
 instruction-following dataset called Code Alpaca5. We iteratively employ the Evol-Instruct technique on this dataset consisting of 20,000 samples to produce evolved data""",,"""The evolved dataset consists of approximately 78k samples""

Not sure how big the samples are.",Likely,,,218.00,15500000000.00,15.5B,,,,,,,,,,,"Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing
models are solely pre-trained on extensive raw code data without instruction finetuning. In this paper, we introduce WizardCoder, which empowers Code LLMs
with complex instruction fine-tuning, by adapting the Evol-Instruct method to
the domain of code. Through comprehensive experiments on four prominent
code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS1000, we unveil the exceptional capabilities of our model. It surpasses all other
open-source Code LLMs by a substantial margin. Moreover, our model even
outperforms the largest closed LLMs, Anthropic’s Claude and Google’s Bard, on
HumanEval and HumanEval+. Our code, model weights, and data are public at
https://github.com/nlpxucan/WizardLM.",2024-05-13 10:15,Anonymous,,0,StarCoder,19503513600000000000,"""The StarCoder [11] serves as our basic foundation model. The evolved dataset consists of approximately 78k samples. To fine-tune the basic models, we employ specific configurations, including a
batch size of 512, a sequence length of 2048, 200 fine-tuning steps, 30 warmup steps, a learning rate
of 2e-5, a Cosine learning rate scheduler, and fp16 mixed precision.""

512*2048*200 = 209,715,200 training tokens

209715200 * 15.5B * 6 = 1.95e19",,,,,,,Industry,,,,,Industry,,,
OPT-66B,Language,Language modelling,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://huggingface.co/facebook/opt-66b,Meta AI,1.100000000001e+23,"OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 ∗ 2e6 ∗ 66e9 ∗ 6 = 1.1e23 FLOP",United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,300000000000,300B tokens,Confident,,,,66000000000.00,,1.67,,,,,NVIDIA A100 SXM4 80 GB,,,,,,2024-04-30 15:03,Robi Rahman,OPT-66B,1,,,,,,,,,,Industry,,,,,Industry,,,
Whisper v2,Speech,Speech recognition,2022-12-05,"Apache 2.0 for weights

code for v1 is MIT: https://github.com/openai/whisper",Open source,,,https://huggingface.co/openai/whisper-large-v2,OpenAI,1.1e+23,"""Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.""

We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23",United States of America,,,,openai/whisper-large-v2,,"""The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.""",9302400000,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h (estimate) * 680,000h = 9,302,400,000 words",Likely,,,,1550000000.00,1550M,7.50,,,,,,Self-supervised learning,,,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.

Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.",2024-04-12 13:58,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Code Llama-7B,Language,Code generation,2023-08-14,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Open access (restricted use),,,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",Meta AI,1.1e+23,"2.5e22 finetune compute + 8.4e22 base compute for Llama 2-7B, for ~1.1e23 compute overall

Table 26: ""In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB""
Suggests all versions took a combined 4.7e23 FLOPs:
3.12e14 * 1400000 * 3600 * 0.3 = 4.7e23

Assuming this refers to the finetune compute only, agrees with our finetune estimate if compute is proportional to parameter count:
7 / (7+13+34+70) = 0.056
0.056 * 4.7e23 = 2.65e22
",United States of America,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",,,Code Llama: Open Foundation Models for Code,,"""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of
publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language
questions or answers. To help the model retain natural language understanding skills, we also sample a small
proportion of our batches from a natural language dataset""",2600000000000,"Llama 2 used 2T tokens, and ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens""

2T + 500B + 100B = 2600000000000",Confident,,,463.00,7000000000.00,7B,,,,,,NVIDIA A100 SXM4 80 GB,,,,,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",2024-05-20 21:28,Epoch AI,,0,Llama 2-7B,2,"Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

7B * (500B+100B) * 6 = 2.5e22

Table 26: ""In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB""
Suggests all versions took a combined 4.7e23 FLOPs:
3.12e14 * 1400000 * 3600 * 0.3 = 4.7e23

Assuming this refers to the finetune compute only, agrees with our finetune estimate if compute is proportional to parameter count:
7 / (7+13+34+70) = 0.056
0.056 * 4.7e23 = 2.65e22
",,,,,,,Industry,,,,,Industry,,,
BlueLM 13B,Language,Chat,2023-10-31,"non-comm
https://github.com/vivo-ai-lab/BlueLM/blob/main/MODEL_LICENSE_EN.pdf",Open access (non-commercial),,Unreleased,https://github.com/vivo-ai-lab/BlueLM,vivo AI lab,1.0920000000001e+23,"C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOP
https://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion
(assuming 1 epoch)",China,,,,,,,1950000000000,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub",Wrong,,,,7000000000.00,"""BlueLM is a large-scale pre-trained language model independently developed by vivo AI Global Research Institute. This release includes 7B base (base) model and 7B conversation (chat) model. At the same time, we have open sourced the long text base (base) model that supports 32K and conversation (chat) model."" from GitHub https://github.com/vivo-ai-lab/BlueLM

",,,,,,,,,,,,2024-04-05 15:35,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
OLMo-7B,Language,"Language modelling/generation,Chat",2024-02-01,"License: The code and model are released under Apache 2.0.

Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo",Open source,,Open source,https://arxiv.org/abs/2402.00838v1,"Allen Institute for AI,University of Washington",1.0332e+23,"direct calculation:
6*7B*2.46trillion=1.0332 × 10^23","United States of America,United States of America","Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",,,OLMo: Accelerating the Science of Language Models,Dolma,,2460000000000,,Unverified,4000000,"For OLMo-1B and -7B models, we use a constant global batch size of 
approximately 4M tokens (2048 instances, each with a sequence length of 2048 tokens).",,7000000000.00,,,,,,,"AMD Instinct MI250X,NVIDIA A100",,,,,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.",2024-04-19 03:50,Natalia Martemianova,,,,,,,,,,"LUMI supercomputer,  MosaicML (Databricks)",,"Research collective,Academia",,,,,"Research collective,Academia",checked,,
Qwen-7B,Language,,2023-09-28,"commercial allowed, can't use to train models
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Open access (restricted use),Unreleased,Unreleased,"https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",Alibaba,1.0099999999999998e+23,"2.4T tokens per Table 1

7b*2.4T*6 = 1.01e23",China,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",,,Qwen Technical Report,,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a
significant portion of the data being in English and Chinese.""",,,Likely,4000000,Table 1,169.00,7000000000.00,7B,,,,,,,,,,,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",2024-04-08 18:27,Epoch AI,,1,,,,,,,,,,Industry,,,,,Industry,,,
Palmyra Large 20B,Language,Language modelling,2023-03-01,"Apache for weights.

data ""available on request"" https://huggingface.co/datasets/Writer/palmyra-data-index",Open source,Unreleased,Unreleased,https://huggingface.co/Writer/palmyra-large,Writer,9.6e+22,"""Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.""

I'm not sure if the 800B is how many tokens the model was trained on, or the size of the dataset. But the dataset linked on HuggingFace has 1T tokens, so 800B as tokens trained is more likely.

20B*800B*6 = 9.6e22",United States of America,,,,,Palmyra dataset,"Writer's custom dataset, English text",750000000000,"1 trillion tokens, or 750B words: https://huggingface.co/datasets/Writer/palmyra-data-index",Speculative,,,,20000000000.00,20B parameters for Palmyra Large. There is also a 43B version called Palmyra X according to HELM.,0.80,,,,,,,,,,"Palmyra Large was primarily pre-trained with English text. Note that there is still a trace amount of non-English data present within the training corpus that was accessed through CommonCrawl. A causal language modeling (CLM) objective was utilized during the process of the model's pretraining. Similar to GPT-3, Palmyra Large is a member of the same family of models that only contain a decoder. As a result, it was pre-trained utilizing the objective of self-supervised causal language modeling.",2024-04-12 12:42,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
GPT-NeoX-20B,Language,,2022-02-09,Apache 2.0,Open source,Open source,Open source,https://arxiv.org/abs/2204.06745,EleutherAI,9.31627008e+22,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,Multinational,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",Historical significance,,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,The Pile,,177167400000,"""In aggregate, the Pile consists of over 825GiB of raw text data""

1 GB ~ 200M words",,3150000,"""we opt to use the same batch size as OpenAI’s 175B model–approximately 3.15M tokens, or 1538 contexts of 2048 tokens each, and train for a total of 150,000 steps""",498.00,20000000000.00,,1.00,,,2160.0,"see other notes
",NVIDIA A100 SXM4 40 GB,Self-supervised learning,$202407.46,,Industry,,2024-04-01 09:03,Robi Rahman,GPT-NeoX-20B,,,,,96,0.3750,GPT-NeoX-20B,,,,Research collective,,,,207360,Research collective,,,
StarCoder,Language,Code generation,2023-05-09,"some restrictions

https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

data is The Stack, which has multiple licenses
https://huggingface.co/datasets/bigcode/the-stack-dedup ",Open access (restricted use),Open access (restricted use),Unreleased,https://arxiv.org/abs/2305.06161,"Hugging Face,ServiceNow,Northeastern University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich",8.46e+22,"FLOP reported here, 8.46e22
https://huggingface.co/bigcode/starcoder


""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""

320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23","Multinational,United States of America,United States of America,Canada,United States of America,United States of America,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,United States of America,Singapore,Israel,Australia,Australia,Australia,Canada,United States of America,Canada,United States of America,Germany,United States of America,United States of America,Sweden,Multinational,United States of America,United States of America,United States of America,India,India,Multinational,Spain,United States of America,Israel,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,Germany","Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",SOTA improvement,"""We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python""",StarCoder: may the source be with you!,The Stack,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process""",,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack""",Likely,4194304,"""We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are observed""",281.00,15500000000.00,"""We trained a 15.5B parameter model""",1.00,,,625.5,"625.5 hours = 320256 /512
512 GPUs from ""We trained our model on a GPU cluster with 512 A100 80 GB GPUs ""

320256 GPU hours from ""Based on the total number of GPU hours that training took (320,256)""
citations from sections 5.6 and 5.7",NVIDIA A100 SXM4 80 GB,,,,,"""The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.""",2024-05-01 09:24,Anonymous,,0,,,,512,,StarCoder,,,,"Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government",checked,,,320256,"Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government",,,
Llama 2-7B,Language,Language modelling,2023-07-18,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Open access (restricted use),,,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,Meta AI,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6*7B*2T = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 * 3600 * 0.3 = 6.21e22",United States of America,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
","Historical significance,Significant use,Highly cited",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,Llama 2: Open Foundation and Fine-Tuned Chat Models,Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",2000000000000,2 trillion tokens,Confident,4000000,,3131.00,70000000000.00,"Llama has been released in 7B, 13B, and 70B variants.",1.00,,,,,NVIDIA A100 SXM4 80 GB,Supervised,,"A100 cost in 2023: $1.10/hour
Training time: 184320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023

184320 * 1.10 / 1.145 = $177,075",Industry,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",2024-05-20 21:27,Epoch AI,,0,,,,,,Llama 2-7B,,Meta’s Research Super Cluster,,Industry,,,,184320,Industry,,,
Switch,Language,Text autocompletion,2021-01-11,"Apache 2
https://github.com/google-research/t5x
https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py

https://huggingface.co/google/switch-c-2048",Open source,,Open source,https://arxiv.org/abs/2101.03961,Google,8.22e+22,"Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",United States of America,"William Fedus, Barret Zoph, Noam Shazeer","Highly cited,SOTA improvement",""" On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7
accuracy versus the prior best of 49.4 (Yang et al., 2020)... Finally, we also conduct an early examination of the model’s knowledge with three closed-book knowledge-based tasks: Natural
Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span
Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM)",Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,C4,,432000000000,"""In our protocol we pre-train with 220 (1,048,576) tokens
per batch for 550k steps amounting to 576B total tokens.""

1 token ~ 0.75 words",,,,1163.00,1600000000000.00,"""Combining expert, model and data parallelism, we design two large Switch Transformer models, one
with 395 billion and 1.6 trillion parameters""",,,,648.0,"see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
",Google TPU v3,Self-supervised learning,$149825.60,,Industry,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.
",2024-04-29 10:24,Robi Rahman,,,,,,1024,0.2800,Switch,,,,Industry,,,,663552,Industry,,,
mT5-XXL,Language,Language modelling,2020-10-20," Apache-2.0 license
https://github.com/google-research/multilingual-t5",Open source,,Open source,https://aclanthology.org/2021.naacl-main.41/,"Google,Google Research",8.2e+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""

1 million steps * 1024 batchsize * 1024 length * 13 billion params * 6 = 8.2e22

Ignores fine-tuning compute; this is likely a small fraction of pre-training compute.","United States of America,Multinational","Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel","Highly cited,SOTA improvement","""Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds state-of-the-art on all classification and QA tasks and is near SOTA on NER (69.2 vs. 70.1).""",mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,mC4,"""The C4 dataset was explicitly designed to be English only: any page that was not given a probability of at least 99% of being English by langdetect2 was discarded. In contrast, for mC4 we use cld33 to identify over 100 languages.
Since some of these languages are relatively scarce on the internet, we make use of all of the 71 monthly web scrapes released so far by Common Crawl. This is dramatically more source data than was used for C4, for which the April 2019 web scrape alone was enough to provide plenty of English-language data.""",1000000000000,"The model was trained on a subset of 1 trillion tokens.
Full mC4 corpus has data ""totaling 6.6B pages and 6.3T tokens""
Distribution by language is in Appendix A.",Confident,1048576,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""",1603.00,13000000000.00,13 billion,1.00,,,,,,,,,,"The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",2024-05-15 19:18,Anonymous,,0,,,,,,mT5-XXL,,,,"Industry,Industry",,,,,"Industry,Industry",,,
ByT5-XXL,Language,Language modelling,2021-05-28,"Apache:
https://github.com/google-research/byt5/blob/master/LICENSE

see mC4 notes for data accessibility",Open source,,Open source,https://arxiv.org/abs/2105.13626,"Google,Google Research",8.1e+22,"""Like mT5, we set our sequence
length to 1024 (bytes rather than tokens), and train
for 1 million steps over batches of 2^20 tokens.""

12.9 billion * 1 million * 2^20 * 6 = ~8.1e22","United States of America,Multinational","Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel",SOTA improvement,"""On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes""",ByT5: Towards a token-free future with pre-trained byte-to-byte models,mC4,,,,Likely,1048576,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens""",304.00,12900000000.00,"12.9B, from Table 1",,,,,,Google TPU v3,,,,,"Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",2024-05-01 09:06,Anonymous,,0,,,,64,,ByT5-XXL,,,,"Industry,Industry",,,,,"Industry,Industry",,,
XGen-7B,Language,Language generation,2023-09-07,"Apache 2.0 
https://github.com/salesforce/XGen",Open source,Unreleased,Unreleased,https://arxiv.org/abs/2309.03450,Salesforce,8.02e+22,"270,336 TPUv4-hours per the carbon emissions section. They must mean chip-hours, not core-hours, because they multiply by 192W, which is the max power consumption of a TPU-v4 chip.

https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4

This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 

270336 * 275 teraflop/s * 3600 * 0.3 = 8.02e22 FLOP

also, using 6ND:

1484 billion tokens * 6.7 billion * 6 = 5.97e22",United States of America,"Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong",,"""Our evaluation on standard benchmarks shows that XGen-7B models
achieve comparable or better results when compared with state-of-the-art opensource LLMs""",XGen-7B Technical Report,,"""Our pre-training dataset is a mixture of data from several public sources, reported in Table 2. We employ a two-stage training strategy, where each stage uses a different data mixture, as shown in
Table 3.
Natural language data for stage 1. Natural language data is a mixture of publicly available data.
We made an effort to improve safety and diversity of the data.
Code data for stage 1. We use the GitHub subset from the recently released RedPajama dataset [9].
We also added Apex code data to enhance our model’s proficiency in Apex code generation. Apex is
a widely used object-oriented programming language in Salesforce products.
BigCode Starcoder data for stage 2. We use all the 86 programming languages from the Starcoder [15] data, preserving the original weight of each. Subsequently, we further filter the data
according to a stronger permissive license guideline.""",1113000000000,1484B tokens per Table 2. 1113B words at 0.75 words/token,Likely,1048576,"""batch size of 128, and a sequence length of 8,192 tokens""",5.00,6700000000.00,"I think this suggests the same number of params as Llama-7b. In any case ~7B. 
""The model architecture follows LLaMA with exact numerical compatibility to ease adoption in third-party frameworks. The hyperparameters closely follow LLaMA-7B [34] with the following alterations""",1.00,,,,,Google TPU v4,,,,,"Large Language Models (LLMs) have become ubiquitous across various domains,
transforming the way we interact with information and conduct research. However,
most high-performing LLMs remain confined behind proprietary walls, hindering
scientific progress. Most open-source LLMs, on the other hand, are limited in their
ability to support longer sequence lengths, which is a key requirement for many
tasks that require inference over an input context. To address this, we have trained
XGen-7B, a series of 7B parameter models on up to 8K sequence length for up
to 1.5T tokens. We have also finetuned the XGen-7B models on public-domain
instructional data, creating their instruction-tuned counterparts (XGen-7B-Inst).
We open-source our models for both research advancements and commercial
applications. Our evaluation on standard benchmarks shows that XGen-7B models
achieve comparable or better results when compared with state-of-the-art opensource LLMs. Our targeted evaluation on long sequence modeling tasks shows the
benefits of our 8K-sequence models over 2K-sequence open-source LLMs.",2024-04-09 09:00,Anonymous,,,,,,,,XGen-7B,,,,Industry,,,,270336,Industry,,,
GOAT,Games,Open ended play,2021-07-27,,Unreleased,,Unreleased,"https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play

https://arxiv.org/abs/2107.12808",DeepMind,7.8e+22,"[Final calculation]
(8 TPUs)(4.20e14 FLOP/s)(0.1 utilisation rate)(32 agents)(7.3e6 s/agent) = 7.8e22 FLOPs

==========================
NOTES BELOW

[Hardware]
- ""Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.""
- TPUv3 (half precision): 4.2e14 FLOP/s
- Number of TPUs: 8
- Utilisation rate: 0.1

[Timesteps]
- Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.
- 3.9e11 / 5e4 = 8e6 s → ~93 days
- 100 million steps is equivalent to 30 minutes of wall-clock time in our setup. (pg 29, fig 27)
- 1e8 steps → 0.5h
- 3.9e11 steps → 1950h → 7.0e6 s → ~82 days
- Both of these seem like overestimates, because:
“Finally, on the largest timescale (days), generational training iteratively improves population performance by bootstrapping off previous generations, whilst also iteratively updating the validation normalised percentile metric itself.” (pg 16)
- Suggests that the above is an overestimate of the number of days needed, else they would have said (months) or (weeks)?
- Final choice (guesstimate): 85 days = 7.3e6 s

[Population size]
- 8 agents? (pg 21) → this is describing the case where they’re not using PBT, so ignore this number
- The original PBT paper uses 32 agents for one task https://arxiv.org/pdf/1711.09846.pdf (in general it uses between 10 and 80)
- (Guesstimate) Average population size: 32",United Kingdom of Great Britain and Northern Ireland,"Open-Ended Learning Team*, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard and Wojciech Marian Czarnecki",SOTA improvement,likely qualitatively SOTA,Open-Ended Learning Leads to Generally Capable Agents,XLand,,390000000000,Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.,,,,143.00,3500000.00,estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing,,,,,"see other notes
",Google TPU v3,Self-supervised learning,$122418.97,,Industry,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",2024-05-01 09:13,Robi Rahman,,,,,,,,GOAT,,,,Industry,checked,,,,Industry,,,
OpenLLaMA-13B,Language,,2023-05-01,,Open source,Open source,Unreleased,https://github.com/openlm-research/open_llama,OpenLM Research,7.8e+22,13b * 1T * 6 = 7.8e22,United States of America,"Xinyang Geng, Hao Liu",,,OpenLLaMA: An Open Reproduction of LLaMA,RedPajama,RedPajama 1T: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,750000000000,"1T tokens, or ~750B words",Likely,,,,13000000000.00,13B,1.00,,,,,Google TPU v4,,,resources provided by Google and Stability AI,Industry,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.

In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture.",2024-04-11 17:18,Anonymous,,0,,,,,,,,,,Research collective,,,,,Research collective,,,
gpt-sw3-40b,Language,"Language modelling/generation,Chat",2023-03-01,"license, commercial, some ethical restrictions: 
https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b/blob/main/LICENSE",Open access (restricted use),Unreleased,Unreleased,https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b,AI Sweden,7.68e+22,"aproximation 6ND = 6*320E9*40e9 = 7.68e22
""GPT-SW3 has been trained on a dataset containing 320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code.""",Sweden,AI Sweden,,,gpt-sw3-40b,"The Pile,Common Crawl,mC4,Wikipedia", https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b#composition,,"320B tokens
""GPT-SW3 has been trained on a dataset containing 320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code.""""",Confident,,,,40000000000.00,40B,,80000000000.00,2N,,,,Self-supervised learning,,,,,2024-05-15 18:48,Bartosz Podkanowicz,,,,,,,,,,,,,,,,,,,,
phi-3-mini 3.8B,Language,"Chat,Language modelling/generation",2024-04-23,"The model is licensed under the MIT license.
https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",Open source,,,https://arxiv.org/abs/2404.14219,Microsoft,7.524e+22,"counting operations: 6×3.3×10^12×3.8×10^9 ≈7.524×10^22 FLOPS
hardware estimate: 7×24×3600*989,000,000,000,000*512*0.3=9.187540992×10^22",United States of America,"Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,Unspecified unreleased,"Our training data of consists of heavily filtered web data (according to the “educational level”) from various open internet sources, as well as synthetic
LLM-generated data. ",3300000000000,"In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens",Likely,,,,3800000000.00,3.8B,,,,168.0,"GPUs: 512 H100-80G
Training time: 7 days
Tensor type BF16
https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",NVIDIA H100 SXM5,,,,,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",2024-05-23 11:36,Natalia Martemianova,,1,,,,512,,,,,,Industry,,,,,Industry,,,
ProtT5-XXL,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2021-05-04,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans",Open source,,Unreleased,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085","Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University",7.370000000000001e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)","A Elnaggar, M Heinzinger, C Dallago, G Rihawi",SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art
without using evolutionary information""",ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,"BFD (Big Fantastic Dataset),UniRef50","First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).",393000000000,"""Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids.""",,,,396.00,11000000000.00,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,,,,Google TPU v3,Self-supervised learning,$123918.36,,Industry,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",2024-04-16 07:59,Robi Rahman,,,,,,512,,ProtT5-XXL,,,,"Academia,Industry,Government,Industry,Academia",,,,,"Academia,Industry,Government,Industry,Academia",,,
ESM2-15B,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-21,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd",Open source,Open source,Open source,https://www.science.org/doi/abs/10.1126/science.ade2574,"Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",7.350000000009999e+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOP

from Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B “connections” x 6. : 7.8e22 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
60 days x 512 V100s x an imputed 30% utilization"": 1e23 FLOP

Geometric mean: 7.35e22","United States of America,United States of America,United States of America,United States of America","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",Evolutionary-scale prediction of atomic-level protein structure with a language model,UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",,,Likely,,,636.00,15000000000.00,"""we train models up to 15B parameters""",,,,1440.0,,NVIDIA V100,Unsupervised,,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",2024-03-28 17:48,Anonymous,,,,,,512,,ESM2-15B,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
CoCa,Vision,Image classification,2022-06-14,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2205.01917v2,Google,7.3e+22,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""

275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22",United States of America,"Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu",SOTA improvement,"""Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.""",CoCa: Contrastive Captioners are Image-Text Foundation Models,"JFT-3B,ALIGN","""CoCa is pretrained from scratch in a single stage on both webscale alt-text data and annotated images by treating all labels simply as texts. We use the JFT-3B dataset [21] with label names as the paired texts, and the ALIGN dataset [13] with noisy alt-texts.""",4800000000,"JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images",Likely,,"65,536 image-text pairs",832.00,2100000000.00,"""Our largest CoCa model (""CoCa"" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder""",7.50,,,120.0,5 days,Google TPU v4,Self-supervised learning,,,Industry,"Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",2024-05-01 09:22,Anonymous,,,,,,2048,,CoCa,,,,Industry,,,,,Industry,,,
CLIP ViT-H/14 - LAION-2B,Vision,Image classification,2022-09-15,"MIT license for weights

dataset is LAION which is open source:
https://laion.ai/blog/laion-5b/",Open source,Open source,Unreleased,https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K,LAION,7.1e+22,"""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""
Per https://laion.ai/blog/large-openclip/ H/14 used 824 A100s, trained 42 samples per gpu-second, saw 32B samples -> implies 256.8 hours of training
824 * 3.12e14 * 256.8 * 3600 * 0.3 = 7.1e22",Multinational,"Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev",,, Model Card for CLIP ViT-H/14 - LAION-2B ,LAION-2B,"""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""",2000000000,"2B size of  LAION-2B
input is image text pair""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""",Likely,,,,986000000.00,986M params from https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K,,,,,,,,,,,,2024-05-21 16:01,Anonymous,,,,,,,,,,,,Research collective,,,,,Research collective,,,
VARCO LLM 2.0 small Finetuning,Language,"Language modelling/generation,Chat",2023-08-16,,API access,,,"https://ncsoft.github.io/ncresearch/varco-llm-details/
https://aws.amazon.com/marketplace/pp/prodview-or3w6j66on53s?sr=0-2&ref_=beagle&applicationId=AWSMPContessa",NCSOFT,6.72e+22,=1600000000000*6*7000000000=6.72 × 10^22,Korea (Republic of),,,,VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.,,,1600000000000,https://ncsoft.github.io/ncresearch/varco-llm-details/,Likely,,,,7000000000.00,,,,,,,,,,,,"VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of various natural language processing-based AI services such as text generation, question answering, chatbots, summarization, and information extraction. NCSOFT's VARCO LLM 2.0 was developed with our own technology, including data construction, pre-training, instruction tuning and alignment tuning. We evaluated VARCO LLM 2.0 on various NLP tasks and its performance has significantly improved compared to VARCO LLM 1.0, and it boasts the highest performance among other Korean LLMs of similar sizes. In particular, it has been trained to be used in high-level natural language processing applications such as creative writing, summarization, question and answering, chatbots and translation, and shows high performance in related quantitative indicators. For inquiries regarding further performance improvement or collaboration for service applications, please contact us by email (varco_llm@ncsoft.com).

Korean Text Generation : VARCO LLM 2.0 is optimized for Korean natural language generation applications. In particular, it provides more natural and creative responses in understanding user instructions and generating text.",2024-05-15 12:25,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,,,
OpenAI Five,Games,Dota 2,2019-12-13,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/1912.06680,OpenAI,6.7e+22,"""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.

Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680

You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",United States of America,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang","Highly cited,SOTA improvement","""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",Dota 2 with Large Scale Deep Reinforcement Learning,,,454321373184,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th
frame which we call a timestep""
--> 7.5 timesteps/s

""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days

296 * 24*3600 * 7.5 = 1.92e8

This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?

EDIT 14/06/2022
Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.
Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",Confident,,,1420.00,159000000.00,"""We define a policy (π) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,7104.0,"""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days",,Self-supervised learning,$166042.11,"Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",Industry,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
",2024-04-22 14:32,Robi Rahman,,,,,,1536,,OpenAI Five,,,,Industry,,,,10911744,Industry,,,
MOSS-Moon-003,Language,Code generation,2023-04-19,"copyleft (permissive, but derivatives must also be open)
https://github.com/OpenMOSS/MOSS/blob/main/MODEL_LICENSE",Open source,Open access (non-commercial),Open source,https://huggingface.co/fnlp/moss-moon-003-base,Fudan University,6.669999999999999e+22,"6.67e22 including pre-training for CodeGen:

""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""
",China,,,,,,"""The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data""",700000000000,Ignoring tokens from fine-tuning; very likely small relative to pre-training data.,Confident,,,,16000000000.00,16B,,,,,,,,,,,"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.",2024-05-21 03:28,Anonymous,,0,CodeGen-Mono 16.1B,1,"""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""


Using the proportion of tokens for fine-tuning against total tokens, we have:
6.67e22 * 120/(120+700) = 9.7e21.

However, the 6.67e22 might be *just* pre-training (this phrasing isn't clear). so that would be
6.67e22 * (120/700) = 1.14e22

alternatively, 16b * 120b * 6 = 1.15e22

I'll go with 1.14e22 but all these numbers are very close",,,,,,,Academia,,,,,Academia,,,
CodeGeeX,Language,Code generation,2022-06-22,"Training code is Apache 2.0. Model has a restricted license: https://github.com/THUDM/CodeGeeX/blob/main/MODEL_LICENSE

data partially open (The Pile)",Open access (restricted use),Unreleased,Open source,https://github.com/THUDM/CodeGeeX,"Zhipu AI,Tsinghua University",6.630000000001e+22,"Assume 1 epoch on 850B tokens.
C=6DN=6*850B*13B
https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion","China,China",,,,,The Pile,"Mix of open and scraped data:

""Our training data contains two parts. The first part is from open-sourced code datasets, The Pile and CodeParrot. The Pile contains a subset of code corpus that collects public repositories with more than 100 stars from GitHub, from which we select codes in 23 popular programming languages. The second part is supplementary data directly scrapped from the public GitHub repositories that do not appear in previous datasets, including Python, Java and C++.""",637500000000,"As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens",Likely,6291456,Table 3. 2048 * 3072,,13000000000.00,"""We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters""",,,,156.1,"Assume 30% utilization on 1536 Ascend 910 calculating in FP16.
https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion+FLOP+%2F+%280.30*1536*256+TFLOPS%29
If they used INT8 precision, the training time would be half of this.",Huawei Ascend 910,Self-supervised learning,,,Industry,"We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages. As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens on a cluster of 1,536 Ascend 910 AI Processors.",2024-04-15 11:48,Anonymous,,0,,,,,,,,,,"Industry,Academia",checked,,,,"Industry,Academia",,,
Falcon-7B,Language,Language modelling/generation,2023-04-24,"apache 2.0 for weights, ODC license (similar to apache) for data:
https://huggingface.co/datasets/tiiuae/falcon-refinedweb",Open source,Open source,,https://huggingface.co/tiiuae/falcon-7b,Technology Innovation Institute,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",United Arab Emirates,,,,Falcon-7B ,Falcon RefinedWeb,"""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""",1500000000000,"1125000000000.0 words  assuming 0.75 words per token (1.5T tokens)
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""",Confident,,,,7000000000.00,7B,,4000000000.00,2N,,"compute divided by flops per seconds
 6.3e22 / 3.e16 = 2100000.0 seconds = 583 hours = 24 days
compute (from other note) = 6.3e22
flops per seconds = (num gpu) * (peak flops) * (assumed utilization rate) = 
384*312e12*0.3 = 3.6e16",NVIDIA A100 SXM4 40 GB,,,,,,2024-05-23 04:50,Bartosz Podkanowicz,,,,,,384,,,,,,Government,,,,,Government,,,
StableLM-3B-4E1T,Language,Language generation,2023-09-29,"cc 4.0
https://creativecommons.org/licenses/by-sa/4.0/",Open source,,,https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo,Stability AI,6.21e+22,"""StableLM-3B-4E1T was trained on the Stability AI cluster across 256 NVIDIA A100 40GB GPUs (AWS P4d instances). Training began on August 23, 2023, and took approximately 30 days to complete.""

256 * 30 * 24* 3600 * 312 trillion * 0.3 utilization (assumption) = 6.21e22",Multinational,"Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz",,,,,"""The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer, 2023) and The Pile (Gao et al., 2020), both without the Books3 subset, and StarCoder (Li et al., 2023). The complete list is provided in Table 1.""",750000000000,Trained on 1T tokens (~750B words),Likely,4194304,"""The batch size is set to 1024 (4,194,304 tokens).""",,2795443200.00,,4.00,,,720.0,approximately 30 days,NVIDIA A100,,,,,"StableLM-3B-4E1T is a 3 billion (3B) parameter language model pre-trained under the multi-epoch regime to study the impact of repeated tokens on downstream performance. Given prior success in this area (Taylor et al., 2022 and Tay et al., 2023), we train on 1 trillion (1T) tokens for 4 epochs following the observations of Muennighoff et al. (2023) in ""Scaling Data-Constrained Language Models"" in which they find ""training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data."" Further inspiration for the token count is taken from ""Go smol or go home"" (De Vries, 2023), which suggests a 2.96B model trained for 2.85 trillion tokens achieves a similar loss to a Chinchilla compute-optimal 9.87B language model.",2024-03-28 11:53,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Megatron-BERT,Language,,2019-09-17,,Unreleased,,,https://arxiv.org/abs/1909.08053,NVIDIA,6.027e+22,"A source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.
https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf

Param-based calculation:
6ND = 6*3.9e9*2e6*1024*1024 = 4.8e22 FLOP

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.
BERT: batch size 1024, 2e6 iterations total.
So we should expect 4B => 1.0 days per epoch (69e3*512 examples)
=> 2e6*1024/(69e3*512) = 58 days training

On 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.
C=15.1 PFLOPS * 58 days = 7.6e22 FLOP.

The param and time calculations seem more trustworthy. Geometric mean is 6.027e22 FLOP",United States of America,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro","Highly cited,SOTA improvement","""Our BERT model achieves SOTA results on the RACE dataset""",Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,,,34800000000,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""",Likely,524288,"""we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearly
over 2 million iterations. Other training parameters are kept
the same as (Devlin et al., 2018).""

in Devlin et al (BERT), sequences are 512 tokens",1152.00,3900000000.00,"Source: https://lair.lighton.ai/akronomicon/

Archive on GitHub: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,,1392.0,"The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.
BERT: batch size 1024, 2e6 iterations total.
So we should expect 4B => 1.0 days per epoch (69e3*512 examples)
=> 2e6*1024/(69e3*512) = 58 days training",NVIDIA Tesla V100S PCIe 32 GB,Self-supervised learning,$208034.39,,Industry,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2024-04-01 09:52,Robi Rahman,,0,,,,512,0.2269,Megatron-BERT,,,,Industry,,,,712704,Industry,,,
StarCoder 2 3B,Language,"Code generation,Code autocompletion",2024-02-29,https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement,Open access (restricted use),,,https://arxiv.org/abs/2402.19173,"Hugging Face,ServiceNow,NVIDIA,BigCode",5.94e+22,estimation is given in Table 6 ,"Multinational,United States of America,United States of America","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",,,StarCoder 2 and The Stack v2: The Next Generation,StarCoder v2,created from repositorites from Github with permissive licences.,3100000000000,"from Table 7, 
31T tokens ",Likely,,,,3000000000.00,3B,,,,,"""A cumulative of 97,120 hours of computation was performed on hardware of type A100 SXM4 80 GB""",NVIDIA A100 SXM4 80 GB,,,,,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",2024-03-27 13:14,Bartosz Podkanowicz,,1,,,,,,,,,,"Industry,Industry,Industry",,,,,"Industry,Industry,Industry",,,
AlphaStar,Games,StarCraft,2019-10-30,"Apache 2.0, training tools only: https://github.com/google-deepmind/alphastar",Unreleased,Unreleased,Open source,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,DeepMind,5.9250000000001e+22,"384 TPUv3 chips for 44 days. Assume 33% utilization.
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days",United Kingdom of Great Britain and Northern Ireland,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",Highly cited,,Grandmaster level in StarCraft II using multi-agent reinforcement learning,,,,"Multiple data types. First supervised learning, then other stuff",,,,2870.00,139000000.00,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",,,,1056.0,"""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",Google TPU v3,,$512765.27,,Industry,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",2024-04-22 08:30,Robi Rahman,,,,,,384,,AlphaStar,,,,Industry,,,,405504,Industry,,,
CodeGen2.5,Language,Code generation,2023-07-06,apache 2.0,Open source,,,https://blog.salesforceairesearch.com/codegen25/,Salesforce,5.899999999999999e+22,"7B parameters, trained on 1.4T tokens

7 billion * 1.4 trillion * 6 = 5.9e22",United States of America,"Erik Nijkamp, Hiroaki Hayashi, Yingbo Zhou, Caiming Xiong",,,"CodeGen2.5: Small, but mighty",StarCoderData,,,280 billion tokens,Likely,,,,7000000000.00,7B,5.00,,,,,,,,,,"The family of Salesforce CodeGen models is growing with CodeGen2.5 – a small, but mighty model! While there has been a recent trend of large language models (LLM) of increasing size, we show that a small model can obtain surprisingly good performance, when being trained well.  

The key contributions towards productization of these models are:

Releasing CodeGen2.5 LLM with state-of-the-art on HumanEval for 7B parameters.
CodeGen2.5 with 7B is on par with >15B code-generation models (CodeGen1-16B, CodeGen2-16B, StarCoder-15B), less than half the size.
Featuring robust infill sampling, that is, the model can “read” text of both the left and right hand size of the current position.
Optimized for fast sampling under Flash attention for optimized serving and local deployment on personal machines.
Permissively licensed in Apache 2.0.",2024-03-28 12:14,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
PanGu-α,Language,,2021-04-25,,Unreleased,,,https://arxiv.org/abs/2104.12369,Huawei Noah's Ark Lab,5.83e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Huawei/PanGu-%CE%B1.json",China,"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian",,,PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,,Custom dataset,200000000000,"""The composition of our corpus and the processing steps adopted to each data source is shown in Table 3.2.
Based on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and 13B) and large (200B) models, respectively""

1 TB = 1000 GB
1 GB ~ 200M words",,,"260k steps, # tokens not too clear (Figure 8 suggests ~40B, seems too low, maybe a typo?)",159.00,207000000000.00,"Table in https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha

Note: Directory of LLMs (https://docs.google.com/spreadsheets/d/1gc6yse74XCwBx028HV_cvdxwXkmXejVjkO-Mz2uwE0k/edit#gid=0)
gives a slightly lower estimate, not sure about source",,,,,,,Self-supervised learning,$97802.06,,Industry,"Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-α, with up to 200 billion parameters. PanGu-α is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-α, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-α in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-α in performing various tasks under few-shot or zero-shot settings.",2024-04-01 09:54,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
FLM-101B,Language,,2023-09-07,"apache 2.0
https://huggingface.co/CofeAI/FLM-101B",Open source,,,https://arxiv.org/abs/2309.03852,"Chinese Academy of Sciences,Harbin Institute of Technology,Nanyang Technological University,Beijing Academy of Artificial Intelligence",5.720000000000001e+22,"192 GPUs * 160 TFLOP/s per GPU (reported, adjusted for utilization) * 21.54 days * 24 * 3600 = 5.72e22 (confident)

6*101000000000*311540000000=1.8879324e+23 (less confident)","China,China,Singapore,China","Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang",,,FLM-101B: An Open LLM and How to Train It with $100K Budget,,"""By design, FLM-101B is an English-Chinese bilingual model pre-trained with causal language modeling. It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling.""",311540000000,"Trained with 311.54B tokens. The dataset is approximately 50/50 English/Chinese: ""It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling"". We assume 1 Chinese word per token and 0.75 English words per token (0.875 on average). 311B/0.875 ~= 350B.",Confident,4310000,Table 1,7.00,101000000000.00,,,,,517.0,"""Under this growth schedule, the total time cost for our 101B model is 21.54 days""",NVIDIA A800,,$84000.00,Authors report $100k. Adjusted for inflation.,,"Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks, among others. Despite these successes, two main challenges remain in developing LLMs: (i) high computational cost, and (ii) fair and objective evaluations. In this paper, we report a solution to significantly reduce LLM training cost through a growth strategy. We demonstrate that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars. Inspired by IQ tests, we also consolidate an additional range of evaluations on top of existing evaluations that focus on knowledge-oriented abilities. These IQ evaluations include symbolic mapping, rule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model, named FLM-101B, trained with a budget of 100K US dollars, achieves performance comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B, especially on the additional range of IQ evaluations.",2024-05-21 07:05,Anonymous,,,,,,,,,,,,"Academia,Academia,Academia,Academia",,,,,"Academia,Academia,Academia,Academia",,,
Calm2-7B,Language,Language modelling/generation,2023-11-01,"apache 2.0.

they say datasets are public, but they don't specify which were used",Open source,,Unreleased,https://huggingface.co/cyberagent/calm2-7b,CyberAgent,5.459999999999999e+22,"6*7B*1.3T = 54600000000000000000000
6ND aproximation",Japan,Ryosuke Ishigami,,, CyberAgentLM2-7B (CALM2-7B)   https://huggingface.co/cyberagent/calm2-7b,,"""1.3T tokens of publicly available Japanese and English datasets. """,1300000000000,"""1.3T tokens of publicly available Japanese and English datasets. "" so around 1.3T words assuming 1 word per token.",Likely,,,,7000000000.00,7B,,14000000000.00,"
2N",,,,Self-supervised learning,,,,,2024-04-02 08:56,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
OPT-30B,Language,Language modelling,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,5.4000000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+30+billion+*+300+billion,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,300000000000,300B tokens,Likely,,,,30000000000.00,,1.67,,,,,,,,,,,2024-04-30 15:03,Robi Rahman,OPT-30B,1,,,,,,,,,,Industry,,,,,Industry,,,
Taiyi-Stable Diffusion,Image generation,"Image generation,Text-to-image",2022-10-31,https://huggingface.co/spaces/CompVis/stable-diffusion-license,Open access (restricted use),,,https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1,IDEA CCNL,5.1e+22,"Fine-tuning: 32 NVIDIA A100 GPUs for 100 hours
32 * 312e12 * 30% * 100 * 60 * 60 = 1.078272e+21 FLOP

Base model: Stable Diffusion, 5e+22 FLOP",China,,Historical significance,"The first open-source, Chinese version of Stable Diffusion.",,,,,,Likely,,,0.00,1000000000.00,,,,,100.0,32 NVIDIA A100 GPUs for 100 hours,NVIDIA A100,,,,,,2024-04-19 10:21,Anonymous,,,Stable Diffusion (LDM-KL-8-G),,,32,,Taiyi-Stable Diffusion,,,,Academia,,,,,Academia,,,
PaLI,"Language,Vision",,2022-09-14,,Unreleased,,,https://arxiv.org/abs/2209.06794v4,Google,5.1e+22,"""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days""

275 teraFLOP/s * 1024 * 7 * 24 * 3600 * 0.3 (utilization assumption) = 5.1e22",United States of America,"Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",SOTA improvement,"""PaLI achieves state-of-the-art in multiple vision and language tasks
(such as captioning, visual question-answering, scene-text understanding)""",PaLI: A Jointly-Scaled Multilingual Language-Image Model,WebLI,"""we introduce WebLI, a multilingual imagelanguage dataset built from images and texts available on the public web... Due to the abundance of multilingual content on the internet, the collection process for the WebLI dataset can be scaled to cover 10 billion images and 12 billion alt-texts. In addition to annotation with web text, we use publicly available automatic service to extract OCR annotations on all images, resulting in 29 billion image-OCR pairs. To balance quality and retain scale, we filter the dataset to the highest quality subset retaining only the top 10% scoring of the original WebLI image-text pairs (about 1B examples), which we use to train PaLI""",,,Likely,,,410.00,16900000000.00,"3.9b Image Encoder, 
14b Multimodal Encoder-Decoder",1.00,,,168.0,7 days,Google TPU v4,,,,,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",2024-05-01 09:22,Anonymous,,,,,,1024,,PaLI,,,,Industry,checked,,,172032,Industry,,,
Stable Diffusion (LDM-KL-8-G),Image generation,"Image generation,Text-to-image",2022-04-13,OpenRAIL license,Open access (restricted use),,,https://arxiv.org/abs/2112.10752,"Runway,Ludwig Maximilian University",5.0000000000000004e+22,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""

[1] https://twitter.com/EMostaque/status/1563870674111832066","United States of America,Germany","Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer","Significant use,Highly cited",,High-Resolution Image Synthesis with Latent Diffusion Models,LAION-400M,"Depends on the specific task; see sec 4

""we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M""",400000000,,,,,6121.00,1450000000.00,See Table 2,,,,585.9,"total chip-hours divided by number of GPUs
150k/256",NVIDIA A100,Self-supervised learning,,,,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",2024-05-03 13:29,Robi Rahman,,,,,,256,,Stable Diffusion (LDM-KL-8-G),,,,"Industry,Academia",,,,150000,"Industry,Academia",,,
Stable Diffusion 3,Image generation,"Image generation,Text-to-image",2024-02-22,"Apr 2014
We have partnered with Fireworks AI, the fastest and most reliable API platform in the market, to deliver Stable Diffusion 3 and Stable Diffusion 3 Turbo.

In keeping with our commitment to open generative AI, we aim to make the model weights available for self-hosting with a Stability AI Membership in the near future.",API access,,,"https://arxiv.org/abs/2403.03206
https://stability.ai/news/stable-diffusion-3",Stability AI,5.0000000000000004e+22,"Finally, we performed
a scaling study of this combination up to a model size of
8B parameters and 5 × 1022 training FLOPs.",Multinational,"Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach",,,Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,"ImageNet,Conceptual 12M (CC12M)","We use two datasets to account for the missing of a standard text-to-image benchmark. As a widely used dataset,
we convert the ImageNet dataset (Russakovsky et al., 2014) into a dataset suitable for text-to-image models by adding
captions of the form “a photo of a 〈class name〉” to images, where 〈class name〉 is randomly chosen from one
of the provided names for the image’s class label. As a more realistic text-to-image dataset, we use the CC12M dataset
(Changpinyo et al., 2021) for training.",,,Confident,,,,8000000000.00,,,,,,,,,,,,"Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",2024-04-19 10:31,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
FLAN 137B,Language,Language modelling,2021-09-03,,Unreleased,,Unreleased,https://arxiv.org/abs/2109.01652,Google Research,4.896e+22,"From section 2.4: ""60 hours on a TPUv3 with 128 cores."" I assume that ""128 cores"" = 128 TPUv3s. Which took less than 2% of total time (see environmental considerations section)",Multinational,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le","Highly cited,SOTA improvement","Abstract: 
""FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.""",Finetuned Language Models Are Zero-Shot Learners,"Wikipedia,Unspecified unreleased","Abstract: ""We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets""",1870000000000,"""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""

2.49e12 tokens ~= 1.87e12 words",,,,1962.00,137000000000.00,"Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?",,,,60.0,,Google TPU v3,Self-supervised learning,,,Industry,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",2024-04-30 04:37,Robi Rahman,,,LaMDA,,"""In our experiments, we use LaMDA-PT, a dense left-to-right,
decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022) [...] Note that
LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog)."" In our entry for LaMDA we only measured pre-training compute, so we just specify LaMDA as the base model of FLAN 137B.",64,,FLAN 137B,,,,Industry,,,,3840,Industry,,,
Florence,Vision,,2021-11-22,,Unreleased,,Unreleased,https://arxiv.org/abs/2111.11432v1,Microsoft,4.831e+22,"""The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.""
512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP",United States of America,"Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, JianFeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang","Historical significance,SOTA improvement",,Florence: A New Foundation Model for Computer Vision,FLD-900M,"900 million image-text pairs curated from internet images and descriptions

""We leverage large quantities of image-text data available
publicly on the internet. Specifically, we construct a 900
million image-text-pair dataset, called FLD-900M (FLD
stands for FLorenceDataset), using a programmatic data
curation pipeline that processes around 3 billion Internet
images and their raw descriptions in parallel.  <..>The final form of the FLD-900M dataset consists of 900M images with 900M free-form texts (ranging from one word, phase to sentences), 9.7M unique queries, and 7.5B tokens in total.
",900000000,,Confident,,,613.00,893000000.00,"""Our Florence pretrained model has in total 893M parameters, including the language transformer with 256M parameters and the CoSwin-H transformer with 637M parameters.""",,,,240.0,10 days on 512 A100 40GB,NVIDIA A100 SXM4 40 GB,Supervised,,,Industry,"Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",2024-04-18 05:28,Anonymous,,,,,,512,,Florence,,,,Industry,checked,,,122880,Industry,,,
Meta Pseudo Labels,Vision,Image classification,2021-03-01,"Apache-2.0 license
https://github.com/google-research/google-research/blob/master/meta_pseudo_labels/README.md",Unreleased,,Open source,https://arxiv.org/abs/2003.10580,"Google Brain,Google AI",4.79e+22,"From communication with author:

22671 TPU days on specific hardware.

Which hardware did you use and in which configuration?
2048 cores of TPU v3.

Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.

2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)

So the compute estimate is:
1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP","United States of America,Multinational","Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",SOTA improvement,,Meta pseudo labels,"ImageNet,JFT-300M",,130000000,"Section 4
Datasets. For this experiment, we use the entire ImageNet
training set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, and
then is filtered down to 130 million images by Noisy Student
using confidence thresholds and up-sampling [77]. We use
the same 130 million images as Noisy Student",,,,535.00,480000000.00,"Table 4
 480M",,,,264.0,"11 days from section 4:
""We train the model for 1 million steps in total,
which takes about 11 days for EfficientNet-L2 and 10 days
for EfficientNet-B6-Wide. ""

""Specifically, our training process runs on a cluster of 2,048
TPUv3 cores. ""
",Google TPU v3,Self-supervised learning,$369462.82,,Industry,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL.",2024-05-01 09:05,Robi Rahman,,,,,,1024,,Meta Pseudo Labels,,,,"Industry,Industry",,,,270336,"Industry,Industry",,,
DALL-E,Image generation,Text-to-image,2021-01-05,,API access,Unreleased,Unreleased,"https://openai.com/blog/dall-e/

https://arxiv.org/abs/2102.12092",OpenAI,4.7e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",United States of America,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever","Significant use,Highly cited",,Zero-Shot Text-to-Image Generation,,"To scale up to 12-billion parameters, we created a dataset of
a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. This dataset
does not include MS-COCO, but does include Conceptual
Captions and a filtered subset of YFCC100M (Thomee et al.,
2016). As MS-COCO was created from the latter, our training data includes a fraction of the MS-COCO validation images (but none of the captions).",250000000,"""To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. """,,,,2969.00,12000000000.00,DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,,,,,"""We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.
At the start of training, we use a linear schedule to ramp up the step size to 4.5 · 10−4 over 5000 updates, and halved the
step size each time the training loss appeared to plateau. We did this a total of five times, ending training with a final step
size that was 32 times smaller than the initial one. """,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,$171537.13,,Industry,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",2024-04-22 10:13,Robi Rahman,,,,,,1024,,DALL-E,,,,Industry,,,,,Industry,,,
Whisper,Speech,Audio speech recognition,2022-09-21,"MIT for code and weights:
https://github.com/openai/whisper",Open source,,Open source,https://cdn.openai.com/papers/whisper.pdf,OpenAI,4.65e+22,See figure 9,United States of America,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",SOTA improvement,,Robust Speech Recognition via Large-Scale Weak Supervision,,,9302400000,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h * 680,000h = 9,302,400,000 words",,,,1180.00,1550000000.00,Table 1,3.00,,,,,,Self-supervised learning,,,,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",2024-05-01 09:22,Robi Rahman,,,,,,,,Whisper,,,,Industry,checked,,,,Industry,,,
Orca 2-13B,Language,Language modelling/generation,2023-11-21,"non commercial

https://huggingface.co/microsoft/Orca-2-13b/blob/main/LICENSE",Open access (non-commercial),,,"https://arxiv.org/abs/2311.11045, https://huggingface.co/microsoft/Orca-2-13b",Microsoft,4.6e+22,4.55e22 base compute from Llama-13 + 8.6e20 finetune compute ~= 4.6e22 ,United States of America,,,"""Orca 2 significantly surpasses models of similar size and attains performance levels similar or better
to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings""",Orca 2: Teaching Small Language Models How to Reason,,"""For Orca 2, we created a new dataset with ~817K training instances, which we will refer as
Orca 2 dataset. Following Orca 1, Orca 2 has been trained with progressive learning, with
subsets of data obtained from combining the original FLAN [33] annotations, Orca 1 dataset
and the Orca 2 dataset. We also describe the details about the progressive learning.""",,See finetune notes,Likely,,,4.00,13000000000.00,based on Llama 13B,,,,80.0,"17+40+23 hours

""We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16.
For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch,
~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continue
training on ~1.8 million GPT-4 data for 4 epochs""",NVIDIA A100 SXM4 80 GB,,,,,"Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at this http URL to support research on the development, evaluation, and alignment of smaller LMs",2024-03-27 14:27,Anonymous,,0,LLaMA-13B,860000000000000000000,"""We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16.
For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch,
~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continue
training on ~1.8 million GPT-4 data for 4 epochs""

32 * (17+40+23) * 3600 * 312 trillion flop/s * 0.3 utilization = 8.6e20

(each 10 hours is ~1e20 flop, so the 5 million ChatGPT data took 4e20 FLOP. 4e20 / 5 million / 13 billion / 6 = 1025, so ~1000 tokens in each example)",,,,,,,Industry,,,,,Industry,,,
LLaMA-13B,Language,Language modelling,2023-02-27,non-commercial license: https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform,Open access (non-commercial),,Unreleased,https://arxiv.org/abs/2302.13971,Meta AI,4.55e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22

from paper, Llama-7B took 135,168 GPU hours using A100s

312 trillion * 135,168 * 3600 * 0.3 = 4.55e22 FLOP",United States of America,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Highly cited,,LLaMA: Open and Efficient Foundation Language Models,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",,1000000000000,Table 2,Confident,4000000,,4640.00,13000000000.00,13.0B,1.09,,,,,NVIDIA A100,,,,,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",2024-05-23 05:20,Robi Rahman,LLaMA-13B,1,,,,,,,,,,Industry,,,,,Industry,,,
StableLM-Base-Alpha-7B,Language,Language modelling,2023-08-05,"CC BY-SA (permissive):

https://creativecommons.org/licenses/by-sa/4.0/",Open source,Unreleased,Unreleased,https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2,Stability AI,4.5e+22,"""StableLM-Base-Alpha-7B-v2 is pre-trained using a multi-stage context length extension schedule following similar work (Nijkamp et al. 2023); first pre-training at a context length of 2048 for 1 trillion tokens, then fine-tuning at a context length of 4096 for another 100B tokens""

6890209280 params * 1.1 trillion tokens * 6 = 4.5e22

alternatively: ""StableLM-Base-Alpha-7B-v2 was trained on the Stability AI cluster - occupying 384 NVIDIA A100 40GB GPUs across AWS P4d instances. Training took approximately 16.33 days to complete across both stages.""

312 teraflops * 384 * 16.33 * 24 * 3600 * 0.3 = 5.07e22",Multinational,,,,,"Falcon RefinedWeb,RedPajama-Data,The Pile","""The first pre-training stage relies on 1 trillion tokens sourced from a mix of the public Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer 2023, The Pile (Gao et al., 2020), and internal datasets with web text sampled at a rate of 71%.

In the second stage, we include the StarCoder (Li et al., 2023) dataset and down sample web text to 55% while increasing sampling proportions of naturally long text examples in the aforementioned sources.""",750000000000,1 trillion tokens,Likely,,,,6890209280.00,,1.00,,,392.0,16.33 days,NVIDIA A100 SXM4 40 GB,,,,,"StableLM-Base-Alpha-7B-v2 is a 7 billion parameter decoder-only language model pre-trained on diverse English datasets. This model is the successor to the first StableLM-Base-Alpha-7B model, addressing previous shortcomings through the use of improved data sources and mixture ratios.",2024-04-09 11:43,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
XGLM-7.5B,Language,"Translation,Question answering,Language modelling/generation",2021-12-20,MIT for weights: https://github.com/facebookresearch/fairseq/blob/main/LICENSE,Open access (non-commercial),Open access (restricted use),Unreleased,https://arxiv.org/abs/2112.10668,"Meta AI,Facebook AI Research",4.347592704e+22,""" The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""
so 312e12 * 256 * 3*7*24*3600 *0.3 = 4.347592704e+22
alternative:
6ND = 6*7.5e9*500e9 = 2.25e22 - we have 7.5B params and 500B tokens from ""we train four multilingual generative language models (up to 7.5 billion parameters),
XGLM’s, and present a comprehensive study of
multilingual zero- and in-context few-shot learning.
We train the models using a large-scale corpus of
500B tokens that comprises 30 diverse languages""","United States of America,United States of America","Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",SOTA improvement,"""Our largest model
with 7.5 billion parameters sets new state of
the art in few-shot learning in more than 20
representative languages""",Few-shot Learning with Multilingual Language Models,"CC100-XL,Common Crawl","""We extend the pipeline used for mining the CC100
corpus (Conneau et al., 2020; Wenzek et al., 2020)
to generate CC100-XL, a significantly larger mul-
tilingual dataset covering 68 Common Crawl (CC)
snapshots (from Summer 2013 to March/April
2020) and 134 languages.""
",565367040000,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""
so 311.6e3 * 3*7*24*3600 = 565367040000 words
- around 1.13 words per token

here it gives a total number of 1640166277942 tokens
https://huggingface.co/facebook/xglm-7.5B#training-data-statistics",Likely,,,161.00,7500000000.00,"""Our largest model
with 7.5 billion parameters sets new state of
the art in few-shot learning in more than 20
representative languages""",1.00,16300408163.00,"""On 256 A100 GPUs, the inference speed can reach 1.47 million words per second.""
(312e12 * 256 * 0.3)/1.47e6 = 16300408163
(peak flop) * (num gpu) * (assumed utilization rate) / (number of words per second during inference)",504.0,"appendix A : ""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",NVIDIA A100,Self-supervised learning,,,Industry,"Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models. ",2024-05-01 09:13,Anonymous,,,,,,256,,XGLM-7.5B,,,,"Industry,Industry",,,,129024,"Industry,Industry",,,
CoAtNet,Vision,Image classification,2021-06-09,,Unreleased,,Unreleased,https://arxiv.org/abs/2106.04803v2,"Google,Google Research,Google Brain",4.27e+22,"20.1K TPU-v3 core-days

TPUs have two cores per chip, and a chip is 123 teraflop/s 
https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3

123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22","United States of America,Multinational,United States of America","Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan",SOTA improvement,"""Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.""","CoAtNet: Marrying Convolution and Attention
for All Data Sizes",JFT-3B,"When only ImageNet-1K is used for training, CoAtNet achieves 86.0% top-1 accuracy, matching the prior art NFNet [20] under similar computation resource and training conditions. Further, when pre-trained on ImageNet-21K with about 10M images, CoAtNet reaches 88.56% top-1 accuracy when finetuned on ImageNet-1K, matching the ViT-Huge pre-trained on JFT-300M, a 23× larger dataset. Finally, when JFT-3B is used for pre-training, CoAtNet exhibits better efficiency compared to ViT, and pushes the ImageNet-1K top-1 accuracy to 90.88% while using 1.5x less computation of the prior art set by ViT-G/14 [26].",,,Likely,,,833.00,2440000000.00,,,2586000000000.00,"2586B FLOP, per table 5, for 512x512 image",,,Google TPU v3,,,,,"Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced “coat” nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",2024-05-01 09:13,Anonymous,,,,,,,,CoAtNet,,,,"Industry,Industry,Industry",,,,10050,"Industry,Industry,Industry",,,
MPT-7B (base),Language,,2023-05-05,"Apache 2.0

""Our MPT model series is:

Licensed for commercial use (unlike LLaMA).",Open source,,Unreleased,https://www.mosaicml.com/blog/mpt-7b,MosaicML,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k. The finetuned models took much less compute and were much cheaper – ranging between a few hundred and few thousand dollars each.""",United States of America,MosaicML NLP Team,,,"""Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs""","mC4,C4,RedPajama (1T),The Stack","""The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:
...""

https://huggingface.co/mosaicml/mpt-7b",,,,,,,7000000000.00,,1.00,,,228.0,"Table 3
https://www.databricks.com/sites/default/files/inline-images/mpt-commercially-usable-llms-img-8.jpg",NVIDIA A100 SXM4 40 GB,,,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k. The finetuned models took much less compute and were much cheaper – ranging between a few hundred and few thousand dollars each.""",,,2024-05-14 10:24,Robi Rahman,MPT-7B,,,,,440,0.3727,,,,,Industry,,,,100320,Industry,,,
BASIC-L,Vision,Image classification,2021-11-19,,Unreleased,,Unreleased,https://arxiv.org/abs/2111.10050,Google,4.12e+22,"6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8

Two cores per chip, and 275 teraflop/s per chip 
(https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)

275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22",United States of America,"Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le",SOTA improvement,"SOTA on ImageNet for a model that was not trained on ImageNet images:
""We present a combined scaling method – named BASIC – that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy
surpasses best-published similar models – CLIP and ALIGN – by 9.3%""",Combined Scaling for Zero-shot Transfer Learning,"JFT,ALIGN","For pretraining (Section 8), we use the JFT dataset. This dataset has been
used in previous publications (Zhai et al., 2021; Dosovitskiy et al., 2021; Kolesnikov et al., 2020), but it has been constantly expanded. The JFT version used in our experiments has 5B images, each of which can be associated to one or multiple labels out of 29K possible classes.


""Starting from the ALIGN dataset, which contains 1.7B weakly-aligned image-text pairs (Jia et al., 2021), we collect 5B more image-text pairs, hence expanding the dataset size by roughly 4 times. We acquire
these 5B image-text pairs from the JFT dataset""",6700000000,6.7B image-text pairs,Likely,,"65536, but these are image-text pairs not tokens
""For the batch size, we use 65536 contrastive
learning examples per minibatch""",135.00,3070000000.00,2.4B image model + 670M text model,3.00,708000000000.00,"per Table 5, if I'm understanding correctly",,,Google TPU v4,,,,,"We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.",2024-05-01 09:13,Anonymous,,,,,,,,BASIC-L,,,,Industry,,,,4350,Industry,,,
Persimmon-8B,Language,Language modelling,2023-09-07,apache 2.0,Open source,,,https://www.adept.ai/blog/persimmon-8b,Adept,4.11246e+22,6*9300000000*737000000000=4.11246e+22,United States of America,"Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani",,,Releasing Persimmon-8B,,"We train the model from start to finish with a sequence length of 16K on 737B tokens uniformly sampled from a much larger dataset, which is a mix of text (~75%) and code (~25%).",737000000000,737B tokens = 552750M words,Confident,,,0.00,9300000000.00,"""The checkpoint we are releasing has approximately 9.3B parameters. In order to make pipelining during training more efficient, we chose to decouple the input and output embeddings. Doing this does not increase the capacity of the model–it is purely a systems optimization to avoid all-reducing the gradients for the (very large) embeddings across potentially slow communication links. In terms of inference cost, the model is equivalent to an 8B parameter model with coupled input/output embeddings.""",,16000000000.00,"The checkpoint we are releasing has approximately 9.3B parameters. In order to make pipelining during training more efficient, we chose to decouple the input and output embeddings. Doing this does not increase the capacity of the model–it is purely a systems optimization to avoid all-reducing the gradients for the (very large) embeddings across potentially slow communication links. In terms of inference cost, the model is equivalent to an 8B parameter model with coupled input/output embeddings.

8B * 2 FLOP/token/parameter = 16B FLOP/token inference compute",,,,,,,Industry,,2024-05-21 06:53,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
RedPajama-INCITE-7B-Base,Language,Chat,2023-06-06,"Apache 2.0 for weights

data TBD: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T",Open source,,Unreleased,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base,Together,4.1e+22,"Trained over 1.001 trillion tokens.
6.9b * 1 trillion * 6 = 4.1e22

",United States of America,,,"one of the fine-tuned versions is Pareto SOTA for open source. wouldn't consider it SOTA overall. ""RedPajama-INCITE-7B-Instruct is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points.""",,RedPajama (1T),https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T,900000000000,"1.2 trillion, or 900b words at 0.75 words/token",Likely,4000000,"""global batch size 4M tokens""",,6900000000.00,6.9b,0.83,,,,,NVIDIA V100,,,,,"We trained 3B and 7B models on the Summit supercomputer, in collaboration with AAI CERC lab at Université de Montréal, EleutherAI & LAION for compute time on Summit within the INCITE program award ""Scalable Foundation Models for Transferable Generalist AI”.

Today we are excited to release the v1 versions of the RedPajama-INCITE family of models, including instruct-tuned and chat versions under the Apache 2.0 license.",2024-04-09 12:19,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
JIANG,Language,Language modelling,2023-08-01,apache 2.0,Open source,,,https://arxiv.org/abs/2308.00624,K.D. Feddersen (KDF),4.03e+22,"""The training was conducted using 96 A100 80G GPUs, and the entire process took approximately 52 days.""

312 teraflop/s * 96 * 52 * 24 * 3600 * 0.3 = 4e22",,"Qinhua Duan, Wenchao Gu, Yujia Chen, Wenxin Mao, Zewen Tian, Hui Cao",,,JIANG: Chinese Open Foundation Language Model,,"""The model is trained on a large quantity of textual data including both English and Chinese and use a standard optimizer.""

Mostly from Chinese internet, and The Pile and GitHub.",467000000000,"467B tokens (inferred from Table 1).

It's a mix of Chinese and English text, I'll use our standard 1:1 token:words ratio for Chinese.",Likely,6000000,"""During the training process, we employed a large batch size of 6 million tokens to enhance the model’s stability""",0.00,,They show a chart with different 400M models they trained to refine the design. Main model probably has more but they don't specify.,,,,1200.0,52 days,NVIDIA A100 SXM4 80 GB,,,,,"With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental results demonstrate the excellent performance of our model.",2024-03-28 11:51,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
LLaMA-7B,Language,Language modelling,2023-02-24,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",Open access (non-commercial),,Unreleased,https://arxiv.org/abs/2302.13971,Meta AI,4.02e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP

from paper, Llama-7B took 82,432 GPU hours using A100s

312 trillion * 82,432 * 3600 * 0.3 = 2.78e22 FLOP",United States of America,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample","Significant use,Highly cited",,LLaMA: Open and Efficient Foundation Language Models,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",,1000000000000,1 trillion tokens * 0.75 words/token = 750 billion words,Likely,4000000,,4640.00,6700000000.00,"6.7B parameters, per table 2: https://arxiv.org/pdf/2302.13971.pdf",1.09,,,,,NVIDIA A100,,,,,"""We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.""",2024-05-23 05:17,Robi Rahman,LLaMA-7B,1,,,,,0.4300,LLaMA-7B,,,,Industry,,,,82432,Industry,,,
WizardLM-7B,Language,Language modelling,2023-04-24,"non commercial: 
https://github.com/nlpxucan/WizardLM",Open access (non-commercial),,,https://arxiv.org/abs/2304.12244,"Microsoft,Peking University",4.02e+22,"""We use pre-trained LLaMA 7B [4] to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 ×10−5, a maximum number of tokens 2048, and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs""

Llama-7b was ~4e22. 8*70 V100-hours is ~2e20, so fine-tuning was <1% of base training.","United States of America,China","Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",SOTA improvement,"""Labelers prefer WizardLM outputs over outputs from ChatGPT under complex test instructions. On Evol-Instruct testset, WizardLM performs worse than ChatGPT, with a win
rate 12.8% lower than ChatGPT (28.0% vs. 40.8%). However, in the high-difficulty section
of Evol-Instruct test set (difficulty level ≥ 8), our WizardLM even outperforms ChatGPT,
with a win rate 7.9% larger than ChatGPT (42.9% vs. 35.0%), that is human annotators even
prefer the output of our model than ChatGPT on those hard questions""",WizardLM: Empowering Large Language Models to Follow Complex Instructions,,"Fine-tuning dataset is made of LLM-generated instructions: ""In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs""",,,Likely,,,421.00,6700000000.00,This is Llama-7b's parameter count,,,,70.0,,NVIDIA V100,,,,,"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL",2024-05-01 09:24,Anonymous,,,LLaMA-7B,,,8,,WizardLM-7B,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Pangu-Weather,Earth science,Weather prediction,2023-07-05,,,,,"https://www.nature.com/articles/s41586-023-06185-3, https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,
https://www.huawei.com/en/news/2023/7/pangu-ai-model-nature-publish",Huawei,3.98e+22,"""Each of the four deep networks was trained for 100 epochs, and
each of them takes approximately 16 days on a cluster of 192 NVIDIA
Tesla-V100 GPUs.""

192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22",China,"Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian",SOTA improvement,"""In meteorology, the Pangu Meteorology Model (or Pangu-Weather) is the first AI model to have surpassed state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy. The prediction speed is also several orders of magnitude faster. In the past, predicting the trajectory of a typhoon over 10 days took 4 to 5 hours of simulation on a high-performance cluster of 3,000 servers. Now, the Pangu model can do it in 10 seconds on a single GPU of a single server, and with more accurate results.""

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html",Accurate medium-range global weather forecasting with 3D neural networks,ERA5,"""We used a single point in time for both input and output. The time resolution
of the ERA5 data is 1 h; in the training subset (1979–2017), there were
as many as 341,880 time points, the amount of training data in one
epoch""",,"""We used a single point in time for both input and output. The time resolution
of the ERA5 data is 1 h; in the training subset (1979–2017), there were
as many as 341,880 time points, the amount of training data in one
epoch... We
fed all included weather variables, including 13 layers of upper-air
variables and the surface variables""

341,880 is the number of hours in ~40 years. But there's lots of data for each hour.",Likely,,,102.00,256000000.00,"4*64 million = 256M params

""We trained four deep networks with lead times (the time difference
between input and output) at 1 h, 3 h, 6 h and 24 h, respectively... 

This modification increases the number of bias parameters by a factor of 527, with each 3D deep network
containing approximately 64 million parameters.""",100.00,,,1536.0,"4*16 = 64 days
""Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16 days on a cluster of 192 NVIDIA
Tesla-V100 GPUs.""
",NVIDIA V100,,,,,"Weather forecasting is important for science and society. At present, the most accurate
forecast system is the numerical weather prediction (NWP) method, which represents
atmospheric states as discretized grids and numerically solves partial diferential
equations that describe the transition between those states1
. However, this procedure
is computationally expensive. Recently, artifcial-intelligence-based methods2
 have
shown potential in accelerating weather forecasting by orders of magnitude, but
the forecast accuracy is still signifcantly lower than that of NWP methods. Here we
introduce an artifcial-intelligence-based method for accurate, medium-range global
weather forecasting. We show that three-dimensional deep networks equipped with
Earth-specifc priors are efective at dealing with complex patterns in weather data,
and that a hierarchical temporal aggregation strategy reduces accumulation errors
in medium-range forecasting. Trained on 39 years of global data, our program,
Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in
all tested variables when compared with the world’s best NWP system, the operational
integrated forecasting system of the European Centre for Medium-Range Weather
Forecasts (ECMWF)3
. Our method also works well with extreme weather forecasts and
ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking
tropical cyclones is also higher than that of ECMWF-HRES.",2024-05-01 09:22,Epoch AI,,0,,,"Possibly based on Pangu 3? Pangu-Weather is mentioned in the Pangu 3 announcement. But the architecture description doesn't seem to resemble Pangu 3. So it seems like Pangu-Weather is one of the higher-level models that can be attached to Pangu 3. 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
",192,,Pangu-Weather,,,,Industry,,,,,Industry,,,
ProtBERT-BFD,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2021-05-04,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans",Open source,Open source,Unreleased,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085","Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology",3.9e+22,"""FLOP = 420M*2*(800k*512*32k+200k*2048*6k) +  420M*4*(800k*512*32k+200k*2048*6k), 1M steps total split into two phases, (1) 800k steps, seq length 512 (bath size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)
single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd""","Germany,United States of America,Korea (Republic of),United States of America,United States of America,China","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,BFD (Big Fantastic Dataset),"ProtBert: BERT2 was trained using both UniRef100
and BFD-100 datasets (referred to as ProtBert and ProtBertBFD, respectively; Table 2)",,"27B proteins during pretraining (Table 1: 2122M proteins, 393B amino acids, 572 GB)",Likely,,,,420000000.00,Table 2,,,,,figure 3 shows 19 hours per epoch,Google TPU v3,Self-supervised learning,,,,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.

Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",2024-04-16 07:55,Anonymous,,,,,,1024,,ProtBERT-BFD,,,,"Academia,Industry,Academia,Industry,Government",,,,,"Academia,Industry,Academia,Industry,Government",,,
Konan LLM 13B,"Language,Vision",Language modelling/generation,2023-08-15,,Hosted access (no API),,Unreleased,"https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",Konan Technology,3.86712e+22,=13100000000*492000000000*6=3.86712 × 10^22,Korea (Republic of),"Yang Seung-hyun, Wiretin, Changmin, Kim Jong-tae",,,Konan LLM: A Korean Large Language Model,,"The Konan LLM introduced this time has 492 billion total tokens, of which 284 billion Korean tokens have been learned. The remaining tokens are in English.",492000000000,"""492 billion total tokens"" from https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model

Since 2007, via the real-time AI analysis service pulseK, over 20.5 billion pieces of data have been independently secured.
Among them, only 2 billion high-quality, large-scale data pieces have been used for training.
",Likely,,,,13100000000.00,'Konan LLM' has 13.1 billion parameters.,,,,,,,,,,,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.",2024-05-15 14:57,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,,,
GLM-10B,Language,,2021-03-18,Apache 2.0 or MIT for code/weights: https://github.com/THUDM/GLM,Open source,,Open source,https://arxiv.org/abs/2103.10360,"Tsinghua University,Beijing Academy of Artificial Intelligence,Massachusetts Institute of Technology (MIT),Shanghai Qi Zhi institute",3.79e+22,,"China,China,United States of America,China","Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",,smaller version of the model in this paper,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,The Pile,see table https://github.com/THUDM/GLM ,,,,524288,"""The models are trained on 64 V100 GPUs for 200K steps with
batch size of 1024 and maximum sequence length
of 512""",617.00,10000000000.00,,1.00,,,,,,,,,,,2024-05-08 15:02,Robi Rahman,GLM-10B,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
ProtT5-XXL-BFD,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2021-05-04,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans",Open source,Open source,Unreleased,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085","Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University",3.7e+22,"FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, ","Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,BFD (Big Fantastic Dataset),"First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).",,"Table 1: 2122M proteins, 393B amino acids, 572 GB",Likely,,,,11000000000.00,Table 2,,,,,,Google TPU v3,Self-supervised learning,,,,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.

Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",2024-04-16 07:59,Anonymous,,,,,,512,,ProtT5-XXL-BFD,,,,"Academia,Industry,Government,Industry,Academia",,,,,"Academia,Industry,Government,Industry,Academia",,,
Student of Games,Games,,2021-12-06,,Unreleased,,Unreleased,https://arxiv.org/abs/2112.03178,DeepMind,3.6679273004682866e+22,"""We trained a version of AlphaZero using its original settings in chess and Go, e.g. , using 800 MCTS simulations during training, with 3500 concurrent actors each on a single TPUv4, for a total of 800k training steps. SOG was trained using a similar amount of TPU resources.""",United Kingdom of Great Britain and Northern Ireland,"Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling",SOTA improvement,"""Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard""",Player of Games,,,,,Speculative,,,9.00,,,,,,,,,,,,Industry,,2024-04-16 09:25,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
AlphaZero,Games,,2017-12-05,,,,,https://arxiv.org/abs/1712.01815,DeepMind,3.6679273004682866e+22,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,United Kingdom of Great Britain and Northern Ireland,"D Silver, T Hubert, J Schrittwieser, I Antonoglou",Highly cited,,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,,,700000,"""We trained a separate instance of AlphaZero for each game. Training proceeded
for 700,000 steps""",,,,1411.00,,,,,"This post claims 0.8 seconds per move for the 40-day training version of AlphaGo Zero
https://www.yuzeh.com/data/agz-cost.html
Compare move time for AlphaZero",,,Google TPU v2,Self-supervised learning,$162054.70,,Industry,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",2024-04-01 09:03,Robi Rahman,,0,,,,64,,AlphaZero,,,,Industry,,,,,Industry,,,
Weblab-10B,Language,Language modelling/generation,2023-08-04,"cc-by-nc-4.0, non commercial",Open access (non-commercial),,,https://huggingface.co/matsuo-lab/weblab-10b,Matsuo Lab,3.6e+22,"6ND = 10B*600B * 6 = 3.6e22"" The model was trained on around 600B tokens from a mixture of the following corpora.""

See also: https://weblab.t.u-tokyo.ac.jp/en/100%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%BA%E3%83%BB%E6%97%A5%E8%8B%B12%E3%83%B6%E5%9B%BD%E8%AA%9E%E5%AF%BE%E5%BF%9C%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1/",, Takeshi Kojima,,, weblab-10b,"The Pile,Japanese C4","""Pre-training

The model was trained on around 600B tokens from a mixture of the following corpora.

    Japanese C4
    The Pile
""",600000000000,"600B tokens , assuming 1 word per token",Likely,,,,10000000000.00,,,20000000000.00,2N,,,,Self-supervised learning,,,,,2024-03-29 16:04,Bartosz Podkanowicz,,,,,,,,,,,,,,,,,,,,
PLUG,Language,,2021-04-19,https://nlp.aliyun.com/portal#/BigText_chinese,Hosted access (no API),,Unreleased,https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg,Alibaba,3.5997695999999996e+22,128 Nvidia A100 for 35 days,China,,SOTA improvement,Was a SOTA in CLUE 1.0 https://www.cluebenchmarks.com/classification10.html,,,,,,,,,0.00,27000000000.00,,,,,840.0,35 days,NVIDIA A100,,,,,,2024-04-22 11:58,Robi Rahman,,,,,,128,,PLUG,,,,Industry,,,,,Industry,,,
Genie,"Video,Games",Video generation,2024-02-23,"We have chosen not to release the trained model checkpoints, the
model’s training dataset, or examples from that data to accompany this paper or the website. We would like to have the opportunity to further engage with the research (and video game) community and to ensure that any future such releases are respectful, safe and responsible.",Unreleased,,Unreleased,https://arxiv.org/abs/2402.15391,Google DeepMind,3.49221e+22,"see figure 9 Right

10^(5.431/10)*10^22=3.49221...*10^22",Multinational,"Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel",,,Genie: Generative Interactive Environments,,"Our approach, Genie, is trained from a large dataset of over 200,000 hours of
publicly available Internet gaming videos

We train Genie on a filtered set of 30,000 hours of Internet gameplay videos from hundreds of 2D platformer games, producing a foundation world model for this setting.",942000000000,"When combined with the tokenizer and action model this brings the total to 10.7B parameters, trained on 942B tokens, which we refer to as the Genie model.",Likely,512,"As a result, for our final model, we train a 10.1B dynamics model with a batch size of 512, for a total of 125k steps, using 256 TPUv5p",,10700000000.00,"When combined with the tokenizer and action
model this brings the total to 10.7B parameters,
trained on 942B tokens, which we refer to as the
Genie model. ",,,,,,Google TPU v5p,,,,,"We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",2024-04-19 13:21,Natalia Martemianova,,,,,,256,,,,,,Industry,,,,,Industry,,,
DeepSeekMoE-16B,Language,Chat,2024-01-11,abuse restrictions: https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/LICENSE-MODEL,Open access (restricted use),,,https://arxiv.org/abs/2401.06066,DeepSeek,3.4e+22,"""With the DeepSeekMoE architecture, we scale up our MoE model to a larger scale with 16B total
parameters and train it on 2T tokens""

""Evaluation results reveal that with only about 40% of computations, DeepSeekMoE 16B achieves comparable performance
with DeepSeek 7B (DeepSeek-AI, 2024), a dense model trained on the same 2T corpus""

40% * 7B = 2.8B, so 2.8B effective parameters

2.8B * 2T * 6 ~= 3.4e22",China,"Damai Dai, Chengqi Deng, Chenggang Zhao, R.X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang",,"""In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations""",DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models,,"""Our training data is sampled from a large-scale multilingual corpus created by DeepSeek-AI. The
corpus primarily focuses on English and Chinese but also encompasses other languages. It is de
rived from diverse sources, including web text, mathematical material, coding scripts, published
literature, and various other textual materials. For the purpose of validation experiments, we
sample a subset containing 100B tokens from the corpus to train our models""",1800000000000,"""Leveraging our architecture, we subsequently scale up the model parameters to 16B and
train DeepSeekMoE 16B on a large-scale corpus with 2T tokens.""

Probably a mix of English and Chinese. 1T English tokens is 0.75T words; 1T Chinese tokens is 1T words, so ~1.8T total

",Likely,,,14.00,16000000000.00,"16B (total, but it's sparse)",1.00,,,,,"NVIDIA A100,NVIDIA H800",,,,,"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating Ks experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.",2024-04-16 15:29,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
T5-11B,Language,Text autocompletion,2019-10-23,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open",Open source,Open source,Open source,https://arxiv.org/abs/1910.10683,Google,3.3e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4, 4.05e22

update: 3.3e22 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf",United States of America,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Highly cited,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,C4,,150000000000,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal
Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB = 1.5e11",Confident,65536,"""We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we “pack” multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens""",12884.00,11000000000.00,The full 11-billion parameter model,,,,481.9,"4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours
https://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",Google TPU v3,Self-supervised learning,$105686.20,,Industry,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",2024-04-19 16:22,Robi Rahman,,,,,,512,0.3707,T5-11B,,,,Industry,,,,246733,Industry,,,
iGPT-XL,"Vision,Image generation",Image completion,2020-06-17,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme",Open source,,Open source,https://openai.com/research/image-gpt,OpenAI,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",United States of America,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",Highly cited,,Generative Pretraining from Pixels,ILSVRC 2012 subset of ImageNet,,9600000,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,1190.00,6801000000.00,source: https://openai.com/blog/image-gpt/#rfref53,,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$120440.96,,Industry,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2024-04-24 13:25,Robi Rahman,,,,,,,,iGPT-XL,,,,Industry,,,,,Industry,,,
Flan-T5 11B,Language,Language modelling/generation,2022-10-20,"apache 2.0 license

https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints",Open source,Unreleased,Unreleased,"https://arxiv.org/abs/2210.11416, https://huggingface.co/google/flan-t5-xxl",Google,3.3e+22,"Table 2: 0.2% greater than T5 xxl, which used 3.3e22 FLOP",United States of America,"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",Highly cited,,Scaling Instruction-Finetuned Language Models,,"Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0,
Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as
described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark
of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ",100000000000,"""For T5 models without instruction finetuning, we use LM-adapted models, which were produced by training T5 on 100B additional tokens from C4 on a standard language modeling objective""",Likely,,,1615.00,11000000000.00,11B,,,,,,Google TPU v4,,,,,"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",2024-05-05 19:58,Epoch AI,,0,T5-11B,76000000000000000000,"7.6e19, per Table 2",,,,,,,Industry,,,,,Industry,,,
Fairseq-dense 13B,Language,,2021-12-20,"data not licensed/released: https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/data_card.md

repo is MIT-licensed
https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/README.md",Open source,Unreleased,Unreleased,https://arxiv.org/abs/2112.10684,Meta AI,3.267e+22,Table 1,United States of America,"Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",,,Efficient Large Scale Language Modeling with Mixtures of Experts,,"""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens corresponding to 453GB:
BookCorpus (Zhu et al., 2019) consists of more
than 10K unpublished books (4GB);
• English Wikipedia, excluding lists, tables and
headers (12GB);
• CC-News (Nagel, 2016) contains 63 millions English news articles crawled between September
2016 and February 2019 (76GB);
• OpenWebText (Gokaslan and Cohen, 2019), an
open source recreation of the WebText dataset
used to train GPT-2 (38GB);
• CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas (31GB);
• English CC100 (Wenzek et al., 2020), a dataset
extracted from CommonCrawl snapshots between January 2018 and December 2018, filtered
to match the style of Wikipedia (292GB)""",112000000000,"112B tokens, or 84B words at 0.75 English words/token. 
""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the English subset of CC100, totalling 112B tokens""
...
""All models are trained for 300B tokens with a sequence length of 2048 tokens.""",Likely,,,125.00,13000000000.00,,2.68,,,,,NVIDIA A100,,,,,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using ∼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",2024-05-21 03:30,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Japanese StableLM Base Alpha 7B,Language,,2023-08-10,"Apache 2.0 for weights

data appears to be open with unclear licenses: https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b",Open source,,Open access (non-commercial),https://stability.ai/news/stability-ai-new-jplm-japanese-language-model-stablelm,Stability AI,3.15e+22,"7b params, 750b tokens
7b * 750b * 6 = 3.15e22",Multinational,"Meng Lee, Fujiki Nakamura, Makoto Shing, Paul McCann, Takuya Akiba, Naoki Orii",,"best open-source Japanese LM: ""It stands as the top-performing publicly available Japanese language model, according to a benchmark suite against four sets of other Japanese LMs.""","Japanese StableLM, Marking Entry into International Language Model  Market",,Japanese language sources + RedPajama,,,Likely,,,,7000000000.00,7B,,,,,,,,,,,"Japanese StableLM is a 7 billion-parameter general-purpose language model. It stands as the top-performing publicly available Japanese language model, according to a benchmark suite against four sets of other Japanese LMs.

Japanese StableLM Base Alpha 7B will be released under the commercially available Apache License 2.0. Japanese StableLM Instruct Alpha 7B is a model created for research purposes and is released exclusively for research use. For details, please refer to the Hugging Face Hub page.",2024-04-09 11:40,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Jais,Language,Language modelling,2023-08-29,apache 2.0,Open source,,,https://inceptioniai.org/jais/docs/Technicalpaper.pdf,"Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence,Inception",3.08e+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,"Multinational,United Arab Emirates,United States of America","Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing",SOTA improvement,SOTA at Arabic language tasks.,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,"Abu El-Khair,Aranews,ArabicText 2022,C4 Arabic,Arabic Wikipedia,ArabicNews 2020,Maktabah,United Nations Parallel Corpus,The Pile,Books3,arXiv,PubMed Central,WebText2,English Wikipedia,FreeLaw,PubMed Abstracts,DeepMind Mathematics,Project Gutenberg,BookCorpus2,EuroParl,PhilPapers,YouTube Subtitles,NIH Grant Abstracts,Enron Emails,GitHub","It was pretrained on 395 billion tokens, including 116 billion Arabic tokens, 232 billion English tokens, and 46 billion tokens of code.
The Arabic data consists of 72 billion tokens, which was augmented by 18 billion tokens of translated English text and then upsampled 1.6 times to reach 116 billion tokens.
The English data is sampled from the Pile dataset and consists of 232 billion tokens.
The code data consists of 46 billion tokens sampled from GitHub.",300000000000,395B tokens ~= 300B words,Confident,3932160,"""After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. """,11.00,13000000000.00,"""With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic""",,26000000000.00,26 billion FLOP per token,600.0,2023 June 25 to July 18 = 25 days = 600 hours,,,,,Industry,"We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture
and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model —the foundation Jais model, and an instruction-tuned Jais-chat variant— with the aim of promoting research on Arabic LLMs.",2024-03-28 11:54,Anonymous,,,,,,,,Jais,,,,"Industry,Academia",checked,,,,"Industry,Academia",,,
ESM2-3B,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-21,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd",Open source,Open source,Open source,https://www.science.org/doi/abs/10.1126/science.ade2574,"Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",3.0000000009999997e+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 1.8e22 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
30 days x 512 V100s x an imputed 30% utilization"": 5e22 FLOP

Geometric mean: 3e22","United States of America,United States of America,United States of America,United States of America","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",Evolutionary-scale prediction of atomic-level protein structure with a language model,UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",,,Likely,,,636.00,3000000000.00,"In the name
",,,,720.0,,,Unsupervised,,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",2024-03-28 17:48,Epoch AI,,1,,,,,,,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
Fugaku-LLM,Language,"Language modelling,Translation,Japanese language modeling",2024-05-10,,Open source,,,https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html,"Tohoku University,CyberAgent,Tokyo Institute of Technology,Fujitsu,RIKEN,Nagoya University,Kotoba Technologies",2.9640000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+13+billion+*+380+billion,"Japan,Japan",,,,Release of “Fugaku-LLM” – a large language model trained on the supercomputer “Fugaku”,,"""Fugaku-LLM was trained on proprietary Japanese data collected by CyberAgent, along with English data, and other data.""",380000000000,"""Fugaku-LLM was trained on 380 billion tokens using 13,824 nodes of Fugaku, with about 60% of the training data being Japanese, combined with English, mathematics, and code.""",Confident,,,,13000000000.00,"""Fugaku-LLM has 13 billion parameters (2)""",,,,,,,,,,,"A team of researchers in Japan released Fugaku-LLM, a large language model (1) with enhanced Japanese language capability, using the RIKEN supercomputer Fugaku. The team is led by Professor Rio Yokota of Tokyo Institute of Technology, Associate Professor Keisuke Sakaguchi of Tohoku University, Koichi Shirahata of Fujitsu Limited, Team Leader Mohamed Wahib of RIKEN, Associate Professor Koji Nishiguchi of Nagoya University, Shota Sasaki of CyberAgent, Inc, and Noriyuki Kojima of Kotoba Technologies Inc.

To train large language models on Fugaku, the researchers developed distributed training methods, including porting the deep learning framework Megatron-DeepSpeed to Fugaku in order to optimize the performance of Transformers on Fugaku. They accelerated the dense matrix multiplication library for Transformers, and optimized communication performance for Fugaku by combining three types of parallelization techniques and accelerated the collective communication library on the Tofu interconnect D.

Fugaku-LLM has 13 billion parameters (2) and is larger than the 7-billion-parameter models that have been developed widely in Japan. Fugaku-LLM has enhanced Japanese capabilities, with an average score of 5.5 on the Japanese MT-Bench (3), the highest performance among open models that are trained using original data produced in Japan. In particular, the benchmark performance for humanities and social sciences tasks reached a remarkably high score of 9.18.

Fugaku-LLM was trained on proprietary Japanese data collected by CyberAgent, along with English data, and other data. The source code of Fugaku-LLM is available on GitHub (4) and the model is available on Hugging Face (5). Fugaku-LLM can be used for research and commercial purposes as long as users comply with the license.

In the future, as more researchers and engineers participate in improving the models and their applications, the efficiency of training will be improved, leading to next-generation innovative research and business applications, such as the linkage of scientific simulation and generative AI, and social simulation of virtual communities with thousands of AIs.",2024-05-13 09:04,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
RWKV-4 14B,Language,Language modelling,2023-05-22,,,,,https://arxiv.org/abs/2305.13048,RWKV Foundation,2.78e+22,"from HuggingFace page: https://huggingface.co/BlinkDL/rwkv-4-pile-14b

trained for 330B tokens
14 billion * 330 billion * 6 = 2.78e22

paper notes that a forward pass is almost exactly 2x parameters (within 2%): ""Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant."" and that 6*params*tokens is a good approximation because it's not a transformer: ""FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D2L), which is the
twice (add and multiply) the number of parameters
in linear layers. The backwards pass FLOPs can be
approximated as twice that of the forward pass. So
the total is 6(V D + 13D2L) per token for training
(3x fw FLOPs). It is noteworthy that FLOPs are
independent of the context length, unlike regular
transformers""",,"Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu",,,RWKV: Reinventing RNNs for the Transformer Era,The Pile,,330000000000,,Confident,262144,"262144 (or 131072?)
""To train the models mentioned, we... switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens""",132.00,14000000000.00,14b,,27780000000.00,Table 2,,,NVIDIA A100 SXM4 80 GB,,,,,"Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.",2024-05-21 04:03,Anonymous,,0,,,,,,,,,,Research collective,,,,,Research collective,,,
LLaMA-7B (protein-oriented instructions finetuned),"Language,Biology",,2023-10-02,"MIT. includes data
https://github.com/zjunlp/Mol-Instructions",Open source,Open source,Unreleased,https://arxiv.org/abs/2306.08018,Zhejiang University,2.78e+22,"Estimate 1: 1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP

from paper, Llama-7B took 82,432 GPU hours using A100s
Estimate 2: 312 trillion FLOP/s * (82,432 * 3600) s * 0.3 = 2.78e22 FLOP",China,"Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen",,,Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,Mol instructions,"""we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions""",,,Confident,,,23.00,7000000000.00,In the name,,,,,,,,,,,"Large Language Models (LLMs), with their remarkable task-handling capabilities
and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as
biomolecular studies remains limited. To address this challenge, we introduce MolInstructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models’ performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",2024-05-06 03:33,Anonymous,,,LLaMA-7B,,,,,,,,,Academia,,,,,Academia,,,
VARCO LLM KO/EN-13B-IST ver.1,Language,"Language modelling/generation,Chat",2023-08-16,,API access,,,"https://ncsoft.github.io/ncresearch/varco-llm-details/
https://aws.amazon.com/marketplace/pp/prodview-usyosolf3an3u?sr=0-1&ref_=beagle&applicationId=AWSMPContessa",NCSOFT,2.7299999999999995e+22,=350000000000*6*13000000000=2.73 × 10^22,Korea (Republic of),,,,,,"""Our LLM is trained with datasets that are either publicly available for pretraining, collected from the Internet or internally constructed,” Jehee Lee, CRO of NCSOFT, told Engadget via email.",350000000000,https://ncsoft.github.io/ncresearch/varco-llm-details/,Likely,,,,13000000000.00,,,,,,,,,,,,"VARCO-LLM is NCSOFT’s large language model, which can be applied to develop various NLP-based AI services such as Q&A, chatbot, summarization, information extraction etc. VARCO-LLM, trained with public pre-training data and internally constructed high-quality Korean data, boasts the highest performance among the Korean LLMs of similar sizes that have been released to date (see https://ncsoft.github.io/ncresearch/  for evaluation results). Our models will continue to be updated and we will also release LLMs that support multiple languages or are fined-tuned to specific tasks. As VARCO-LLM is currently in beta service (29 Aug to 10 Sep 2023), usage fees will not be charged temporally for this period. For inquiries regarding further performance improvement or collaboration for service applications, please contact us via email (varco_llm@ncsoft.com).

The VARCO LLM KO/EN-13B-IST is a bilingual instruction-tuned model that is trained with the Korean/English pre-training data and the instruction dataset, both constructed by NCSOFT.",2024-05-15 12:40,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,,,
CogAgent,"Vision,Language","Instruction interpretation,Visual question answering",2023-12-14,Code is Apache License 2.0; model is under a more restrictive custom licence which still allows commercial usage but which limits uses undermining Chinese national security and unity.,Open access (restricted use),,Open source,https://arxiv.org/abs/2312.08914,"Tsinghua University,Zhipu AI",2.69e+22,"States 12.6 TFLOP per 1120x1120 image forward pass. Trained 60k steps with 4608 batch size, and then 10k with 1024 batch size.
12.6 TFLOP * (60000*4608 + 10000*1024) = 3.76e21

Uses pretrained CogVLM as base (2.311e+22 FLOP), along with EVA2-CLIP-L. EVA2-CLIP-L's FLOPs are potentially estimable, but based on details about EVA2-CLIP-g/14 (a larger model), they likely contribute negligibly to CogAgent.

Sum: 2.69e22","China,China","Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang",SOTA improvement,See Table 1,CogAgent: A Visual Language Model for GUI Agents,,"From section 2.3, pretraining data includes:
- Synthetic renderings with text from language pre-training dataset (80M image-text pairs)
- OCR from natural images from COYO [6] and LAION-2B (18M image-text pairs)
- Academic documents (9M image-text pairs)
- Visual grounding dataset with bounding boxes (see CogVLM) (40M image-text pairs)
- Common Crawl Screenshot 400K (400k images with  140M QA pairs)",287000000,,Unverified,,,50.00,18000000000.00,,,12600000000000.00,12.6 TFLOP for a 1120x1120 image,,,,Supervised,,,,"People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at this https URL .",2024-05-21 06:34,Luke Frymire,,0,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
CogView,Image generation,Text-to-image,2021-05-26,"Apache 2 license

https://github.com/THUDM/CogView",Open source,,Open source,https://arxiv.org/abs/2105.13290,"Tsinghua University,Alibaba DAMO Academy",2.68e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","China,China","M Ding, Z Yang, W Hong, W Zheng, C Zhou",SOTA improvement,"""CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E""",CogView: Mastering Text-to-Image Generation via Transformers,WuDao Corpora,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""",50000000000,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""

250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokens

So 30M text-image pairs and 50 billion words",Likely,,,502.00,4000000000.00,"""We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.""",,,,,,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,$44452.39,,Industry,"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",2024-05-01 09:05,Robi Rahman,,,,,,512,,CogView,,,,"Academia,Industry",checked,,,,"Academia,Industry",,,
ALIGN,"Multimodal,Vision,Language","Representation learning,Image classification",2021-06-11,,Unreleased,,Unreleased,https://arxiv.org/abs/2102.05918,Google Research,2.5986700000009994e+22,"From author communication
14.82K TPUv3 core-days
Precision: bfloat16

Estimation
TPUv3 at float16: 123 TFLOPS/chip

123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOP
https://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33",Multinational,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig","Highly cited,SOTA improvement","""The aligned visual and language representations... set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks""",Scaling up visual and vision-language representation learning with noisy text supervision,Conceptual Captions (CC3M),,1600000000,"Dataset contains 1.8B image-text pairs, then some duplicates are removed.",Likely,,,2205.00,820000000.00,"From author communication

 480M (image tower) + 340 M (text tower)",,,,347.3,14820 TPU core-hours / 1024 TPU cores = 347.3 hours,Google TPU v3,Self-supervised learning,$357760.33,,Industry,"Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",2024-05-13 09:41,Robi Rahman,,,,,,512,,ALIGN,,,,Industry,,,,177818,Industry,,,
OPT-13B,Language,Language modelling,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,2.340000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+13+billion+*+300+billion,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Likely,,,,13000000000.00,,1.67,,,,,,,,,,,2024-04-30 15:03,Robi Rahman,OPT-13B,1,,,,,,,,,,Industry,,,,,Industry,,,
CogVLM,"Multimodal,Vision,Language","Image captioning,Visual question answering,Chat",2023-11-06,"code is Apache, model more restrictive, commercial allowed, subject to PRC laws and interests",Open access (restricted use),,Open source,"https://arxiv.org/abs/2311.03079
https://huggingface.co/THUDM/cogvlm-chat-hf
https://github.com/THUDM/CogVLM
","Tsinghua University,Zhipu AI,Beihang University",2.311e+22,"from table 8 on page 17

230.1 FLOPS*days 
so 
10**15*24*3600*230.1= 1.988e22

Since this training uses pretrained weights from EVA02-CLIP-E and Vicuna1.5-7B, we report the full number of FLOPs baked into the model.

EVA02-CLIP-g/14 is stated to have taken 25 days to train 12B samples using 64 A100-40GB GPUs, implying: 
25 days * 24 hr/day * 3600 sec/hr * 64 GPU * 7.80E+13 FLOP/GPU-sec * 30% efficiency = 3.23e21

EVA02-CLIP-E doesn't give a training time; it saw 1/4 as many samples as the g/14 model but has 4.27x more parameters; as a rough estimate, assume it took the same number of FLOPs to train.

Vicuna1.5-7B is stated to have cost $140, so training compute is likely negligible.","China,China,China","Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang",SOTA improvement,"""CogVLM-17B
achieves state-of-the-art performance on 17 clas-
sic cross-modal benchmarks, including 1) im-
age captioning datasets: NoCaps, Flicker30k, 2)
VQA datasets: OKVQA, TextVQA, OCRVQA,
ScienceQA, 3) LVLM benchmarks: MM-
Vet, MMBench, SEED-Bench, LLaVABench,
POPE, MMMU, MathVista, 4) visual grounding
datasets: RefCOCO, RefCOCO+, RefCOCOg,
Visual7W. Codes and checkpoints are available at
https://github.com/THUDM/CogVLM""",CogVLM: Visual Expert for Pretrained Language Models,"VQAv2,LAION-2B,COYO-700M,OKVQA,TextVQA,OCR-VQA,ScienceQA,LLaVA-Instruct-150k,LRV-Instruction,LLaVAR,Flickr30K Entities,RefCOCO,Visual7W,VisualGenome,COCO,TextCaps","Pretraining uses LAION-2B, COYO-700M, plus a newly created visual grounding dataset of 40M images.
Generalist models CogVLM-Chat and CogVLM-Grounding are additionally finetuned on VQAv2, OKVQA, TextVQA, OCRVQA, ScienceQA, LLaVA-Instruct, LRV-Instruction, LLaVAR, Flickr30K Entities, RefCOCO, Visual7W, and VisualGenome.
Additional tests finetune on the training sets from COCO and TextCaps.",1518534581,"After filtering, about 1.5B image-text pairs are left for pretraining in stage one. Stage two of pretraining adds a visual grounding dataset of 40M images with generated noun bounding boxes. These are filtered from LAION-115M so that 75% of images contain at least two bounding boxes.

Two different kinds of finetuning are done, each using a number of datasets:
- CogVLM-Chat: VQAv2 (11059040), OKVQA (70275), TextVQA (453360), OCRVQA (1002146), ScienceQA (21208), LLaVAInstruct (150000), LRV-Instruction (300000), LLaVAR (1633000)
- CogVLM-Grounding: Flickr30K Entities (520000), RefCOCO (142209), Visual7W (889388), VisualGenome (1700000)

Additional experiments finetune using the training sets from COCO (413915 in train) and TextCaps (109765 in train)

In sum, pretraining and finetuning appear to contain 1,500,000,000 and 18,534,581 datapoints, respectively.",Confident,,"8192 in pretraining stage 1, 1024 in stage 2",95.00,17000000000.00,"CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. However, ""the total number of trainable parameters is 6.5B"".

""CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module.""

ViT: EVA2-CLIP-E, last layer removed (5B params with last layer, non-trainable)
MLP adapter: 2 layers, parameter count unavailable
GPT: Vicuna1.5-7B (7B params)
Visual expert module: parameter count unclear",,,,,,,Supervised,,,,"We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at this https URL. ",2024-05-01 09:22,Bartosz Podkanowicz,,,Vicuna-7B,2,Trained from Vicuna1.5-7B weights,,,CogVLM,,,,"Academia,Industry,Academia",checked,,,,"Academia,Industry,Academia",,,
Cerebras-GPT-13B,Language,Language modelling,2023-04-06,,,,,https://arxiv.org/abs/2304.03208,Cerebras Systems,2.3e+22,"2.3e22, per table 2",Multinational,"Nolan Dey, Gurpreet Gosal, Zhiming (Charles)Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness",,,Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,The Pile,,278000000000,"371B tokens, or 278B words",Confident,2210000,"""For the 13B parameter model, we train with a batch size of 720 sequences of length 2048 tokens for the first 84B tokens. At that point, we observed the gap between validation and train loss growing, indicating that the gradient noise was growing, so we increased the batch size to 1080 sequences for the rest of training.""

batch sizes ramp from 1.47M to 2.21M",51.00,13000000000.00,13 billion,0.69,,,,,Cerebras CS-2,,,,,"We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization (μP) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: this https URL.",2024-04-01 09:52,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Phi-2,Language,Language generation,2023-12-12,MIT,Open source,,,https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/,Microsoft,2.27e+22,"2.7B params, trained on 1.4T tokens

2.7 billion * 1.4 trillion * 6 = 2.27e22

96*14 A100-days

14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22",United States of America,"Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, Yi Zhang",,"""With only 2.7 billion parameters, Phi-2 surpasses the performance of Mistral and Llama-2 models at 7B and 13B parameters on various aggregated benchmarks. Notably, it achieves better performance compared to 25x larger Llama-2-70B model on muti-step reasoning tasks, i.e., coding and math. Furthermore, Phi-2 matches or outperforms the recently-announced Google Gemini Nano 2, despite being smaller in size.""",Phi-2: The surprising power of small language models,,"""Our training data mixture contains synthetic datasets specifically created to teach the model common sense reasoning and general knowledge, including science, daily activities, and theory of mind, among others. We further augment our training corpus with carefully selected web data that is filtered based on educational value and content quality. Secondly, we use innovative techniques to scale up, starting from our 1.3 billion parameter model, Phi-1.5, and embedding its knowledge within the 2.7 billion parameter Phi-2. This scaled knowledge transfer not only accelerates training convergence but shows clear boost in Phi-2 benchmark scores.""",,,Likely,,,,2700000000.00,2.7B,,,,336.0,14 days,NVIDIA A100,,,,,"Over the past few months, our Machine Learning Foundations team at Microsoft Research has released a suite of small language models (SLMs) called “Phi” that achieve remarkable performance on a variety of benchmarks. Our first model, the 1.3 billion parameter Phi-1(opens in new tab), achieved state-of-the-art performance on Python coding among existing SLMs (specifically on the HumanEval and MBPP benchmarks). We then extended our focus to common sense reasoning and language understanding and created a new 1.3 billion parameter model named Phi-1.5(opens in new tab), with performance comparable to models 5x larger.

We are now releasing Phi-2(opens in new tab), a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.",2024-03-27 13:56,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
ERNIE 3.0,Language,,2021-07-05,apache 2.0 https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0,Open source,,Open source,http://research.baidu.com/Blog/index-view?id=160,Baidu,2.25e+22,"Section 3.3.3: 
""""The model is trained for
a total of 375 billion tokens""

Total compute approximated as 6*N*D",China,"Y Sun, S Wang, S Feng, S Ding, C Pang",SOTA improvement,"""ERNIE 3.0 achieved new state-of-the-art results across 54 Chinese NLP tasks""",ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,,,668000000000,"""To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.""

1 GB ~ 167M chinese words",,,,267.00,10000000000.00,"""We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.""",,,,,,NVIDIA V100,Self-supervised learning,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,384,,ERNIE 3.0,,,,Industry,,,,,Industry,,,
XGLM,Language,"Language modelling/generation,Language generation,Language modelling,Translation,Question answering",2021-12-20,"MIT license
https://github.com/facebookresearch/fairseq/tree/main/examples/xglm

https://github.com/facebookresearch/fairseq/blob/main/examples/xglm/model_card.md#primary-intended-use",Open access (non-commercial),Open source,Unreleased,https://arxiv.org/abs/2112.10668,"Meta AI,Facebook AI Research",2.25e+22,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""

256 * 312 teraFLOP/s * 21 * 24 * 3600 * 0.3 utilization assumption ~= 4.3e22

also, it was trained for 500B tokens. Using Compute = 6ND, we have
6 * 500B * 7.5B = 2.25e22

311k tokens per second * 7.5B params * 6 is 1.35e16 FLOP/s. divide that by 312 teraFLOP/s, which is A100 peak compute, gets 43, suggesting low utilization (17%) of the 256-GPU cluster, or somewhat higher if there's more than one token per word. So I'll use the 6ND number.","United States of America,United States of America","Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",SOTA improvement,"""Our largest model (XGLM7.5B) sets a new state of the art performance for few-shot learning in more than 20 representative languages (including medium- and low-resource languages) for the tasks of commonsense reasoning, natural language inference and machine translation.""",Few-shot Learning with Multilingual Language Models,"Subset of CC100-XL,CC100-XL,Common Crawl","*they built a closed dataset based on open Common Crawl
""We extend the pipeline used for mining the CC100 corpus (Conneau et al., 2020; Wenzek et al., 2020) to generate CC100-XL, a significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from Summer 2013 to March/April 2020) and 134 languages.",1740000000,,Likely,,,161.00,7500000000.00,7.5B,,,,,,,Self-supervised learning,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
MoE-1.1T,Language,,2021-12-20,,,,,https://arxiv.org/abs/2112.10684,Meta AI,2.227e+22,Reported directly in paper. Authors calculate FLOPs analytically in appendix G,United States of America,"Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",,,Efficient Large Scale Language Modeling with Mixtures of Experts,,"""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens corresponding to 453GB:
BookCorpus (Zhu et al., 2019) consists of more
than 10K unpublished books (4GB);
• English Wikipedia, excluding lists, tables and
headers (12GB);
• CC-News (Nagel, 2016) contains 63 millions English news articles crawled between September
2016 and February 2019 (76GB);
• OpenWebText (Gokaslan and Cohen, 2019), an
open source recreation of the WebText dataset
used to train GPT-2 (38GB);
• CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas (31GB);
• English CC100 (Wenzek et al., 2020), a dataset
extracted from CommonCrawl snapshots between January 2018 and December 2018, filtered
to match the style of Wikipedia (292GB)""",84000000000,"112B tokens, or 84B words at 0.75 English words/token. 
""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the
English subset of CC100, totalling 112B tokens""",Confident,,,125.00,1100000000000.00,,2.68,,,,,NVIDIA A100,,,,,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using ∼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",2024-04-01 09:02,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
TinyLlama-1.1B (3T token checkpoint),Language,Chat,2023-10-01,apache 2.0: https://github.com/jzhang38/TinyLlama,Open source,,Open source,https://arxiv.org/abs/2401.02385,Singapore University of Technology & Design,2.173796352e+22,"6ND approximation: 6*1.1B * 3T = 19800000000000000000000
Extrapolation from the 1T checkpoint:
flops = (16) * (312 * 10**12) * (3 * 30 * 24 * 3600) * (0.56) = 7245987840000001048576
(num gpu) * (peak flops) * (time in seconds) * (reported utilization rate)

source: https://github.com/jzhang38/TinyLlama
""Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization""
and Releases Schedule from the same link",Singapore,"Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu",,,TinyLlama: An Open-Source Small Language Model,"SlimPajama,StarCoderData",Slimpajama & Starcoderdata from Training Details from https://github.com/jzhang38/TinyLlama,750000000000,1T tokens checkpoint so around 0.75T words,Confident,,,39.00,1100000000.00,1.1B,,2200000000.00,1.1B*2,2160.0,"1T checkpoint was released after 1 month. Assume the 3T checkpoint took 3 months.

source: https://github.com/jzhang38/TinyLlama",NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,,"We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama",2024-04-02 10:14,Robi Rahman,,0,,,,16,0.5600,,,,,Academia,,,,34560,Academia,,,
Pythia-12b,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",2.1590000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+12+billion+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,2097152,"""The most notable divergence from standard training procedures is that we use a much larger batch size than what is standard for training small language models... we use a batch size of 1024 samples with a sequence length of 2048 (2,097,152 tokens) for all models""",429.00,12000000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:28,Robi Rahman,Pythia-12b,0,,,,256,0.2659,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,72300,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
Stable Code 3B,Language,"Language modelling/generation,Code generation",2024-01-09,"non-commercial by default. looks like they charge for commercial licenses? 
https://stability.ai/news/introducing-stability-ai-membership",Open access (non-commercial),,,https://huggingface.co/stabilityai/stable-code-3b,Stability AI,2.106e+22,"6ND = 2.7e9 * 1.3e12 * 6 = 2,106E+22
""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. """,Multinational,"Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and  and Cooper, Nathan",,,Stable Code 3B,"Falcon RefinedWeb,GitHub,StarCoder","""Training Dataset

The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), along with CommitPackFT and Github Issues (BigCode., 2023), and StarCoder (Li et al., 2023). We further supplement our training with data from mathematical domains (Azerbayev, Zhangir, et al., 2023 and, Yu, Longhui, et al., 2023). """,,"1.3T tokens
""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. """,Likely,,,,2796431360.00,"2796431360 from https://huggingface.co/stabilityai/stable-code-3b#model-architecture
""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. """,,5592862720.00,2N,,,NVIDIA A100 SXM4 40 GB,,,,Industry,,2024-03-27 13:51,Bartosz Podkanowicz,,,,,,256,,,,,,Industry,,,,,Industry,,,
GraphCast,Earth science,Weather prediction,2023-11-14,,,,,https://www.science.org/doi/epdf/10.1126/science.adi2336,Google DeepMind,2.1000000000000002e+22,"""Training GraphCast took roughly four weeks on 32 Cloud TPU v4 devices using batch parallelism.""

4.6: ""we use bfloat16 floating point precision""

2.1e22 = 2.75E+14 FLOP/s * 32 * 60* 60 * 24 * 7 * 4",Multinational,"Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, Peter Battaglia",SOTA improvement,"""Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute""",Learning skillful medium-range globalweather forecasting,,"According to the blog post, ""we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather.""
https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/",,,Speculative,,,,,Not mentioned in paper.,,,,,,,,,,,,2024-04-23 21:48,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Evo,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2024-02-27,Apache License 2.0,Open source,,,"https://arcinstitute.org/news/blog/evo
https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1","Stanford University,UC Berkeley",2.0000000001e+22,"""In total, Evo was trained on approximately 340B tokens, using approximately 2e22 FLOPS""","United States of America,United States of America","Eric Nguyen, Michael Poli, Matthew G. Durrant, Armin W. Thomas, Brian Kang, Jeremy Sullivan, Madelena Y. Ng, Ashley Lewis, Aman Patel, Aaron Lou, Stefano Ermon, Stephen A. Baccus, Tina Hernandez-Boussard, Christopher Ré, Patrick D. Hsu, Brian L. Hie",,Competitive with SOTA protein language models,Sequence modeling and design from molecular to genome scale with Evo,,"""We're also open-sourcing a large 300B token training dataset we compiled, which we call OpenGenome, consisting of 2.7M publicly available prokaryotic and phage genomes""",300000000000,"300 billion nucleotides. Since these are tokenized at the single-nucleotide level, 300B tokens.",Likely,,,3.00,7000000000.00,"Based on a StripedHyena architecture, context length of 131 kilobases",1.00,,,2968.0,"Trained for first stage on 64 H100s, then for second stage on 128 A100s. Trained primarily with BF16 precision; FP32 used for ""long convolutional parameters"". I use BF16 performance for simplicity.

This gives a plausible range for completing 2e22 FLOPs.

If all 2e22 FLOPs were performed on H100s:
2e22 FLOP / (64 GPUs * 1.3e14 FLOP/GPU-sec) * 1/0.3 efficiency *  1/3600 hour/sec  = 2226 hours

If all 2e22 FLOPs were performed on A100s:
2e22 FLOP / (128 GPUs * 3.9e13 FLOP/GPU-sec) * 1/0.3 efficiency * 1/3600 hour/sec = 3710 hours

Assume each stage is roughly half the total time, so:
(2226+3710)/2 = 2968 hours","NVIDIA A100,NVIDIA H100 SXM5",Self-supervised learning,$341168.00,"""We trained Evo in stage 1 on 64 Nvidia H100 GPUs and on 128 Nvidia A100 GPUs in stage 2. In total, Evo was trained on approximately 340B tokens, using approximately 2e22 FLOPS""

This gives a plausible range for completing 2e22 FLOPs.

If all 2e22 FLOPs were performed on H100s:
2e22 FLOP / (1.3e14 FLOP/GPU-sec) * 1/0.3 efficiency *  1/3600 hour/sec * $1.69/GPU-hour = $240,741

If all 2e22 FLOPs were performed on A100s:
2e22 FLOP / (3.9e13 FLOP/GPU-sec) * 1/0.3 efficiency * 1/3600 hour/sec * $0.93/GPU-hour = $441,595

Assume each stage is roughly half the total compute, so:
(240741+441595)/2 = $341,168",Industry,"The genome is a sequence that completely encodes the DNA, RNA, and proteins that orchestrate the function of a whole organism. Advances in machine learning combined with massive datasets of whole genomes could enable a biological foundation model that accelerates the mechanistic understanding and generative design of complex molecular interactions. We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at single-nucleotide, byte resolution. Trained on whole prokaryotic genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multielement generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multi-scale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.",2024-05-06 14:37,Anonymous,,0,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
StableLM-2-1.6B,Language,Language modelling/generation,2024-01-18,"non-commercial:

https://huggingface.co/stabilityai/stablelm-2-1_6b/blob/main/LICENSE",Open access (non-commercial),,,https://huggingface.co/stabilityai/stablelm-2-1_6b,Stability AI,1.92e+22,"6 * 1.6B * 2T = 19200000000000000000000
",Multinational,Stability AI Language Team,,,Stable LM 2 1.6B,"Falcon RefinedWeb,RedPajama-Data,The Pile,StarCoder","""The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without the Books3 subset, and StarCoder (Li et al., 2023). We further supplement our training with multi-lingual data from CulturaX (Nguyen et al., 2023) and, in particular, from its OSCAR corpora, as well as restructured data in the style of Yuan & Liu (2022).""",2000000000000,"""model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.""
assuming 1 word per token ",Likely,,,,1600000000.00,Stable LM 2 1.6B,,3200000000.00,2N,,,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,,,2024-03-27 13:40,Anonymous,,,,,,512,,,,,,Industry,,,,,Industry,,,
LiMoE,"Multimodal,Vision,Language",Image classification,2022-06-06,,,,,https://arxiv.org/abs/2206.02770,Google,1.8e+22,"Section 3.2: ""The model contains 5.6B parameters in total, but only applies 675M parameters per token""

From Section A.3, ""batch size 21502 with resolution 288 and text sequence length16"". ""The model was trained for 700k steps pre-cooldown. There was one cooldown of length 125k steps
from the final step, and 3 of length 40k steps starting from step 650k"". Patch size 14 for images.

Assume C = 6*N*D. 
C = 6*675e6*21.5e3*1e6*(16+(288/14)**2)/2 = 1.8e22

This is broadly consistent with ViT-H/14's compute",United States of America,"Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby",,"
",Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts,,,7600000000,"Section 3: ""Training data. By default, all models are trained on paired image-text data used in [16], consisting of 3.6B images and alt-texts scraped from the web. For large LIMoE-H/14 experiment, we also co-train with JFT-4B [17]. """,,,,95.00,5600000000.00,"Section 1: ""We scale this up to a large 5.6B parameter LIMoE-H/14""",,,,,,,,,,Industry,"Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6% zero-shot ImageNet accuracy (vs. 76.2%), and when further scaled to H/14 (with additional data) it achieves 84.1%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.",2024-05-13 09:14,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ResNet-RS,Vision,Image classification,2021-03-13,"apache 2.0: 
https://github.com/tensorflow/tpu/tree/master/models/official/resnet/resnet_rs/configs",Open source,Unreleased,Unreleased,https://arxiv.org/abs/2103.07579,"Google Brain,UC Berkeley",1.763328e+22,"(350) * (128000000000) * (1312 * 10**5) * 3 = 17633280000000000000000
(epochs) * (inference FLOP) * (dataset size) * (constant to account for backpropagation)
from 4.2 ""Our training method closely matches that of EfficientNet, where we train for 350 epochs, but with a few small differences""350 epochs from description of Table 8 in appendix C","United States of America,United States of America","Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph",,,Revisiting ResNets: Improved Training and Scaling Strategies,,"""In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudo-labeled images.""
""We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Studen""
 ""We use the same dataset of 130M images pseudo-labeled as Noisy Student, """,131200000,"1.2M + 130M = 131.2M 
""In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudo-labeled images.""""We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Studen""
 ""We use the same dataset of 130M images pseudo-labeled as Noisy Student""",Likely,,,274.00,192000000.00,Table 7 appendix B,350.00,128000000000.00,Table 7 in appendix B last row (page 14),,,Google TPU v3,,,,,"Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan & Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research. ",2024-04-04 12:07,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
NLLB,Language,Translation,2022-07-06,"MIT
",Open source,Open source,Open source,https://research.facebook.com/publications/no-language-left-behind/,Meta AI,1.751113728e+22,"Section 8.8:
"" To train NLLB-200, a cumulative
of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""
See also Table 48

Section 8.2.4 states they use FP16

NVIDIA Datasheet states 312TFLOPS for FP16
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf

Assuming 0.3 utilization:

312e12*3600*51968*0.3

Also:
""Our final model is a Transformer
encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in
every 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128
experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder
layers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in
Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder
input embedding and decoder output embedding layers. We use an overall dropout of 0.3,
attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model.""",United States of America,"Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",SOTA improvement,"""Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art""",No Language Left Behind: Scaling Human-Centered Machine Translation,,,360000000000,"[WORDS]

Section 8.2.2: ""As we prepare to train on the final 202 language dataset comprising of over 18B sentence
pairs and 2440 language directions""

18B sentences * 20 words/sentence",,,,574.00,54500000000.00,"Section 8.2.4: ""The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model""",,,,,,NVIDIA A100 SXM4 80 GB,Self-supervised learning,$39175.64,,Industry,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",2024-05-01 09:22,Robi Rahman,,,,,,,,NLLB,,,,Industry,,,,59168,Industry,,,
Stockmark-13B,Language,Language modelling/generation,2023-10-23,MIT license,Open source,Unreleased,Unreleased,https://huggingface.co/stockmark/stockmark-13b,Stockmark,1.7160000000000002e+22,"6ND = 6*13B*220B = 17160000000000000000000
""Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens. This model is developed by Stockmark Inc.""
",Japan,,,, stockmark/stockmark-13b,"CC100,mC4,Wikipedia (ja),Common Crawl (ja)", Training dataset section from https://huggingface.co/stockmark/stockmark-13b,220000000000,220B tokens so -  assuming 1 word per token - 220B words,Likely,,,,13200000000.00,13.2B from https://huggingface.co/stockmark/stockmark-13b,,,,,,,Self-supervised learning,,,Industry,,2024-04-02 10:12,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
RETRO-7B,Language,,2022-02-07,,Unreleased,,Unreleased,https://arxiv.org/abs/2112.04426,DeepMind,1.68e+22,C=6ND = 6 * 7e9 * 400e9 = 1.7e22 ,United Kingdom of Great Britain and Northern Ireland,"Sebastian Borgeaud†, Arthur Mensch†, Jordan Hoffmann†, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae‡, Erich Elsen‡ and Laurent Sifre",SOTA improvement,"""Our largest model obtains state-of-the-art results on a range of downstream evaluation
datasets including Wikitext103""",Improving language models by retrieving from trillions of tokens,,,315000000000,"""we train for 419,430,400,000 training tokens"" ~= 315B words.",,,,587.00,7500000000.00,"""Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. """,,,,,,,Self-supervised learning,,,Industry,,2024-05-06 14:30,Robi Rahman,RETRO-7B,,,,,,,,,,,Industry,,,,,Industry,,,
MADLAD-400 10B,Language,Translation,2023-09-09,"Apache 2.0:
https://github.com/google-research/google-research/tree/master",Open source,,,https://arxiv.org/abs/2309.04662,"Google DeepMind,Google Research",1.605e+22,"6ND = 10.7B * 250B = 1.6e22
'MADLAD-400-10B-MT is a multilingual machine translation model based on the T5 architecture that was trained on 250 billion tokens covering over 450 languages using publicly available data. '
10.7B  params from appendix A.8","Multinational,Multinational","Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat",,,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,MADLAD-400," 'We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.'",3000000000000,"MADLAD-400, dataset released with paper, is 3T tokens: 'We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.'

However, model in question was trained on only 250B tokens: ""We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data""",Confident,,,19.00,10700000000.00,10.7B  from appendix A.8,,,,,,,,,,,"We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community. ",2024-05-15 16:56,Bartosz Podkanowicz,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
ProteinLM,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2021-08-17,"apache 2.0
https://github.com/THUDM/ProteinLM",Open source,Open source,Open source,https://arxiv.org/abs/2108.07435,"Tsinghua University,Beijing Academy of Artificial Intelligence,Tencent",1.5999999999999998e+22,"""We pretrained two large models on a 480 GPUs (TeslaV100-32GB) cluster for about three weeks""

21 * 24* 3600 * 480 * 125 teraFLOP/s * 0.3 (utilization) * 0.5 (two models) = 1.6e22","China,China,Multinational","Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, Jie Tang
",,,Modeling Protein Using Large-scale Pretrain Language Model,Pfam,"""PFAM[7] is a widely-used database consisting of more than 32 million protein sequences""",,,Likely,,,21.00,3000000000.00,"""We have trained multiple largescale models on the PFAM[7] dataset, the largest with
3 billion parameters""",,,,252.0,,NVIDIA V100,Self-supervised learning,,,Industry,"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token- level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",2024-04-01 09:53,Anonymous,,,,,,48,,,,,,"Academia,Academia,Industry",,,,12096,"Academia,Academia,Industry",,,
Griffin,Language,"Language modelling/generation,Chat",2024-02-29,,,,,https://arxiv.org/abs/2402.19427,Google DeepMind,1.5848931924611135e+22,"Figure 1.a
10^(1/5)*10^22 = 15848931924611134852021.0137339150701326944213382503906831629",Multinational,"Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre",,,Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,,,300000000000,,Likely,,,,14000000000.00,,,,,,,Google TPU v3,,,,,"Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.",2024-04-19 13:34,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
Turing-NLG,Language,Text autocompletion,2020-02-13,,Unreleased,,,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,Microsoft,1.57e+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",United States of America,Corby Rosset,SOTA improvement,"from paper: ""Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks""",Turing-NLG: A 17-billion-parameter language model by Microsoft,,,34800000000,"Authors say they pretrain on the same data as for Megatron-LM. 

From the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf

""The resulting aggregate
corpus contains 174 GB of deduplicated text.""

174GB * 2e8words/GB = 3.48e10 words",Likely,,,114.00,17000000000.00,,3.39,36000000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$58395.62,,Industry,,2024-03-29 10:57,Robi Rahman,Turing-NLG,,,,,256,,Turing-NLG,,,,Industry,,,,,Industry,,,
GPT-J-6B,Language,,2021-05-01,Apache 2.0,Open source,,,https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/,"EleutherAI,LAION",1.5e+22,source: zero shot evaluation table in GitHub,"Multinational,Multinational","Ben Wang, Aran Komatsuzaki",,,GPT-J-6B: 6B JAX-Based Transformer,The Pile,,400000000000,"""The model was trained on 400B tokens from The Pile dataset with 800GB text.""

1 GB ~ 200M words",Confident,,,0.00,6053381344.00,source: model details table in GitHub,1.00,,,,,,Self-supervised learning,$25176.80,,Industry,"Summary:

We have released GPT-J-6B, 6B JAX-based (Mesh) Transformer LM (Github).
GPT-J-6B performs nearly on par with 6.7B GPT-3 (or Curie) on various zero-shot down-streaming tasks.
You can try out this Colab notebook or free web demo.
This library also serves as an example of model parallelism with xmap on JAX.",2024-05-22 13:08,Robi Rahman,GPT-J-6B,,,,,,,,,,,"Research collective,Research collective",,,,,"Research collective,Research collective",,,
BLOOM-7.1B,Language,,2022-07-05,no harmful use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,Open access (restricted use),,Unreleased,https://huggingface.co/bigscience/bloom-7b1,"Hugging Face,BigScience",1.48e+22,,"Multinational,Multinational","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,BigScience ROOTS Corpus,,,,,,,404.00,7070000000.00,,1.00,,,,,,,,,,,2024-04-30 14:42,Robi Rahman,BLOOM-7.1B,,,,,,,,,,,"Industry,Research collective",,,,,"Industry,Research collective",,,
Imagen,Image generation,Text-to-image,2022-05-23,,API access,,,https://imagen.research.google/,Google Brain,1.4600000000000002e+22,"256 TPU v4 chips for 64x64, for 4 days
128 TPU v4 chips for 64->256, for 2 days
128 TPU v4 chips for 256->1024, for 2 days

256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP",United States of America,"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li","Significant use,SOTA improvement,Highly cited",,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,"LAION-400M,other",,860000000,"""We train on a combination of internal datasets, with ≈ 460M
image-text pairs, and the publicly available Laion dataset [61], with ≈ 400M image-text pairs.""",Likely,,,3017.00,3000000000.00,"2B 64x64 generation model, 600M 64->256 super-resolution model, 400M 256->1024 super-resolution model",,,,96.0,4 days,Google TPU v4,Self-supervised learning,,,Industry,,2024-04-01 09:45,Robi Rahman,,,,,,256,,Imagen,,,,Industry,checked,,,24576,Industry,,,
mBART-50,Language,Translation,2020-08-02,"Repo has MIT license:

https://github.com/facebookresearch/fairseq/tree/main/examples/mbart",Open source,,,"https://arxiv.org/abs/2008.00401

https://huggingface.co/facebook/mbart-large-50",Facebook AI,1.4515200000000002e+22,"flops = (256) * (125000000000000) * (2.5 * 7 * 24 * 3600) * (0.3) = 1.45152e+22
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)


""mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs""
V100 have peak flop 28.26 TFLOPS  from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957",United States of America,"Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan",,,"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning
",,"multiple sources
Table 6 in the appendix
additionally XLMR from section 5.1
",,"multiple sources
203686055 sentences - summing column 2 in Table 6 in the appendixadditionally XLMR from section 5.1
There are multiple languages so it is hard to estimate number of words.",Confident,,,325.00,610000000.00,"610M
from https://github.com/facebookresearch/fairseq/tree/main/examples/mbart",,,,420.0,"""mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs""",NVIDIA V100,,,,,"Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch. ",2024-05-21 10:19,Anonymous,,,,,,256,,,,,,Industry,,,,107520,Industry,,,
OmegaPLM,Biology,"Proteins,Protein folding prediction",2022-07-22,,,,,https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1,"Massachusetts Institute of Technology (MIT),Westlake University",1.38018816e+22,"""OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days."" 
""Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.""

Assume 0.4 utilization.

Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.4 * = 1.38e2","United States of America,China","Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, Jian Peng",Historical significance,"""Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures""",High-resolution de novo structure prediction from primary sequence,UniRef50,"""After pretraining on sequences in UniRef50 (dated at 2021/04)""",,,Likely,,,206.00,670000000.00,"""Our model contains 66 layers with around 670 million parameters without sharing parameters, which doubles the layer count of ESM-1b but roughly retains the parameter count.""",,,,,"2,560 GPU Nvidia A100 80G days",NVIDIA A100 SXM4 80 GB,,,,,"Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein’s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.",2024-03-11 16:24,Anonymous,,,,,,,,OmegaPLM,,,,"Academia,Academia",,,,61440,"Academia,Academia",,,
ProGen2-xlarge,Biology,"Proteins,Protein generation,general-purpose protein or nucleotide language model (pLM/nLM)",2022-06-27,"BSD license (permissive)
https://github.com/salesforce/progen?tab=BSD-3-Clause-1-ov-file#readme",Open source,,Open source,https://arxiv.org/abs/2206.13517,"Salesforce Research,Columbia University,Johns Hopkins University",1.35e+22,"Estimate 1:
""350,000 steps x 1m batch size x 6.4 B “connections” x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)
Steps and batches from Table 1. 
FLOP estimate: 1.3e22

Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf
FLOP estimate: 1.4e22

Geometric mean = 1.35e22 FLOP","United States of America,United States of America,United States of America","Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani
",SOTA improvement,"""ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and pre- dicting protein fitness without additional finetuning.""",ProGen2: Exploring the Boundaries of Protein Language Models,"UniRef90,BFD30","""The standard PROGEN2 models are pretrained on a mixture of Uniref90 (Suzek et al., 2015) and BFD30 (Steinegger & Söding, 2018) databases""",,,Likely,,,123.00,6400000000.00,"""We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters""",,,,,,Google TPU v3,Self-supervised learning,,,Industry,"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence- driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.",2024-05-01 09:22,Anonymous,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
GShard (600B),Language,Translation,2020-06-30,"code is open, Apache: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm",Unreleased,,Open source,https://arxiv.org/abs/2006.16668,Google,1.33e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",United States of America,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",,this is the smaller model in this paper,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,,,260000000000,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",,,,671.00,600000000000.00,"""The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3 core-years.""",,,,,,,Self-supervised learning,$27609.81,,Industry,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2024-05-01 09:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
OpenAI Five Rerun,Games,Dota 2,2019-12-13,,Unreleased,Unreleased,Unreleased,https://cdn.openai.com/dota-2.pdf,OpenAI,1.3e+22,"THIS CALCULATION IS FOR RERUN

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",United States of America,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
Przemysław “Psyho"" Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang","Highly cited,SOTA improvement","""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",Dota 2 with Large Scale Deep Reinforcement Learning,,,53084160000,"54k iterations (Fig 7)
with a batch size of 983040 (Table 2)",,,,1420.00,159000000.00,"""We define a policy (π) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,,,,Self-supervised learning,$32217.13,,Industry,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",2024-04-22 14:33,Robi Rahman,,,,,,512,,OpenAI Five Rerun,,,,Industry,,,,,Industry,,,
GShard (dense),Language,Translation,2020-06-30,"code is open, Apache: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm",Unreleased,,Open source,https://arxiv.org/abs/2006.16668,Google,1.28e+22,"""The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3
core-years""

Assume 30% utilization. 2 TPU v3 cores = 1 TPU v3 chip.
TPU v3 performance is 123 teraFLOPS per chip

2048 TPU cores * (1 chip / 2 cores) * 123 TFLOPS/chip * 0.30 = 1.28e22 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+22+years+*+0.30


Meanwhile their best dense model was trained on 235.5 TPU v3 core-years or 1.3702e23 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+235.5+years+*+0.30

Effective model FLOPs utilization could have been lower since this model has very high training compute compared to parameter count (2.3B). (Compare to Chinchilla-optimal?)",United States of America,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",SOTA improvement,"""such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art""",GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,,,260000000000,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",Confident,4000000,"Table 3, bolded row is best model",671.00,600000000000.00,"""The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days""",,,,1008.0,6 weeks = 1008 hours,Google TPU v3,Self-supervised learning,,,Industry,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2024-05-01 09:06,Robi Rahman,,0,,,,1024,,GShard (dense),,,,Industry,,,,1032192,Industry,,,
GPT3-6.7B + muP,Language,,2022-03-07,"their repo is open: https://github.com/microsoft/mup

The technique is open, not the model. GPT-3 isn't open so it wouldn't be possible for people to recreate GPT-3 + muP with this code",Unreleased,,Unreleased,https://arxiv.org/abs/2203.03466,"Microsoft,OpenAI",1.28e+22,,"United States of America,United States of America","Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",,,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,,,,,,,,83.00,6700000000.00,,1.00,,,,,,,,,,,2024-04-25 16:15,Robi Rahman,GPT3-6.7B + muP,,,,,,,,,,GPT3-6.7B + muP,"Industry,Industry",,,,,"Industry,Industry",,,
Polyglot-Ko-12.8B,Language,,2023-06-04,apache 2.0 for weights,Open source,Unreleased,,https://arxiv.org/abs/2306.02254; https://huggingface.co/EleutherAI/polyglot-ko-12.8b,EleutherAI,1.28e+22,"trained for 167 billion tokens

167b * 12.8b * 6 = 1.28e22",Multinational,"Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Jiwung Hyun, Sungho Park, Kyubyong Park",,,A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models,,"""We collaborated with TUNiB to collect a largescale Korean language dataset for our research. The
dataset, totaling 1.2TB, was meticulously gathered
through our collaborative efforts. Subsequently, we
performed preprocessing on this dataset, resulting
in 863GB of text data that served as the foundation
for our analysis and model training.""",96000000000,"863 GB of Korean language data after processing

~111m Korean words per GB, so ~95,793,000,000 or ~96B words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",Likely,554817,"from HuggingFace: ""Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework.""

from the paper: ""The overall batch size was maintained through the use of gradient accumulation steps (GAS). The model was trained for a total of 301,000 steps.""

GAS is a technique to train larger batches if you have limited memory. I don't think this text says anything in particular about whether the batch sizes changed over the course of training? 167B / 301k = 554,817",15.00,12898631680.00,,,,,,,NVIDIA A100,,,,Industry,"Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated by multiple factors: firstly, the Korean models facilitated performance comparisons with existing multilingual models; and finally, they catered to the specific needs of Korean companies and researchers. This paper presents our work in developing the Polyglot Korean models, which propose some steps towards addressing the non-English language performance gap in multilingual language models.",2024-04-09 12:22,Anonymous,,0,,,,,,,,,,Research collective,,,,,Research collective,,,
Pythia-6.9b,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",1.2420000000001001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+6.9+billion+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,6900000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-6.9b,1,,,,128,0.3300,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,33500,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
OPT-6.7B,Language,Language modelling,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,1.2060000000001e+22,https://www.wolframalpha.com/input?i=6+FLOP+*+6.7+billion+*+300+billion,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Likely,,,,6700000000.00,,1.67,,,,,,,,,,,2024-04-30 15:03,Robi Rahman,OPT-6.7B,1,,,,,,,,,,Industry,,,,,Industry,,,
GPT3-6.7B (rerun of original),Language,,2020-05-28,"repo here, don't think there's GPT-3 code though https://github.com/microsoft/mup/blob/main/README.md ",Unreleased,,Unreleased,https://arxiv.org/abs/2203.03466,"Microsoft,OpenAI",1.2e+22,,"United States of America,United States of America","Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",,,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,,,,,,,,83.00,6700000000.00,,1.00,,,,,,,,,,,2024-05-10 16:30,Robi Rahman,GPT3-6.7B (rerun of original),,,,,,,,,,GPT3-6.7B (rerun of original),"Industry,Industry",,,,,"Industry,Industry",,,
$\infty$-former (SM),Language,,2021-09-01,"code, no clear license: https://github.com/deep-spin/infinite-former ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2109.00301,"Universidade de Lisboa (ULisboa),DeepMind",1.2e+22,,"Portugal,United Kingdom of Great Britain and Northern Ireland","Pedro Henrique Martins, Zita Marinho, André F. T. Martins",,,$\infty$-former: Infinite Memory Transformer,"WikiText-103,PG-19 (Project Gutenberg)","""we fine-tune GPT-2 small (Radford et al., 2019) on Wikitext103 (Merity et al., 2017) and a subset of PG-19 (Rae et al., 2019) containing the first 2,000 books (≈ 200 million tokens) of the training set""",,,,,,31.00,117000000.00,,1.00,,,,,,,,,,"Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \infty-former's attention complexity becomes independent of the context length, trading off memory length with precision. In order to control where precision is more important, \infty-former maintains ""sticky memories"" being able to model arbitrarily long contexts while keeping the computation budget fixed. Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \infty-former's ability to retain information from long sequences.",2024-05-09 13:46,Robi Rahman,$\infty$-former (SM),,GPT-2 (117M),,"""To understand if long-term memories can be used to
extend a pre-trained language model, we fine-tune
GPT-2 small (Radford et al., 2019) on Wikitext103 (Merity et al., 2017) and a subset of PG-19
(Rae et al., 2019) containing the first 2,000 books
(≈ 200 million tokens) of the training set""",,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
bilingual-gpt-neox-4b,Language,Language generation,2023-07-31,"MIT for weights. open data, multiple licenses: https://huggingface.co/rinna/bilingual-gpt-neox-4b",Open source,Open access (non-commercial),Unreleased,https://huggingface.co/rinna/bilingual-gpt-neox-4b,rinna,1.2e+22,3.8 billion params * 524b tokens * 6 = 1.2e22,Japan,"Tianyu Zhao, Toshiaki Wakatsuki, Akio Kaga, Koh Mitsuda, Kei Sawada",,,,"Japanese CC-100,Japanese C4,The Pile,RedPajama",,,,Likely,,,,3800000000.00,3.8 billion,,,,,,,,,,,This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.,2024-04-09 11:44,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Nucleotide Transformer,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2023-01-15,,,,,https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf,"NVIDIA,Technical University of Munich",1.2e+22,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""

Assuming 78 TFLOP / s for 32-bit calculations and 0.5 utilization rate

Estimate: 78e12 FLOP/s * 128 GPUs * 28 days * 86400 seconds * 0.5 utilization rate = 1.2e22","United States of America,Germany","Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir,
Marie Lopez, Thomas Pierrot",SOTA improvement,"""We show that the representations alone match or outperform specialized
methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning.""","The Nucleotide Transformer: Building and Evaluating Robust
Foundation Models for Human Genomics",Human reference genome (GRCh38/hg38),"""pre-trained them on three different datasets encompassing the Human
reference genome, a collection of 3,200 diverse human genomes, and 850 genomes from several species""",,,Unverified,,,22.00,2500000000.00,"""We built four distinct foundation language models of different sizes, ranging from 500M up to 2.5B parameters""",,,,672.0,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""",NVIDIA A100,Self-supervised learning,,,,"Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.
",2024-03-11 16:13,Anonymous,,,,935000000000000000,"""All fine-tuning runs were performed on a single node with eight A100 GPUs. [...] On average, a fine-tuning run lasted 20 minutes for the 500M parameter models, and 50 minutes for the 2.5B parameter models.""

Estimate: 78e12 FLOP/s * 8 GPUs * 50 min * 60 seconds * 0.5 utilization rate = 9.36e17",128,,Nucleotide Transformer,,,,"Industry,Academia",,,,86016,"Industry,Academia",,,
Refact-1.6B,Language,Language modelling/generation,2023-08-29,"OpenRAIL-M license, responsible use restrictions: https://bigscience.huggingface.co/blog/bigscience-openrail-m",Open access (restricted use),Unreleased,Unreleased,https://huggingface.co/smallcloudai/Refact-1_6B-fim,Refact AI,1.152e+22,"6ND = 6 * 1.6B * 1.2T = 11520000000000000000000 = 1.152e22
citation ""model trained for 1.2T tokens. ""
alternative
flops = (64) * (27770 * 10**9) * (28 * 24 * 3600) * (0.3) = 1.2898787328e+21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
Precision: bfloat16
GPUs 64 NVidia A5000
Training time 28 days
27.77 TFLOPS - peak flop from https://www.techpowerup.com/gpu-specs/rtx-a5000.c3748",United Kingdom of Great Britain and Northern Ireland,,,, Refact-1.6B ,,"""For the base model, we used our own dataset that contains code with permissive licenses only, and open text datasets. Filtering is the key to success of this model:

    We only used text in English
    Only topics related to computer science
    Applied heavy deduplication

The text to code proportion was 50:50, model trained for 1.2T tokens. """,1200000000000,"1.2T from ""The text to code proportion was 50:50, model trained for 1.2T tokens. """,Likely,,,,1600000000.00,1.6B,,3200000000.00,2N,672.0,"from ""Model Stats""",NVIDIA RTX A5000,Self-supervised learning,,,,,2024-05-21 06:17,Bartosz Podkanowicz,,,,,,6,,,,,,Industry,,,,4032,Industry,,,
CLIP (ViT L/14@336px),"Multimodal,Vision,Language,Video","Zero-shot image classification,Character recognition,Video description",2021-01-05,"MIT License

https://github.com/OpenAI/CLIP",Open source,,Open source,https://arxiv.org/abs/2103.00020,OpenAI,1.05e+22,https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing,United States of America,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever","Highly cited,SOTA improvement",,Learning Transferable Visual Models From Natural Language Supervision,Unspecified unreleased,"Custom image-text pairs from the internet

we constructed a new dataset of 400 million (image,
text) pairs collected form a variety of publicly available
sources on the Internet. To attempt to cover as broad a set
of visual concepts as possible, we search for (image, text)
pairs as part of the construction process whose text includes
one of a set of 500,000 queries",400000000,,,,,12732.00,370000000.00,"Image encoder
Vision Transformer
Table 1 in https://arxiv.org/pdf/2010.11929.pdf
Authors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)
307M params

Text encoder
~Transformer (from paper)
63M params",,110000000.00,Figure 10 https://arxiv.org/pdf/2103.00020.pdf,288.0,"“The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs”",NVIDIA V100,Self-supervised learning,$40146.99,"https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html
",Industry,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",2024-05-13 09:59,Robi Rahman,,,,,,256,,CLIP (ViT L/14@336px),,,,Industry,,,,,Industry,,,
Distilled Grandmaster,Games,Chess,2024-02-07,,,,,https://arxiv.org/abs/2402.04494,DeepMind,1.035671832e+22,"10356718320000000065536 FLOP
""Board states 𝑠 are encoded as FEN strings which we convert to fixed-length strings of 77 characters where the ASCII-code of each character is one token."" so 77 tokens for board + 1 token for action ""For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates""
so input is 78 tokens for each action-value
number of tokens = 1194960000000.0
The model is dense transformer
"" We train for 10 million
steps, which corresponds to 2.67 epochs for a batch
size of 4096 with 15.32B data points "", but in appendix A.2 there is mention of 5.35 epochs
I have used higher value from 5.35 and 2.67,
Probably final they trained model for 5.35 epochs and used checkpoint from 2.67 as final model.
aproximation 6ND for 5.35 epochs = 6*270e6*1194960000000.0 * 5.35 =  10356718320000000065536
",United Kingdom of Great Britain and Northern Ireland,"Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein",,,Grandmaster-Level Chess Without Search,,"custom
""To get a large corpus of “ground-truth” action-values we
use Stockfish 16 as an oracle to annotate millions of
board states obtained from randomly drawn games on
lichess.org, which are mostly played by humans vary-
ing significantly in playing strength.""",1194960000000,"15.32B examples * 78 tokens per example = 1.19e12
Training is supervised. I count each action-value (board state, action and numeric evaluation of state from Stockfish 16) as 1 data point.""For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates""",Confident,,,2.00,270000000.00,"""Our largest model has roughly 270 million parameters.""",,,,,,,Supervised,,,,"The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters. ",2024-05-15 16:39,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
DeciLM 6B,Language,Chat,2023-09-13,"Llama 2 license (restrictive)
dataset: https://huggingface.co/datasets/cerebras/SlimPajama-627B",Open access (restricted use),Open access (non-commercial),Unreleased,https://huggingface.co/Deci/DeciLM-6b,Deci AI,1.026e+22,"assume they used 50% of SlimPajama dataset (300B tokens) - then we are  within 'Likely' confidence interval

6*300B*5700000000=1.026e+22",Israel,DeciAI Research Team,,,DeciLM 6B,SlimPajama,"""DeciLM 6B underwent training utilizing a subset of the SlimPajamas dataset"" from https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/?utm_campaign=repos&utm_source=hugging-face&utm_medium=model-card&utm_content=decilm-6b",,"subset of the SlimPajamas dataset, but we don't know which subset
""DeciLM 6B underwent training utilizing a subset of the SlimPajamas dataset"" from https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/?utm_campaign=repos&utm_source=hugging-face&utm_medium=model-card&utm_content=decilm-6b",Likely,,,,5700000000.00,"""DeciLM 6B is a 5.7 billion parameter decoder-only text generation model. """,,,,,,,,,,Industry,,2024-05-21 02:29,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Cohere Command Light,Language,Language generation,2023-11-30,,API access,,,https://txt.cohere.com/embed-command-light-fine-tuning-on-amazon-bedrock/,Cohere,1.001e+22,"https://docs.cohere.com/docs/environmental-impact

2700kg CO2 equivalent. Cohere used this calculator: https://mlco2.github.io/impact/ 

This calculator claims that ~40000 TPUv3 hours causes ~3000 kg CO2 emissions in the ""us-west1"", ""us-west2"", and ""us-west3"" regions. Not clear what region the data center Cohere used was in. Google has data centers around the world; *most* regions are similarly carbon intensive as us-west but north-america-northeast is 10x less carbon intensive and south Asia is 5x more carbon intensive. So the calculation below could be quite off.

Cohere most likely used TPUv4s, which the calculator does not support, which seem to be much more efficient (2.7x more, according to this https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)

40000 hours * 123 teraflops * 3600 * 0.3 utilization * 2.7 = 1.4e22

",Canada,,,,Cohere’s Embed and Command Light Models with Fine-tuning Now Available on Amazon Bedrock,,,,,Speculative,,,,6000000000.00,"6B

https://aws.amazon.com/bedrock/cohere-command-embed/",,,,,,Google TPU v4,,,,,,2024-03-20 09:28,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
BTLM-3B,Language,"Language generation,Code generation",2023-09-20,"Apache for weights: https://huggingface.co/cerebras/btlm-3b-8k-base

dataset is SlimPajama, with various licenses: https://huggingface.co/datasets/cerebras/SlimPajama-627B",Open source,,Unreleased,https://arxiv.org/abs/2309.11568,Cerebras Systems,9.8e+21,2.6b params * 627b tokens * 6 = 9.8e21,Multinational,"Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming (Charles)Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness",,"SOTA for its parameter class: ""BTLM-3B-8K achieves state-of-the-art performance among 3B parameter models""",BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model,SlimPajama,"""To bolster BTLM’s performance, we create a high quality
627B token dataset called SlimPajama ((Soboleva et al., 2023)). Starting from the 1.21T token RedPajama dataset Computer (2023), we apply filtering and deduplication to improve data quality. First, we
remove documents containing fewer than 200 characters, as we find these typically contain only metadata. Next, we perform global deduplication using MinHashLSH (Leskovec et al., 2014) to extensively
remove documents with significant overlapping text.""",470000000000,"627B tokens, equivalent to 470B english words",Likely,3932160,,,2600000000.00,"2.6B, per paper",1.00,,,,,Cerebras CS-2,,,,,"We introduce the Bittensor Language Model, called ""BTLM-3B-8K"", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.
On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: this https URL.",2024-04-08 18:31,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
VideoMAE V2,Video,Action recognition,2023-03-29,"MIT
https://github.com/OpenGVLab/VideoMAEv2/blob/master/LICENSE",Open source,Open source,Open source,https://arxiv.org/abs/2303.16727v2,"Nanjing University,Shenzhen Institute of Advanced Technology,Shanghai AI Lab",9.7e+21,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21","China,China,China","Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao",SOTA improvement,"""Finally, we successfully train a video ViT model with a
billion parameters, which achieves a new state-of-the-art
performance on the datasets of Kinetics (90.0% on K400
and 89.9% on K600) and Something-Something (68.7% on
V1 and 77.0% on V2).""",VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking,,"""To well support the billion-level ViT model pretraining, we build two large-scale video datasets for our proposed progressive training. For self-supervised pre-training of VideoMAE V2, we build a million-level unlabeled video
dataset by collecting clips from multiple resources such
as Movie, Youtube, Instagram, General Webs, and manual recordings from scripts, and the dataset is termed as
UnlabeledHybrid""",,"1.35 million video clips. Not sure about average length (34 seconds, but that's only reported for Instagram portion).

""In total, there are around 1.35M clips in our mixed dataset and
this is the largest dataset ever used for video masked autoencoding.",Confident,,,121.00,1000000000.00,1B,1200.00,,,336.0,2 weeks,NVIDIA A100 SXM4 80 GB,Self-supervised learning,,,,"Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner. The code and model is available at \url{this https URL}.",2024-05-14 17:59,Anonymous,,0,ViT-G/14,9,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21
",64,,VideoMAE V2,,,,Academia,,,,,Academia,,,
Megatron-LM (8.3B),Language,,2019-09-17,"code (2.5B model is a GPT model): https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-overview  
open license: https://github.com/NVIDIA/Megatron-LM?tab=License-1-ov-file#readme ",Unreleased,,Open source,https://arxiv.org/abs/1909.08053,NVIDIA,9.100000000000001e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb


other estimates:

8.3B is a GPT-2-based model (Table 2). ""For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations"" 

I interpret the above as 1024*512*300k = 157B training tokens 

6 * 157 billion * 8.3 billion  = 7.8e21

Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.
(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)
2.1 days per epoch, ~4.4 epochs
2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22

These are both close to the akronomicon estimate",United States of America,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro","Highly cited,SOTA improvement","""Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA"" 

GPT-2 model here meaning model similar to GPT-2",Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,,"""we aggregate several of the largest language
modeling datasets. We create an aggregate dataset consisting of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &
Le, 2018), RealNews (Zellers et al., 2019), and OpenWebtext (Radford et al., 2019). To avoid training set leakage
into our downstream tasks we remove the Wikipedia articles
present in the WikiText103 test set (Merity et al., 2016).""",34800000000,"""The resulting aggregate
corpus contains 174 GB of deduplicated text.""",Likely,,,1152.00,8300000000.00,"Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/

Data also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json",4.40,18000000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",327.0,"Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs
Assume total compute is 9.1e21 FLOP.
Then training time is 327 hours.
https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29",NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$78689.00,"327 hours * 512 GPUs * $0.55/V100 GPU-hour = $92,083
Convert to 2020 dollars: $78,689",Industry,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2024-05-22 11:49,Robi Rahman,Megatron-LM (8.3B),,,,,512,0.2269,Megatron-LM (8.3B),,,,Industry,,,,167424,Industry,,,
iGPT-L,"Image generation,Vision",Image completion,2020-06-17,modified MIT: https://github.com/openai/image-gpt?tab=License-1-ov-file#readme,Open source,,Open source,https://openai.com/blog/image-gpt/,OpenAI,8.91e+21,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]

I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].

I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.
Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].

In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.

[1] https://openai.com/blog/image-gpt/
[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf
[3] https://openai.com/blog/ai-and-compute/",United States of America,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",Highly cited,,Generative Pretraining from Pixels,ILSVRC 2012 subset of ImageNet,,9600000,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,1190.00,1362000000.00,source: https://openai.com/blog/image-gpt/#rfref53,,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$32482.56,,Industry,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2024-04-24 13:29,Robi Rahman,,,,,,,,iGPT-L,,,,Industry,,,,60000,Industry,,,
ResNeXt-101 32x48d,Vision,Image classification,2018-05-02,,,,,https://arxiv.org/abs/1805.00932,Facebook,8.74395e+21,"Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP",United States of America,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten","Highly cited,SOTA improvement","""We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%",Exploring the Limits of Weakly Supervised Pretraining,"ImageNet,Instagram","Instagram images, captioned with hashtags",9525000000,Table 3: (300+1925+300+7000) million images,Likely,,,1216.00,829000000.00,"Table 6
",,31200000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,"""Mahajan et al. (2018) required 19
GPU years to train their ResNeXt101-32x48d""
https://arxiv.org/abs/2103.00020",,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,336,,ResNeXt-101 32x48d,,,,Industry,,,,,Industry,,,
GPT-Neo-2.7B,Language,,2021-03-21,MIT,Open source,Open source,,https://github.com/EleutherAI/gpt-neo,EleutherAI,7.9e+21,source: https://www.aitracker.org/,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,The Pile,,885837004800,"""In aggregate, the Pile consists of over 825GiB of raw text data""

(see GPT-NeoX)",,,,0.00,2700000000.00,"source: https://www.eleuther.ai/projects/gpt-neo/

Note: Directory of LLMs (https://docs.google.com/spreadsheets/d/1gc6yse74XCwBx028HV_cvdxwXkmXejVjkO-Mz2uwE0k/edit#gid=0) gives a somewhat lower estimate (2e9)",,,,,,,Self-supervised learning,$13685.99,,Industry,,2024-04-17 05:50,Robi Rahman,GPT-Neo-2.7B,,,,,,,,,,,Research collective,checked,,,,Research collective,,,
Segment Anything Model,Vision,Image segmentation,2023-04-05,"Apache 2.0 license
don't see pretrain code in the repo, could be wrong

https://github.com/facebookresearch/segment-anything",Open source,Open access (non-commercial),Unreleased,https://arxiv.org/abs/2304.02643,Meta AI,7.8e+21,"""SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training
large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh""

68*256 A100-hours = 
17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)
= 7.82e21

max A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)",United States of America,"Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick",Highly cited,,Segment Anything,Segment Anything 1B,"""Dataset (§5). Our final dataset, SA-1B, includes more than
1B masks from 11M licensed and privacy-preserving images (see Fig. 2). SA-1B, collected fully automatically using the final stage of our data engine, has 400× more masks
than any existing segmentation dataset [66, 44, 117, 60],
and as we verify extensively, the masks are of high quality
and diversity. Beyond its use in training SAM to be robust
and general, we hope SA-1B becomes a valuable resource
for research aiming to build new foundation models.""",1100000000,"""SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.""
segmentation mask is a map that identifies segments in an image",Likely,,,1967.00,636000000.00,"This is an undercount. SAM has multiple components (figure 1B), but looks like the bulk of it is VIT-H (636M params, figure 13)",2.00,,,68.0,"""SAM was trained on 256 A100 GPUS for 68 hours""",NVIDIA A100,Supervised,,,,"We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.",2024-04-11 17:29,Anonymous,,0,ViT-Huge/14,7,see Training Compute notes,256,,Segment Anything Model,,,,Industry,,,,,Industry,,,
Conformer + Wav2vec 2.0 + Noisy Student,Speech,Speech recognition,2020-10-20,,Unreleased,,Unreleased,https://arxiv.org/abs/2010.10504v2,"Google,Google Research,Google Brain",7.6e+21,"""We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...
We fine-tune the pre-trained checkpoints (400k steps) with global batch
size 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models""

TPU v3 chips are 123 teraflop/s. 2 chips per core

512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21","United States of America,Multinational,United States of America","Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu",SOTA improvement,"""By doing so, we are able to achieve
word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against
the current state-of-the-art WERs 1.7%/3.3%.""",Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition,LibriLight,"""We aim to improve the LibriSpeech task [1] by utilizing the unlabeled audio in the ""unlab-60k""
subset of the Libri-Light [2] dataset. The 960h of transcribed audio of the LibriSpeech dataset is
used as the supervised data""",,,Likely,,,280.00,1000000000.00,1B for XXL model,,,,168.0,7 days,Google TPU v3,,,,,"We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.",2024-04-16 09:48,Anonymous,,,,,,256,,Conformer + Wav2vec 2.0 + Noisy Student,,,,"Industry,Industry,Industry",,,,,"Industry,Industry,Industry",,,
PeptideBERT,Biology,Proteins,2023-08-28,MIT,Open source,Open source,Open source,https://arxiv.org/abs/2309.03099,Carnegie Mellon University (CMU),7.6e+21,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",United States of America,"Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani",SOTA improvement,"""Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide’s potential to induce red blood cell lysis.""",PeptideBERT: A language Model based on Transformers for Peptide Property Prediction,,,,,Confident,,,,,,30.00,,,4.1,244 minues from Table 1,NVIDIA Geforce GTX 1080 Ti,,,,,"Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non- fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide’s potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide’s capacity to resist non-specific interactions. This model, trained predominantly on shorter sequences, benefits from the dataset where negative examples are largely associated with insoluble peptides. Codes, models, and data used in this study are freely available at: https://github.com/ChakradharG/PeptideBERT",2024-03-29 16:00,Anonymous,,,,49805280000000000,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",1,,PeptideBERT,,,,Academia,,,,4,Academia,,,
ESM2-650M,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-21,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd",Open source,Open source,Open source,https://www.science.org/doi/abs/10.1126/science.ade2574,"Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",7.560000000001e+21,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 4.4e21 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
8 days x 512 V100s x an imputed 30% utilization"": 1.3e22 FLOP

Geometric mean: 7.56e21 FLOP
","United States of America,United States of America,United States of America,United States of America","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",Evolutionary-scale prediction of atomic-level protein structure with a language model,UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",,,Likely,,,636.00,650000000.00,In the name,,,,192.0,,NVIDIA V100,Unsupervised,,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",2024-03-28 17:48,Epoch AI,,1,,,,512,,ESM2-650M,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
DeciCoder-6B,Language,"Chat,Code autocompletion",2024-01-15,Apache 2.0,Open source,,,https://huggingface.co/Deci/DeciCoder-6B,Deci AI,7.56e+21,"Python, Java, Javascript, Rust, C++, C, and C# subset of Starcoder Training Dataset

From the Starcoder paper, that's 7.9% + 11.3% + 8.4% + 1.2% + 6.4% + 7% + 5.8% of the total, so 48% of 815 GB, say 410 GB of code. If we assume they trained for 2 epochs, 820 GB.

820 GB * 200M word per GB = 1.6e11 words
1.6e11 / 0.75 = 2.1e11 tokens

C = 6ND = 6 * 2.1e11 * 6e9 = 7.6e21",Israel,DeciAI Research Team,,, Model Card for DeciCoder-6B,StarCoder,,,,Likely,,,,6000000000.00,6B,,,,,,,,,,,,2024-05-20 11:29,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
DINOv2,Vision,Image representation,2023-04-14,apache 2.0,Open source,,Open source,https://arxiv.org/abs/2304.07193,"Facebook AI Research,INRIA",7.41851136e+21,"table 14

22016 * 3600 * 312 * 10 ** 12 * 3/10 = 7.41851136e+21
gpu hours in seconds * flops of A100 * assumed utilization  rate","United States of America,France","Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",SOTA improvement,"""Our family of models drastically improves over
the previous state of the art in self-supervised learning and reaches performance comparable with weakly-
supervised features.""
",DINOv2: Learning Robust Visual Features without Supervision,,new dataset  - named LVD142M Table 15,142000000,new dataset  - named LVD142M Table 15,Confident,,,753.00,1140000000.00,1.14B from https://huggingface.co/facebook/dinov2-giant,,,,,,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,,"The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. ",2024-05-16 17:01,Anonymous,,,,,,,,DINOv2,,,,"Industry,Academia",,,,,"Industry,Academia",,,
FTW,Games,Capture the flag,2018-07-03,,,,,https://arxiv.org/abs/1807.01281,DeepMind,7.26e+21,"We assume that most operations happen in the visual embedding.

2* 84^2*84^2 * 32 * 3 / 1^2 = 9.5 *10^9
new image size: 76 x 76 x 32
ignore ReLU/additions becaue probably very little influence 
2 * 76^2 * 76^2 * 10* 64 = 4 *10^10
new image size: 72 x 72 x 64
2 * 72^2 *72^2 * 64 * 64 * 3=  6.6 * 10^11
new image size: 69 x 69 x 64
2 * 69^2 *69^2 * 64 * 64 * 3=  5.5 * 10^11
new image size: 66 x 66 x 64
Linear layer: 2* ( 66*66*64)*256 = 1.4*10^8
Total aprox: 1.21e+12 FLOP/forward pass

",United Kingdom of Great Britain and Northern Ireland,"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",SOTA improvement,"""In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag (28), using only pixels and game points as input.""",Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,,,,,,,,625.00,126001330.00,"Architecture described in figure S11 of the supplement

The architecture includes modules for visual embedding, reward prediction, recurrent processing, policy, baseline and pixel control.

Input is 84x84x3 pixels as seen in figure S10 of the supplement

""We elected to use a resolution of 84x84 pixels as in previous related work in this environment. Each pixel is represented by a triple of three bytes""

Visual embedding (84x84x3 -> 256)
32*(8*8*3+1)+64*(4*4*32+1)+64*(3*3*64+1)+64*(3*3*64+1) + (84/(S^4)*84/(S^4)*64+1)*256
Note there is no information about the stride S used in the convolutions; we assume S = 1

Reward prediction (256 -> 3)
(256+1)*128 + (128+1)*3

Recurrent processing (n-> 512)
VU1 (256 -> 512)
4*(799+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(799+2*32)+1) + 2*(256+1)*256

VU2 (512 -> 512)
4*(512+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(512+2*32)+1) + 2*(256+1)*256

LSTMs usually have 4*(n*m+n*n+n) parameters, where n=input size and m=output size.

This DNS + LSTM takes as input the concatenation of the previous layer of size n and R read vectors of size W=32; and outputs m units plus an interface vector of size (W*R) + 3*W + 5*R + 3, for a total of about 4*(n+R*W)*((m+(W*R) + 3*W + 5*R + 3)+(n+R*32)+1) parameters

I assume R=2 since that seems implied by the previous paper (?)

The first VU has as input the visual embedding (size 256), the previous action (size 540) and the previous reward (size 3), for a total size of 256+540+3 = 799. The output is size 512.

The second VU has input size 512 and output size 512

The DNC memory architecture is described in https://www.nature.com/articles/nature20101.epdf

Policy (512 -> 5x3x3x3x2x2)
6*(512+1)*256 + (256+1)*5 + 3*(256+1)*3 + 2*(256+1)*2

Baseline
(512+1)*256 + (256+1)*1

Pixel control
(512+1)*32*7*7 + 32*(9*9+1) + 5*(4*4+1) + 3*2*(4*4+1) + 2*2*(4*4+1) + 1*(4*4+1)
""we trained independent pixel control policies for each of the six action groups""",,1210000000000.00,,,,,Self-supervised learning,$21045.02,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,FTW,,,,Industry,,,,,Industry,,,
TinyLlama-1.1B (1T token checkpoint),Language,Chat,2023-10-01,apache 2.0: https://github.com/jzhang38/TinyLlama,Open source,,Open source,https://arxiv.org/abs/2401.02385,Singapore University of Technology & Design,7.24598784e+21,"6ND approximation: 6*1.1B * 1T = 6600000000000000000000
Based on reported GPU-time and utilization:
flops = (num gpu) * (peak flops) * (time in seconds) * (reported utilization rate) = (16) * (312 * 10**12) * (30 * 24 * 3600) * (0.56) = 7245987840000001048576


source: https://github.com/jzhang38/TinyLlama
""Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization""
and Releases Schedule from the same link",Singapore,"Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu",,,TinyLlama: An Open-Source Small Language Model,"SlimPajama,StarCoderData",Slimpajama & Starcoderdata from Training Details from https://github.com/jzhang38/TinyLlama,750000000000,1T tokens checkpoint so around 0.75T words,Confident,,,39.00,1100000000.00,1.1B,,2200000000.00,1.1B*2,720.0,"1T checkpoint was released after 1 month

source: https://github.com/jzhang38/TinyLlama",NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,,"We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama",2024-04-02 10:14,Anonymous,,1,,,,16,0.5600,,,,,Academia,,,,11520,Academia,,,
Tranception,Biology,"Proteins,Protein variant pathogenicity prediction",2022-05-27,MIT,Open source,,,https://arxiv.org/abs/2205.13760,"University of Oxford,Harvard Medical School,Cohere",7.240000000000001e+21,"Trained using 64 A100 GPUs for two weeks.
64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)
= 7.24e21","United Kingdom of Great Britain and Northern Ireland,United States of America,Canada","Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan Gomez, Debora S. Marks, Yarin Gal",SOTA improvement,"""We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches.""",Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,UniRef100,"""We therefore train our final model (700M parameters) on UniRef100""",,250 million proteins after filtering,Likely,,,106.00,700000000.00,"""Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014)""",,,,336.0,2 weeks,NVIDIA A100,Self-supervised learning,,,,"The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",2024-05-01 09:22,Anonymous,,,,,,64,,Tranception,,,,"Academia,Academia,Industry",,,,21504,"Academia,Academia,Industry",,,
AudioGen,Audio,Audio generation,2023-03-05,"MIT license, but non-commercial for weights: https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights",Open access (non-commercial),Open source,Open source,https://arxiv.org/abs/2209.15352,"Meta AI,Hebrew University of Jerusalem",7.2e+21,"""the large model was trained on 128 A100 GPUs for 200k steps (∼1 week)""
A100s are 312 teraflop/s

128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21","United States of America,Israel","Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi",SOTA improvement,"""We propose a state-of-the-art auto-regressive audio generation model conditioned on textual descriptions or audio prompts, as evaluated with objective and subjective (human
listeners) scores.""",AudioGen: Textually Guided Audio Generation,"AudioSet,AudioCaps","""We use a set of several datasets: AudioSet (Gemmeke et al., 2017), BBC sound effects,
AudioCaps (Kim et al., 2019), Clotho v2 (Drossos et al., 2020), VGG-Sound (Chen et al., 2020),
FSD50K (Fonseca et al., 2021), Free To Use Sounds 2
, Sonniss Game Effects 3
, WeSoundEffects 4
,
Paramount Motion - Odeon Cinematic Sound Effects 5
. All audio files were sampled at 16kHz.
For textual descriptions we use two types of annotations. The first one is multi-label annotations,
available for the datasets: AudioSet, VGG-Sound, FSD50K, Sinniss Game Effects, WeSoundEffects, Paramount Motion - Odeon Cinematic Sound Effects.""",,"""Overall we are left with ∼4k hours for training data.""
mix of speech and other sounds",Likely,,,127.00,1000000000.00,"""We trained two sets of ALMs, one with 285M parameters (base) and the other
with 1B parameters (large).""",,,,168.0,1 week,NVIDIA A100,,,,,"We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: this https URL",2024-05-01 09:24,Anonymous,,0,,,,,,AudioGen,,,,"Industry,Academia",checked,,,,"Industry,Academia",,,
Wu Dao - Wen Lan,"Multimodal,Vision",Image captioning,2021-03-01,,,,,https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,Beijing Academy of Artificial Intelligence,7.1995392e+21,"128 Nvidia A100 GPUs for 7 days

128 GPUs * 3.1e14 FLOP/s /GPU * 7*24*60*60s* 0.3 [utilization rate]

",China,,,,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',,,,,,,,0.00,1000000000.00,"""Currently, the model has 1 billion parameters and is trained on 50 million graphic pairs collected from open sources.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",,,,,,,Self-supervised learning,,,Industry,"Wu Dao — Wen Lan, meanwhile, is the first publicly available Chinese universal graphic multimodal pretraining model. The ultra-large-scale multimodal pretraining model aims to break through the theoretical challenges of pretraining multimodal data based on a combination of graphics, text and video, and eventually generate industrial-grade Chinese graphics pretraining models and applications that exceed SOTA performance. Currently, the model has 1 billion parameters and is trained on 50 million graphic pairs collected from open sources. The Wu Dao — Wen Lan model has reached SOTA performance, scoring 5 percent higher than the champion team on the Image Caption task on the Chinese public multimodal test set AIC-ICC and 20 percent higher than the most popular UNITER model on the Visual Entailment task.",2024-05-13 09:53,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Primer,Language,,2022-01-24,,,,,https://arxiv.org/abs/2109.08668,Google Brain,7.1e+21,"From the email they claim to have use 72K TPUv4 hours for training

Thus: 
72000 h * 0.1 * 275e12 FLOP/s 3600s/h = 7.1e21 FLOP",United States of America,"DavidR.So, WojciechMan ́ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le",,,Primer: Searching for Efficient Transformers for Language Modeling,C4,,173284750600,"In GB - TODO convert to words

""Dataset size: 806.92 GiB""
https://www.tensorflow.org/datasets/catalog/c4

This was the largest dataset that the authors used 
""These benefits are robust and hold across model sizes (20M
to 1.9B parameters), across compute scales (10 to 105
accelerator hours), across datasets (LM1B,
C4, PG19 [22])""

802.92 GiB ~ 866.42 GB
1 GB ~ 200M words",,,,110.00,1900000000.00,"""For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer""",,,,,,,,$9690.72,,Industry,"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
GNMT,Language,Translation,2016-09-26,presumably deployed via Google translate,Hosted access (no API),,,https://arxiv.org/abs/1609.08144,Google,6.620000000001001e+21,"From AI and Compute:
""sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days""
source: https://openai.com/blog/ai-and-compute/

https://www.wolframalpha.com/input?i=96+*+9+days+*+8.5+TFLOPS+*+0.33+*+sqrt%281000%29
",United States of America,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",Highly cited,,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,,,1230000000,"[WORDS]
"" On WMT En→Fr, the training set contains 36M sentence pairs. On WMT En→De, the training set contains 5M sentence pairs.""

41M sentence pairs * 2 sentences per pair * 15 words/sentence",,,,6196.00,278000000.00,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538",1.00,,,,"Test model used 96 K80 for 9 days, then this was scaled up by 31x for the production model, but unclear how many GPUs were used or how long it was trained for. The production run used 96 * 9 days * sqrt(1000) ~= 655730 chip-hours.",NVIDIA Tesla K80,Reinforcement learning,$307573.50,,Industry,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",2024-05-21 09:31,Robi Rahman,,0,,,,,,GNMT,,,,Industry,,,,655730,Industry,,,
Ankh_large,Biology,"Protein generation,Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2023-01-16,"cc non-commercial:
https://github.com/agemagician/Ankh/blob/main/LICENSE.md",Open access (non-commercial),,,https://arxiv.org/abs/2301.06568,"Technical University of Munich,Columbia University",6.5e+21,Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf,"Germany,United States of America","Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",SOTA improvement,"""On average, Ankh improved the PLM SOTA performance by 4.8%""",Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling,UniRef50,"""We build upon the same results by pre-training our baseline on UniRef50.""",,"45M proteins and 14B amino acids, per Table 2",Likely,,,31.00,1150000000.00,See figure 1,68.00,,,,,Google TPU v4,,,,,"As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",2024-05-01 09:22,Anonymous,,,,,,,,Ankh_large,,,,"Academia,Academia",,,,,"Academia,Academia",,,
WeLM,Language,Language modelling/generation,2023-05-16,,,,,"https://arxiv.org/abs
/2209.10372",WeChat AI,6.2084579328e+21,">>> num_gpu = 128 
>>> flops = 7797 * 10**10 
>>> time = 24 * 24 * 3600 
>>> utilization = 0.3 
>>> num_gpu * flops * time * utilization 
6.2084579328e+21 
>>> int(num_gpu * flops * time * utilization) 
6208457932800000000000 
from citations:
""The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 days”, ""All models are trained with FP16 mixed precision.""
from https://www.techpowerup.com/gpu-specs/a100-sxm4-40-gb.c3506 A100 have  77.97 TFLOPS for FP16 (half), ",China,"Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, Jie Zhou",,,WeLM: A Well-Read Pre-trained Language Model for Chinese,,,262000000000,"from the paper ""After all the above filtering process, our corpus contains 262B tokens”

Data is mostly in Chinese language - we assume that 1 token correspond to 1 word",Likely,,,12.00,10000000000.00,"""WeLM is trained with 10B parameters”",,,,576.0,"""The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 days”, ",NVIDIA A100 SXM4 40 GB,Unsupervised,,,,"Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by ""reading"" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM with multi-prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself, which can be promising directions for future research. Our models can be applied from this https://welm.weixin.qq.com/docs/api/",2024-02-05 08:51,Anonymous,,,,,,128,,,,,,Industry,,,,73728,Industry,,,
ADM,Image generation,Image generation,2021-05-11,"These models are intended to be used for research purposes only. In particular, they can be used as a baseline for generative modeling research, or as a starting point to build off of for such research.
These models are not intended to be commercially deployed. Additionally, they are not intended to be used to create propaganda or offensive imagery.

MIT License
https://github.com/openai/guided-diffusion",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2105.05233,OpenAI,6.199999999999999e+21,"Largest run with their architecture improvements is the ImageNet 512 variant. Table 7 suggests utilization is around 30% for largest models (though we only see 256 x 256 and 128 -> 512)

Table 10: ImageNet 512 variant took 1914 V100-days of training
125e12 FLOP/sec * 1914 days * 24 h/day * 3600 sec/h * 0.3 = 6.2e21",United States of America,"Prafulla Dhariwal, Alex Nichol","Highly cited,SOTA improvement","""We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models""",Diffusion Models Beat GANs on Image Synthesis,"LSUN,ILSVRC 2012 subset of ImageNet","""To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN [71] classes: bedroom, horse, cat""",1281167,"Biggest models are trained on ImageNet 512x512. ImageNet ILSVRC has 1,281,167 images in the training set, but it is possible some were filtered due to size.

Note that a smaller model was trained on LSUN {bedroom, horse, cat}, which forms a larger dataset:
3,033,042 + 2,000,340 + 1,657,266 = 6,690,648 images",Confident,,,3542.00,559000000.00,"Largest model is denoted ImageNet 512, has 559M parameters",,,,,,NVIDIA V100,,,,,"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.",2024-05-21 01:10,Anonymous,,,,,,,,ADM,,,,Industry,,,,,Industry,,,
DeBERTa,Language,,2021-06-10,"MIT

https://github.com/microsoft/DeBERTa",Open source,,Open source,https://arxiv.org/abs/2006.03654,Microsoft,6.00000000001e+21,"From section 5.1.1: ""We use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K batch size and 1M steps takes about 20 days."" 

This specifically refers to the largest models referred to in the paper, and smaller models are described elsewhere, but I'm assuming the large models are what we care about here. 

Apparently there are multiple types of GPUs referred to as V100s. I'm guessing these are NVIDIA Tesla SMX2s.",United States of America,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen","Highly cited,SOTA improvement","""DeBERTa significantly outperforms all existing PLMs of similar size on MNLI and creates a new state of the art""",DeBERTa: Decoding-enhanced BERT with Disentangled Attention,"Wikipedia,CC-Stories,OPENWEBTEXT,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","We pre-train our large models following the setting of BERT (Devlin et al., 2019), except that we use the BPE vocabulary of Radford et al. (2019); Liu et al. (2019c). For training data, we use Wikipedia (English Wikipedia dump3; 12GB), BookCorpus (Zhu et al., 2015) (6GB), OPENWEBTEXT (public Reddit content (Gokaslan & Cohen, 2019); 38GB), and STORIES (a subset of CommonCrawl (Trinh & Le, 2018); 31GB). The total data size after data deduplication (Shoeybi et al., 2019) is about 78G",15600000000,""" DeBERTa is pretrained on 78G training data""

1GB ~ 200M words",,,,1667.00,1500000000.00,"""...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters""

Other versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).",,,,240.0,20 days,NVIDIA V100,Self-supervised learning,,,Industry,"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",2024-04-30 14:30,Robi Rahman,,,,,,96,,DeBERTa,,,,Industry,,,,,Industry,,,
MosaicML Diffusion,Image generation,"Image generation,Text-to-image",2023-04-28,"Model weights not released, training code is open source with Apache 2.0 license",Unreleased,,,"https://www.databricks.com/blog/stable-diffusion-2
https://www.databricks.com/blog/diffusion
https://github.com/mosaicml/diffusion",Databricks,5.9e+21,"21,000 A100-hours * 7.80E+13 FLOP/GPU-sec * 3600 sec/hour = 5.9e21

Uses automatic mixed precision, which uses half precision (fp16) in most layers, but fp32 in a few numerically unstable layers like normalization and softmax. Unlike original stable diffusion 2, used half precision LayerNorm and GroupNorm layers.",United States of America,"Mihir Patel, Erica Ji Yuen, Cory Stephenson, Landan Seguin",,Included due to high degree of transparency in training process,Training Stable Diffusion from Scratch for <$50k with MosaicML,LAION,"We trained on a subset of LAION-5B that includes samples with English-only captions and an aesthetic score of 4.5+. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 million image-caption samples. For the second phase of training, we only used images with resolution >=512x512, amounting to 300 million image-caption samples.",790000000,"For the first phase of training, we used all images with resolution >=256x256, amounting to 790 million image-caption samples. For the second phase of training, we only used images with resolution >=512x512, amounting to 300 million image-caption samples.

Note that second phase data is a strict subset of first stage.",Unverified,,,,1289952427.00,"Manually loaded model with following code snippet:

from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
import torch

repo_id = ""stabilityai/stable-diffusion-2""
pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=""fp16"")

n_params_vae = sum(p.numel() for p in pipe.components['vae'].parameters())
n_params_text_encoder = sum(p.numel() for p in pipe.components['text_encoder'].parameters())
n_params_unet = sum(p.numel() for p in pipe.components['unet'].parameters())
n_params = n_params_vae + n_params_text_encoder + n_params_unet

print(f""Total number of parameters: {n_params}"")",,,,163.0,Table 1 at https://www.databricks.com/blog/diffusion,NVIDIA A100,Self-supervised learning,$35624.00,"$41,710 in April 2023 –> $35,624 in 2020 dollars",Industry,"We've replicated Stable Diffusion 2 for less than $50k, and we've open-sourced the training code so you can too! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, making training large-scale diffusion models from scratch more accessible than ever before.",2024-04-24 16:50,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
PULI GPTrio,Language,Chat,2023-08-23,License: cc-by-nc-4.0 (non commercial),Open access (non-commercial),Unreleased,Unreleased,https://link.springer.com/chapter/10.1007/978-3-031-40498-6_9; https://huggingface.co/NYTK/PULI-GPTrio,Hungarian Research Centre for Linguistics,5.8e+21,"8 A100s for three months

8 * 312 trillion * 24 * 3600 * 90 * 0.3 (utilization assumption) = 5.8e21",Hungary,"Zijian Győző Yang, László János Laki, Tamás Váradi & Gábor Prószéky ",,,Mono- and Multilingual GPT-3 Models for Hungarian,,"Mix of Hungarian, English, and Chinese text",214000000000,"adding up column in Table 2.

might be slightly off because it's counting non-Chinese tokens in the Chinese data, rather than non-Chinese words, but close.",Likely,,,,6700000000.00,6.7B,,,,2200.0,3 months,NVIDIA A100,,,,,"In recent years, the growth in size of Transformer-based language models has accelerated significantly. Global technology companies are training larger and larger models that require enormous resources and training data. With these experiments, they aim to demonstrate that sufficiently large models with abundant training data can solve any natural language processing task even without fine-tuning. It may not be feasible to compete directly in this race, but there is an opportunity to conduct experiments in the direction of larger models in their shadow. Our aim is to train large language models for Hungarian. According to the knowledge transfer researches, a language model can adapt valuable knowledge from other languages. Furthermore, in order for the model to be able to solve translation tasks, it also needs multilingual knowledge. In our research, we trained a Hungarian monolingual and a Hungarian-English-Chinese trilingual 6.7 billion parameter GPT language model with more than 1TB text data. In our experiments, we also fine-tuned our model with the prompts provided by the Stanford Alpaca dataset. Thus, employing this methodology, an instruct GPT was built, which, as far as we know, is the first multilingual large language model in this region that can follow instructions.",2024-04-11 16:15,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,,,
HuBERT,Speech,Speech recognition,2021-07-27,"https://github.com/facebookresearch/fairseq/tree/main/examples/hubert

fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.",Open source,,Open source,https://arxiv.org/abs/2106.07447,Facebook AI Research,5.54e+21,"GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s

Numbers from Section IV part C
0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU",United States of America,"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed","Highly cited,SOTA improvement","Abstract: 
"" the
HuBERT model either matches or improves upon the state-ofthe-art wav2vec 2.0 performance on the Librispeech (960h) and
Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and
960h fine-tuning subsets.""",HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,"LibriSpeech,LibriLight",,820800000,"""When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.""

1h ~ 13,680 words
13,680 * 60,000 = 820800000",,,,1498.00,1000000000.00,"From abstract:
""Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets""",,,,,,,Self-supervised learning,$8632.11,,Industry,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",2024-04-11 13:25,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
M6-10T,"Multimodal,Language,Vision",Language modelling/generation,2021-10-08,,,,,https://arxiv.org/abs/2110.03888,Alibaba,5.53e+21,"512 GPUs in 10 days - using NVIDIA V100 GPUs

Using the NVIDIA V100 Specifications this works out to be: 
0.30 * 125E12 * 512 * 10 * 86400 = 1.66E22

(Assuming 30% utilisation, and 125 TFLOPS)",China,"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang",,,M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,"""BookCorpus (BooksCorpus, Toronto Book Corpus)"",English Wikipedia",,8000000000,"""We conduct experiments for pretraining and finetuning to analyze model competence in upstream and
downstream tasks. Following the classical data setup for pretraining and finetuning, we pretrain the model on BookCorpus [52] and English Wikipedia [9], which are corpora with around 16GB of plain
texts.""

I used http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html for the conversion to number of words",,,,29.00,10000000000000.00,"""We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days""",,,,,512 GPUs * 10 days * 24 h/day,,Self-supervised learning,$20073.49,,Industry,"Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called ""Pseudo-to-Real"" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.",2024-05-13 09:37,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
M6-T,"Multimodal,Language,Vision","Chat,Image captioning",2021-03-05,,Unreleased,,Unreleased,https://arxiv.org/abs/2105.15082,Alibaba,5.5e+21,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,China,"An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang",SOTA improvement,"Improves on hardware SOTA for similar problems

Abstract: 
""We push the model
scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs [11; 6] on 2048 TPU cores.""",M6-T: Exploring Sparse Expert Models and Beyond,M6-Corpus,,1900000000000,Images,Likely,,,76.00,1000000000000.00,"Section 4, pg 8:
""Due to limited computational resources, we attempt to figure out solutions to implement a 1-trillion-parameter model on solely 480 NVIDIA V100-32GB GPUs.""",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,,Industry,"Mixture-of-Experts (MoE) models can achieve promising results with outrageous large amount of parameters but constant computation cost, and thus it has become a trend in model scaling. Still it is a mystery how MoE layers bring quality gains by leveraging the parameters with sparse activation. In this work, we investigate several key factors in sparse expert models. We observe that load imbalance may not be a significant problem affecting model quality, contrary to the perspectives of recent studies, while the number of sparsely activated experts k and expert capacity C in top-k routing can significantly make a difference in this context. Furthermore, we take a step forward to propose a simple method called expert prototyping that splits experts into different prototypes and applies k top-1 routing. This strategy improves the model quality but maintains constant computational costs, and our further exploration on extremely large-scale models reflects that it is more effective in training larger models. We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores. The proposed giant model achieves substantial speedup in convergence over the same-size baseline.",2024-05-13 09:47,Robi Rahman,,,,,,480,,M6-T,,,,Industry,,,,,Industry,,,
MSA Transformer,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2021-02-13,MIT: https://github.com/facebookresearch/esm,Open source,Open source,Open source,https://proceedings.mlr.press/v139/rao21a/rao21a.pdf,"Facebook AI Research,UC Berkeley,New York University (NYU)",5.49e+21,"Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0

Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8

Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp","United States of America,United States of America,United States of America","Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives",SOTA improvement,"""The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models""",MSA Transformer,"UniRef50,UniRef30 (FKA UniClust30)","""Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.""",26000000,"""Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.""",Likely,,,366.00,100000000.00,"""We train an MSA Transformer model with 100M parameters..."" ",,,,,,NVIDIA Tesla V100 DGXS 32 GB,,,,,"Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models. ",2024-04-16 08:22,Anonymous,,,,,,32,,MSA Transformer,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Gato,"Multimodal,Robotics,Games,Language","Atari,Image captioning,Block stacking,Chat",2022-05-12,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2205.06175,DeepMind,5.44e+21,256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.5 utilization = 5.44e21 FLOPs,United Kingdom of Great Britain and Northern Ireland,"Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas",SOTA improvement,"SOTA at Meta-World MT50 tasks (96.6%) page 14, section 5.5",A Generalist Agent,,,,,,,,530.00,1180000000.00,"""This section focuses on in-simulation evaluation.
Figure 10 compares the full 1.18B parameter Gato"" p.10",,,,96.0,4 days,Google TPU v3,,$6781.08,,Industry,"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",2024-05-13 22:08,Robi Rahman,,,,,,256,,Gato,,,,Industry,checked,,,,Industry,,,
Mamba-2.8B,Language,Language generation,2023-12-01,"Apache 2.0 for model.

Dataset is The Pile, which doesn't have a clear license",Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/2312.00752,"Carnegie Mellon University (CMU),Princeton University",5.400000000000001e+21,"""Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models.""

3b * 300b * 6 = 5.4e21

Note: this is a new architecture so not sure how well 6*params*data works as a heuristic

Figure 4 shows perplexity curves where Mamba is trained up to 2e20 FLOP, but those are for the 125M and 1.3B variants.","United States of America,United States of America","Albert Gu, Tri Dao",,,Mamba: Linear-Time Sequence Modeling with Selective State Spaces,The Pile,,,,Likely,,,277.00,2800000000.00,"2.8B

https://github.com/state-spaces/mamba",,,,,,,,,,,"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",2024-05-01 09:22,Epoch AI,,0,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ContextNet + Noisy Student,Speech,Speech recognition,2020-01-19,,Unreleased,,Unreleased,https://arxiv.org/abs/2005.09629v2,Google,5.4e+21,"""Five generations of models numbered 0 to 4, are trained,
where the baseline model is taken to be the generation-zero
model. The baseline ContextNet model has the same encoder
as CN-2, but has a one-layer RNN decoder with dimension 640.
Meanwhile, CN-w with w=1.25, 1.75, 2.25 and 2.5 have been
set to be the ASR model from generation 1 through 4. Each
generation is trained on 128 Google Cloud TPU chips for 1-3
days""

Roughly assuming each generation is an average of 2 days. The TPU version is likely v3 given this is a 2020 paper.

we get 128 * 10 * 24 * 3600 * 123 tflops * 0.4  (assumed utilization) = 5.4e21",United States of America,"Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, Quoc V. Le",SOTA improvement,"""We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%)""",Improved Noisy Student Training for Automatic Speech Recognition,"LibriSpeech,LibriLight",,,,Likely,,,217.00,,,,,,240.0,roughly 10 days,Google TPU v3,,,,,"Recently, a semi-supervised learning method known as ""noisy student training"" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).",2024-04-22 16:02,Anonymous,,,,,,,,ContextNet + Noisy Student,,,,Industry,,,,,Industry,,,
ZymCTRL,Biology,Protein generation,2022-12-01,,,,,https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf,"Basecamp Research,Friedrich-Alexander-Universität,University of Girona",5.05e+21,"""We trained for 179,000 steps on 48 NVIDIA A100s 80GB for about 15,000 GPU hours""

15000  * 3600 * 312 teraFLOPS * 0.3 (utilization assumption) = 5.05e21","United Kingdom of Great Britain and Northern Ireland,Germany,Spain","Geraldene Munsamy, Sebastian Lindner, Philipp Lorenz, Noelia Ferruz",,,ZymCTRL: a conditional language model for the controllable generation of artificial enzymes,BRENDA,"""ZymCTRL was trained on the BRENDA database, a dataset of 37M enzyme sequences classified according to their enzymatic class""",,,Likely,,,8.00,738000000.00,"""ZymCTRL contains 36 layers totalling 738M parameters""",8.00,,,,,NVIDIA A100,,,,,"The design of custom-tailored proteins has the potential to provide novel and
groundbreaking solutions in many fields, including molecular medicine or environmental sciences. Among protein classes, enzymes are particularly attractive because their complex active sites can accelerate chemical transformations by several orders of magnitude. Since enzymes are biodegradable nanoscopic materials, they hold an unmatched promise as sustainable, large-scale industrial catalysts. Motivated by the enormous success of language models in designing novel yet nature-like proteins, we hypothesised that an enzyme-specific language model could provide new opportunities to design purpose-built artificial enzymes. Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods. We release the model to the community.",2024-04-01 09:28,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Pythia-2.8b,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",5.038000000001e+21,https://www.wolframalpha.com/input?i=6+FLOP+*+2.8+billion+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,2800000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-2.8b,1,,,,64,0.3150,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,14240,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
LongNet,Language,Language modelling/generation,2023-07-05,,,,,https://arxiv.org/abs/2307.02486,"Microsoft,Xi’an Jiaotong University",4.86e+21,"2.7B params * 300B tokens * 6 = 4.86e21

Note: not sure if there are very long sequences in the training data that would affect this calculation. Per paper, complexity of their attention mechanism scales linearly with sequence length.","United States of America,China","Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei
",,Very long sequence length (1b),"LongNet: Scaling Transformers to 1,000,000,000 Tokens",The Stack,,,,Likely,,,56.00,2700000000.00,2.7B,,,,,,,,,,,"Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",2024-04-01 09:53,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
NÜWA,"Multimodal,Vision,Image generation,Video,Language","Image generation,Video generation",2021-11-24,https://github.com/microsoft/NUWA,Unreleased,,Unreleased,https://arxiv.org/abs/2111.12417,"Microsoft Research,Peking University",4.8384e+21,"From AI Tracker:
""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". Info sheet from NVIDIA (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) gives single precision TensorFloat 32 performance of 156 TFLOPs/s. So we get 64 x 14 x 156 = 140,000 TFLOPs/s x days.""

Multiply by seconds/day and 30% utilization","United States of America,China","Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",SOTA improvement,"""NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc""",NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,"Conceptual Captions (CC3M),Moments in Time,VATEX",,,"we first pre-train N  ̈UWA on three
datasets: Conceptual Captions [22] for text-to-image (T2I)
generation, which includes 2.9M text-image pairs, Mo-
ments in Time [26] for video prediction (V2V), which in-
cludes 727K videos, and VATEX dataset [43] for text-to-
video (T2V) generation, which includes 241K text-video
pairs.",,,,212.00,870000000.00,Section 4.1,,,,,,,Self-supervised learning,$10446.84,,Industry,"This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.",2024-05-13 09:19,Robi Rahman,,,,,,,,NÜWA,,,,"Industry,Academia",,,,,"Industry,Academia",,,
SEER,Vision,"Image embedding,Image classification",2021-07-29,"https://github.com/facebookresearch/vissl/tree/main/projects/SEER

We share instructions on how to train SEER model on GPUs using PyTorch. First, Install VISSL and follow the data setup instructions to easily setup your data input with VISSL.

https://github.com/facebookresearch/vissl/blob/main/projects/SEER/MODEL_LICENSE.md",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2103.01988,"Facebook AI Research,INRIA",4.42e+21,"Numbers from section 3.2

512 GPUs * 0.1 * 8days * 24h/day * 3600s/h * 125 TFLOP/s","United States of America,France","Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",SOTA improvement,"SOTA for self-supervised models on ImageNet, which seems fair to consider a different benchmark than ImageNet for supervised models.

""Our final SElf-supERvised (SEER) model,
a RegNetY with 1.3B parameters trained on 1B random
images with 512 GPUs achieves 84.2% top-1 accuracy,
surpassing the best self-supervised pretrained model by 1%""",Self-supervised Pretraining of Visual Features in the Wild,Instagram,"Section 3.3:
""For our billion scale pretraining, we consider a dataloader that directly samples random, public, and non-EU images from Instagram""

Note the dataset is not static - it is refreshed every 90 days",1000000000,"""Overall, we train
on 1B images for a total of 122K iterations.""",,,,220.00,1300000000.00,"From abstract:
"" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...""",,,,192.0,8 days,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$16058.80,,Industry,"Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL",2024-05-01 09:13,Robi Rahman,,,,,,512,,SEER,,,,"Industry,Academia",,,,98304,"Industry,Academia",,,
AlphaFold-Multimer,Biology,"Protein folding prediction,Proteins",2021-10-04,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

https://github.com/google-deepmind/alphafold",Open source,,Open source,https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1,"Google DeepMind,DeepMind",4.3499999999999995e+21,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""

Assuming: FP16 and utilization 0.4

Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs","Multinational,United Kingdom of Great Britain and Northern Ireland","Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper and Demis Hassabis","Highly cited,SOTA improvement","""On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2])""

""For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively""

""For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances""",Protein complex prediction with AlphaFold-Multimer,PDB (Protein Data Bank),"""The training dataset comprised structures from the Protein Data Bank (PDB) [13] with a maximum release date of 2018-04-30"" [2.5. Training Regimen]",147328,See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018,Confident,,,1314.00,,"""Multiple changes to the AlphaFold system were made to adapt it to training on protein complexes, which are detailed below. Summarizing briefly, we [...] make various small adjustments to the structure losses and the model architecture."" [2. Methods]

Hence, this will have approximately the same amount of parameters as AlphaFold2",,,,384.0,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""",Google TPU v3,,,,,"While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances.",2024-04-16 07:05,Anonymous,,,AlphaFold 2,,,64,,AlphaFold-Multimer,,,,"Industry,Industry",,,,24576,"Industry,Industry",,,
GPT-2 (1.5B),Language,,2019-02-14,"modified MIT
https://github.com/openai/gpt-2?tab=License-1-ov-file#readme",Open source,Unreleased,Unreleased,https://openai.com/blog/better-language-models/,OpenAI,4.3e+21,"We use COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT* N EPOCHS * N TOKENS IN TRAINING DATASET

The number of epochs is not reported, but this other paper [1] claims in table 1 that it is 20 or 100 epochs. 100 epochs is consistent with the original GPT paper. 

40GB dataset is 8B words, or 1/0.75 * 8B = 10.66B tokens.

6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e21
6 * (40 * 200 million * 1/0.75 * 100) * 1.5 billion parameters = 9.6e21

Geometric mean is 4.29e21

[1] https://arxiv.org/abs/1906.06669",United States of America,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",Highly cited,,Language Models are Unsupervised Multitask Learners,WebText,,3000000000,"“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”
40GB is approximately 8e9 words.
",,,,15225.00,1500000000.00,"""GPT-2 is a large transformer-based language model with 1.5 billion parameters""",20.00,3400000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,Self-supervised learning,$4692.89,"https://en.wikipedia.org/wiki/GPT-2#:~:text=The%20cloud%20compute%20costs%20for,full%201.5%20billion%20parameter%20model).",Industry,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",2024-04-18 15:52,Robi Rahman,GPT-2 (1542M),,,,,,,GPT-2 (1.5B),,,,Industry,,,,,Industry,,,
XLNet,Language,,2019-06-01,Apache 2.0 for code and weights: https://github.com/zihangdai/xlnet,Open source,Unreleased,Open source,https://arxiv.org/abs/1906.08237,"Carnegie Mellon University (CMU),Google Brain",4.3e+21,"""Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.""

123 teraflops * 5.5 days * 512 * 0.3 utilization (assumption) ~= 9e21 FLOP

Alternatively, 500k steps * batch size 8192 * sequence length 512 = 2.1T training passes. 340 million * 6 * 2 trillion = 4.3e21 FLOP. This suggests very low utilization, ~15%","United States of America,United States of America","Z Yang, Z Dai, Y Yang, J Carbonell",Highly cited,,XLNet: Generalized Autoregressive Pretraining for Language Understanding,"Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","""Following BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.""",,,Likely,,,7041.00,340000000.00,"Same size as BERT-Large, which was 340M",,,,,,,,,,Industry,,2024-04-17 14:49,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
ViT-Huge/14,Vision,Image representation,2020-10-22,"Apache-2.0 license
https://github.com/google-research/vision_transformer",Open source,,Open source,https://arxiv.org/abs/2010.11929,"Google Brain,Google Research",4.262e+21,from Table 6,"United States of America,Multinational","Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",Highly cited,,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"ImageNet-1k,ImageNet21k,JFT-300M","To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and 303M high-resolution images. ",1280000,,Confident,,,20003.00,632000000.00,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,,,,Google TPU v3,,,,Industry,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",2024-04-16 11:45,Robi Rahman,,,,,,,0.3200,ViT-Huge/14,,,,"Industry,Industry",,,,30000,"Industry,Industry",,,
RoBERTa Large,Language,,2019-07-01,"code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md

repo is MIT license",Open source,Unreleased,Open source,https://arxiv.org/abs/1907.11692,"Facebook,University of Washington",4.15383552e+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision.

C=5*1024*3.13E+13*60**2*24*0.3 = 4.2e21","United States of America,United States of America","Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",Highly cited,,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"CC-News,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",WebText2,Wikipedia","""We consider five English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text. We use the following text
corpora:
• BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
• CC-NEWS, which we collected from the English portion of the CommonCrawl News
dataset (Nagel, 2016). The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after filtering).4
• OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText corpus described in Radford et al. (2019). The text
is web content extracted from URLs shared on
Reddit with at least three upvotes. (38GB).5
• STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data filtered to match the story-like style of
Winograd schemas. (31GB).""",32000000000,160GB*200M words/GB = 3.2e10 words,Likely,,,17972.00,355000000.00,,,,"Authors of KEPLER say their model has the same inference compute as RoBERTa, so if we calculate this we may use it for KEPLER, too

""It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.""",120.0,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",NVIDIA Tesla V100 DGXS 32 GB,,,,Industry,,2024-04-24 16:02,Robi Rahman,,0,,,,1024,,RoBERTa Large,,,,"Industry,Academia",,,,122880,"Industry,Academia",,,
ProtGPT2,Biology,"Proteins,Protein generation,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-27,"apache

https://huggingface.co/nferruz/ProtGPT2",Open source,,,https://www.nature.com/articles/s41467-022-32007-7,University of Bayreuth,4.1e+21,"""The model trained on 128 NVIDIA A100s in 4 days""

128 * 4 * 24 * 3600 * 312 trillion FLOP/s * 0.3 = 4.1e21",Germany,"Noelia Ferruz, Steffen Schmidt & Birte Höcker ",,,ProtGPT2 is a deep unsupervised language model for protein design,UniRef50,"""We took Uniref50 version 2021_04 as the dataset for training, containing 49,874,565 sequences""",,,Confident,,,221.00,738000000.00,"""Here, we introduce ProtGPT2, an autoregressive Transformer model with 738 million parameters capable of generating de novo protein sequences in a high-throughput fashion.""",,,,96.0,,NVIDIA A100,Unsupervised,,,Academia,"Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.",2024-04-01 09:35,Anonymous,,,,,,128,,,,,,Academia,,,,12288,Academia,,,
RetNet,Language,Language modelling,2023-07-17,MIT for code,Unreleased,,Open source,https://arxiv.org/abs/2307.08621,"Microsoft Research,Tsinghua University",4.02e+21,C = 6ND = 6 * 6.7 billion * 100 billion,"United States of America,China","Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei",,,Retentive Network: A Successor to Transformer for Large Language Models,,,75000000000,,Likely,4000000,4M,75.00,6700000000.00,"Table 2

They later mention testing the memory and throughput of a 13B-parameter model, but it doesn't sound like they trained it long enough to test its perplexity.",,,,,,,,,,,"In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models.",2024-04-01 09:52,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
EVA-01,Vision,"Image classification,Object detection,Semantic segmentation",2022-11-14,MIT: https://github.com/baaivision/EVA,Open source,Open source,Open source,https://arxiv.org/abs/2211.07636,"Beijing Academy of Artificial Intelligence,Huazhong University of Science and Technology,Zhejiang University,Beijing Institute of Technology",3.7509433344e+21,"flops = (128) * (77.97 * 10**12) * (14.5 * 24 * 3600) * (0.3) = 3.75e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16","China,China,China,China","Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao",SOTA improvement,"from abstract 'Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training.'",EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"ImageNet21k,COCO","from table 3 : ImageNet-21K, CC12M, CC3M, Object365, COCO, ADE",29600000,from table 3: 29.6M images,Likely,,,322.00,1011000000.00,1011M from table 3,150.00,,,348.0,from Table 3 14.5 days = 348 hours,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,,,"We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research,
we release all the code and billion-scale model.",2024-05-01 09:05,Bartosz Podkanowicz,,,,,,128,,EVA,,,,"Academia,Academia,Academia,Academia",,,,44544,"Academia,Academia,Academia,Academia",,,
Hawk,Language,"Language modelling/generation,Chat",2024-02-29,,,,,https://arxiv.org/abs/2402.19427,Google DeepMind,3.668600000000001e+21,"Figure 1.a
10^(5.645/10)*10^21 = 3.66860... × 10^21
",Multinational,"Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre",,,Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,,,300000000000,,Likely,,,,7000000000.00,,,,,,,Google TPU v3,,,,,"Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.",2024-04-19 13:34,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
Unified-IO,"Multimodal,Vision,Language","Object detection,Language modelling/generation,Image generation",2022-06-17,Apache 2.0: https://github.com/allenai/unified-io-inference,Open source,Open source,Open source,https://arxiv.org/abs/2206.08916,"Allen Institute for AI,University of Washington",3.5e+21,"1M steps, batch size 1024. Sequence length may be 128-256:
""We use a maximum of 256 and 128 text tokens for inputs and outputs respectively, and a maximum
length of 576 (i.e. 24 × 24 patch encoding from a 384 × 384 image) for image inputs and 256 (i.e.
16 × 16 latent codes from a 256 × 256 image) for image outputs

6 * 1 million * 1024 * 128 * 2.9 billion = 2.3e21
6 * 1 million * 1024 * 256 * 2.9 billion = 4.6e21
average is 3.5e21

No hardware details.","United States of America,United States of America","Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi",,"Milestone in generality: ""Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning""","Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",,"""we gather 95 vision, language, and multi-modal
datasets from 62 publicly available data sources as targets for our model to learn during multi-task
training""",,,Speculative,,,227.00,2925000000.00,"2952M, Table 2",,,,,,Google TPU v4,,,,,"We propose UNIFIED-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection,
depth estimation and image generation, vision-and-language tasks such as region
captioning and referring expression, to natural language processing tasks such
as question answering and paraphrasing. Developing a single unified model for
such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps,
binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a
single transformer-based architecture, jointly on over 90 diverse datasets in the
vision and language fields. UNIFIED-IO is the first model capable of performing
all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse
benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos
for UNIFIED-IO are available at: unified-io.allenai.org",2024-05-13 09:11,Anonymous,,0,,,,,,,,,,"Research collective,Academia",,,,,"Research collective,Academia",,,
InternLM-XComposer,"Multimodal,Language,Vision","Chat,Visual question answering",2023-09-26,"code is apache 2.0
weights allow for free commercial use, apply for full commercial license
https://github.com/InternLM/InternLM-XComposer",Open access (restricted use),,Open source,https://arxiv.org/abs/2309.15112,InternLM,3.4504704e+21,"flops = (128) * (312 * 10**12) * (80 * 3600) * (0.3) = 3.45e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section A.1 we have 128xA100 used for 80 hours",China,"Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang",,Table 9 and Table 10 suggests that model is better in image selection from GPT4-V based on human preference,InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition,,"WanJuan, CC 3M, SBU-Caption, LAION400M, CC 12M, In-house Concept data, Multimodal C4, TaiSu, WuKong, LAION-CN - form Table 1",64900000000,from appendix A.1  55.6B English tokens and 22.1B Chineese tokens and 1.1B images so 0.75*55.6e9+22.1e9+1.1e9 = 64900000000.0,Likely,,,55.00,7000000000.00,7B from https://huggingface.co/internlm/internlm-xcomposer-vl-7b,,,,80.0,from appendix A.1,NVIDIA A100,,,,,"We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition. The innovative nature of our model is highlighted by three appealing properties: 1) Interleaved Text-Image Composition: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. Simply provide a writing instruction, and our system will generate the corresponding manuscript. It can intelligently identify the areas in the text where images would enhance the content and automatically insert the most appropriate visual candidates. 2) Comprehension with Rich Multilingual Knowledge: The text-image comprehension is empowered by training on an extensive multi-modal multilingual database with carefully crafted strategies, resulting in a deep understanding of visual content. 3) State-of-the-art Performance: Our model consistently achieves state-of-the-art results across various mainstream benchmarks for vision-language foundational models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, CCBench (Chinese Cultural Benchmark), QBench and Tiny LVLM. Owing to the absence of established metrics for quantitatively assessing text-image composition, we have devised a robust evaluation procedure that comprises both human and GPT4-Vision (GPT4-V) to ensure reliability. Notably, our InternLM-XComposer achieves competitive text-image composition scores compared to public solutions, including GPT4-V and GPT3.5. Collectively, InternLM-XComposer seamlessly blends advanced text-image comprehension and composition, revolutionizing vision-language interaction and offering new insights and opportunities. The InternLM-XComposer model series are publicly available at this https URL. ",2024-05-13 08:32,Bartosz Podkanowicz,,,,,,128,,,,,,,,,,10240,,,,
ViT-G/14,Vision,Image classification,2021-06-08,"About the weights: https://twitter.com/giffmana/status/1402507421029916672

About the code: Apache 2.0
https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/scaling_laws/train_vit_g.py",Unreleased,,Open source,https://arxiv.org/abs/2106.04560,"Google Brain,Google Research",3.4e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb


Alternatively: per paper, ViT-G required between 20-30k TPUv3 core-days to train (from eyeballing the tick marks in Figure 9).

TPUv3 is 123 teraflop/s per chip, 2 cores per chip

123 trillion * (1/2) * 25,000 * 3600 * 0.4 = 2.2e21

","United States of America,Multinational","X Zhai, A Kolesnikov, N Houlsby, L Beyer",SOTA improvement,"""we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy""",Scaling Vision Transformers,"JFT-3B,ImageNet","We trained a large Vision Transformer, ViT-G/14, which
contains nearly two billion parameters. Section 3.6 details
the architecture’s shape. We evaluate the ViT-G/14 model on
a range of downstream tasks, and compare it to recent stateof-the-art results. We fine-tune on ImaegNet",3000000000,"""For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used
in many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists of
nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic
pipeline""",Confident,,,744.00,1843000000.00,Table 2 of paper,,,,,,Google TPU v3,Self-supervised learning,$5541.84,,Industry,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",2024-05-21 04:24,Robi Rahman,,,,,,,,ViT-G/14,,,,"Industry,Industry",,,,,"Industry,Industry",,,
ViT-G (model soup),Vision,Image classification,2022-03-10,no license,Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2203.05482v3,"University of Washington,Columbia University,Google,Meta AI,Tel Aviv University",3.4e+21,"This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon. Fine-tuning compute is likely minor in comparision.","United States of America,United States of America,United States of America,United States of America,Israel","Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt",SOTA improvement,"""When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art.""",Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,,,,,Likely,,,463.00,1843000000.00,This is from the original ViT-G paper,,,,,,,,,,,"The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results ""model soups."" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at this https URL.",2024-05-01 09:22,Anonymous,,,,,,,,ViT-G (model soup),,,,"Academia,Academia,Industry,Industry,Academia",,,,,"Academia,Academia,Industry,Industry,Academia",,,
Rita-XLarge,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-14,"MIT license
https://github.com/lightonai/RITA",Open source,,Unreleased,https://arxiv.org/abs/2205.05789,"LightOn,Harvard University,University of Oxford",3.4e+21,"""The models were trained for a total training time of over 25 thousand Nvidia-V100 GPU hours""

125 teraFLOP/s (uncertain which V100 model, tensor performance varies from 112-130tFLOP/s) * 25000 * 3600 * 0.3 (utilization) = 3.4e+21""","France,United States of America,United Kingdom of Great Britain and Northern Ireland","Daniel Hesslow, Niccolo Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks ",,,RITA: a Study on Scaling Up Generative Protein Sequence Models,"UniRef100,MGnify,Metaclust","""We focus on three different pre-training corpora: UniRef100 (The UniProt Consortium, 2020), MGnify (Mitchell et al., 2020) and Metaclust (Steinegger & Soding ¨ , 2018), each providing a sufficient amount of tokens for model pretraining without having to repeat the data.""",,,Likely,,,49.00,1200000000.00,"""with up to 1.2 billion parameters""",,,,,,NVIDIA V100,,,,,"In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid pre- diction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",2024-04-01 09:03,Anonymous,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
TransformerXL+RelationLM,Language,,2022-01-24,,Unreleased,,Unreleased,https://arxiv.org/abs/2201.09680,"DeepMind,University of Oxford",3.2e+21,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Qi Liu, Dani Yogatama, Phil Blunsom",,,Relational Memory-Augmented Language Models,WikiText-103,,,,,,,26.00,124000000.00,,,,,,,,,,,,,2024-05-06 14:33,Robi Rahman,TransformerXL+RelationLM,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
ELECTRA,Language,Text autocompletion,2020-03-23,Apache 2.0: https://github.com/google-research/electra,Open source,,Open source,https://arxiv.org/abs/2003.10555v1,"Stanford University,Google,Google Brain",3.0999999999999995e+21,"Table 8: ""ELECTRA-1.75M"" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.","United States of America,United States of America,United States of America","Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",Highly cited,,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,"""BookCorpus (BooksCorpus, Toronto Book Corpus)"",Wikipedia,ClueWeb,Gigaword","""For most experiments we pre-train on the same data as BERT, which consists
of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large
model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT
dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and
Gigaword (Parker et al., 2011).""",25000000000,"33B tokens or ~25B words

""For most experiments we pre-train on the same data as BERT, which consists
of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large
model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT
dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and
Gigaword (Parker et al., 2011).""",,,,2968.00,335000000.00,https://github.com/google-research/electra,,79000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,table 1,,Self-supervised learning,,,Industry,"Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",2024-04-23 10:15,Robi Rahman,,,,,,,,ELECTRA,,,,"Academia,Industry,Industry",,,,,"Academia,Industry,Industry",,,
Incoder-6.7B,Language,Code generation,2023-04-09,"CC-BY-NC 4.0 (non commercial)

data is open: ""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.""",Open access (non-commercial),Open source,Open access (non-commercial),https://arxiv.org/abs/2204.05999,"Facebook AI Research,University of Washington,UC Berkeley,Carnegie Mellon University (CMU),Toyota Technological Institute at Chicago",3.00001e+21,"per table 5, required 3 zettaflop (3e21) to train.

also, ""INCODER-6.7B was trained on 248 V100 GPUs for 24 days""

hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.
","United States of America,United States of America,United States of America,United States of America,United States of America","Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis",SOTA improvement,"""Zero-shot infilling with bidirectional context substantially outperforms approaches based on left-to-right-only models, and on several tasks
obtains performance comparable to state-of-the-art models fine-tuned on the tasks""",InCoder: A Generative Model for Code Infilling and Synthesis,,"Code from GitHub and StackOverflow

""To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.
Our primary focus in this paper is on the Python language, but we also include code files from
28 total languages and StackOverflow content from all available languages.""",,"216 GB: ""Our final pre-training corpus contains a total of 159 GB of code, 52 GB of it
in Python, and a total of 57 GB of content from StackOverflow""",Confident,,,365.00,6700000000.00,6.7B,1.00,,,576.0,24,NVIDIA V100,,,,,"Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. this https URL",2024-05-01 09:24,Anonymous,,0,,,,,,Incoder-6.7B,,,,"Industry,Academia,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia,Academia",,,
AlphaFold 2,Biology,"Protein folding prediction,Proteins",2020-11-30,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

https://github.com/google-deepmind/alphafold",Open source,,Open source,https://www.nature.com/articles/s41586-021-03819-2,DeepMind,2.99e+21,"123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4

""Training regimen"" section: 
""We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU
core, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.""",United Kingdom of Great Britain and Northern Ireland,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.","Historical significance,Highly cited","""Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known"" [Abstract]

>17790 citations",Highly accurate protein structure prediction with AlphaFold,"PDB (Protein Data Bank),UniRef30 (FKA UniClust30),UniRef90,MGnify,BFD (Big Fantastic Dataset),UniProtKB","""Inputs and data sources"" section:
""The following versions of public datasets were used in this study. Our models were trained on a copy of the PDB downloaded on 28 August 2019. For finding template structures at prediction time, we used a copy of the PDB downloaded on 14 May 2020, and the PDB70 clustering database downloaded on 13 May 2020. For MSA lookup at both training and prediction time, we used Uniref90 v.2020_01, BFD, Uniclust30 v.2018_08 and MGnify v.2018_12. For sequence distillation, we used Uniclust30 v.2018_08 to construct a distillation structure dataset. Full details are provided in Supplementary Methods 1.2.""

AlphaFold needs multiple genetic (sequence) databases to run:

BFD,
MGnify,
PDB70,
PDB (structures in the mmCIF format),
PDB seqres – only for AlphaFold-Multimer,
UniRef30 (FKA UniClust30),
UniProt – only for AlphaFold-Multimer,
UniRef90",530000,"3 different types of input data to the network:
(1) Amino acid sequence
(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins
(3) Template structures (3D atom coordinates of homologous structures), where available

Training data is processed into the following two datasets that are sampled with different probabilities. 
Supplementary Material, Section 1.2.4. Training data:
""With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank""

Supplementary Material, Section 1.3 Self-distillation dataset:
""This gives a final dataset of 355,993 sequences"". An initial model was used to predict structures for these sequences.

PDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures
172788

Therefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]",,,,17790.00,,,,,,264.0,7 days pretrain and 4 days finetune,Google TPU v3,,,,Industry,"Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’—has been an important open research problem for more than 50 years. Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",2024-04-16 06:28,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DeciCoder-1B,Language,Code generation,2023-08-15,"Apache 2.0

data is StarCoder, license unclear",Open source,,Unreleased,https://huggingface.co/Deci/DeciCoder-1b,Deci AI,2.9e+21,446b * 1.1b * 6 = 2.9e21,Israel,DeciAI Research Team,,, Model Card for DeciCoder-1b,StarCoder,"""DeciCoder was trained on StarCoder Training Dataset, filtered for Python, Java, and Javascript code""",446000000000,Total Tokens: 446B ,Confident,,,,1100000000.00,"1.1B, per model card https://huggingface.co/Deci/DeciCoder-1b",,,,,,,,,,,"DeciCoder 1B is a 1 billion parameter decoder-only code completion model trained on the Python, Java, and Javascript subsets of Starcoder Training Dataset. The model uses Grouped Query Attention and has a context window of 2048 tokens. It was trained using a Fill-in-the-Middle training objective. The model's architecture was generated by Deci's proprietary Neural Architecture Search-based technology, AutoNAC.",2024-05-20 11:19,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Adaptive Agent,Games,,2023-01-18,,,,,https://arxiv.org/abs/2301.07608,DeepMind,2.8e+21,"""AdA was implemented using JAX (Bradbury et al., 2018) and the DeepMind JAX Ecosystem (Babuschkin et al., 2020) and trained on 64 Google TPUv3 devices. The wall-clock time for training this version of AdA from scratch was approximately 5 weeks: 1 week to train the teacher, and 4 weeks to train AdA""

64 * 123 teraflop/s * 35 days * 24 * 3600 * 0.4 = 9.5e21

This might be for all single-agent experiments in the paper, or just for the 76M model in Table D.1, I'm not sure.

In Table E.2, the 533M-param model takes 2e20 FLOP to go through 5B learner steps, and was trained on 70B steps in total (Table 1). That would be 2.8e21 for 70B steps. That might be an underestimate because there are also teacher(?) steps.",United Kingdom of Great Britain and Northern Ireland,"Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, Lei Zhang",,,Human-Timescale Adaptation in an Open-Ended Task Space,,"RL in XLand 2.0 task space, ""an environment supporting procedural generation of diverse 3D worlds and
multi-player games""",,,Speculative,,,61.00,533000000.00,Table D.9,,,,840.0,5 weeks. Possible that this is for multiple models,Google TPU v3,Reinforcement learning,,,,"Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.",2024-04-01 09:35,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
CodeT5-large,Language,Code generation,2022-07-05,https://github.com/salesforce/CodeT5?tab=BSD-3-Clause-1-ov-file#readme,Open source,,,https://arxiv.org/abs/2207.01780,Salesforce,2.72e+21,"""We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days""

16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21",United States of America,"Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi ",SOTA improvement,"""Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""",CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning,,"""We enlarge the Python pretraining dataset using the recently released
large-scale Github Code dataset5. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code (e.g. “mit”, “apache-2”, “bsd-3-clause”, “bsd-2- 126clause”, “cc0-1.0”, “unlicense”, “isc”). The resulting Python dataset (GCPY) has 10.5B tokens and is 10x larger than the CodeSearchNet (CSN) corpus [Husain et al., 2019] used in the original CodeT5 [Wang et al., 2021]""",,10.5b tokens,Likely,,,127.00,770000000.00,"""We pretrain a CodeT5-large model (770M) from scratch following T5-large’s architecture""",150.00,,,504.0,21 days,NVIDIA A100,,,,,"""Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose ""CodeRL"", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.""",2024-05-01 09:22,Anonymous,,,,,,,,CodeT5-large,,,,Industry,,,,,Industry,,,
Emu (BAAI),"Vision,Multimodal,Language","Image generation,Text autocompletion",2023-07-11,,,,,https://arxiv.org/abs/2307.05222,"Beijing Academy of Artificial Intelligence,Tsinghua University,Peking University",2.70000000001e+21,"""We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.""
https://www.wolframalpha.com/input?i=128*312+TFLOPS+*+2+days+*+0.4","China,China,China","Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang",,,Generative Pretraining in Multimodality,"LAION-2B,WebVid-10M,LAION-COCO,BLIP,YT-Storyboard-1B",,,,Confident,,,,14000000000.00,"""The total number of parameters of Emu is 14B and is trained end-to-end.""",,,,48.0,"""We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.""",NVIDIA A100 SXM4 80 GB,,,,,"We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",2024-02-29 16:08,Robi Rahman,,,,,,128,,,,,,"Academia,Academia,Academia",,,,6144,"Academia,Academia,Academia",,,
Ankh_base,Biology,"Protein generation,Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2023-01-16,"cc non-commercial:
https://github.com/agemagician/Ankh/blob/main/LICENSE.md",Open access (non-commercial),,,https://arxiv.org/abs/2301.06568,"Technical University of Munich,Columbia University",2.6e+21,Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf,"Germany,United States of America","Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",SOTA improvement,"""On average, Ankh improved the PLM SOTA performance by 4.8%""",Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling,UniRef50,"""We build upon the same results by pre-training our baseline on UniRef50.""",,"45M proteins and 14B amino acids, per Table 2",Likely,,,31.00,450000000.00,See figure 1,68.00,,,,,Google TPU v4,,,,,"As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",2024-05-01 09:22,Epoch AI,,,,,,,,Ankh_base,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Pythia-1.4b,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",2.5190000000001e+21,https://www.wolframalpha.com/input?i=6+FLOP+*+1.4+billion+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,1400000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-1.4b,1,,,,64,,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,7120,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
ALBERT-xxlarge,Language,,2020-02-09,Apache 2.0 code/weights: https://github.com/google-research/ALBERT,Open source,,Open source,https://arxiv.org/abs/1909.11942,"Toyota Technological Institute at Chicago,Google",2.39e+21,"32 hours of training
512 TPU V3s
0.33 utilization rate
","United States of America,United States of America","Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",Highly cited,,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.,"Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","""To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. W""",3300000000,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,,,5199.00,235000000.00,,,2500000000000.00,Source: https://github.com/amirgholami/ai_and_memory_wall,32.0,,Google TPU v3,Self-supervised learning,$5924.43,,Industry,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",2024-04-23 09:51,Robi Rahman,,,,,,512,,ALBERT-xxlarge,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Prithvi-100M,Earth science,,2023-11-08,https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M,Open source,,Unreleased,https://arxiv.org/abs/2310.18660,"IBM,NASA",2.299133952e+21,"compute hardware: 
0.3*311.84 (peak TFLOPS)*384 (s per epoch)*64 (GPUs)*1000 (epochs) *(10^12) = 2299133952000000000000 = 2.299133952×10^21","United States of America,United States of America","Johannes Jakubik, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi (Steve)Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Ganti, Kommy Weldemariam, Rahul Ramachandran",,,Foundation Models for Generalist Geospatial Artificial Intelligence,HLS / Harmonized Landsat and Sentinel-2,model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset,,"

",Speculative,1024,"We use AdamW optimizer with β1 = 0.9, β2 = 0.999, batch size of 1024, one-cycle cosine learning rate scheduler, with a maximum learning rate of 5e-4. We experimented with ViTbase and ViT-large backbones and trained the models for 1000 epochs. ",,100000000.00,,1000.00,,,,,NVIDIA A100,,,,,"Significant progress in the development of highly adaptable and reusable Artificial Intelligence (AI) models is expected to have a significant impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. This paper introduces a first-of-a-kind framework for the efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation tasks that have not been tackled by previous work on foundation models involving multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compares well against the state-of-the-art, e.g., outperforming a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Finally, due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face.",2024-05-06 09:32,Natalia Martemianova,,,,,,64,,,IBM,,,"Industry,Government",,,,,"Industry,Government",,,
GBERT-Large,Language,"Document classification,Named entity recognition",2020-10-21,MIT: https://huggingface.co/deepset/gbert-large,Open source,,Unreleased,https://arxiv.org/abs/2010.10906,"deepset,Bayerische Staatsbibliothek Muenchen,Munich Digitization Center",2.24446464e+21,"flops = (64) * (123* 10**12) * (11 * 24 * 3600) * (0.3) = 2.24e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1it was trained for 11 days from Table 2","Germany,Germany","Branden Chan, Stefan Schweter, Timo Möller",SOTA improvement,'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.',German's Next Language Model,"Wikipedia,OPUS,OSCAR,OpenLegalData",Table 1 in the paper,27287800000,"163.4GB from Table 1 in the paper
assuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0",Likely,,,215.00,335000000.00,335M from Table 5,,,,264.0,11 days from Table 2,Google TPU v3,,,,Industry,"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. ",2024-04-23 13:02,Bartosz Podkanowicz,,,,,,64,,,,,,Industry,,,,16896,Industry,,,
NASv3 (CIFAR-10),Vision,,2016-11-05,,,,,https://arxiv.org/abs/1611.01578,Google Brain,2.2e+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",United States of America,"Barret Zoph, Quoc V. Le",Highly cited,,Neural Architecture Search with Reinforcement Learning,,,50000,CIFAR-10 (does not factor in augmentation procedures),Likely,,,4781.00,37400000.00,Table 1,,,,,,,,$13069.35,,Industry,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",2024-04-26 16:35,Robi Rahman,,0,,,,800,,NASv3 (CIFAR-10),,,,Industry,,,,,Industry,,,
SantaCoder,Language,Code generation,2023-01-09,"license - commercial, usage restrictions against things like discrimination and misinformation 
https://huggingface.co/spaces/bigcode/license",Open access (restricted use),Open source,Unreleased,https://arxiv.org/abs/2301.03988,"Hugging Face,ServiceNow,Massachusetts Institute of Technology (MIT),Wellesley College,Saama,EleutherAI,Huawei Noah's Ark Lab,Carnegie Mellon University (CMU)",2.1e+21,Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 × 10^21 FLOPs. The final model described in Section 6.2 uses twice the amount of compute.,"Multinational,United States of America,United States of America,United States of America,United States of America,Multinational,China,United States of America","Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra",,"""Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model""",SantaCoder: don't reach for the stars!,The Stack v1.1,"""The base training dataset for the experiments in this paper contains 268 GB of Python, Java and JavaScript files from The Stack v1.1 (Kocetkov et al., 2022) after removing data from optout requests, near-deduplication, PII-redaction (see Section 4)""",,268 GB,Confident,,,94.00,1100000000.00,1.1B,,,,150.0,"Their initial training runs took 3.1 days. The final training run was run for twice as many iterations with ""all other hyper-parameters the same"" and used twice as much compute as this. So likely 6 days or ~150 hours, but they don't explicitly say whether they used the same hardware.",NVIDIA V100,,,,,"The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at this https URL.",2024-05-15 16:31,Anonymous,,0,,,,,,,,,,"Industry,Industry,Academia,Academia,Research collective,Industry,Academia",checked,,,,"Industry,Industry,Academia,Academia,Research collective,Industry,Academia",,,
AraGPT2-Mega,Language,,2020-12-31,"apache-like license:
https://github.com/aub-mind/arabert/blob/master/aragpt2/LICENSE",Open source,,,https://arxiv.org/abs/2012.15520,American University of Beirut,1.9999999999999997e+21,"source: https://github.com/lightonai/akronomicon/blob/10adaca9c74afa7d11f196947e410d248f25abe9/akrodb/American%20University%20of%20Beirut/AraGPT2-Mega.json

Akronomicon uses units of petaflop/s-days. 20 petaflop/s-days ~= 2e21 FLOP.

Our own validation of this estimate is below.

For the Mega model: 9 days on a TPUv3-128, bfloat16 precision  (from author communication)

A TPUv3-128 has 128 cores (you can infer this from footnote 9 on p.4 of the paper - 128 * 16GB = 2TB). TPUv3 has 2 cores per chip. So 64 chips.

TPUv3 FLOP/s: 1.23E+14

Utilization: use default value of 30% for Language domain (https://epochai.org/blog/estimating-training-compute)

64 chips * 30% * 1.23E+14 FLOP/s * 9 days * 24h/day * 3600s/h
~= 2e21 FLOP",Lebanon,"W Antoun, F Baly, H Hajj",,,AraGPT2: Pre-Trained Transformer for Arabic Language Generation,,,8800000000,"""The total dataset size is 77GB with 8.8B words [word count was done after preprocessing, where a white
space is inserted before and after punctuations, brackets, numbers... which increased the total word count]""",,,,64.00,1500000000.00,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,,,,,Self-supervised learning,$3685.43,,Academia,"Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.",2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
wave2vec 2.0 LARGE,Speech,Speech completion,2020-10-22,"https://github.com/facebookresearch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/wav2vec/README.md

fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.",Open source,,Open source,https://arxiv.org/abs/2006.11477,Facebook,1.9e+21,"From surveying the authors:

We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.

V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)
Assume 40% utilization based on default for non-Language domain (https://epochai.org/blog/estimating-training-compute)

64 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h
~= 1.9E+21 FLOP",United States of America,"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli","Highly cited,SOTA improvement","Arguably an ""important"" paper? 

Abstract: 
""We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.""",wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"LibriSpeech,LibriLight",,727776000,"pg 4, section 4.1

""As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.""

53.2k h * 13,680 words/h = 727776000 words",,,,3437.00,317000000.00,"Section 5.1:
""We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,$1569.38,,Industry,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",2024-04-11 13:37,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
AlphaGo Lee,Games,Go,2016-01-27,,,,,https://www.nature.com/articles/nature16961,DeepMind,1.9e+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",United Kingdom of Great Britain and Northern Ireland,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",Highly cited,,Mastering the game of Go with deep neural networks and tree search,,,29400000,"We trained the policy network pσ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",Speculative,,,14887.00,,,,,,,,,,$14041.80,,Industry,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",2024-04-01 09:35,Robi Rahman,,,,,,,,AlphaGo Lee,,,,Industry,,,,,Industry,,,
CPM-Large,Language,,2020-12-01,"MIT license

https://github.com/TsinghuaAI/CPM-1-Generate",Open source,Open source,Unreleased,https://arxiv.org/abs/2012.00413,"Tsinghua University,Beijing Academy of Artificial Intelligence",1.8e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb","China,China","Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su",SOTA improvement,"""CPM outperforms CDial-GPT with a large margin in the few-shot experiment, showing the generalization ability of our model.""",CPM: A Large-scale Generative Chinese Pre-trained Language Model,Unspecified unreleased,"we construct a new sub-word vocabulary, containing both words and characters.",16700000000,"""language model, with 2.6 billion parameters and 100GB Chinese training data.""

We use the conversion factor 1GB ~ 167M words",,,,94.00,2600000000.00,"""To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode""",,,,336.0,"""It takes two weeks to train our largest model using 64 NVIDIA V100.""",NVIDIA V100,Self-supervised learning,$6569.51,"https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9.",Academia,"Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL.",2024-05-14 14:33,Robi Rahman,,,,,,64,,CPM-Large,,,,"Academia,Academia",,,,,"Academia,Academia",,,
BigGAN-deep 512x512,Image generation,Image generation,2018-09-28,"repo license is Apache:

https://github.com/tensorflow/tfhub.dev/blob/master/assets/docs/deepmind/models/biggan-deep-512/1.md",Open source,,Unreleased,https://arxiv.org/abs/1809.11096,"Heriot-Watt University,DeepMind",1.8e+21,"3e21, estimate taken from:

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening","United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","A Brock, J Donahue, K Simonyan",Highly cited,,Large Scale GAN Training for High Fidelity Natural Image Synthesis,JFT-300M,,292000000,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. """,Likely,,,4398.00,112694781.00,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)",,,,48.0,"""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for most models""",Google TPU v3,,$10448.44,,Industry,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",2024-04-15 14:47,Robi Rahman,,0,,,,256,,BigGAN-deep 512x512,,,,"Academia,Industry",,,,12288,"Academia,Industry",,,
Pythia-1b,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",1.799000000001e+21,https://www.wolframalpha.com/input?i=6+FLOP+*+1+billion+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,1000000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-1b,1,,,,64,,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,4830,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
Once for All,Vision,Image classification,2020-04-29,MIT license: https://github.com/mit-han-lab/once-for-all,Open source,,Open source,https://arxiv.org/abs/1908.09791,"MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT),IBM",1.7842809599999997e+21,"4.2k V100-hours (table 1)
0.33 utilization rate
","United States of America,United States of America,United States of America","Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",SOTA improvement,"""In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting""",Once for all: Train one network and specialize it for efficient deployment.,ImageNet,,,,,,,1017.00,7700000.00,,,,,,,NVIDIA V100,,$13000.00,from Table 1,Industry,"We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.",2024-05-01 09:05,Robi Rahman,,,,,,,,Once for All,Amazon Web Services,,,"Academia,Academia,Industry",,,,4200,"Academia,Academia,Industry",,,
GPT2-Large+LHOPT,Language,,2021-06-02,there's a repo for the optimizer but no training code for this model: https://github.com/openai/LHOPT,Unreleased,,Unreleased,https://arxiv.org/abs/2106.00958,OpenAI,1.6e+21,,United States of America,"Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba",,,A Generalizable Approach to Learning Optimizers,,,,,,,,21.00,760000000.00,,1.00,,,,,,,,,,,2024-05-07 12:00,Robi Rahman,GPT2-Large+LHOPT,,,,,,,,,,GPT2-Large+LHOPT,Industry,,,,,Industry,,,
CodeT5-base,Language,Code generation,2021-11-01,"BSD-3-Clause license

https://github.com/salesforce/CodeT5",Open source,,Open source,https://aclanthology.org/2021.emnlp-main.685/,"Salesforce,Nanyang Technological University",1.56e+21,"""We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""

16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21","United States of America,Singapore","Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi",SOTA improvement,"""Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE.""",CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,"CodeSearchNet,BigQuery","""We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from
BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining""",,"""In total, we employ around 8.35 million instances for pretraining"" 
Instances meaning code snippets/examples, not tokens.",Likely,,,874.00,220000000.00,"""We build CodeT5 based on Huggingface’s T5 (Raffel et al., 2020) PyTorch implementation and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M)""",150.00,,,288.0,"""The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""",NVIDIA A100,,,,,"""Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.""",2024-05-01 09:13,Anonymous,,,,,,,,CodeT5-base,,,,"Industry,Academia",,,,,"Industry,Academia",,,
PIXART-α,Image generation,"Image generation,Text-to-image",2023-09-30,copyleft license: https://github.com/PixArt-alpha/PixArt-alpha,Open source,,,"https://arxiv.org/abs/2310.00426
https://openreview.net/pdf?id=eAKmQPe3m1
https://github.com/PixArt-alpha/PixArt-alpha","Huawei Noah's Ark Lab,The University of Hong Kong,Hong Kong University of Science and Technology",1.541475e+21,"PixArt-α only takes 12% of Stable Diffusion v1.5's training time (753 vs. 6,250 A100 GPU days), saving nearly $300,000 ($28,000 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%.
To ensure fairness, we convert the V100 GPU days (1656) of our training to A100 GPU days (753)

they compare their compute with Imagen as 7132:753 (see Table 2)
Imagen compute was 1.4600000000000002e+22 FLOPS (from Epoch table) then PIXART-α is
(1.4600000000000002e+22/7132)*753=1.541475e+21 FLOPS (the most likely)

Another calculation: 

753 A100 GPU days (most likely A100 SXM4 80 GB)
753 days * 24 hours * 3600 s * 77970000000000 FLOPS/s (assume FP16) * 40% utilization rate = 2.0290663296e+21 FLOPS
753 days * 24 hours * 3600 s * 312000000000000 FLOPS/s (assume FP16 Tensor core) * 40% utilization rate = 8.11938816e+21 FLOPS (unlikely)

1656 V100 GPU days 
1656*24*3600*125000000000000 (assume Tensor float 32)*40%=7.15392e+21 FLOPS (unlikely)
1656*24*3600*31330000000000 (assume the most popular NVIDIA Tesla V100 DGXS 32 GB FP16)*40% = 1.7930585088e+21 FLOPS

The count of GPU days excludes the time for data labeling.","China,Hong Kong","Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li",,,PIXART-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis,"Segment Anything 1B,JourneyDB,ImageNet","we propose an autolabeling pipeline utilizing the state-of-the-art vision-language model (LLaVA (Liu et al., 2023)) to
generate captions on the SAM (Kirillov et al., 2023). Referencing in Section 2.4, the SAM dataset is
advantageous due to its rich and diverse collection of objects, making it an ideal resource for creating
high-information-density text-image pairs, more suitable for text-image alignment learning

In the third stage, we construct our training dataset by incorporating JourneyDB (Pan et al., 2023)
and a 10M internal dataset to enhance the aesthetic quality of generated images beyond realistic
photographs.",25000000,Table 2,Unverified,,,,600000000.00,0.6B,,,,,,NVIDIA V100,,$23685.58,"Our effective designs result in remarkable training efficiency for our model, costing only 753 A100 GPU days and $28,400
CPI Inflation calculator: $28,400 in 2024 is equivalent in purchasing power to about $23,685.58 in 2020",Academia,"PixArt-α is a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), and the training speed markedly surpasses existing large-scale T2I models, e.g., PixArt-α only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days).",2024-04-16 15:37,Anonymous,,0,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
MnasNet-A3,Vision,"Image classification,Object detection",2019-05-29,"Apache: https://github.com/tensorflow/tpu/blob/master/LICENSE

data is ImageNet",Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/1807.11626,Google,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",United States of America,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",Highly cited,,MnasNet: Platform-Aware Neural Architecture Search for Mobile,ImageNet,,1280000,"""In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. """,Speculative,,,2554.00,5200000.00,From https://arxiv.org/pdf/1807.11626.pdf,,806000000.00,"Table 1: 403M mult-adds per image
1 multiply-add = 2 FLOP",108.0,,Google TPU v3,,$4331.00,,Industry,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",2024-04-17 14:08,Robi Rahman,,,,,,256,,MnasNet-A3,,,,Industry,,,,,Industry,,,
MnasNet-A1 + SSDLite,Vision,"Image classification,Object detection",2019-05-29,Apache: https://github.com/tensorflow/tpu/blob/master/LICENSE,Open source,Open source,Open source,https://arxiv.org/abs/1807.11626,Google,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",United States of America,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",Highly cited,,MnasNet: Platform-Aware Neural Architecture Search for Mobile,COCO,,118000,,Speculative,,,2554.00,4900000.00,From https://arxiv.org/pdf/1807.11626.pdf,,1600000000.00,"Table 3: 0.8B mult-adds per image
1 multiply-add = 2 FLOP",108.0,,Google TPU v3,,$4331.00,,Industry,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",2024-04-18 07:21,Robi Rahman,,,,,,256,,MnasNet-A1 + SSDLite,,,,Industry,,,,,Industry,,,
German ELECTRA Large,Language,"Document classification,Named entity recognition",2020-10-21,MIT: https://huggingface.co/deepset/gelectra-large,Open source,,,https://arxiv.org/abs/2010.10906,"deepset,Bayerische Staatsbibliothek Muenchen,Munich Digitization Center",1.42829568e+21,"flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2","Germany,Germany","Branden Chan, Stefan Schweter, Timo Möller",SOTA improvement,'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.',German's Next Language Model,"Wikipedia,OPUS,OSCAR,OpenLegalData",Table 1 in the paper,27287800000,"163.4GB from Table 1 in the paper
assuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0",Likely,,,215.00,335000000.00,335M from Table 5,,,,168.0,7 days from Table 2,Google TPU v3,,,,Industry,"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. ",2024-04-04 12:41,Bartosz Podkanowicz,,,,,,64,,German ELECTRA Large,,,,Industry,,,,10752,Industry,,,
GenSLM,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2022-10-11,,,,,https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf,"University of Chicago,NVIDIA,Harvard University,Cerebras Systems,Technical University of Munich,California Institute of Technology",1.42e+21,"See Table 3
Overall ZettaFlops 1.42","United States of America,United States of America,United States of America,Multinational,Germany,United States of America","Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Sam Foreman, Zhen Xie, Diangen Lin, Maulik Shukla, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Rick Stevens, Anima Anandkumar, Venkatram Vishwanath, Arvind Ramanathan",SOTA improvement,"""Together, these capabilities go beyond state-of-the-art techniques
for global-scale whole genome surveillance of pandemic-causing
viruses and address a critical infrastructure need for the global
public health organization"" - SOTA improvement on very specific task",GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,SARS-CoV-2 genome dataset,"SARS-CoV-2 genome datasets from multiple sources:

""we used >1.5 million high-quality BV-BRC SARSCoV-2 complete genome sequences""

""We also utilized a dataset collected by the Houston Methodist Hospital System - one of the largest single-institution collections of SARS-CoV-2 genome sequences in the United States. [...]  Sequences with >256 ambiguous characters were discarded, leaving 16,545 total sequences""

Prokaryotic gene sequence dataset from BV-BRC:
""To allow for better generalization and avoid overfitting of the models to the SARS-CoV-2 data, we used >110 million unique prokaryotic gene sequences from BV-BRC""",,"Multiple datasets used:

""we used >1.5 million high-quality BV-BRC SARS CoV-2 complete genome sequences""

HMHS dataset: ""leaving 16,545 total sequences""

BV-BRC prokaryotic dataset: ""We queried BV-BRC to find 10,206 unique PGfams, each with >30,000 unique members.""",Confident,,,30.00,25000000000.00,See Table 3,,,,,,,,,,,"Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 10 million prokaryotic gene sequences, and then finetuning a SARS-CoV-2 specific model on 1.5 million genomes, we show that GenSLM can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLM represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of GenSLMs on both GPU-based supercomputers and AI-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining GenSLMs in tracking the evolutionary dynamics of SARS-CoV-2, noting that its full potential on large biological data is yet to be realized.",2024-05-01 09:22,Anonymous,,,,,,,,GenSLM,,,,"Academia,Industry,Academia,Industry,Academia,Academia",checked,,,,"Academia,Industry,Academia,Industry,Academia,Academia",,,
GPT-SW3,Language,Language modelling,2022-06-25,"commercial allowed 
https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b/blob/main/LICENSE",Open access (restricted use),,,"http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf
","AI Sweden,RISE",1.3e+21,"From section 4: ""Training was performed on GPU resources from the Berzelius Superpod, which is currently the fastest super
computer in Sweden, equipped with 60 Nvidia DGX
A100 servers, each of which consists of 8 Nvidia A100
GPUs with 320 GB Total GPU memory. Our training
process took 2.5 days utilizing 16 of the DGX A100
servers (in total 128 GPUs).""

2.5*24*60**2 * 128 * 1.56E+14 * 0.3 = 1.3e21","Sweden,Sweden","Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Ohman, Fredrik Carlsson, Magnus Sahlgren",,,Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish,,Novel Swedish 100GB corpus from news articles.,16700000000,"100GB Swedish corpus, assume Swedish has similar 167M words per GB as German.
100*167e6 = 1.67e10",,,,8.00,3500000000.00,,,,,60.0,"""Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs).""",NVIDIA A100,Self-supervised learning,,,,"Large-scale generative language models such as the GPT series (Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020) have enjoyed considerable attention in recent years. This has been partly due to their unprecedented ability to generate coherent text, but also for their capacity for zero-shot performance - without any training examples, on a wide range of different tasks. A prerequisite for building such models is access to both large amounts of high-quality text data and powerful computational resources. This has proven to be a limiting factor for the development of large-scale models for languages other than English. With the goal of promoting the development of largescale generative models for other languages, we here present our work on developing and evaluating GPTSW3, a 3.5 billion parameter autoregressive language model, trained on a newly collected 100 GB Swedish corpus. To the best of our knowledge, this is the largest generative model for Swedish to date, and probably one of the bigger non-English models at the moment. In this paper, we collect the lessons learned by developing and evaluating this model, including challenges with data collection, training procedures, and validation activities.",2024-03-28 11:51,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Transformer (Adaptive Input Embeddings),Language,,2018-09-28,,,,,https://arxiv.org/abs/1809.10853,Facebook AI Research,1.25e+21,"Table 1 shows their biggest adaptive input embeddings run:  145 hours with 64 V100 GPUs
125e12 FLOP/sec * 64 * 145 * 3600 * 0.3 = 1.25e21 FLOPs
 
Roughly aligns with 6NC method:
BILLION WORD models train for a total of 975K updates
batch size = 2048 * 64 GPUs = 131,072
6NC = 6 * 131072 * 975000 * 1026000000 = 7.9e20",United States of America,"Alexei Baevski, Michael Auli",SOTA improvement,"""On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result""",Adaptive Input Representations for Neural Language Modeling,"WikiText-103,One Billion Word benchmark","""We experiment on the BILLION WORD benchmark and WIKITEXT-103. BILLION WORD contains 768M word tokens... The training data of WIKITEXT-103 comprises about 100M tokens""",768000000,"""BILLION WORD contains 768M word tokens... The training data of WIKITEXT-103 comprises about 100M tokens""
Datasets are not combined but used to train separate models. BILLION WORD is the larger dataset and used for the largest training runs, so is reported here.",Confident,,,340.00,1026000000.00,Table 1,166.40,,,145.0,,NVIDIA V100,,,,,"We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.",2024-05-23 03:50,Robi Rahman,Transformer (Adaptive Input Embeddings),,,,,64,,Transformer (Adaptive Input Embeddings),,,,Industry,,,,4288,Industry,,,
EMDR,Language,Question answering,2021-06-09,https://github.com/DevSinghSachan/emdr2,Open source,,Open source,https://arxiv.org/abs/2106.05346v2,"Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),McGill University,DeepMind",1.24e+21,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete,
while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also
perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.""

1 week + 25+25+8 hours * 16 A100s
= ~230 * 16 A100-hours
= 230 * 16 * 3600 * 312 trillion * 0.3 = 1.24e21","Canada,Canada,United Kingdom of Great Britain and Northern Ireland","Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama",SOTA improvement,"""Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results.""",End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,"Wikipedia,NQ (Natural Questions),TriviaQA","pre-train on Wikipedia (Table 6), then training on the QA datasets",,,Likely,,,111.00,440000000.00,Table 2,,,,230.0,,NVIDIA A100,,,,,"We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.",2024-05-01 09:13,Anonymous,,0,,,,,,EMDR,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
BLIP-2 (Q-Former),"Vision,Language",,2023-01-30,,,,,https://arxiv.org/abs/2301.12597,Salesforce Research,1.20000000001e+21,https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33,United States of America,"Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",SOTA improvement,"""BLIP-2 achieves state-of-the-art performance on various vision-language tasks""",BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,"COCO,LAION-400M","""We use the same pre-training dataset as
BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017),
CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from
the LAION400M dataset (Schuhmann et al., 2021).""",,,Likely,,,1576.00,1480000000.00,"Q-Former has 188M params. The BLIP-2 system overall has ""54x fewer trainable parameters"" than Flamingo80B.",,,,200.0,"""For example, using
a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""
9 days = 216 hours",NVIDIA A100 SXM4 40 GB,,,,,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",2024-05-01 09:06,Robi Rahman,,,,,,16,,BLIP-2 (Q-Former),,,,Industry,,,,3200,Industry,,,
Flan T5-XXL + BLIP-2,"Multimodal,Language,Vision","Vision-language generation,Chat",2023-01-30,"BSD license (commercial)
https://github.com/salesforce/LAVIS/tree/main/projects/blip2",Open source,Open source,Open source,"https://arxiv.org/abs/2301.12597, https://huggingface.co/Salesforce/blip2-flan-t5-xl",Salesforce,1.2e+21,"fine-tuned from Flan-T5 XXL (11B) and ViT-g

fine-tuning compute:

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21",United States of America,"Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",Highly cited,,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,"COCO,LAION-400M","""We use the same pre-training dataset as BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from the LAION400M dataset""",,,Likely,,,1576.00,12100000000.00,"12.1B, per Table 2. 

only 108M trainable params (i.e. params trained during the BLIP process)",,,,200.0,"""less than 6 days for the first
stage and less than 3 days for the second stage""
9*24 is 216, rounding down a bit is 200 hours",NVIDIA A100 SXM4 40 GB,,,,,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",2024-05-13 09:04,Anonymous,,0,Flan-T5 11B,1,"ViT-g is the other base model.

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21",,,Flan T5-XXL + BLIP-2,,,,Industry,,,,,Industry,,,
Phi-1.5,Language,Language generation,2023-09-11,MIT license for weights,Open source,Unreleased,Unreleased,"https://arxiv.org/abs/2309.05463, https://huggingface.co/microsoft/phi-1_5",Microsoft,1.17e+21,"150B training tokens

150B*1.3B*6 = 1.17e21

also, took 1.5k GPU-hours with A100s, per Table 1

1500 * 312 trillion * 3600 * 0.3 (utilization assumption) = 5.05e20

so utilization was likely ~60%. may be high due to the relatively small cluster.
",United States of America,"Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee",,,Textbooks Are All You Need II: phi-1.5 technical report,,"Synthetic ""textbook"" data: 

""Our training data for phi-1.5 is a combination of phi-1’s training data (7B tokens) and newly created
synthetic, “textbook-like” data (roughly 20B tokens) for the purpose of teaching common sense reasoning
and general knowledge of the world (science, daily activities, theory of mind, etc.)""",22500000000,"30B tokens, or ~22.5B words",Likely,,,98.00,1300000000.00,,5.00,,,192.0,,NVIDIA A100 SXM4 40 GB,,,,,"We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality"" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need"" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step"" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.",2024-04-09 08:55,Anonymous,,0,,,,,,,,,,Industry,checked,,,,Industry,,,
DLRM-2022,Recommendation,,2021-09-15,,,,,https://arxiv.org/abs/2104.05158,Facebook,1.1e+21,"Figure 1

https://arxiv.org/abs/2104.05158",United States of America,"D Mudigere, Y Hao, J Huang, A Tulloch",,,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,,,,,,,,93.00,3000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,"NVIDIA Tesla V100 DGXS 32 GB,NVIDIA A100",,$2394.07,,Industry,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
PolyCoder,Language,Code generation,2022-02-26,,,,,https://arxiv.org/abs/2202.13169,Carnegie Mellon University (CMU),1.1e+21,"""We use GPT-NeoX toolkit 11 to
train the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall
time used to train the largest 2.7B model is about 6 weeks""

8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21",United States of America,"Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn",SOTA improvement,"""In the C programming language, PolyCoder outperforms
all models including Codex""",A Systematic Evaluation of Large Language Models of Code,,"Code scraped from GitHub. ""249GB of code across 12 programming languages on a single machine.""",,"249GB

They trained on 39B tokens per Table 3, but I'm not sure how many epochs that is. May be <1. ",Likely,,,343.00,2700000000.00,2.7B for largest model,,,,1000.0,6 weeks,NVIDIA Quadro RTX 8000,,,,Academia,"Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL, which enables future research and application in this area.",2024-05-01 09:13,Anonymous,,,,,,,,PolyCoder,,,,Academia,checked,,,,Academia,,,
ESM2-150M,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-21,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd",Open source,Open source,Open source,https://www.science.org/doi/abs/10.1126/science.ade2574,"Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",1.1e+21,from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 1.1e21 FLOP,"United States of America,United States of America,United States of America,United States of America","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",Evolutionary-scale prediction of atomic-level protein structure with a language model,UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",,,Likely,,,636.00,150000000.00,In the name,,,,,,,Unsupervised,,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",2024-03-28 17:48,Epoch AI,,1,,,,,,ESM2-150M,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
Swin Transformer V2,"Vision,Video","Action recognition,Image classification",2021-11-18,"MIT license

https://github.com/microsoft/Swin-Transformer",Open source,,Open source,https://arxiv.org/abs/2111.09883v2,Microsoft Research Asia,1.1e+21,"trained on ""<0.5k"" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)

A core is 123/2 teraflops

500 core-days
= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization
~= 1.1e21",China,"Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo","SOTA improvement,Highly cited","""It set new performance records on 4 representative vision tasks, including ImageNet-V2
image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification.""",Swin Transformer V2: Scaling Up Capacity and Resolution,"ImageNet,COCO,ADE20K","""We conduct experiments on ImageNet-1K image classification (V1 and V2) [18, 55], COCO object detection [44], and ADE20K semantic segmentation [85]. For the 3B model experiments, we also report the accuracy on
Kinetics-400 video action recognition [37].""

• Image classification. ImageNet-1K V1 and V2 val are
used [18,55] for evaluation. ImageNet-22K [18] which
has 14M images and 22K categories is optionally employed for pre-training. For the pre-training our largest
model SwinV2-G, a privately collected ImageNet22K-ext dataset with 70 million images is used.",,,Likely,,,1083.00,3000000000.00,,,,,,,NVIDIA A100 SXM4 40 GB,,,,,"Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \url{this https URL}.",2024-04-18 05:09,Anonymous,,0,,,,,,Swin Transformer V2,,,,Industry,,,,,Industry,,,
FRED-T5-XL,Language,Chat,2024-04-18,"https://huggingface.co/ai-forever/FRED-T5-1.7B
License:
apache-2.0
",Open source,,Unreleased,https://arxiv.org/abs/2309.10931,Sber,939600000000000000000.00,"=6*90000000000*1740000000.00=9.396 × 10^20
",Russia,"Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Vitalii Kadulin, Sergey Markov, Tatiana Shavrina, Vladislav Mikhailov, Alena Fenogenova",,,A Family of Pretrained Transformer Language Models for Russian,"Wikipedia,Corus,C4,OpenSubtitles","In general, different domains and sizes of the subcorpora are included in the resulting pretraining corpora of our LMs, which range from 30GB (ruBERT) to 450GB (ruGPT-3).",90000000000,"450GB *200M words per GB = 90000000000 words
In general, different domains and sizes of the subcorpora are included in the resulting pretraining corpora of our LMs, which range from 30GB (ruBERT) to 450GB (ruGPT-3).",Likely,2048,,,1740000000.00,1.74B,,,,1920.0,"""The FRED-T5-large and FREDT5-XL models are pretrained with a total batch size of 2048 for 35 days on 160 V100 GPUs, followed by 5 days on 80 A100 GPUs, and for 45 days on 112 A100 GPUs, respectively.""

Total hardware-time: 264960 chip-hours
https://www.wolframalpha.com/input?i=%2835*160%2B5*80%2B45*112%29*24.","NVIDIA V100,NVIDIA A100",,,,,"Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However, developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs, which spans encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. We provide a report on the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.",2024-05-15 10:20,Natalia Martemianova,,,,,,160,,,,,,Industry,,,,264960,Industry,,,
T0-XXL,Language,Language modelling,2021-10-15,"Apache-2.0 license
https://github.com/bigscience-workshop/t-zero",Open source,,Open source,https://arxiv.org/abs/2110.08207,"Hugging Face,Brown University",918190000000000000000.00,"From Table 1 and section B.1, a single run uses 27 hours of a 512 core slice of a TPU-v3 pod. 
512 * 0.5 * 1.23e14 * 3600 * 27 * 0.3 = 9.18e20
(cores) * (chip/core) * (FLOP/chip-sec) * (sec/hour) * (hours) * (utilization assumption)","Multinational,United States of America","Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,  Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael McKenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush",Highly cited,"""we compare T0 to the zero-shot performance of the largest language models available as of writing, i.e., various GPT-3 models up to 175B parameters...
We find that T0 matches or exceeds the performance
of all GPT-3 models on 9 out of 11 held-out datasets""",Multitask Prompted Training Enables Zero-Shot Task Generalization,P3 (Public Pool of Prompts),,,"Multitask - 12 tasks, 62 datasets. See fig 2 for details. 

This is going to be a nightmare to figure out! TODO figure out the sizes of each of these 62 datasets!

All datasets from here: https://arxiv.org/pdf/2109.02846.pdf

From B.2: ""across all of our training runs (including preliminary test experiments not described in this paper) we trained for 250 billion tokens""",Confident,,,1180.00,11000000000.00,"""Unless specified otherwise, we use the XXL version which
has 11B parameters.""",,,,27.0,"For main model, 27 hours (Table 1)

Total time taken to train for all experiments was 270 hours ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.""",Google TPU v3,,,,Industry,"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.",2024-05-17 11:28,Robi Rahman,,,,,,256,,T0-XXL,,,,"Industry,Academia",,,,69120,"Industry,Academia",,,
T5-3B,Language,Text autocompletion,2019-10-23,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open",Open source,Open source,Open source,https://arxiv.org/abs/1910.10683,Google,865865406873600000000.00,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5",United States of America,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Highly cited,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,C4,,25500000000,"""This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""
750GB * 200M word/GB = 1.5e11

""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.""
The fraction is 25.5 billion / 150 billion = 0.17 epochs.",,,,12884.00,2800000000.00,"page 37, 3B and 11B. ""To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B” variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters""",0.17,,,,,Google TPU v3,Self-supervised learning,,,Industry,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",2024-04-19 16:21,Robi Rahman,,,,,,,,T5-3B,,,,Industry,,,,,Industry,,,
Rubik's cube ADR robot,Robotics,,2019-10-15,,,,,https://arxiv.org/abs/1910.07113,OpenAI,854000000000000000000.00,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,United States of America,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang
",,,Solving Rubik’s Cube with a Robot Hand,,,62400000,""" The cumulative amount of experience over that period used for training on the
Rubik’s cube is roughly 13 thousand years, which is on the same order of magnitude as the 40 thousand years used by
OpenAI Five""

13/40 * 1.92e8 = 6.24e7",,,,975.00,27769565.00,"Table 13 on pg. 44 of the Cube paper, saved in ""RL papers"" folder. Sum of all the trainable parameters (dominated by the value and policy networks).

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,,,NVIDIA Tesla V100 DGXS 32 GB,,$3102.27,,Industry,"We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: this https URL",2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Noisy Student (L2),Vision,Image classification,2019-11-11,apache 2.0 license: https://github.com/google-research/noisystudent,Unreleased,,Open source,https://arxiv.org/abs/1911.04252v4,"Carnegie Mellon University (CMU),Google",849346560000000000000.00,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""

2048*4.00E+12*60**2*24*4*0.3 = 8.5e20","United States of America,United States of America","Q Xie, MT Luong, E Hovy, QV Lee","Highly cited,SOTA improvement","""Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model""",Self-training with Noisy Student improves ImageNet classification,"ImageNet,JFT",,81000000,"""Due to duplications, there are only 81M unique images among these 130M images.""",,,,1972.00,480000000.00,,,1040000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",144.0,6 days,Google TPU v3,,,,Industry,,2024-04-22 09:01,Robi Rahman,,,,,,1024,,Noisy Student (L2),,,,"Academia,Industry",,,,,"Academia,Industry",,,
Hybrid H3-2.7B,Language,,2022-12-28,apache 2.0,Open source,,Open source,https://arxiv.org/abs/2212.14052,"Stanford University,University at Buffalo",849000000000000000000.00,,"United States of America,United States of America","Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",SOTA improvement,Results table shows SOTA performance for some benchmarks,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,,,,,,,,125.00,2700000000.00,,509.02,,,,,,,,,,,2024-05-01 09:22,Robi Rahman,Hybrid H3-2.7B,0,,,,,,Hybrid H3-2.7B,,,,"Academia,Academia",,,,,"Academia,Academia",,,
JFT,Vision,"Image classification,Object detection,Semantic segmentation,Pose estimation",2017-07-10,,,,,https://arxiv.org/abs/1707.02968,"Google Research,Carnegie Mellon University (CMU)",843000000000000000000.00,"Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP","Multinational,United States of America","Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta",Highly cited,,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,JFT-300M,,300000000,,,,,2022.00,,,,,,1440.0,,NVIDIA Tesla K80,,$21396.42,,Industry,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",2024-04-01 09:45,Robi Rahman,,,,,,50,,JFT,,,,"Industry,Academia",,,,72000,"Industry,Academia",,,
GPT-2-Medium+Pixelfly,Language,,2021-11-30,apache for code: https://github.com/HazyResearch/fly,Unreleased,,Open source,https://arxiv.org/abs/2112.00029,"Stanford University,""SambaNova Systems, Inc"",Peking University,Adobe,University at Buffalo",834000000000000000000.00,,"United States of America,United States of America,China,United States of America,United States of America","Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",,,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,WikiText-103,"""We show training GPT-2-Small, Medium and its
Pixelfly model from scratch on a commonly used
NLP benchmarking dataset, wikiText-103.""",,,,,,53.00,203000000.00,,100.00,,,,,,,,,,,2024-05-06 14:58,Robi Rahman,GPT-2-Medium+Pixelfly,,,,,,,,,,,"Academia,Industry,Academia,Industry,Academia",,,,,"Academia,Industry,Academia,Industry,Academia",,,
CamemBERT,Language,Language modelling/generation,2019-11-10,"MIT: 
https://camembert-model.fr/",Open source,,Unreleased,https://arxiv.org/abs/1911.03894,"Facebook,INRIA,Sorbonne University",830000000000000000000.00,"""Unless otherwise specified,
our models use the BASE architecture, and are
pretrained for 100k backpropagation steps on 256
Nvidia V100 GPUs (32GB each) for a day""

256 V100-days

256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)
= 8.3e20


""Following (Liu et al., 2019), we
optimize the model using Adam (Kingma and Ba,
2014) (β1 = 0.9, β2 = 0.98) for 100k steps with
large batch sizes of 8192 sequences, each sequence
containing at most 512 tokens""

Using compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20","United States of America,France,France","Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah, Benoît Sagot",SOTA improvement,"""Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."" (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks)",CamemBERT: a Tasty French Language Model,CCNet,"""we train another model with the LARGE architecture, referred to as CamemBERTLARGE, for a fair comparison with XLM-RLARGE. This model is trained with the CCNet corpus, described in Sec. 6, for 100k steps""

Other models in paper are trained with the French portion of OSCAR. See footnote 12.",24000000000," 31.9B tokens, Table 6. 24B words using 0.75 words/token",Likely,,,990.00,335000000.00,"CamemBERT Large, Table 4",13.00,,,24.0,1 day for each model (may not have been a full 24 hours),NVIDIA V100,,,,,"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",2024-04-04 12:58,Anonymous,,0,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
"ALiBi (L=3072, Lvalid = 3072)",Language,,2021-08-27,"weights and code, MIT: https://github.com/ofirpress/attention_with_linear_biases",Open source,,Open source,https://arxiv.org/abs/2108.12409,"University of Washington,Facebook AI Research,Allen Institute for AI",810000000000000000000.00,"From figure 5, 6000 GPU hours (Nvidia V100) 

6000*  125 teraflop/s * 3600 * 0.3 = 8.1e20","United States of America,United States of America,United States of America","Ofir Press, Noah A. Smith, Mike Lewis",,,"""Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation""",WikiText-103,"""We first test the extrapolation abilities of various position methods on the WikiText-103 corpus... The training set is about 103 million tokens from English Wikipedia""",103000000,"""The training set is about 103 million tokens from English Wikipedia""",Confident,,,335.00,1300000000.00,,205.00,,,,,,,,,,,2024-05-17 16:17,Robi Rahman,"""ALiBi (L=3072, Lvalid = 3072)""",,,,,,,,,,,"Academia,Industry,Research collective",,,,,"Academia,Industry,Research collective",,,
DD-PPO,Robotics,Object detection,2019-12-19,MIT license for environment used to train. doesn't seem like it has training code for this model. https://github.com/facebookresearch/habitat-lab,Unreleased,,Unreleased,https://openreview.net/forum?id=H1gX8C4YPr,"Georgia Institute of Technology,Facebook AI Research,Oregon State University,Simon Fraser University",780000000000000000000.00,"""Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days – 180 GPU-days of training""

125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20","United States of America,United States of America,United States of America,Canada","Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra",SOTA improvement,"""This agent achieves state-of-art on the Habitat Challenge 2019 RGB track (rank 2 entry has 0.89 SPL).""",DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,,"""We experiment with several different sources of data. First, we utilize the training data released
as part of the Habitat Challenge 2019, consisting of 72 scenes from the Gibson dataset (Xia et al.,
2018). We then augment this with all 90 scenes in the Matterport3D dataset (Chang et al., 2017) to
create a larger training set (note that Matterport3D meshes tend to be larger and of better quality).2
Furthermore, Savva et al. (2019) curated the Gibson dataset by rating every mesh reconstruction on
a quality scale of 0 to 5 and then filtered all splits such that each only contains scenes with a rating of
4 or above (Gibson-4+), leaving all scenes with a lower rating previously unexplored. We examine
training on the 332 scenes from the original train split with a rating of 2 or above (Gibson-2+).""",,,Likely,,,365.00,,"no parameter count but some architecture details: ""The policy is parameterized by a 2-layer LSTM with a 512-dimensional hidden state. It takes three inputs: the previous action, the target relative to the current state, and the output of the visual encoder. The LSTM’s output is used to produce a softmax distribution over the action space and an estimate of the value function. See Appendix C for full details.""",,,,66.0,2.75 days,NVIDIA V100,Reinforcement learning,,,,"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever ""stale""), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. ",2024-05-01 09:05,Anonymous,,,,,,64,,DD-PPO,,,,"Academia,Industry,Academia,Academia",,,,,"Academia,Industry,Academia,Academia",,,
DreamLLM,"Multimodal,Language,Vision,Image generation","Language modelling/generation,Vision-language generation,Image generation",2023-09-20,,,,,https://arxiv.org/abs/2309.11499,"Xi’an Jiaotong University,Megvii Inc,Tsinghua University,Huazhong University of Science and Technology",754790400000000000000.00,"Table 13: 128xA800 GPU for (6+10+1.5) hours

flops = (128) * ( 312 * 10**12) * (17.5 * 3600) * (0.3) = 7.5e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
","China,China,China,China","Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi",,,DreamLLM: Synergistic Multimodal Comprehension and Creation,,"LLaVAPretrain (558K), MMC4 (2M), LLaVAInstruct (80K), BLIP-LAION (8M), BLIP-LAION (2M), InstructMMC4 (20K), LAION400M (11M), Instruct-BLIP-LAION (20K), LAION-COCO (11M) from Table 11",,"from Table 13: 30M pairs image and text, 4M interleaved image-text documents, and 120k instruction examples",Confident,,,42.00,7000000000.00,7B,1.00,,,17.5,from Table 11: (6+10+1.5) hours,NVIDIA A800,,,,,"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. ",2024-05-17 14:06,Bartosz Podkanowicz,,,,,,128,,,,,,"Academia,Industry,Academia,Academia",,,,2240,"Academia,Industry,Academia,Academia",,,
Pythia-410m,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",737700000001000000000.00,https://www.wolframalpha.com/input?i=6+FLOP+*+410+million+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,410000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-410m,1,,,,32,,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,2540,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
MegaMolBART,Biology,Drug discovery,2021-09-14,apache: https://github.com/NVIDIA/MegaMolBART/blob/dev/LICENSE/license.txt,Open source,Open access (non-commercial),Open source,"https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html, https://github.com/NVIDIA/MegaMolBART",NVIDIA,720000000000000000000.00,"""MegaMolBART was trained with data parallelism on 64 V100 32 GB GPUs (4 nodes x 16 GPUs) for 8 epochs (approximately 160k iterations or ~80 wall-clock hours) using a batch size of 32 molecules per GPU (micro batch)""

https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html

64 * 130 teraflops * 80 * 3600 * 0.3 = 7.2e20",United States of America,,,,MegaMolBART,ZINC 15,"""The ZINC-15 database was used for training [Sterling and Irwin, 2015]. Approximately 1.54 Billion molecules (SMILES strings) were selected from tranches meeting the following constraints: molecular weight <= 500 Daltons, LogP <= 5, reactivity level was “reactive,” and purchasability was “annotated.” The compounds were filtered to ensure a maximum length of 512 characters.""",,"1.54B molecules, maximum length of 512 characters. Perhaps ~100B tokens",Likely,,,,45000000.00,"""MegaMolBART has eight layers, four attention heads, a hidden space dimension of 256, and contains 45M parameters""",8.00,,,80.0,,NVIDIA V100,,,,,"MegaMolBART is a model that understands chemistry and can be used for a variety of cheminformatics applications in drug discovery. The embeddings from its encoder can be used as features for predictive models. Alternatively, the encoder and decoder can be used together to generate novel molecules by sampling the model’s embedding space.",2024-04-15 14:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Wu Dao - Wen Yuan,Language,,2021-01-11,,API access,,,https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg,Beijing Academy of Artificial Intelligence,650280960000000000000.00,"64 Nvidia V100 GPUs for two weeks

64 GPUs * 2.8e13 FLOP/s /GPU * 14*24*60*60s * 0.3 [utilization rate]

",China,,,,"Tencent: Facing cognition, Zhiyuan Research Institute and several units released a super-large-scale new pre-training model ""Enlightenment·Wenhui""",,,,,,,,0.00,2600000000.00,"""It has 2.6 billion parameters and is capable of performing cognitive activities such as memorization, comprehension, retrieval, numerical calculation, multi-language, etc.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",,,,,,,Self-supervised learning,,,Industry,,2024-03-27 17:24,Robi Rahman,,,,,,,,,,,Wu Dao - Wen Yuan,Academia,,,,,Academia,,,
OpenAI TI7 DOTA 1v1,Games,DOTA,2017-08-11,,,,,https://openai.com/research/dota-2,OpenAI,604609522259200200000.00,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,United States of America,,"Historical significance,SOTA improvement",,Dota 2,,,,,,,,0.00,,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",,,,,,,,$2873.99,,Industry,"We’ve created a bot which beats the world’s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.",2024-03-07 14:22,Robi Rahman,,,,,,,,OpenAI TI7 DOTA 1v1,,,,Industry,,,,,Industry,,,
S4,Language,,2021-10-31,"Apache 2 0

https://github.com/state-spaces/s4",Open source,,Open source,https://arxiv.org/abs/2111.00396,Stanford University,600000000000000000000.00,,United States of America,"Albert Gu, Karan Goel, Christopher Ré",SOTA improvement,"""S4 achieves strong empirical results across a diverse range of established benchmarks, including... SoTA on every task from the Long Range Arena benchmark""",Efficiently Modeling Long Sequences with Structured State Spaces,WikiText-103,,,,,,,490.00,249000000.00,,509.02,,,,,,,,,,,2024-05-01 09:13,Robi Rahman,S4,,,,,,,S4,,,,Academia,,,,,Academia,,,
"GPT-2 (1.5B, Curriculum Learning 45K)",Language,,2021-08-13,there's a repo for the technique but I don't see training code for this model: https://github.com/microsoft/DeepSpeed ,Unreleased,,Unreleased,https://arxiv.org/abs/2108.06084,Microsoft,600000000000000000000.00,,United States of America,"Conglong Li, Minjia Zhang, Yuxiong He",,,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,,,,,,,,20.00,1500000000.00,,2.20,,,,,,,,,,,2024-05-07 09:00,Robi Rahman,"""GPT-2 (1.5B, Curriculum Learning 45K)""",,,,,,,,,,,Industry,,,,,Industry,,,
DiT-XL/2,Image generation,Image generation,2023-03-02,,,,,https://arxiv.org/abs/2212.09748,"New York University (NYU),UC Berkeley",600000000000000000000.00,"~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. 

3M iterations with a batch size of 256.","United States of America,United States of America","William Peebles, Saining Xie",SOTA improvement,"""our largest DiT-XL/2 models outperform all prior diffusion models on the classconditional ImageNet 512×512 and 256×256 benchmarks,
achieving a state-of-the-art FID of 2.27 on the latter.""",Scalable Diffusion Models with Transformers,ImageNet,,,didn't state which ImageNet set,Likely,,,335.00,675000000.00,675M,,524600000000.00,524.6 billion FLOP for 512x512 (table 4),,,Google TPU v3,,,,,"We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",2024-05-01 09:24,Anonymous,,0,Stable Diffusion (LDM-KL-8-G),,,,,DiT-XL/2,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Libratus,Games,Poker,2017-01-01,,,,,https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf,Carnegie Mellon University (CMU),551000000000000000000.00,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""

""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""

I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).
The first system has 752 nodes a 2CPUs a 14cores each.

source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/



1. 12M core hours for 196 cores
2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores
2.1 That's 42.5 GFLOPS per core.
3. Running this for 12M h
3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs
4. Assuming 30% utilization
 1.823e21 * 0.3
→ 5.51e20 FLOPs",United States of America,"N Brown, T Sandholm, S Machine",SOTA improvement,Claims to be first ML system to reach superhuman level at No Limit Poker Texas Hold Em,Libratus: The Superhuman AI for No-Limit Poker,,,,,,,,97.00,,,,,,,"In total, Libratus used about 25 million core hours. Of those,
about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent
on the initial abstraction and equilibrium finding component,
another 3 million were used for nested subgame solving, and
about 3 million were used on the self-improvement algorithm.",,,$6253.49,,Academia,"No-limit Texas Hold’em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold’em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first—and so far only—AI to defeat top human professionals in that game. Libratus’s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.",2024-03-17 09:26,Robi Rahman,,,,,,,,Libratus,,,,Academia,,,,857000,Academia,,,
BERT-Large-CAS (PTB+WT2+WT103),Language,,2019-04-20,Apache 2.0 license: https://github.com/cgraywang/gluon-nlp-1/blob/lmtransformer/scripts/language_model/train/transformer_lm.py,Unreleased,Open access (non-commercial),Open source,https://arxiv.org/abs/1904.09408,Amazon,521000000000000000000.00,,United States of America,"Chenguang Wang, Mu Li, Alexander J. Smola",SOTA improvement,"""CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs""",Language Models with Transformers,"Penn TreeBank,WikiText-2,WikiText-103",,,,,,,110.00,395000000.00,,50.00,,,,,,,,,,,2024-04-16 12:47,Robi Rahman,BERT-Large-CAS (PTB+WT2+WT103),,,,,,,BERT-Large-CAS (PTB+WT2+WT103),,,,Industry,,,,,Industry,,,
AR-LDM,Image generation,Text-to-image,2022-11-20,"no weights, no license:
https://github.com/xichenpan/ARLDM",Unreleased,Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/2211.10950,"Alibaba,University of Waterloo,Vector Institute",510000000000000000000.00,8 NVIDIA A100 GPUs for 8 days,"China,Canada,Canada","Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen",SOTA improvement,"The first latent diffusion model for coherent visual story synthesizing.
""Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images""",Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models,,,,"PororoSV, FlintstonesSV and VIST. All storytelling datasets, sizes would be possible to look up.",Confident,,,27.00,1500000000.00,Table 1,50.00,,,194.0,8 NVIDIA A100 GPUs for 8 days,NVIDIA A100,,,,,"Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",2024-05-13 09:05,Anonymous,,,,,,,,AR-LDM,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
ESM1-670M (UR50/D),Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-08-31,"MIT. looks like inference code, not training code: https://github.com/facebookresearch/esm",Open source,Open source,Unreleased,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,"Facebook AI Research,New York University (NYU)",480000000000000000000.00,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
906k steps [See Table S2: Hyperparameters]
131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]

Estimate:  906e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 4.8e20 FLOP","United States of America,United States of America","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,UniRef50,"""the high-diversity dense dataset (UR50/D) samples the UniRef100 sequences evenly across the UniRef50 clusters.""",,,Confident,,,1282.00,669200000.00,See Table 1,,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-05-07 10:09,Epoch AI,,,,,,128,,ESM1-670M (UR50/D),,,,"Industry,Academia",,,,,"Industry,Academia",,,
ESM1b,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-12-15,MIT: https://github.com/facebookresearch/esm,Open source,Open source,Open source,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,"Facebook AI Research,New York University (NYU)",460000000000000070000.00,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
8.5 hours on 64 GPUs per epoch, 56 epochs [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]
128 NVIDIA V100 GPU, assuming  V100 PCIe single precision 14 TFLOPS and 0.3 utilization rate

Estimate: (8.5*56*3600) s * 14e12 FLOP/s * 0.3 *64 = 4.6e20 FLOPs","United States of America,United States of America","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,UniParc,"In our experiments, we explore datasets withup to 250 million sequences of the UniParc database (33), whichhas 86 billion amino acids.",,,Confident,,,1282.00,652400000.00,See Table 9,56.00,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-04-16 08:18,Epoch AI,,1,,,,128,,ESM1b,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Masked Autoencoders,Vision,"Semantic segmentation,Image classification,Image generation",2021-11-11,"Code at https://github.com/facebookresearch/mae

This project is under the CC-BY-NC 4.0 license",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2111.06377,Facebook AI Research,460000000000000070000.00,"128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 × 632000000 connections × 3 × 1281167 training examples × 1600 of epochs  = 7.8e18 FLOP

Manual calculation with `calflops` package roughly agrees with hardware-time calculation: 
286.21 GFLOPS/observation * 1281167 observations * 1600 epochs = 5.86e20 FLOP

See reproduction here: https://colab.research.google.com/drive/1KCsmrfPzT9BgGO_YQthnz4oP3QRqbw5o?usp=sharing

Weighting three estimates equally:
(7.84e20 + 7.8e18 + 5.86e20)/3 = 4.6e20",United States of America,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick","Highly cited,SOTA improvement","""By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data. The previous best accuracy, among all methods using only IN1K data, is 87.1% (512 size)... We improve over the state-of-the-art by a nontrivial margin in the highly competitive benchmark of IN1K (no external data). Our result is based on vanilla ViT, and we expect advanced networks will perform better.""

See Table 3",Masked Autoencoders Are Scalable Vision Learners,ImageNet-1k,,1281167,,Speculative,,,5077.00,632000000.00,"Three models:
ViT-B (86M), ViT-L (304M), ViT-H (632M)",1600.00,,,69.0,"Table 2 gives wall times for training ViT-L and ViT-H to 800 epochs; later it is stated that the systems are each trained for 1600 epochs.
(34.5 hours / 800 epochs) * 1600 epochs = 69 hours",,Self-supervised learning,$155.25,"$0.62 / 32 cores of TPU-v3 * (128 cores / 32 cores) = $2.65/hour
CPI conversion to 2020: $2.25
$2.25/hour * 69 hours = $155.25",Industry,"This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",2024-04-11 13:52,Anonymous,,0,ViT-Huge/14,,"UNCERTAIN
128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 × 632000000 connections × 3 × 1281167 training examples × 1600 of epochs  = 7.8e18 FLOP
",,,,,,,Industry,,,,,Industry,,,
Kosmos-2,"Language,Vision","Visual question answering,Image captioning,Named entity recognition",2023-06-26,"data: https://huggingface.co/datasets/zzliang/GRIT

weights and code: https://github.com/microsoft/unilm/tree/master/kosmos-2

license for full repo (MIT):
https://github.com/microsoft/unilm/blob/master/LICENSE",Open source,Open source,Open source,https://arxiv.org/abs/2306.14824,Microsoft,455003542840000000000.00,"""We train the model on 256 V100 GPUs and the training takes approximately one day to complete""
""We train KOSMOS-2 for 60k steps, equivalent to around 25 billion tokens""

GPU-time method
(256) * (1.3e14) * (24 * 3600) * (0.3) = 8.626176e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

Parameter-data method
6ND = 6*25B*1.6B = 2.4e20

Used geometric mean of two estimates.",United States of America,"Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei",,,Kosmos-2: Grounding Multimodal Large Language Models to the World,,"""We train the model on newly added grounded image-text pairs, monomodal text
corpora, image-caption pairs, and interleaved image-text data. Our training process involves a
batch size of 419K tokens, consisting of 185K tokens from text corpora, 215K tokens from original
and grounded image-caption pairs, and 19K tokens from interleaved data. We train KOSMOS-2
for 60k steps, equivalent to around 25 billion tokens""",,"text and images
 ""We train KOSMOS-2 for 60k steps, equivalent to around 25 billion tokens""
""We train the model on newly added grounded image-text pairs, monomodal text
corpora, image-caption pairs, and interleaved image-text data. Our training process involves a
batch size of 419K tokens, consisting of 185K tokens from text corpora, 215K tokens from original
and grounded image-caption pairs, and 19K tokens from interleaved data. We train KOSMOS-2
for 60k steps, equivalent to around 25 billion tokens""
",Likely,,,230.00,1600000000.00,1.6B,,3200000000.00,2N,24.0,""" We train the model on 256 V100 GPUs and the training takes approximately one day to complete""",NVIDIA V100,Self-supervised learning,,,,"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at this https://github.com/microsoft/unilm/tree/master/kosmos-2",2024-05-15 19:56,Anonymous,,,,,,256,,,,,,Industry,,,,6144,Industry,,,
ESM1-670M (UR50/S),Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-08-31,MIT: https://github.com/facebookresearch/esm,Open source,Open source,Open source,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,"Facebook AI Research,New York University (NYU)",440000000000000000000.00,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
840k steps [See Table S2: Hyperparameters]
131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]

Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 4.4e20 FLOP","United States of America,United States of America","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,UniRef50,"""high-diversity sparse dataset (UR50/S) uses the UniRef50 representative sequences""",,,Confident,,,1282.00,669200000.00,See Table 1,,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-04-04 11:48,Anonymous,,1,,,,128,,ESM1-670M (UR50/S),,,,"Industry,Academia",,,,,"Industry,Academia",,,
Xception,Vision,Image classification,2016-10-07,,,,,https://arxiv.org/abs/1610.02357,Google,435999999999999930000.00,60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20,United States of America,François Chollet,Highly cited,,Xception: Deep Learning with Depthwise Separable Convolutions,JFT,"Also ImageNet, but JFT is significantly larger",350000000,"""JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k""",Likely,,,11578.00,22855952.00,Table 3,,16800000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",720.0,"""while the JFT experiments took over one month each.""",NVIDIA Tesla K80,,$1961.34,,Industry,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2024-04-01 09:45,Robi Rahman,,,,,,60,,Xception,,,,Industry,,,,43200,Industry,,,
Monarch-GPT-2-Medium,Language,,2022-04-01,"apache for code: https://github.com/HazyResearch/fly/tree/master

https://github.com/HazyResearch/fly/tree/master/configs/model/gpt2model ",Unreleased,,Open source,https://arxiv.org/abs/2204.00595,"Stanford University,University at Buffalo,University of Michigan",435999999999999930000.00,,"United States of America,United States of America,United States of America","Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",,,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,,,,,,,,48.00,165000000.00,,110.00,,,,,,,,,,,2024-05-06 14:10,Robi Rahman,Monarch-GPT-2-Medium,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
DETR,Vision,Object detection,2020-05-26,Apache 2.0: https://github.com/facebookresearch/detr,Open source,,Open source,https://arxiv.org/abs/2005.12872,Facebook,400000000000000000000.00,"""Training the baseline model for 300 epochs on 16 V100 GPUs
takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the
longer schedule used to compare with Faster R-CNN we train for 500 epochs
with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared
to the shorter schedule.""

48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.

125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20",United States of America,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko",Highly cited,,End-to-End Object Detection with Transformers,COCO 2017,"""We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18], containing 118k training images and 5k validation images""",123000,,Likely,,,8594.00,60000000.00,60M per Table 1,500.00,25300000000.00,"253 GFLOPS inference cost, for 10 FPS, so 25.3 billion FLOP per image. ",,,NVIDIA V100,Supervised,,,,"Abstract. We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation
that explicitly encode our prior knowledge about the task. The main
ingredients of the new framework, called DEtection TRansformer or
DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given
a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output
the final set of predictions in parallel. The new model is conceptually
simple and does not require a specialized library, unlike many other
modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation
in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr.",2024-04-24 13:23,Anonymous,,,,,,,,DETR,,,,Industry,,,,,Industry,,,
AmoebaNet-A (F=448),Vision,Image classification,2018-02-05,,,,,https://arxiv.org/abs/1802.01548,Google Brain,385296912000000000000.00,"450 K40 GPUs for 20k models (approx. 7 days).
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",United States of America,"Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le",Highly cited,,Regularized Evolution for Image Classifier Architecture Search,ImageNet-1k,,1280000,,,,,2596.00,469000000.00,Table 2,,,,168.0,"""Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).""",NVIDIA Tesla K40s,,$5858.75,,Industry,"The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",2024-04-01 09:45,Robi Rahman,,,,,,450,,AmoebaNet-A (F=448),,,,Industry,,,,75600,Industry,,,
AlphaGo Fan,Games,Go,2015-10-01,,,,,https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ,DeepMind,380000000000000070000.00,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs",United Kingdom of Great Britain and Northern Ireland,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis","Highly cited,SOTA improvement",,Mastering the game of Go with deep neural networks and tree search,,,,Supervised learning + self-play,,,,14887.00,8209984.00,"The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",,,"Distributed: 176 GPUs + 1202 PUs + 40 search threads
Single machine: 8 GPUs + 48 CPUs 

https://www.nature.com/articles/nature16961",,,,,$3076.07,,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.",2024-04-09 09:00,Robi Rahman,,,,,,,,AlphaGo Fan,,,,Industry,,,,,Industry,,,
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Vision,Image generation,2021-06-11,"https://github.com/hojonathanho/diffusion

everything is openly avaiable but no license or terms of use information",Open source,,Open source,https://arxiv.org/abs/2006.11239,UC Berkeley,380000000000000070000.00,"Numbers in Appendix B
10.6h for the CIFAR model (batch size 128, 21 step/s)
2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours

This is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores)
https://cloud.google.com/tpu/docs/regions-zones

1.25E14 FLOP/s * (8 cores / 2 cores/chip) * 702.8h * 3600s/h * 0.3 = 3.8e20",United States of America,"Jonathan Ho, Ajay Jain, Pieter Abbeel","Highly cited,SOTA improvement","Novel approach to image synthesis that yields SOTA results on datasets like CIFAR-10

Abstract: 
""On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. """,Denoising Diffusion Probabilistic Models,LSUN Bedroom,,3033042,"""We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.""

""The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.""
https://paperswithcode.com/dataset/celeba-hq

LSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.
https://www.tensorflow.org/datasets/catalog/lsun
",,,,6932.00,256000000.00,"Appendix B: 
"" Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.""",,,,,,Google TPU v3,,$2.60,,Academia,,2024-04-16 11:23,Robi Rahman,,,,,,,,Denoising Diffusion Probabilistic Models (LSUN Bedroom),,,,Academia,,,,,Academia,,,
ProGen,Biology,"Protein generation,Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-03-13,,,,,https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2,"Salesforce Research,Stanford University",370000000000000000000.00,"Our model was implemented in TensorFlow (Abadi et al.,
2016) and trained with a global batch size of 64 distributed
across 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)

4.00E+12*256*60**2*24*14*0.3 = 3.7e20","United States of America,United States of America","Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,  View ORCID ProfilePo-Ssu Huang, Richard Socher",,,ProGen: Language Modeling for Protein Generation,,,,,,,,217.00,1200000000.00,"""We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences""",5.00,,,,,,Self-supervised learning,$623.75,,Industry,"Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
DDPM-IP (CelebA),Image generation,Image generation,2023-01-27,,,,,https://arxiv.org/abs/2301.11706v3,Utrecht University,350000000000000000000.00,"""We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In
more detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32×32
for 34 days. For LSUN tower 64×64, CelebA 64×64 and FFHQ 128×128, we used 16 GPUs to train the models for 3 days,
5 days and 4 days, respectively""

5*16 V100-days for CelebA.

5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",Netherlands,"Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara",SOTA improvement,"""For instance, on CelebA 64×64, we achieve a new state-of-theart FID score of 1.27, while saving 37.5% of the training time""",Input Perturbation Reduces Exposure Bias in Diffusion Models,CelebA,,203000,,Likely,,,24.00,295000000.00,"295M for CelebA model, per Table 9",681.00,,,120.0,5 days,NVIDIA V100,,,,,"Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64×64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at this https URL",2024-04-01 09:53,Anonymous,,,,,,,,DDPM-IP (CelebA),,,,Academia,,,,,Academia,,,
HSO,Language,,2021-12-16,,Unreleased,,Unreleased,https://arxiv.org/abs/2112.08653,Toyota Technological Institute at Chicago,345000000000000000000.00,,United States of America,"Davis Yoshida, Kevin Gimpel",,,Reconsidering the Past: Optimizing Hidden States in Language Models,,,,,,,,2.00,345000000.00,,,,,,,,,,,,,2024-05-06 14:38,Robi Rahman,HSO,,,,,,,,,,HSO,Academia,,,,,Academia,,,
Megatron-LM (355M),Language,,2019-09-17,MIT-like license: https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE,Open source,Unreleased,Open source,https://arxiv.org/abs/1909.08053,NVIDIA,335000000000000000000.00,"355M is a GPT-2-based model (Table 2).

""For GPT-2 models, all training is performed with sequences
of 1024 subword units at a batch size of 512 for 300k iterations"" 

I interpret the above as 1024*512*300k = 157B training tokens (or things that require a forward+backward pass). 

6 * 1024*512*300,000 * 355,000,000  = 3.35e20",United States of America,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",Highly cited,,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,"Wikipedia,WebText","""we aggregate several of the largest language
modeling datasets. We create an aggregate dataset consisting of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &
Le, 2018), RealNews (Zellers et al., 2019), and OpenWebtext (Radford et al., 2019). To avoid training set leakage
into our downstream tasks we remove the Wikipedia articles
present in the WikiText103 test set (Merity et al., 2016).""",34800000000,"""The resulting aggregate
corpus contains 174 GB of deduplicated text.""
assuming 200M tokens per GB",Likely,,,1152.00,355000000.00,355M,4.40,,,,,,,,,,,2024-04-18 15:53,Robi Rahman,Megatron-LM (355M),1,,,,,,,,,,Industry,,,,,Industry,,,
DLRM-2021,Recommendation,,2020-07-01,,,,,https://www.arxiv-vanity.com/papers/2104.05158/,Facebook AI,300000000000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",United States of America,"D Mudigere, Y Hao, J Huang, A Tulloch",,,"High-performance, Distributed Training of Large scale Deep Learning Recommendation Models",,,,,,,,10.00,1000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,$1094.92,"https://bdtechtalks.com/2020/02/03/google-meena-chatbot-ai-language-model/
",Industry,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebookand are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40 × speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Pythia-160m,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",287900000001000050000.00,https://www.wolframalpha.com/input?i=6+FLOP+*+160+million+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,160000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-160m,1,,,,32,,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,1030,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
BERT-Large,Language,Next sentence prediction,2018-10-11,apache 2.0,Open source,,Open source,https://arxiv.org/abs/1810.04805,Google,285000000000000000000.00,more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing,United States of America,"J Devlin, MW Chang, K Lee, K Toutanova",Highly cited,,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,,,3300000000,"""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,,,71316.00,340000000.00,,,79000000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",96.0,"from appendix A.2: ""Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.""",Google TPU v2,Self-supervised learning,$999.93,,Industry,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",2024-04-01 09:52,Robi Rahman,,,,,,64,0.2900,BERT-Large,,,,Industry,,,,6144,Industry,,,
ConSERT,Language,Language modelling,2021-05-25,https://github.com/yym6472/ConSERT,Open source,,Open source,https://arxiv.org/abs/2105.11741,"Meituan University,Beijing University of Posts and Telecommunications",280000000000000000000.00,"Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)

Foundation model is BeRT with 2.8e+20 FLOP.

So total compute is 2.8e+20.","China,China","Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu",SOTA improvement,Trains an effective BERT model on small sample sizes and achieves an 8% improvement over previous SOTA on STA datasets.,ConSERT: A contrastive framework for self-supervised sentence representation transfer,Chinese STS,,,,Likely,,,418.00,345000000.00,,,,,0.1,,NVIDIA Tesla V100S PCIe 32 GB,,,,,"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",2024-05-01 09:06,Anonymous,,,,,,,,ConSERT,,,,"Academia,Academia",,,,,"Academia,Academia",,,
PAGnol-XL,Language,Language modelling,2021-10-16,,API access,,,https://arxiv.org/abs/2110.08554,"LightOn,Laboratoire de Physique de l'Ecole Normale (LPENS),INRIA",259200000000000000000.00,"They report their compute directly.

From section 8: ""About 62k GPU-hours on the Jean Zay HPC Cluster."" Jean Zay uses both A100 and V100 GPUs, and maybe other stuff as well?

Note they explicitly call out V100 in their Appendix A.

https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/","France,France,France","Julien Launay, E.L. Tommasone, Baptiste Pannier, François Boniface, Amélie Chatelain, Alessandro Cappelli, Iacopo Poli, Djamé Seddah",,,PAGnol: An Extra-Large French Generative Model,CCNet,They mostly use CCNet but also OSCAR for a comparison.,24000000000,Section 4.1: 32G tokens => 32e9*0.75 = 24e9 words,,,,4.00,1500000000.00,,,,,,,NVIDIA Tesla V100 SXM2,Self-supervised learning,,,,"Access to large pre-trained models of varied architectures, in many different languages, is central to the democratization of NLP. We introduce PAGnol, a collection of French GPT models. Using scaling laws, we efficiently train PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a model 13 times smaller. PAGnol-XL is the largest model trained to date for the French language. We plan to train increasingly large and performing versions of PAGnol, exploring the capabilities of French extreme-scale models. For this first release, we focus on the pretraining and scaling calculations underlining PAGnol. We fit a scaling law for compute for the French language, and compare it with its English counterpart. We find the pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text. We evaluate our models on discriminative and generative tasks in French, comparing to other state-of-the-art French and multilingual models, and reaching the state of the art in the abstract summarization task. Our research was conducted on the public GENCI Jean Zay supercomputer, and our models up to the Large are made publicly available.",2024-03-27 17:23,Robi Rahman,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
LLaVA-NeXT-34B (LLaVA-1.6),"Multimodal,Language,Vision","Visual question answering,Chat,Question answering",2024-01-30,Apache 2.0,Open source,,,"https://llava-vl.github.io/blog/2024-01-30-llava-next/, https://huggingface.co/liuhaotian/llava-v1.6-34b","University of Wisconsin Madison,ByteDance,Nanyang Technological University,UC Berkeley",258785280000000000000.00,"2.6e20 = 32 * 312e12 * 0.3 * 24* 3600 = num gpus * peak flops * assumed utilization rate * time in seconds
""The largest 34B variant finishes training in ~1 day with 32 A100s.""","United States of America,China,Singapore,United States of America","Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee",,"""SoTA Performance! LLaVA-NeXT achieves the best performance compared with open-source LMMs such as CogVLM or Yi-VL. Compared with commercial ones, it catches up to Gemini Pro and outperforms Qwen-VL-Plus on selected benchmarks.""","LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",,"""Data: Coming soon.""",,"'1318K"" image-text pairs",Likely,,,,34750000000.00,34.75B,,,,24.0,"""The largest 34B variant finishes training in ~1 day with 32 A100s.""",NVIDIA A100,,,,Industry,,2024-03-27 13:18,Bartosz Podkanowicz,,,,,,32,,,,,,"Academia,Industry,Academia,Academia",,,,768,"Academia,Industry,Academia,Academia",,,
DALL-E mega,Vision,Text-to-image,2022-06-28,apache 2.0,Open source,Unreleased,Open source,"https://huggingface.co/dalle-mini/dalle-mega
https://github.com/borisdayma/dalle-mini",Craiyon,228527308800000000000.00,"flops = (128) * (1230 * 10**9) * (1344 * 3600) * (0.3) = 2.3e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact",,"Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and Lê Khắc, Phúc and Melas, Luke and Ghosh, Ritobrata",,,DALL·E Mega Model Card,,possible the same data as DALL-E mini,,possible the same data as DALL-E mini,Likely,,,,,,,,,1344.0,from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact,Google TPU v3,,,,,,2024-04-10 08:51,Bartosz Podkanowicz,,,,,,128,,,,,,,,,,172032,,,,
Discriminator Guidance,Image generation,Image generation,2022-11-28,,,,,https://arxiv.org/abs/2211.17091v4,"Korea Advanced Institute of Science and Technology (KAIST),NAVER",215700000010000000000.00,481 hours * 312 TFLOPS (A100) * 40% utilization,"Korea (Republic of),Korea (Republic of)","Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",SOTA improvement,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).""
https://paperswithcode.com/paper/refining-generative-process-with",Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,,,,,Likely,,,44.00,,,,,,481.0,Table 6,NVIDIA A100 PCIe,,,,,"The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).",2024-05-01 09:22,Anonymous,,0,,,,,,Discriminator Guidance,,,,"Academia,Industry",,,,,"Academia,Industry",,,
RoseTTAFold All-Atom (RFAA),Biology,"Protein folding prediction,Proteins",2023-10-09,MIT-like license: https://github.com/baker-laboratory/RoseTTAFold-All-Atom?tab=License-1-ov-file#readme,Open source,,Open source,https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1,"University of Washington,Seoul National University",214000000000000030000.00,"Supplementary material: ""This took 8 days on 8 NVIDIA A6000 GPUs.""
8*3.87E+13*60*60*24*8","United States of America,Korea (Republic of)","Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix S Morey-Burrows, Ivan Anishchenko, Ian R Humphreys, Ryan McHugh, Dionne Vafeados, Xinting Li, George A Sutherland, Andrew Hitchcock, C Neil Hunter, Minkyung Baek, Frank DiMaio, David Baker",,,Generalized Biomolecular Modeling and Design with RoseTTAFold All-Atom,"PDB (Protein Data Bank),Cambridge Structural Dataset",More than a PLM. Models other kinds of molecules too. ,,,,,,,,,,,,,,,,,,,"Although AlphaFold2 (AF2) and RoseTTAFold (RF) have transformed structural biology by enabling high-accuracy protein structure modeling, they are unable to model covalent modifications or interactions with small molecules and other non-protein molecules that can play key roles in biological function. Here, we describe RoseTTAFold All-Atom (RFAA), a deep network capable of modeling full biological assemblies containing proteins, nucleic acids, small molecules, metals, and covalent modifications given the sequences of the polymers and the atomic bonded geometry of the small molecules and covalent modifications. Following training on structures of full biological assemblies in the Protein Data Bank (PDB), RFAA has comparable protein structure prediction accuracy to AF2, excellent performance in CAMEO for flexible backbone small molecule docking, and reasonable prediction accuracy for protein covalent modifications and assemblies of proteins with multiple nucleic acid chains and small molecules which, to our knowledge, no existing method can model simultaneously. By fine-tuning on diffusive denoising tasks, we develop RFdiffusion All-Atom (RFdiffusionAA), which generates binding pockets by directly building protein structures around small molecules and other non-protein molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we design and experimentally validate proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and optically active bilin molecules with potential for expanding the range of wavelengths captured by photosynthesis. We anticipate that RFAA and RFdiffusionAA will be widely useful for modeling and designing complex biomolecular systems.",2024-04-03 09:54,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ESM2-35M,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-21,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd",Open source,Open source,Open source,https://www.science.org/doi/abs/10.1126/science.ade2574,"Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",209999999999999970000.00,"""All language models were trained for 500K updates, except the 15B language model"" 
""All models used 2 million tokens as batch size except the 15B model""
[Supplementary Materials]

Hence: 1000B training tokens (500k steps, 2M tokens/batch)

Estimate: 35M*2*1000B + 35M*4*1000B","United States of America,United States of America,United States of America,United States of America","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",Evolutionary-scale prediction of atomic-level protein structure with a language model,UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",,,Likely,,,636.00,35000000.00,In the name,,,,,,,Unsupervised,,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",2024-03-28 17:48,Epoch AI,,1,,,,,,ESM2-35M,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
ViT + DINO,Vision,Image classification,2021-04-29,"https://github.com/facebookresearch/dino

Apache-2.0 license",Open source,,Open source,https://arxiv.org/abs/2104.14294,"INRIA,Facebook AI Research",209999999999999970000.00,"""Overall, training DINO with Vision Transformers
achieves 76.1 top-1 accuracy using two 8-GPU servers for 3
days""

GPU is V100

16 * 125 teraflops * 3 days * 0.4 utilization
= 2.1e20

However, this isn't the best result in the paper (which is 80.1% with ViT-B/8). 76.1% is the result from ViT-B/16 per Table 2, which may be 5x cheaper than ViT-B/8 based on Table 1?","France,United States of America","Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin",Highly cited,,Emerging Properties in Self-Supervised Vision Transformers,ImageNet,"""We pretrain the models on the ImageNet dataset [60] without labels""",,,Likely,,,3272.00,85000000.00,"85M, table 1",300.00,,,,,NVIDIA V100,Self-supervised learning,,,,"In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",2024-04-11 14:48,Anonymous,,0,,,,,,ViT + DINO,,,,"Academia,Industry",,,,,"Academia,Industry",,,
ruDalle: Kandinsky 3.0,Image generation,"Text-to-image,Image generation",2023-12-11,"Apache 2 license

https://github.com/ai-forever/Kandinsky-3",Open source,,Open source,https://arxiv.org/abs/2312.03511,Sber,201490800000000000000.00,11900000000*2822000000*6 = 2.014908 × 10^20,Russia,"Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov",,,KANDINSKY 3.0 TECHNICAL REPORT,"Unspecified unreleased,LAION,COYO-700M","see list on the page 5
The training dataset consists of popular open-source datasets and our internal dataset of approximately 150 million text-image pairs.",2822000000,,Speculative,,,,11900000000.00, 11.9 billion parameters,,,,,,NVIDIA A100,,,,,"We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation. Compared to previous versions of Kandinsky 2.x, Kandinsky
3.0 leverages a two times larger U-Net[1] backbone, a ten times larger text encoder and removes diffusion mapping. We describe the architecture of the model, the data collection procedure, the training technique, and the production system of user interaction. We focus on the key components
that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others. By our side-by-side comparisons, Kandinsky becomes better in text understanding and works better on specific domains. The project is
available at https://ai-forever.github.io/Kandinsky-3",2024-05-01 02:42,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,,,
ERNIE-GEN (large),Language,Language generation,2020-08-06,"https://github.com/PaddlePaddle/ERNIE

code/weights with unclear license",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2001.11314,Baidu,200000000000000000000.00,"430GB text for 1 epoch

approx 430 * 200 million words = 86B words, or 100B tokens per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0

6 * 340 million params * 100 billion tokens ~= 2e20",China,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA improvement,"""Empirically, ERNIE-GEN is particularly effective and
achieves state-of-the-art results on a range of NLG tasks
including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA)""",ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,"CC-News,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",WebText2,Wikipedia,C4","""Recent works for pre-training verify that larger scaled pretraining corpora can improve the performances on downstream tasks. We pre-train ERNIE-GENLARGE model on
the 430GB text corpora with 1 epoch and 1M training steps.
Our 430GB text corpora is extracted from the corpus used by
RoBERTa [Liu et al., 2019], T5 [Raffel et al., 2019] and ALBERT [Lan et al., 2020]. We fine-tune ERNIE-GENLARGE
on two abstractive summarization datasets including Gigaword and CNN/Daily Mail, the evaluation results are reported
in Table 9""

RoBERTa and T5 datasets are CC-News, BookCorpus, Wikipedia, WebText2, and C4",86000000000,"approx 430 * 200 million words = ~86B words, per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",Speculative,,,107.00,340000000.00,"""We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively""",,,,,,,,,,Industry,,2024-05-01 09:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
InstructBLIP,"Multimodal,Language,Vision","Visual question answering,Chat",2023-05-11,"LlaMA/Vicuna license, non-comm:
https://github.com/salesforce/LAVIS/tree/main/projects/instructblip",Open access (non-commercial),,,https://arxiv.org/abs/2305.06500,"Salesforce Research,Hong Kong University of Science and Technology,Nanyang Technological University",193999999999999970000.00,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""
16 * 3.12e14 * 1.5 * 24 * 3600 * 0.3 = 1.94e20","United States of America,Singapore","Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi",SOTA improvement,from abstract - SOTA on ScienceQA,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,,"COCO Caption, Web CapFilt, NoCaps, Flickr30K, TextCaps, VQAv2, VizWiz, GQA, Visual Spatial Reasoning, IconQA, OKVQA, A-OKVQA, ScienceQA, Visual Dialog, OCR-VQA, TextVQA, HatefulMemes, LLaVA-Instruct-150K, MSVD-QA, MSRVTT-QA, iVQA",,,Confident,,,984.00,13000000000.00,13B form 2.6,,,,36.0,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",NVIDIA A100 SXM4 40 GB,,,,,"Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at this https URL. ",2024-05-21 04:10,Bartosz Podkanowicz,,,Vicuna-13B,194088960000000000000,"flops = (16) * (312 * 10**12) * (1.5* 24 * 3600) * (0.3) = 1.9e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",16,,,,,,"Industry,Academia,Academia",,,,576,"Industry,Academia,Academia",,,
DistilProtBert,Biology,"Proteins,Protein folding prediction",2022-09-18,,,,,https://academic.oup.com/bioinformatics/article/38/Supplement_2/ii95/6701995,Bar-Ilan University,190000000000000030000.00,"""Pretraining was done on five v100 32-GB Nvidia GPUs from a DGX
cluster with a local batch size of 16 examples... Every epoch run took approximately 4 days, resulting in total pretraining time of 12 days""

5 * 125 teraFLOP/s * 12 * 24 * 3600 * 0.3 (assumed utilization) = 1.9e20",Israel,"Yaron Geffen, Yanay Ofran and Ron Unger",,,DistilProtBert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts,UniRef50,"""DistilProtBert was pretrained on 43 M sequences from UniRef50 with length ranging from 20 to 512 amino acids""",,43M sequences,Likely,,,15.00,230000000.00,"""we were able to reduce the number of DistilProtBert parameters by almost half, to 230 M""",3.00,,,288.0,12 days,NVIDIA Tesla V100 DGXS 32 GB,,,,Academia,"Recently, deep learning models, initially developed in the field of natural language processing (NLP), were applied successfully to analyze protein sequences. A major drawback of these models is their size in terms of the number of parameters needed to be fitted and the amount of computational resources they require. Recently, 'distilled' models using the concept of student and teacher networks have been widely used in NLP. Here, we adapted this concept to the problem of protein sequence analysis, by developing DistilProtBert, a distilled version of the successful ProtBert model. Implementing this approach, we reduced the size of the network and the running time by 50%, and the computational resources needed for pretraining by 98% relative to ProtBert model. Using two published tasks, we showed that the performance of the distilled model approaches that of the full model. We next tested the ability of DistilProtBert to distinguish between real and random protein sequences. The task is highly challenging if the composition is maintained on the level of singlet, doublet and triplet amino acids. Indeed, traditional machine-learning algorithms have difficulties with this task. Here, we show that DistilProtBert preforms very well on singlet, doublet and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91 and 0.87, respectively. Finally, we suggest that by examining the small number of false-positive classifications (i.e. shuffled sequences classified as proteins by DistilProtBert), we may be able to identify de novo potential natural-like proteins based on random shuffling of amino acid sequences.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
ONE-PEACE,"Multimodal,Vision,Speech","Image classification,Speech recognition,audio question answering,audio classification",2023-05-18,"Apache 2.0:
https://github.com/OFA-Sys/ONE-PEACE",Open source,Open source,Open source,https://arxiv.org/abs/2305.11172v1,"Alibaba,Huazhong University of Science and Technology",180000000000000000000.00,"4 billion params * 7.5 billion data * 6 = 1.8e20.

see training dataset size notes. this estimate required some more assumptions than usual.","China,China","Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, Chang Zhou",SOTA improvement,""" ONEPEACE achieves leading results in both uni-modal and multi-modal tasks, including image classification (89.8%
accuracy on ImageNet w/o privately labeled data), semantic segmentation (63.0% mIoU on ADE20K), audio-text
retrieval (outperforming previous SOTAs on AudioCaps and Clotho by a large margin), audio classification (91.8%
zero-shot accuracy on ESC-50, 69.7% accuracy on FSD50K, 59.6% accuracy on VGGSound w/o visual information),
audio question answering (86.2% accuracy on AVQA w/o visual information), image-text retrieval (84.1% I2T R@1
on MSCOCO and 97.6% I2T R@1 on Flickr30K w/o intermediate finetuning and ranking), and visual grounding
(89.26%/83.23%/89.27% scores on RefCOCO/+/g test sets).""",ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities,"LAION-2B,LAION-Audio-630K","""For image-text pairs, we use LAION-2B... For audio-text pairs, we mainly use the environmental sound datasets processed by [76].""

looks like there's additional fine-tuning data as well",1600000000,"""After these steps, we retain about 1.5 billion image-text pairs""
...
""We also perform simple cleaning on the data, which involves removing samples with text lengths less than 3 or greater than
512, as well as texts containing non-English or emoji characters. Ultimately, we obtain about 2.4 million audio-text pairs, with a total duration of around 8,000 hours""

8000 hours = 480,000 minutes = ~109,440,000 words at 228 wpm

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq

Trained on 10 epochs for audio. For text, they train on ""200K steps with a batch size of 32768"" = 6,533,600,000
Adding together, they train on ~ 7.5b data points on a dataset of 1.6b, for ~4.7 epochs on average.",Speculative,,,37.00,4000000000.00,"""we propose ONE-PEACE, a model with 4B parameters""",4.70,,,,,,,,,,"In this work, we explore a scalable way for building a general representation model toward unlimited modalities. We release ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities. The architecture of ONE-PEACE comprises modality adapters, shared self-attention layers, and modality FFNs. This design allows for the easy extension of new modalities by adding adapters and FFNs, while also enabling multi-modal fusion through self-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic pretraining tasks, cross-modal aligning contrast and intra-modal denoising contrast, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently. With the scaling-friendly architecture and pretraining tasks, ONE-PEACE has the potential to expand to unlimited modalities. Without using any vision or language pretrained model for initialization, ONE-PEACE achieves leading results on a wide range of uni-modal and multi-modal tasks, including image classification (ImageNet), semantic segmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio classification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA), image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g). Code is available at this https URL.",2024-05-13 08:54,Anonymous,,0,,,,,,ONE-PEACE,,,,"Industry,Academia",,,,,"Industry,Academia",,,
LUKE,Language,Question answering,2020-10-02,"apache 2.0: https://github.com/studio-ousia/luke?tab=readme-ov-file

data is wikimedia, which has a commercial license: https://dumps.wikimedia.org/legal.html",Open source,Open source,Open source,https://arxiv.org/abs/2010.01057v1,"University of Washington,National Institute of Informatics",175799808000000000000.00,"(16) * (1413 * 10**10) * (30 * 24 * 3600) * (0.3) = 175799808000000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Docker
container 19.02 hosted on a server with two Intel
Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30
days.""
peak flops for fp32 from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957","United States of America,Japan","Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto",SOTA improvement,"from abstract ""In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).""",LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,Wikipedia,"""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. """,3500000000,"""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. """,Likely,2048,table in appendix A,599.00,484000000.00,"from https://github.com/studio-ousia/luke section - 484 M (LUKE model)
(mLUKE is model from different paper)",,,,720.0,see compute notes,NVIDIA V100,,,,,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https://github.com/studio-ousia/luke",2024-04-29 10:20,Anonymous,,,,,,16,,LUKE,,,,Academia,,,,11520,Academia,,,
IMPALA,Games,Atari,2018-02-05,,,,,https://arxiv.org/abs/1802.01561,DeepMind,168000000000000000000.00,"Source: Ajeya Cotra and Tom Davidson, https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",United Kingdom of Great Britain and Northern Ireland,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu","Highly cited,SOTA improvement","""IMPALA is able to achieve better performance than previous agents with less data""",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,,,240000000000,"From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the ""dataset size""",,,,1344.00,1600000.00,"""Figure 3 in the paper states that the large architecture has 1.6 million parameters. I am using the large model because it was the only one trained on all the Atari games at once, which seems like the most impressive task in the suite.""

Source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,100.0,Maximum training time for IMPALA is 100 hours according to Figure 6. This seems to refer to the 1 GPU model. The 8 GPU model looks to have been trained about 1/8 as long.,NVIDIA P100,Self-supervised learning,,,Industry,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",2024-04-01 09:52,Robi Rahman,,,,,,1,,IMPALA,,,,Industry,,,,100,Industry,,,
Mesh-TensorFlow Transformer 4.9B (language modelling),Language,Language modelling/generation,2018-11-05,,,,,https://arxiv.org/abs/1811.02084,Google Brain,161740800000000000000.00,"flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips",United States of America,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman",SOTA improvement,"'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",Mesh-TensorFlow: Deep Learning for Supercomputers,"Wikipedia,One Billion Word benchmark",from section 9.1 Wikipedia and one-billion-word language modeling benchmark.,4750000000,"from section 9.1 Wikipedia and one-billion-word language modeling benchmark.
there were around 5B tokens from wikipedia - assuming 0,75 words per token we have 3750000000.0 words from wikipedia and 1B words from the benchmark
and citation from section 9.1: 'We have included random samples from these models in Appendix C. On the languagemodel_wiki_noref_v128k_l1k dataset from the Tensor2Tensor library1, consisting of over 5 billion tokens of text from Wikipedia, perplexity continued to improve significantly with a model size of 5 billion parameters.'",Likely,,,357.00,4900000000.00,4.9B from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.',10.00,,,13.0,"from section 9.1 ""For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.""",Google TPU v2,,,,,"Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the ""batch"" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .",2024-03-03 14:50,Bartosz Podkanowicz,,,,,,256,,,,,,Industry,,,,3328,Industry,,,
NoPos,Language,,2022-03-30,,Unreleased,,Unreleased,https://arxiv.org/abs/2203.16634,"Tel Aviv University,University of Washington,Intel Labs,Meta AI",161000000000000000000.00,,"Israel,United States of America,Multinational,United States of America","Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy",,,Transformer Language Models without Positional Encodings Still Learn Positional Information,The Pile (NoPos subset),,,,,,,50.00,1300000000.00,,199.92,,,,,,,,,,,2024-04-25 15:58,Robi Rahman,NoPos,,,,,,,,,,,"Academia,Academia,Industry,Industry",,,,,"Academia,Academia,Industry,Industry",,,
Compressive Transformers for Long-Range Sequence Modelling,Language,,2019-11-13,,Unreleased,,Unreleased,https://arxiv.org/abs/1911.05507,DeepMind,160000000000000000000.00,,United Kingdom of Great Britain and Northern Ireland,"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap",,,Compressive Transformers for Long-Range Sequence Modelling,WikiText-103,,,,,,,417.00,,,328.32,,,,,,,,,,"We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",2024-05-22 11:36,Robi Rahman,Compressive Transformers for Long-Range Sequence Modelling,,,,,,,,,,,Industry,,,,,Industry,,,
Sandwich Transformer,Language,,2019-11-10,non-commercial training code: https://github.com/ofirpress/sandwich_transformer,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1911.03864,"Allen Institute for AI,Facebook AI Research",158000000000000000000.00,,"United States of America,United States of America","Ofir Press, Noah A. Smith, Omer Levy",SOTA improvement,"""Sandwich transformers achieve state-of-the-art results on the enwik8 character-level language modeling dataset and on an additional word-level corpus,
but have no significant effect on machine translation""",Improving Transformer Models by Reordering their Sublayers,"""BookCorpus (BooksCorpus, Toronto Book Corpus)"",enwik8,text8",,,,,,,70.00,209000000.00,209M,180.00,,,,,,,,,,,2024-05-07 10:15,Robi Rahman,Sandwich Transformer,,,,,,,Sandwich Transformer,,,,"Research collective,Industry",,,,,"Research collective,Industry",,,
gLM,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2023-04-08,"non-commercial license
https://github.com/y-hwang/gLM?tab=License-1-ov-file",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),"https://www.biorxiv.org/content/10.1101/2023.04.07.536042v1.full
https://github.com/y-hwang/gLM",Harvard University,151000000000000000000.00,"""The training stage takes several weeks on four NVIDIA A100 GPUs.""

Assumption: 2 weeks, 40% utilization rate, 78 TFLOP peak rate

Estimate: =(2*7*24*3600) s * 78e12 FLOP/s *4 GPU * 0.4",United States of America,"Yunha Hwang, Andre L. Cornman, Sergey Ovchinnikov, Peter R. Girguis",,,Deep learning of genomic contexts predicts protein co-regulation and function,MGnify,"""seven million metagenomic contig fragments consisting of 15 to 30 genes from the MGnify database""",,,Likely,,,,1000000000.00,"""Our model consists of ~1B parameters which is at least a magnitude smaller  compared to state-of-the-art pLMs.""",,,,,,NVIDIA A100,,,,,"Deciphering the relationship between a gene and its genomic context is fundamental to understanding and engineering biological systems. Machine learning has shown promise in learning latent relationships underlying the sequence-structure-function paradigm from massive protein sequence datasets. However, to date, limited attempts have been made in extending this continuum to include higher order genomic context information. Evolutionary processes dictate the specificity of genomic contexts in which a gene is found across phylogenetic distances, and these emergent genomic patterns can be leveraged to uncover functional relationships between gene products. Here, we trained a genomic language model (gLM) on millions of metagenomic scaffolds to learn the latent functional and regulatory relationships between genes. gLM learns contextualized protein embeddings that capture the genomic context as well as the protein sequence itself, and appears to encode biologically meaningful and functionally relevant information (e.g. phylogeny, enzymatic function). Our analysis of the attention patterns demonstrates that gLM is learning co-regulated functional modules (i.e. operons). Our findings illustrate that gLM’s unsupervised deep learning of the metagenomic corpus is an effective approach to encode functional semantics and regulatory syntax of genes in their genomic contexts, providing a promising avenue for uncovering complex relationships between  genes in a genomic region.",2024-04-02 15:48,Anonymous,,,ESM2-650M,,,,,,,,,Academia,,,,,Academia,,,
DNABERT,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2021-08-15,Apache 2.0: https://github.com/jerryji1993/DNABERT,Open source,Open source,Open source,https://academic.oup.com/bioinformatics/article/37/15/2112/6128680,Northeastern University,140000000000000000000.00,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",United States of America,"Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri",SOTA improvement,"""We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data."" [Abstract] - SOTA improvement on very specific task",DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,Human genome,"""We generated training data from human genome [...]"" [2.2.2 Pre-training]",,,Likely,,,334.00,110000000.00,"""We used the same model architecture as the BERT base, which consists of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, and the same parameter setting across all the four DNABERT models during pre-training""

Known to have 110 million parameters as reported in: https://arxiv.org/pdf/1810.04805v2.pdf
""We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) [...]""",,,,600.0,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""",NVIDIA GeForce RTX 2080 Ti,,,,,"Motivation
Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.

Results
To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.",2024-05-01 09:13,Anonymous,,,,,,,,DNABERT,,,,Academia,,,,,Academia,,,
BERT-RBP,Biology,"Proteins,Protein interaction prediction",2022-04-07,"No clear license: https://github.com/kkyamada/bert-rbp

data also doesn't have a clear license: http://www.csbio.sjtu.edu.cn/bioinf/RBPsuite/dataset_new.html",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689,Waseda University,140000000000000000000.00,"See DNABert entry:

""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",Japan,"Keisuke Yamada, Michiaki Hamada",SOTA improvement,"""Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs"" [Abstract] - SOTA improvement on a very specific task",Prediction of RNA–protein interactions using a nucleotide language model,"Human reference genome [pre-training],RBPSuite","See DNABert entry: ""We generated training data from human genome [...]"" [2.2.2 Pre-training]

""An eCLIP-seq dataset previously generated from the ENCODE3 database by Pan et al. (2020) was used. The original dataset consisted of 154 RBP sets with up to 60 000 positive RNA sequences that bind to the corresponding RBP and the same number of negative sequences."" [2.2 Data preparation]",,,Confident,,,23.00,110000000.00,"Base model is BERT base (110M parameters), pre-trained on human reference genome (DNABert: https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)",,,,,,,,,,,"Motivation
The accumulation of sequencing data has enabled researchers to predict the interactions between RNA sequences and RNA-binding proteins (RBPs) using novel machine learning techniques. However, existing models are often difficult to interpret and require additional information to sequences. Bidirectional encoder representations from transformer (BERT) is a language-based deep learning model that is highly interpretable. Therefore, a model based on BERT architecture can potentially overcome such limitations.

Results
Here, we propose BERT-RBP as a model to predict RNA–RBP interactions by adapting the BERT architecture pretrained on a human reference genome. Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs. The detailed analysis further revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only based on sequence information. Overall, the results provide insights into the fine-tuning mechanism of BERT in biological contexts and provide evidence of the applicability of the model to other RNA-related problems.",2024-05-22 12:39,Anonymous,,,DNABERT,22000000000000000,"""The models were trained on four NVIDIA Tesla V100 GPUs (128
GB memory). The training of one RBP model using 19 200 samples
took <10 min.""

Calculation assuming FP16 and 30% utlization and NVIDIA Tesla V100 SMX2 model: 
10 min * 60 sec/min * 3.1e13 FLOP/s * 4 GPU * 0.3 utilization = 2.2e16",,,BERT-RBP,,,,Academia,,,,,Academia,,,
ESM1-670M (UR100),Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-08-31,MIT: https://github.com/facebookresearch/esm,Open source,Open source,Open source,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,"Facebook AI Research,New York University (NYU)",140000000000000000000.00,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
275k steps [See Table S2: Hyperparameters]
131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]

Estimate:  275e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 1.4e20 FLOP","United States of America,United States of America","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,UniRef100,"""the low-diversity dataset (UR100) uses the UniRef100 representative sequences""",,,Confident,,,1282.00,669200000.00,See Table 1,,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-04-04 11:48,Epoch AI,,1,,,,128,,ESM1-670M (UR100),,,,"Industry,Academia",,,,,"Industry,Academia",,,
ESM1v,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM),Protein variant pathogenicity prediction",2021-11-17,"MIT for code/weights.

Uniref has commercial license

https://github.com/facebookresearch/esm",Open source,Open source,Open source,https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2,"Facebook AI Research,New York University (NYU),UC Berkeley",135000000000000070000.00,"""ESM-1v models are pre-trained for 6 days on 64 V100 GPUs"" [F - Compute costs]
Assuming  V100 PCIe single precision 14 TFLOPS and 0.3 utilization rate 

Estimate: (6*24*3600) s * 14e12 FLOP/s * 0.3 *64 = 1.4e20 FLOPs

Alternative estimate based on Figure 7: 
10^(7.5) GPU-seconds * 14e12 FLOP/s * 0.3 = 1.3e20 FLOPs

Mean: 1.35e20 FLOP","United States of America,United States of America,United States of America","Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, Alexander Rives",,,Language models enable zero-shot prediction of the effects of mutations on protein function,UniRef90,"""We use Uniref90 2020-03""",98000000,"""We train ESM-1v, a 650M parameter transformer language model for prediction of variant effects, on 98 million diverse protein sequences across evolution""",Confident,,,291.00,650000000.00,"""We train ESM-1v, a 650M parameter transformer language model for prediction of variant effects""",,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-04-04 11:46,Epoch AI,,1,,,,64,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Fusion in Encoder,Language,Question answering,2022-11-18,,,,,https://arxiv.org/abs/2211.10147,Samsung,130000000000000000000.00,"""The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.""

2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20",Korea (Republic of),"Akhil Kedia, Mohd Abbas Zaidi, Haejun Lee",SOTA improvement,"""Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset""",FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering,TriviaQA,,,79k per table 11 (probably number of question-answer pairs),Likely,,,5.00,330000000.00,330M,,,,48.0,2 days,NVIDIA A100 SXM4 80 GB,,,,,"Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",2024-03-11 16:13,Anonymous,,0,,,,,,Fusion in Encoder,,,,Industry,,,,,Industry,,,
Pythia-70m,Language,,2023-04-03,apache 2.0 for model/code/data,Open source,,Open source,https://arxiv.org/abs/2304.01373,"EleutherAI,""Booz Allen Hamilton, McLean"",University of Cambridge,""Indraprastha Institute of Information Technology
Delhi"",Stability AI,datasaur.ai,University of Amsterdam",126000000001000010000.00,https://www.wolframalpha.com/input?i=6+FLOP+*+70+million+*+299892736000,"Multinational,United States of America,United Kingdom of Great Britain and Northern Ireland,India,Multinational,United States of America,Netherlands","Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",,,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,The Pile,,299892736000,"""We train all models for 299,892,736,000 ≈ 300B tokens""",Confident,,,429.00,70000000.00,See Table 1 for non-embedding parameters,1.00,,,,,NVIDIA A100 SXM4 40 GB,,,,,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at",2024-05-22 15:35,Robi Rahman,Pythia-70m,1,,,,32,,,Amazon Web Services,AWS US East,,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,,510,"Research collective,Industry,Academia,Academia,Industry,Industry,Academia",,,
KEPLER,Language,Relation extraction,2020-11-23,"MIT License

https://github.com/THU-KEG/KEPLER",Unreleased,,Open source,https://arxiv.org/abs/1911.06136,"Tsinghua University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),HEC,CIFAR AI Research,Princeton University,University of Montreal / Université de Montréal",124000000000000000000.00,"From author communication

""About 128 GPU-days using Nvidia V100 (16GB). ""

precision: float16

V100 GPU for float16: 28000000000000 (2.8E+13)

0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h
= 1.24E+20


","China,Canada,France,Canada,United States of America,Canada","Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.",SOTA improvement,"""Experimental results show that KEPLER achieves state-of-the-art performances
on various NLP tasks""",KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.,"Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",Wikidata5M","From author communication

    For the language modeling objective, we use Wikipedia+BookCorpus datasets (about 13GB).    For the knowledge embedding objective, we use Wikidata5m (about 1GB).",3300000000,"For BookCorpus + English Wikipedia: 800M + 2500M

For Wikidata5M: 20614279
See table 1. Contains ""entities"", ""relations"", and ""triplets""",,,,479.00,125000000.00,,,,"From author communication

It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.",,,,Self-supervised learning,$437.97,,Academia,"Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL.",2024-05-10 04:51,Robi Rahman,,,,,,,,KEPLER,,,,"Academia,Academia,Academia,Research collective,Academia,Academia",,,,,"Academia,Academia,Academia,Research collective,Academia,Academia",,,
KnGPT2,Language,,2021-10-15,,Unreleased,,Unreleased,https://arxiv.org/abs/2110.08152,"Huawei Noah's Ark Lab,McGill University",124000000000000000000.00,,"China,Canada","Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh",,,Kronecker Decomposition for GPT Compression,,,,,,,,21.00,83000000.00,,1.00,,,,,,,,,,,2024-05-07 08:30,Robi Rahman,KnGPT2,,,,,,,,,,KnGPT2,"Industry,Academia",,,,,"Industry,Academia",,,
NMST+GPT-2,Language,,2022-10-03,BSD 3-Clause License. Looks like just training code  https://github.com/nyu-dl/non-monotonic-self-terminating-lm,Unreleased,,Open source,https://arxiv.org/abs/2210.00660,New York University (NYU),120000000000000000000.00,,United States of America,"Eugene Choi, Cheolhyoung Lee, Kyunghyun Cho",,,A Non-monotonic Self-terminating Language Model,WikiText-103,,,,,,,0.00,124000000.00,,2.98,,,,,,,,,,,2024-04-30 13:01,Robi Rahman,NMST+GPT-2,,GPT-2 (117M),,,,,,,,NMST+GPT-2,Academia,,,,,Academia,,,
Transformer+Recurrent Windows of Context,Language,,2020-08-16,,Unreleased,,Unreleased,https://arxiv.org/abs/2008.07027,"Toyota Technological Institute at Chicago,University of Chicago",116999999999999980000.00,,"United States of America,United States of America","Davis Yoshida, Allyson Ettinger, Kevin Gimpel",,,Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size,,,,,,,,5.00,124000000.00,,2.00,,,,,,,,,,,2024-05-10 15:24,Robi Rahman,Transformer+Recurrent Windows of Context,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Wu Dao - Wen Hui,"Language,Multimodal,Video,Image generation","Language modelling/generation,Image generation,Video generation",2021-03-01,,,,,https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,Beijing Academy of Artificial Intelligence,116121600000000000000.00,"64 Nvidia V100 GPUs for 2.5 days

64 GPUs * 2.8e13 FLOP/s /GPU * 2.5*24*60*60s* 0.3 [utilization rate]

",China,,,,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',,,,,,,,0.00,11300000000.00,"""Wu Dao — Wen Hui has reached 11.3 billion parameters, and through simple fine-tuning can generate poetry, make videos, draw pictures, retrieve text, perform complex reasoning, etc.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",,,,,,,Self-supervised learning,,,Industry,"Wu Dao — Wen Hui is an ultra-large-scale cognitive-oriented pretraining model that focuses on a series of essential problems in general artificial intelligence from a cognitive perspective, aiming to develop and enhance the logic-, consciousness- and reasoning-based cognitive capabilities of pretraining models. Wu Dao — Wen Hui has reached 11.3 billion parameters, and through simple fine-tuning can generate poetry, make videos, draw pictures, retrieve text, perform complex reasoning, etc. BAAI says the model achieves near-human performance on poetry generation on the Turing test.",2024-05-13 09:51,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Big Transformer for Back-Translation,Language,Translation,2018-08-28,,,,,https://arxiv.org/abs/1808.09381,"Facebook AI Research,Google Brain",108084326400000000000.00,"(128) * (28.26 * 10**12) * (27*3600 + 40*60) * (0.3)  = 108084326400000000000
(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  

""We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are intercon-
nected by Infiniband. Experiments are run on 16
machines and we perform 30K synchronous up-
dates. ""
""We train models with 16-bit floating point
operations""
from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957 V100 have 28.26 TFLOPS

in section 5.6 we have

""train this system we perform 300K training up-
dates in 27h 40min on 128 GPUs;""","United States of America,United States of America","Sergey Edunov, Myle Ott, Michael Auli, David Grangier","Highly cited,SOTA improvement","""Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. """,Understanding Back-Translation at Scale,WMT English-German,"""Finally, for WMT English-German we train
on all 226M available monolingual training sen-
tences and perform 250K updates in 22.5 hours on 128 GPUs. """,3390000000,"""Finally, for WMT English-German we train on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs.""

We assume that 1 sentence have 15 words",Likely,,,1155.00,,"""We re-implemented the Transformer model in py-
torch using the fairseq toolkit.1 All experiments
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. ""

I am not sure what authors mean by 'Big Transformer architecture'",,,,27.7,"""training updates in 27h 40min on 128 GPUs""",NVIDIA V100,Supervised,,,,"An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. ",2024-03-06 15:55,Anonymous,,,,,,128,,Big Transformer for Back-Translation,,,,"Industry,Industry",,,,3541,"Industry,Industry",,,
AlphaFold,Biology,"Protein folding prediction,Proteins",2020-01-15,,Unreleased,,Unreleased,https://www.nature.com/articles/s41586-019-1923-7,DeepMind,100000000000000000000.00,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening

""AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.""",United Kingdom of Great Britain and Northern Ireland,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis","SOTA improvement,Highly cited","""AlphaFold represents a considerable advance
in protein-structure prediction."" [Abstract]",Improved protein structure prediction using potentials from deep learning,"PDB (Protein Data Bank),UniRef30 (FKA UniClust30)","""Our models are trained on structures extracted from the PDB"" [""Data"" section]

""For each training sequence, we searched for and aligned to the training sequence similar protein sequences in the Uniclust3035 dataset"" [""Data"" section]",,Multiple tasks! Different units,Speculative,,,2773.00,16340840.00,"""Neural network hyperparameters"" section of https://www.nature.com/articles/s41586-019-1923-7:
“7 × 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8”
“48 × 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8”

""Distogram prediction"" section:
""For the final layer, a position-specific bias was used""

Extended Data Fig.1 (b): 
Shows that each block consists of 9 layers:
(1) Batch norm
(2) Elu
(3) Project down (halves number of dimensions)
(4) Batch norm
(5) Elu
(6) 3x3 kernel with dilation
(7) Batch norm
(8) Elu
(9) Project up (doubles number of dimensions)

Dilations don't change the number of parameters in each filter
Assuming that projection layers are convolutional layers with 1x1 kernels

Parameter estimate for each layer in a 256 channel block:
(1) 256*2            = 512
(2) 0
(3) 1*1*256*128 = 32768
(4) 128*2            = 256 
(5) 0
(6) 3*3*128*128 = 147456
(7) 128*2            = 256 
(8) 0
(9) 1*1*128*256 + 256 = 33024
Total                             = 214272

Parameter estimate for each layer in a 128 channel block:
(1) 128*2            = 256
(2) 0
(3) 1*1*128*64   = 8192
(4) 64*2              = 128
(5) 0
(6) 3*3*64*64     = 36864
(7) 64*2              = 128
(8) 0
(9) 1*1*64*128 + 128 = 8320
Total                   = 53897

Estimate total network = 7*4*214272 + 48*4*53897 = 5992616 + 10348224
                                     = 16340840
                                     ~ 16e6

Within a factor of 2 of the estimate of 21M parameters stated in: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305407/

[Previous approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624]",,,,120.0,"""Training time: about 5 days for 600,000 steps""",,Self-supervised learning,$241.59,,Industry,"Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined.",2024-04-22 15:50,Robi Rahman,,,,,,,,AlphaFold,,,,Industry,,,,,Industry,,,
Mixture-of-Depths,Language,,2024-04-02,,Unreleased,,,https://arxiv.org/abs/2404.02258,"Google DeepMind,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",100000000000000000000.00,"Figure 4: ""We used the 12.5% capacity MoD variant to perform an isoFLOP analysis for 6e18, 2e19, and 1e20 FLOPs, training models varying in size from 60M to 3B parameters""","Multinational,Canada,Canada","David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro",SOTA improvement,"SOTA improvements on transformer loss for a given compute budget, or can match loss with >60% fewer FLOPs per training step. See Figure 3.",Mixture-of-Depths: Dynamically allocating compute in transformer-based language models,,Training set not described.,,,Confident,,,1.00,3000000000.00,"Figure 4: ""We used the 12.5% capacity MoD variant to perform an isoFLOP analysis for 6e18, 2e19, and 1e20 FLOPs, training models varying in size from 60M to 3B parameters""",,,,,,,,,,Industry,"Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (k) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-k routing mechanism. Since k is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the k tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\% faster to step during post-training sampling.",2024-05-22 13:23,Anonymous,,0,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
EfficientNetV2,Vision,Image classification,2021-06-23,"https://github.com/google/automl/tree/master/efficientnetv2

Apache-2.0 license",Open source,,Open source,EfficientNetV2: Smaller Models and Faster Training,"Google,Google Brain",95600000000000000000.00,"Table 7, page 7: 45 hours on 32 TPUv3 cores.

""Each v3 TPU chip contains two TensorCores.""
TPU performance per chip = 123e12 FLOP/s
32 cores = 16 chips

123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOP

https://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3","United States of America,United States of America","Mingxing Tan, Quoc V. Le","Highly cited,SOTA improvement","""EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while 
 training 5x-11x faster using the same computing resources.""",EfficientNetV2: Smaller Models and Faster Training,ImageNet21k,,14197122,,Likely,,,1474.00,208000000.00,"Table 7, page 7",,,,45.0,Table 7,Google TPU v3,Supervised,,,Industry,"This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.
Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.
With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at this https URL.",2024-04-16 10:33,Anonymous,,,,,,,,EfficientNetV2,,,,"Industry,Industry",,,,,"Industry,Industry",,,
MoE,Language,"Language modelling,Translation",2017-01-23,,Unreleased,,,https://arxiv.org/abs/1701.06538,"Jagiellonian University,Google Brain",93939056640000000000.00,"12 days 
64 NVIDIA K40 GPUS (see hardware data sheet for performance)
0.33 util rate
 ","Poland,United States of America","N Shazeer, A Mirhoseini, K Maziarz, A Davis","Highly cited,SOTA improvement","""On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost""",Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,,,100000000000,"[WORDS]

""We constructed a similar training set consisting of shuffled unique sentences from Google’s internal
news corpus, totalling roughly 100 billion words""",,,,1531.00,8700000000.00,"Table 5

https://arxiv.org/abs/1701.06538",,,,288.0,12 days,NVIDIA Tesla K40t,,$8484.35,,Industry,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",2024-04-01 09:03,Robi Rahman,,,,,,64,,MoE,,,,"Academia,Industry",,,,,"Academia,Industry",,,
OpenCALM,Language,Chat,2023-05-15,"CC BY-SA 4.0 license, commercial",Open source,,Unreleased,https://huggingface.co/cyberagent/open-calm-7b,CyberAgent,89510400000000000000.00,6*2131200000*7000000000=8.95104e+19,Japan,Ryosuke Ishigami,,,OpenCALM-7B,"Wikipedia (ja),Japanese CC-100",,2131200000,"Wikipedia ja:
size of en wikipedia: 20.28 GB (https://huggingface.co/datasets/wikipedia_
size of japanese wikipedia: 20.28 GB / 6,825,683 english articles  * 1,416,129 japanese articles = 4.2 GB
wikipedia statistics: https://meta.wikimedia.org/wiki/List_of_Wikipedias
111M japanese words per GB * 4.2 GB = 466200000 words

CommonCrawl ja:
here they mention using cc100
cc-100 has 15G of japanese -> 1665000000 words
https://huggingface.co/datasets/cc100


1665000000+466200000=2131200000 (confidence - speculative because we don't know what subset they used)",Speculative,,,,7000000000.00,7B,,,,,,,,,,,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.",2024-05-21 10:40,Anonymous,,0,,,"They say ""Library: GPT-NeoX"". could mean it's fine-tuned from GPT-NeoX, or just that it uses the same architecture or something? ",,,,,,,Industry,,,,,Industry,checked,,
SciBERT,Language,"Relation extraction,Sentiment classification,Text classification",2019-03-26,apache 2.0: https://github.com/allenai/scibert/,Open source,Open source,Open source,https://arxiv.org/abs/1903.10676,Allen Institute for AI,89268480000000000000.00,"4*123e12*0.3*(7*24*3600) = 8.926848e+19
(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)
We have:
 4 TPUv3 chips.123teraFLOPs per chip.
7 days of training
""We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). """,United States of America,"Iz Beltagy, Kyle Lo, Arman Cohan","Highly cited,SOTA improvement","""We demon-
strate statistically significant improvements
over BERT and achieve new state-of-the-
art results on several of these tasks""",SciBERT: A Pretrained Language Model for Scientific Text,,"""We train SCIBERT on a random
sample of 1.14M papers from Semantic
Scholar (Ammar et al., 2018). """,2475000000,"assuming 0.75 words per token 3.3B*0.75 = 2475000000
""The average paper length is154 sentences (2,769 tokens) resulting in a corpus
size of 3.17B tokens, similar to the 3.3B tokens
on which BERT was trained.""",Likely,,,2808.00,110000000.00,"110M
size of bert base from https://huggingface.co/google-bert/bert-base-uncased
relevant citation: 
""We use the original BERT code to
train SCIBERT on our corpus with the same con-
figuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or un-
cased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.""",,,,168.0,1 week,Google TPU v3,,,,,"Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https://github.com/allenai/scibert/",2024-04-04 13:03,Bartosz Podkanowicz,,,,,,4,,SciBERT,,,,Research collective,,,,672,Research collective,,,
Sparse Wide GPT-3 Small,Language,,2023-03-21,,Unreleased,,,https://arxiv.org/abs/2303.11525,Cerebras Systems,88400000000000000000.00,,Multinational,"Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie",,,Sparse Iso-FLOP Transformations for Maximizing Training Efficiency,,,,,,,,2.00,1300000000.00,,110.00,,,,,,,,,,,2024-03-28 17:40,Robi Rahman,Sparse Wide GPT-3 Small,,,,,,,,,,,Industry,,,,,Industry,,,
Adaptive Input Transformer + RD,Language,Translation,2021-06-28,https://github.com/dropreg/R-Drop,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2106.14448,"Microsoft Research Asia,Soochow University",82000000000000000000.00,,"China,Taiwan","Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu",SOTA improvement,"""In particular, it yields substantial
improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model """,R-Drop: Regularized Dropout for Neural Networks,WMT14,,,,,,,300.00,247000000.00,,,,,,,,,,,,,2024-05-01 09:13,Robi Rahman,Adaptive Input Transformer + RD,,,,,,,Adaptive Input Transformer + RD,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Decaying Fast Weights Transformer (WT-103),Language,,2022-10-09,"They have CUDA kernels, don't see pretrain code: https://github.com/jenni-ai/T2FW",Unreleased,,Unreleased,https://arxiv.org/abs/2210.04243,Jenni,79000000000000000000.00,"Pre-trained model is Transformer (Adaptive Input Embeddings) which was 7.3e19. This is from 8 * 67 V100-hours. 

fine-tuning:

""Training was performed on a single NVIDIA A40 GPU for 35 hours""

35h, 1 GPU, 149.7e12, 30% = 5.7e18 FLOP""

5.7e18 + 7.3e19 is 7.9e19",,Huanru Henry Mao,,,Fine-Tuning Pre-trained Transformers into Decaying Fast Weights,WikiText-103,,,,,,,3.00,242000000.00,"""We fine-tune starting from the checkpoint provided by Baevski
and Auli (2019)8, which has 242M parameters""",192.12,,,,,,,,,,,2024-04-30 12:50,Robi Rahman,Decaying Fast Weights Transformer,,Transformer (Adaptive Input Embeddings),5700000000000000000,"""Training was performed on a single NVIDIA A40 GPU for 35 hours""

35h, 1 GPU, 149.7e12, 30%
5.7e18 FLOP""",,,,,,,,,,,,,,,
DiffDock,Biology,Proteins,2022-10-04,,Open source,,,"https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",Massachusetts Institute of Technology (MIT),72000000000000000000.00,"""We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).""

4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19

https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",United States of America,"Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola",SOTA improvement,"""DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods""","DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",PDB (Protein Data Bank),"""We evaluate our method on the complexes from PDBBind [Liu et al., 2017],
a large collection of protein-ligand structures collected from PDB [Berman et al., 2003], which
was used with time-based splits to benchmark many previous works""",,,Unverified,,,219.00,20240000.00,"""For determining the hyperparameters of DIFFDOCK’s score model, we trained
smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up
to the final model (20.24 million parameters) that was trained on four 48GB GPUs""

There's a separate 4.77M ""confidence model"" that helps make predictions along with the score model",850.00,,,432.0,18 days,NVIDIA RTX A6000,,,,,"Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.",2024-03-20 12:32,Anonymous,,0,,,,,,DiffDock,,,,Academia,,,,,Academia,,,
BEIT-3,"Multimodal,Vision,Language","Object detection,Semantic segmentation,Image classification,Visual question answering,Image captioning,Language generation",2022-08-22,,,,,https://arxiv.org/abs/2208.10442,Microsoft,70000000000000000000.00,"from Table 11, 1M training steps with batch size 6144. 
From Table 2 we have that model have 1.9B parameters.
Model is VIT",United States of America,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,SOTA improvement,"from abstract: 'In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks.'",Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,"ImageNet21k,COCO,English Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""",from Table 3,,"from Table 3
21M pairs image text,
14M images,160GB documents",Likely,,,458.00,1900000000.00,1.9B from Table 2,,,,,,,,,,,"A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked ""language"" modeling on images (Imglish), texts (English), and image-text pairs (""parallel sentences"") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). ",2024-05-13 09:10,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
Mesh-TensorFlow Transformer 2.9B (translation),Language,Language modelling/generation,2018-11-05,,,,,https://arxiv.org/abs/1811.02084,Google Brain,68428800000000000000.00,"flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips",United States of America,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman",SOTA improvement,"'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",Mesh-TensorFlow: Deep Learning for Supercomputers,WMT14,"from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",,,Likely,,,357.00,2900000000.00,"2.9B from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",10.00,,,22.0,"from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",Google TPU v2,,,,,"Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the ""batch"" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .",2024-03-03 14:50,Bartosz Podkanowicz,,,,,,64,,,,,,Industry,,,,1408,Industry,,,
PNASNet-5,Vision,Image classification,2017-12-02,,,,,https://arxiv.org/abs/1712.00559,"Johns Hopkins University,Google AI,Stanford University",66290400000000010000.00,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.
(From Imagenet paper-data, Besiroglu et al., forthcoming) ","United States of America,Multinational,United States of America","C Liu, B Zoph, M Neumann, J Shlens",Highly cited,,Progressive Neural Architecture Search,ImageNet-1k,,1280000,,,,,1819.00,,,,,,,,,,$991.48,,Industry,"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",2024-04-01 09:02,Robi Rahman,,,,,,,,PNASNet-5,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
ProteinBERT,Biology,"Proteins,Protein generation",2022-02-10,,,,,https://academic.oup.com/bioinformatics/article/38/8/2102/6502274,"Hebrew University of Jerusalem,Ben-Gurion University of the Negev,Deep Trading",65000000000000000000.00,"""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ∼670M records""

28 * 24 * 3600 * 89 TFLOP/s * 0.3 (assumed utilization) = 6.5e19
https://www.wolframalpha.com/input?i=28+days+*+89+TFLOP%2Fs+*+0.3","Israel,United States of America,United States of America","Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial",SOTA improvement,"""ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes)""",ProteinBERT: a universal deep-learning model of protein sequence and function,UniRef90,"""ProteinBERT was pretrained on ∼106M UniRef90 records for ∼6.4 epochs""",,,Likely,,,268.00,16000000.00,"""Altogether, it includes ∼16M trainable parameters, making it substantially smaller than other protein language models""",6.40,,,672.0,28 days,NVIDIA Quadro RTX 5000,Self-supervised learning,,,,"Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data.",2024-05-01 09:13,Anonymous,,,,,,,,ProteinBERT,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
LLaVA 1.5,"Multimodal,Language,Vision","Chat,Question answering,Visual question answering",2023-11-05,Llama 2 license,Open access (restricted use),,,"https://arxiv.org/abs/2310.03744,
https://huggingface.co/liuhaotian/llava-v1.5-13b","University of Wisconsin Madison,Microsoft Research",65000000000000000000.00,"6.5e19 finetuning compute. fine-tuned from Vicuna-13B which is fine-tuned Llama-13B, which was 4.55e22 FLOP","United States of America,United States of America","Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",SOTA improvement,"from abstract: ""we establish stronger baselines that achieve state-of-the-art across 11 benchmark""",Improved Baselines with Visual Instruction Tuning,,from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset,1200000,1.2M text-image pairs from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset,Confident,,,504.00,13000000000.00,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,,,,24.0,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,NVIDIA A100,,,,,"Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available. ",2024-05-21 03:44,Bartosz Podkanowicz,,,Vicuna-13B,65000000000000000000,"8 * 312e12 * 24 * 3600 * 0.3 = 6.469632e+19 =  num gpus * peak flops * time in seconds * assumed utilization rate
from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node.""",8,,LLaVA 1.5,,,,"Academia,Industry",,,,192,"Academia,Industry",,,
CTR-BERT,Recommendation,Click-through rate prediction,2021-12-06,,,,,https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf,Amazon,64696320000000000000.00,"flops = (8) * (312 * 10**12) * (24 * 3600) * (0.3)
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'CTR-BERT
uses about 70 million parameters which can be trained on 8 A100 GPUs in less than a day (<1000
USD).
'",United States of America,"Aashiq Muhamed, Iman Keivanloo, Sujan Perera, James Mracek, Yi Xu, Qingjun Cui, Santosh Rajagopalan, Belinda Zeng, Trishul Chilimb
",,,CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models,,"citations: '
Our CTR dataset is sampled from online traffic and is different from existing public CTR datasets in
that it comprises of text features in addition to numeric/categorical features.

We sample random train-test splits from 2020 online traffic. As

The train-test splits are sampled from 2021 online traffic and balanced the
same way as OOD data. The train set comprises 200 million data points and the test and validation
sets comprise 25 million points each'",,"more than 200M
citations: 'Our CTR dataset is sampled from online traffic and is different from existing public CTR datasets in
that it comprises of text features in addition to numeric/categorical features.

We sample random train-test splits from 2020 online traffic. As

The train-test splits are sampled from 2021 online traffic and balanced the
same way as OOD data. The train set comprises 200 million data points and the test and validation
sets comprise 25 million points each'",Likely,,,29.00,70000000.00,"
'CTR-BERTuses about 70 million parameters which can be trained on 8 A100 GPUs in less than a day (<1000
USD).
'",,,,,,NVIDIA A100,,,,,"While pre-trained large language models (LLM) like BERT have achieved state-of-
the-art in several NLP tasks, their performance on tasks with additional grounding
e.g. with numeric and categorical features is less studied. In this paper, we study the
application of pre-trained LLM for Click-through-rate (CTR) prediction for product
advertisement in e-commerce. This is challenging because the model needs to a)
learn from language as well as tabular data features, b) maintain low-latency (<5 ms)
at inference time, and c) adapt to constantly changing advertisement distribution.
We first show that scaling the pre-trained language model to 1.5 billion parameters
significantly improves performance over conventional CTR baselines. We then
present CTR-BERT, a novel lightweight cache-friendly factorized model for CTR
prediction that consists of twin-structured BERT-like encoders for text with a
mechanism for late fusion for text and tabular features. We train the CTR-BERT
model using cross-architecture knowledge distillation (KD) and empirically study
the interaction between KD and distribution shift in this setting, by a) experimenting
with pre-training, distillation pre-finetuning and fine-tuning strategies b) factorizing
features based on their distribution shift time scales, that allows the model to
readily adapt and be re-trained. Finally, we show that CTR-BERT significantly
outperforms a traditional CTR baseline with a 2.3% relative ROC-AUC lift in
offline experiments and a 2% CTR lift in an online experiment",2024-04-01 09:35,Bartosz Podkanowicz,,,,,,8,,,,,,Industry,,,,,Industry,,,
PolyNet,Vision,Image classification,2016-11-17,,,,,https://arxiv.org/abs/1611.05725,Chinese University of Hong Kong (CUHK),64000000000000000000.00,"Section 5: ""ResNet-500 [has] similar computation
costs to our Very Deep PolyNet"".

ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.

560k iterations, batch size 512:
Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",Hong Kong,"X Zhang, Z Li, C Change Loy",SOTA improvement,"""The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.""",PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,ImageNet,Section 4,1280000,,Likely,,,282.00,92000000.00,,,,,,,NVIDIA GTX Titan X,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,32,,PolyNet,,,,Academia,,,,,Academia,,,
GPT-2 + Progressive LRD,Language,,2022-10-12,,Unreleased,,Unreleased,https://neurips2022-enlsp.github.io/papers/paper_33.pdf,"Huawei,Huawei Noah's Ark Lab",62000000000000000000.00,,"China,China","Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, Yang Liu",,,Strategies for Applying Low Rank Decomposition to Transformer-Based Models,,They compressed pre-trained GPT-2. Possibly without finetuning on additional data?,,,,,,0.00,31000000.00,Table 2,,,,,,,,,,,,2024-04-30 12:36,Robi Rahman,Progressive LRD,,GPT-2 (117M),,,,,,,,Progressive LRD,"Industry,Industry",,,,,"Industry,Industry",,,
T2R + Random Init,Language,,2021-03-24,"There's a repo but it's kind of inscrutable with no docs about T2R, not clear if the training code for this paper is in it:

https://github.com/jungokasai/T2R/tree/master ",Unreleased,,Unreleased,https://arxiv.org/abs/2103.13076,"University of Washington,Microsoft,DeepMind,Allen Institute for AI",61000000000000000000.00,,"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America","Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",,,Finetuning Pretrained Transformers into RNNs,,,,,,,,42.00,450000000.00,,205.48,,,,,,,,,,,2024-05-08 11:48,Robi Rahman,T2R + Random Init,,,,,,,,,,,"Academia,Industry,Industry,Research collective",,,,,"Academia,Industry,Industry,Research collective",,,
Sparse all-MLP,Language,,2022-04-14,,Unreleased,,,https://arxiv.org/abs/2203.06850,Meta AI,60770304000000000000.00,"112 hours on 32 V100 GPUs
assumed 0.33 util rate
32*112*60*60*0.3*1.57E+13
",United States of America,"Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li",SOTA improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",Efficient Language Modeling with Sparse all-MLP,RoBERTa dataset,,75000000000,100B tokens (Table 2) so 75B words.,,,,10.00,9400000000.00,"Table 2: ""In Section 4.4, we run our large model (9.41B parameters)""",,,,112.0,,,Self-supervised learning,$320.00,,Industry,"All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2× improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",2024-04-01 09:02,Robi Rahman,,,,,,,,Sparse all-MLP,,,,Industry,,,,,Industry,,,
ConvS2S (ensemble of 8 models),Language,Translation,2017-07-25,,,,,https://arxiv.org/abs/1705.03122,Meta AI,56400000000000000000.00,"All models are implemented in Torch (Collobert et al., 2011) and trained on a single Nvidia M40 GPU except for WMT’14 English-French for which we use a multi-GPU setup on a single
machine. We train on up to eight GPUs synchronously by
maintaining copies of the model on each card and split the
batch so that each worker computes 1/8-th of the gradients;
at the end we sum the gradients via Nvidia NCCL.

1. English-Romanian: ""Training took between 6 and 7.5 days on a single GPU.""
7 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 1.2e18 FLOP

2. English-German: "" We trained this model on a single GPU over a
period of 18.5 days with a batch size of 48"".
18.5 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 3.3e18 FLOP

3. English-French: ""Our results are based on training
with 8 GPUs for about 37 days and batch size 32 on each
worker.6 ""
37 days * 24 * 3600 * 8 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 5.2e19 FLOP

the minimum compute needed to train ensemble model: 1.2e18 FLOP + 3.3e18 FLOP + 5.2e19 FLOP = 5.65e19 FLOP

I am not sure how much to add more (they say ensemble model consists of 8 models), probably summarization training takes at least 1.2e18 FLOP more. 

",United States of America,"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin","Highly cited,SOTA improvement","""We achieve a new state of the art on several public translation benchmark data sets. On the WMT’16 EnglishRomanian task we outperform the previous best result by 1.9 BLEU, on WMT’14 English-French translation we improve over the LSTM model of Wu et al. (2016) by 1.6 BLEU in a comparable setting, and on WMT’14 EnglishGerman translation we ouperform the same model by 0.5
BLEU""",Convolutional Sequence to Sequence Learning,"WMT English-German,WMT14,Gigaword",,46600000," 2.8M + 4.5M + 35.5M + 3.8M = 46.6M

We consider three major WMT translation tasks as well as
a text summarization task.

WMT’16 English-Romanian. We use the same data and pre-processing as Sennrich et al. (2016b) but remove sentences with more than 175 words. This results in 2.8M sentence pairs for training and we evaluate on newstest2016.2

WMT’14 English-German. We use the same setup as Luong et al. (2015) which comprises 4.5M sentence pairs for training and we test on newstest2014.

WMT’14 English-French. We use the full training set of
36M sentence pairs, and remove sentences longer than 175
words as well as pairs with a source/target length ratio exceeding 1.5. This results in 35.5M sentence-pairs for training.

Abstractive summarization. We train on the Gigaword
corpus (Graff et al., 2003) and pre-process it identically
to Rush et al. (2015) resulting in 3.8M training examples
and 190K for validation.",Likely,,,,,,,,,,,NVIDIA M40,,,,Industry,"The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",2024-04-10 13:00,Natalia Martemianova,,,,,,,,,,,,Industry,,56500000000000000000,,,Industry,,,
Seq2Seq LSTM,Language,Translation,2014-09-10,,,,,https://arxiv.org/abs/1409.3215,Google,56000000000000000000.00,"384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP",United States of America,"I Sutskever, O Vinyals, QV Le",Highly cited,,Sequence to Sequence Learning with Neural Networks,WMT14,,652000000,"[WORDS]
""We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”
subset from [29].""",,,,18626.00,1920000000.00,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
for the “decoder” LSTM).
The paper uses an ensemble of 5 LSTMs.",,,,,,,,$79.60,,Industry,,2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ESM1-85M,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-08-31,MIT: https://github.com/facebookresearch/esm,Open source,Open source,Open source,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,"Facebook AI Research,New York University (NYU)",56000000000000000000.00,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
840k steps [See Table S2: Hyperparameters]
131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]

Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 85.1e6 parameters = 5.6e19 FLOP","United States of America,United States of America","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,UniRef50,"""the high-diversity sparse dataset (UR50/S) uses the UniRef50 representative sequences""",,,Confident,,,1282.00,85100000.00,See Table 1,,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-04-04 11:48,Epoch AI,,1,,,,128,,ESM1-85M,,,,"Industry,Academia",,,,,"Industry,Academia",,,
YOLOv3,Vision,Object detection,2018-04-08,,,,,https://arxiv.org/abs/1804.02767,University of Washington,50939199919999990000.00,"We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples

Assuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf",United States of America,"Joseph Redmon, Ali Farhadi",Highly cited,,YOLOv3: An Incremental Improvement,ImageNet,,1281167,Source: https://image-net.org/download.php,,,,17124.00,56933216.00,"Feature extractor (ignoring biases)
32*3*3*3 +
64*3*3*32 +
32*1*1*64 +
64*3*3*32 +
128*3*3*64 +
2*(64*1*1*128 +
128*3*3*64) +
256*3*3*128 +
8*(128*1*1*256 +
256*3*3*128) +
512*3*3*256 + 
8*(256*1*1*512 + 
512*3*3*256) + 
1024*3*3*512 + 
4*(512*1*1*1024 +
1024*3*3*512) +
4*4*1024*1000

source: table 1
This is assuming the average pooling step changes the output size from 8x8 to 4x4.

The weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.",,18700000000.00,"Table 2, Darknet-53. Note that the inference compute depends on the image resolution..",,,"NVIDIA M40,NVIDIA GTX Titan X",,$295.76,,Academia,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",2024-04-01 09:30,Robi Rahman,,,,,,,,YOLOv3,,,,Academia,,,,,Academia,,,
LLaVA,"Multimodal,Vision,Language","Chat,Question answering,Visual question answering",2023-04-17,apache 2.0,Open source,Open source,Open source,https://arxiv.org/abs/2304.08485,"University of Wisconsin Madison,Microsoft Research,Columbia University",48522240000000000000.00,"8*312e12*(10+8)*3600*0.3 = 4.852224e+19
num gpus * peak flops * time *assumed utilization rate 
""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time, 8 A100,","United States of America,United States of America,United States of America","Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",SOTA improvement,"When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.",Visual Instruction Tuning,,"""We pre-train our model
on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and
fine-tune on the proposed LLaVA-Instruct-158K dataset """,,"595K + 158K = 753K image text pairs
""This results in
around 595K image-text pairs""
""We collect 158K unique language-image instruction-following samples in total, including 58K in
conversations, 23K in detailed description, and 77k in complex reasoning, respectively. """,Likely,,,1091.00,13000000000.00,13B,,,,10.0,"""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning
on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.""",NVIDIA A100,,,,,"Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. ",2024-05-01 09:24,Bartosz Podkanowicz,,,,,,8,,LLaVA,,,,"Academia,Industry,Academia",,,,80,"Academia,Industry,Academia",,,
MuZero,Games,Atari,2019-11-19,,Unreleased,,Unreleased,https://arxiv.org/abs/1911.08265v2,DeepMind,48000000000000000000.00,"third-generation Google Cloud TPU
(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)
For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play
Training for 12 hours (for Atari)
Data from Parameter, Compute and Data Trends in Machine Learning
Google v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)
Utilization rate 
In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%
Calculations for Atari:
12 hours → 43200 seconds
(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP
Training time missing for boardgames
Assumption also 12 hours 
Also: 2.4017472 * 10^19 FLOP
Total cost ≈ 4.8 * 10^19 FLOP",United Kingdom of Great Britain and Northern Ireland,"J Schrittwieser, I Antonoglou, T Hubert, K Simonyan","Highly cited,SOTA improvement",,Mastering Atari Go Chess and Shogi by Planning with a Learned Model,,,20000000000,"Table 1
https://arxiv.org/pdf/1911.08265.pdf",,,,1494.00,36864000.00,"Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.

Previous downsampling:
•  1 convolution with stride 2 and 128 output planes, output resolution 48x48.•  2 residual blocks with 128 planes•  1 convolution with stride 2 and 256 output planes, output resolution 24x24.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 12x12.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 6x6.",,,,,,,Self-supervised learning,$121.18,,Industry,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",2024-04-22 13:28,Robi Rahman,,,,,,,,MuZero,,,,Industry,,,,,Industry,,,
ESM2-8M,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2022-07-21,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd",Open source,Open source,Open source,https://www.science.org/doi/abs/10.1126/science.ade2574,"Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",48000000000000000000.00,"""All language models were trained for 500K updates, except the 15B language model"" 
""All models used 2 million tokens as batch size except the 15B model""
[Supplementary Materials]

Hence: 1000B training tokens (500k steps, 2M tokens/batch)

Estimate: 8M*2*1000B + 8M*4*1000B","United States of America,United States of America,United States of America,United States of America","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",Evolutionary-scale prediction of atomic-level protein structure with a language model,UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",1000000000000,"""All language models were trained for 500K updates, except the 15B language model"" 
""All models used 2 million tokens as batch size except the 15B model""
[Supplementary Materials]

Hence: 1000B training tokens (500k steps, 2M tokens/batch)",Likely,,,636.00,8000000.00,In the name,,,,,,,Unsupervised,,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",2024-05-06 10:10,Epoch AI,,1,,,,,,ESM2-8M,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
All-attention network + adaptive span,Language,,2019-07-02,,Unreleased,,Unreleased,https://arxiv.org/abs/1907.01470,Facebook AI Research,46000000000000000000.00,,United States of America,"Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin",,,Augmenting Self-attention with Persistent Memory,WikiText-103,,,,,,,103.00,133000000.00,,,,,,,,,,,,,2024-05-22 16:34,Robi Rahman,All-attention network + adaptive span,,,,,,,,,,,Industry,,,,,Industry,,,
ASE,Robotics,,2022-05-05,,,,,https://arxiv.org/abs/2205.01906,"NVIDIA,UC Berkeley",44928000000000000000.00,"Training was done using the Isaac Gym simulator on an NVIDIA V100 GPU. The model was trained on over 10 billion samples, which equates to 10 years of simulated experience time. Training took around 10 days on a single GPU.
1.3e14 * 10 * 24 * 3600 * 0.4 = 4.49e19","United States of America,United States of America","Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler",,,ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters,,"The model is trained on a dataset of 187 motion clips, about 30 minutes of human motion capture data, depicting locomotion and sword wielding motions.",,,Likely,,,56.00,,,,,,240.0,Training took around 10 days on a single GPU.,NVIDIA Tesla V100 PCIe 16 GB,Reinforcement learning,,,Industry,"The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.",2024-05-21 04:24,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Feedback Transformer,Language,,2020-02-21,,Unreleased,,Unreleased,https://arxiv.org/abs/2002.09402,"LORIA,University of Lorraine,Facebook AI Research",44100000000000000000.00,,"France,France,United States of America","Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar",SOTA improvement,"""As shown in Table 4, the Feedback
Transformer model achieves a new SOTA performance (on Enwiki8) of 0.96 bit-per-byte despite its small size.""",Addressing Some Limitations of Transformers with Feedback Memory,WikiText-103,,,,,,,41.00,126000000.00,,267.23,,,,,,,,,,,2024-04-23 10:03,Robi Rahman,Feedback Transformer,,,,,,,Feedback Transformer,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
SeqVec,Biology,Proteins,2019-12-17,"MIT license. doesn't look like it has training code
https://github.com/rostlab/SeqVec",Open source,Open source,Unreleased,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8,Technical University of Munich,41000000000000000000.00,"3 weeks, 5 NVIDIA Titan GPUs (Assuming NVIDIA Titan V and 30% utilization rate for calculation) with 12 GB memory, ",Germany,"Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes & Burkhard Rost ",,,Modeling aspects of the language of life through transfer-learning protein sequences,UniRef50,,,,Likely,,,,93000000.00,"""The model had about 93 M (mega/million) free parameters""",,,,508.0,,NVIDIA Titan V,,,,,"Background
Predicting protein function and structure from sequence is one important challenge for computational biology. For 26 years, most state-of-the-art approaches combined machine learning and evolutionary information. However, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary information is less powerful for small families, e.g. for proteins from the Dark Proteome. Both these problems are addressed by the new methodology introduced here.
Results
We introduced a novel way to represent protein sequences as continuous vectors (embeddings) by using the language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively captured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new embeddings as SeqVec (Sequence-to-Vector) and demonstrate their effectiveness by training simple neural networks for two different tasks. At the per-residue level, secondary structure (Q3 = 79% ± 1, Q8 = 68% ± 1) and regions with intrinsic disorder (MCC = 0.59 ± 0.03) were predicted significantly better than through one-hot encoding or through Word2vec-like approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10 = 68% ± 1) and membrane-bound were distinguished from water-soluble proteins (Q2 = 87% ± 1). Although SeqVec embeddings generated the best predictions from single sequences, no solution improved over the best existing method using evolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary information and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of protein sequences. Overall, the important novelty is speed: where the lightning-fast HHblits needed on average about two minutes to generate the evolutionary information for a target protein, SeqVec created embeddings on average in 0.03 s. As this speed-up is independent of the size of growing sequence databases, SeqVec provides a highly scalable approach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis.
Conclusion
Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for various protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein sequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary information, however, that information is not available on the level of a single sequence.",2024-03-29 16:53,Anonymous,,,,,,5,,,,,,Academia,,,,2540,Academia,,,
RNMT+,Language,Translation,2018-04-26,,,,,https://arxiv.org/abs/1804.09849,Google AI,39506227200000000000.00,"32 * 9.526 TFLOPS * (120 * 3600) * 0.3 = 39506227200000000000
(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)
""All models were trained with synchronous
training. RNMT+ and ConvS2S were trained with
32 NVIDIA P100 GPUs ""
performence of P100 is 9.526 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-16-gb.c2888
training time is 120h from Table 1",Multinational,"Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, Macduff Hughes",,,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,WMT14,"""Table 1 shows our results on the WMT’14 En→Fr task.""",544500000,"""We train our models on the standard WMT’14 En→Fr and En→De datasets that comprise 36.3M""

We estimate 15 english words in one sentence.",Likely,,,504.00,378900000.00,from Table 3 RNMT+ ,8.50,28100000000.00,from Table 3,120.0,from Table 1,NVIDIA P100,Supervised,,,,"The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets. ",2024-02-02 16:45,Anonymous,,,,,,32,,,,,,Industry,,,,3840,Industry,,,
ATLAS,Language,Question answering,2020-05-02,Apache 2.0 license: https://github.com/allenai/unifiedqa,Open source,,Open source,https://arxiv.org/abs/2005.00700,"Allen Institute for AI,University of Washington",38257920000000000000.00,"flops = (8) * (123 * 10**12) * (36 * 3600) * (0.3)
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Appendix A.2: ""Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""
so 36h for T5

""Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for BART models.""

from https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_chip
tpu chip have peak flops 123 teraflops
so 8 chips have peak flops 123 * 8","United States of America,United States of America","Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",SOTA improvement,"from abstract: ""Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets""",UnifiedQA: Crossing Format Boundaries With a Single QA System,SQuAD 1.1,"from appendix A.1 - multiple QA datasets, In section 3 there is description how batches are created from multiple datasets.",,"from appendix A.1 - multiple QA datasets - it may be possible to estimate by summing sizes of all datasets
I am not sure if all data is used as system is trained for 100K steps (from appendix A.2)
with batch size 8 (appendix A.1)",Likely,,,599.00,11000000000.00,"11B from appendix A.2 : Model sizes: ""Most of the experiments are done on T5(11B) which has 11 billion parameters. We also report experiments with BART (large) with 440 million parameters.""",,,,36.0,"Appendix A.2: Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.",Google TPU v3,,,,,"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.",2024-05-01 09:06,Anonymous,,,,,,,,ATLAS,,,,"Research collective,Academia",,,,,"Research collective,Academia",,,
DALL-E mini,Vision,Text-to-image,2021-10-26,"Apache 2.0

train code here: https://github.com/borisdayma/dalle-mini/blob/main/tools/train/train.py

data is open with various licenses",Open source,,Open source,https://huggingface.co/dalle-mini/dalle-mini,Craiyon,38000000000000000000.00,"trained on a TPU v3-8 for 72 hours. That's 8 TPUv3 cores, or 4 TPUv3 chips (123 teraflop/s each)

flops = (4) * (123 * 10**12) * (72 * 3600) * (0.3) = 3.8e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions",,"Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and Lê Khắc, Phúc and Melas, Luke and Ghosh, Ritobrata",,,DALL·E Mini Model Card ,,"Conceptual Captions Dataset, which contains 3 million image and caption pairs.
﻿Conceptual 12M, which contains 12 million image and caption pairs.
The OpenAI subset of YFCC100M, which contains about 15 million images and that we further sub-sampled to 2 million images due to limitations in storage space. 
from https://huggingface.co/dalle-mini/dalle-mini#training-data",30000000,3M+12M+15M = 30M from https://huggingface.co/dalle-mini/dalle-mini#training-data,Likely,,,,,,,,,72.0,from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions,Google TPU v3,,,,,,2024-04-10 08:51,Bartosz Podkanowicz,,,,,,4,,,,,,,,,,288,,,,
ProxylessNAS,Vision,Image classification,2019-02-23,"MIT for code+weights

https://github.com/MIT-HAN-LAB/ProxylessNAS",Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/1812.00332,Massachusetts Institute of Technology (MIT),37065600000000000000.00,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.

At FP32, a V100 GPU has a peak performance of 1.56E+14 FLOPS.

Utilization rate of 0.33.",United States of America,"Han Cai, Ligeng Zhu, and Song Han",Highly cited,,ProxylessNAS: Direct neural architecture search on target task and hardware,ImageNet,,1280000,,,,,1806.00,,,,262548000000.00,"5.1 Miliseconds on a V100 GPU
",,,NVIDIA V100,,$135.04,,Academia,"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",2024-04-16 08:58,Robi Rahman,,,,,,,,ProxylessNAS,,,,Academia,,,,200,Academia,,,
Population-based DRL,Games,Capture the flag,2018-07-03,,,,,https://arxiv.org/abs/1807.01281,DeepMind,34900000000000000000.00,"Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",United Kingdom of Great Britain and Northern Ireland,"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",SOTA improvement,"Qualitatively clearly SOTA: ""In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag (28), using only pixels and game points as input... proved far stronger than existing state-of-the-art agents""",Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,,,,,,,,625.00,122000000.00,"Calculated from the architecture schematic in Figure S11 on pg 55 of the Capture the Flag supplementary materials. This is dominated by the size of the vision module, which is 116 million parameters, followed by the temporal processors which is 4.3 million parameters. The RL policy itself is only 0.79 million parameters. Also, I'm pretty uncertain if I'm right about how I calculated these parameters.

Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,60000000000.00,"""Agents were trained for two billion steps, corresponding to approximately 450K
games.""

""We train a
population of 30 different agents together, which provides a diverse set of teammates and opponents to
play with, and is also used to evolve the internal rewards and hyperparameters of agents and learning
process""

30 * 2e9 = 6e10",,,,Self-supervised learning,$130.36,,Industry,"Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.",2024-05-01 09:13,Robi Rahman,,,,,,,,Population-based DRL,,,,Industry,,,,,Industry,,,
QT-Opt,"Robotics,Vision",,2018-06-27,There is a public implementation of the architecture at https://github.com/quantumiracle/QT_Opt (not from any of the paper's co-authors),Unreleased,,,https://arxiv.org/abs/1806.10293,"Google Brain,UC Berkeley",34875000000000000000.00,"""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  9.30E+12 FLOP/sec/GPU * 10 GPU = 3.4875E+19","United States of America,United States of America","Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine",Highly cited,,QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation,,"""... we collected over 580k grasps over the course of several weeks across 7 robots""
",5984870,"Observations take up 4TB of disk space, and the input space is a 472x472 RGB image.

Assuming 24 bit depth color (8 bits per channel), that suggests 472 * 472 * 3 * 8 bits = 668.352 kB per image (this could be off by a factor of 2 depending on actual bit depth)

4 TB / 668.352 kB = 5,984,870 images; around 10 per grasp attempt.

15M gradient steps with batchsize 32 implies:
15M steps * 32 images/step * 1/5984870 images ~= each image seen 80 times",Likely,,,1442.00,1200000.00,"""The Q-function Qθ(s, a) is represented in our system by a large convolutional neural network with 1.2M parameters""",80.00,,,104.2,"""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  1/3600 hours/second = 104.2 hours",NVIDIA P100,Reinforcement learning,$957.60,"Using cost from ML Hardware Data spreadsheet,
$0.919/hr/GPU * 104.2 hours * 10 GPUs = $957.60

Likely an underestimate, as the cloud pricing comes from 2023 and incorporates 5 additional years of depreciation on the P100.",Industry,"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",2024-04-09 21:06,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
TRIMELMext (247M),Language,,2022-05-25,"code and weights, no clear license: https://github.com/princeton-nlp/TRIME?tab=readme-ov-file ",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2205.12674,Princeton University,31200000000000000000.00,,United States of America,"Zexuan Zhong, Tao Lei, Danqi Chen",,,Training Language Models with Memory Augmentation,WikiText-103,,,,,,,80.00,247000000.00,,204.72,,,,,,,,,,,2024-05-06 13:21,Robi Rahman,TRIMELMext (247M),,,,,,,,,,,Academia,,,,,Academia,,,
TAPE Transformer,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2019-06-19,"BSD license. code, data, and weights:
https://github.com/songlab-cal/tape?tab=readme-ov-file#data",Open source,Open source,Open source,https://arxiv.org/abs/1906.08230,"UC Berkeley,Covariant,Google,Chan Zuckerberg Initiative",30000000000000000000.00,"""All self-supervised models are trained on four NVIDIA V100 GPUs for one week""

(7 * 24 * 3600) s * 4 GPUs * 3.1e13 FLOP/s * 0.4 (utilization assumption) = 3e19","United States of America,Multinational,United States of America,United States of America","Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, Yun S. Song",,,Evaluating Protein Transfer Learning with TAPE,Pfam,"""We use Pfam [33], a database of thirty-one million protein domains used extensively in bioinformatics, as the pretraining corpus for TAPE""",,31M proteins,Likely,,,667.00,38000000.00,"""We use a 12-layer Transformer with a hidden size of 512 units and 8 attention heads, leading to a 38M-parameter model""",,,,168.0,,NVIDIA V100,Self-supervised learning,,,,"Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling
paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.",2024-04-01 16:59,Anonymous,,,,,,4,,,,,,"Academia,Industry,Research collective",,,,672,"Academia,Industry,Research collective",,,
ERNIE-Doc (247M),Language,,2020-12-31,"weights available, not sure there's training code for WT-103: https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-doc",Open source,,Unreleased,https://arxiv.org/abs/2012.15688,Baidu,29100000000000000000.00,,China,"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA improvement,"""ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText103""",ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,"Wikipedia,CC-News,CC-Stories,""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,,,,,,37.00,247000000.00,,190.88,,,,,,,,,,,2024-05-10 10:58,Robi Rahman,ERNIE-Doc (247M),,,,,,,ERNIE-Doc (247M),,,,Industry,,,,,Industry,,,
DEQ-Transformer (Post-LN) + Jacobian Regularisation,Language,,2021-06-28,"code and pretrained models, MIT: https://github.com/locuslab/deq/tree/master/DEQ-Sequence",Open source,,Open source,https://arxiv.org/abs/2106.14342,"Carnegie Mellon University (CMU),Intel Labs",29000000000000000000.00,,"United States of America,Multinational","Shaojie Bai, Vladlen Koltun, J. Zico Kolter",,,Stabilizing Equilibrium Models by Jacobian Regularization,WikiText-103,,,,,,,45.00,98000000.00,,23.00,,,,,,,,,,,2024-05-07 09:52,Robi Rahman,DEQ-Transformer (Post-LN) + Jacobian Regularisation,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
CaLM,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2022-12-19,,,,,https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf,University of Oxford,29000000000000000000.00,"""4 NVIDIA Quadro RTX4000 GPUs for 40 days""

Calculation assuming FP32, utilization 30%:
= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU",United Kingdom of Great Britain and Northern Ireland,Carlos Outeiral and Charlotte M. Deane,SOTA improvement,"""We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters"" [Abstract]",Codon language embeddings provide strong signals for protein engineering,European Nucleotide Archive (ENA),"""The training set was constructed from the European Nucleotide Archive [39], with significant preprocessing to limit redundancy and save computational cost.""",9000000,"""a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing""",Likely,,,5.00,86000000.00,"""We trained a large language model with 86M parameters""",14.00,,,960.0,"""The model reported in this work was trained on 4 NVIDIA Quadro
RTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)""",NVIDIA Quadro RTX 4000,,,,,"Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models’ capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks.
In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.",2024-03-11 16:13,Anonymous,,,,,,4,,CaLM,,,,Academia,,,,3840,Academia,,,
B2T connection (16L),Language,,2022-06-01,"code and weights, MIT: https://github.com/takase/b2t_connection?tab=readme-ov-file
",Open source,,Open source,https://arxiv.org/abs/2206.00330,"LINE Corporation,Tohoku University",28000000000000000000.00,"192*7 GPU (P100) hours per Table 6

19 TFLOP/s 

19 trillion * 192 * 7 * 3600 * 0.3 = 2.76e19
","Japan,Japan","Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki",,,On Layer Normalizations and Residual Connections in Transformers,WikiText-103,,,,,,,6.00,247000000.00,,150.00,,,,,,,,,,,2024-05-06 13:17,Robi Rahman,B2T connection (16L),,,,,,,,,,B2T connection (16L),"Industry,Academia",,,,,"Industry,Academia",,,
ESM1-43M,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2020-08-31,MIT: https://github.com/facebookresearch/esm,Open source,Open source,Open source,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,"Facebook AI Research,New York University (NYU)",28000000000000000000.00,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
840k steps [See Table S2: Hyperparameters]
131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]

Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 42.6e6 parameters = 2.8e19 FLOP","United States of America,United States of America","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus","Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,UniRef50,"""the high-diversity sparse dataset (UR50/S) uses the UniRef50 representative sequences""",,,Confident,,,1282.00,42600000.00,See Table 1,,,,,,NVIDIA V100,,,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2024-04-04 11:48,Epoch AI,,1,,,,128,,ESM1-43M,,,,"Industry,Academia",,,,,"Industry,Academia",,,
TaLK Convolution,Language,,2020-02-08,"MIT, code and weights (though this repo is for translation not WT-103):

https://github.com/lioutasb/TaLKConvolutions?tab=readme-ov-file",Unreleased,,Open source,https://arxiv.org/abs/2002.03184,Carleton University,27800000000000000000.00,,Canada,"Vasileios Lioutas, Yuhong Guo",SOTA improvement,"""[We] set a new state-of-the-art result on the
IWSLT De-En and CNN-DailyMail datasets""",Time-aware Large Kernel Convolutions,WikiText-103,,,,,,,28.00,240000000.00,Table 5,187.43,,,,,,,,,,,2024-04-22 16:57,Robi Rahman,TaLK Convolution,,,,,,,TaLK Convolution,,,,Academia,,,,,Academia,,,
"Segatron XL large, M=384",Language,,2020-04-30,"training code and weights, no clear license: https://github.com/rsvp-ai/segatron_aaai?tab=readme-ov-file ",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2004.14996,"University of Waterloo,Peking University,RSVP.ai",26500000000000000000.00,,"Canada,China,China","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",,,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,WikiText-103,,,,,,,13.00,257000000.00,,167.02,,,,,,,,,,,2024-05-16 14:43,Robi Rahman,"""Segatron XL large, M=384""",,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
"Segatron-XL large, M=384 + HCP",Language,,2022-03-21,"code, no license
https://github.com/richardbaihe/robustlm",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2203.10692,"Microsoft Research,University of Waterloo",26500000000000000000.00,,"United States of America,Canada","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",SOTA improvement,"""Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV""",Better Language Model with Hypernym Class Prediction,,,,,,,,8.00,257000000.00,,167.02,,,,,,,,,,,2024-05-01 09:22,Robi Rahman,"""Segatron-XL large, M=384 + HCP""",,,,,,,"""Segatron-XL large, M=384 + HCP""",,,,"Industry,Academia",,,,,"Industry,Academia",,,
DeepSpeech2 (English),Speech,Speech recognition,2015-12-08,,,,,https://arxiv.org/abs/1512.02595,Baidu Research - Silicon Valley AI Lab,26000000000000000000.00,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",United States of America,"Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu",Highly cited,,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,,,163339200,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""

11,940 * 13,680 = 163339200",,,,2749.00,38000000.00,All networks have 38 million parameters.,,1800000000.00,,120.0,"""5 days"" from AI and Compute https://openai.com/index/ai-and-compute/",NVIDIA GTX Titan X,,$150.78,,Industry,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",2024-05-13 15:55,Robi Rahman,,,,,,16,0.4500,DeepSpeech2 (English),,,,Industry,,,,1920,Industry,,,
MultiBand Diffusion,Audio,Audio generation,2023-11-08,MIT for weights and code,Open source,,Open source,https://arxiv.org/abs/2308.02560,"Meta AI,Hebrew University of Jerusalem,LORIA",26000000000000000000.00,"""It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.""

125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number)
4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19","United States of America,Israel,France","Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre Défossez",SOTA improvement,"""At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality""",From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion,"Common Voice,DNS","""We train on a diverse set of domains and data. We use speech from the train set of Common Voice 7.0
(9096 hours) [Ardila et al., 2019] together with the DNS challenge 4 (2425 hours) [Dubey et al., 2022].
For music, we use the MTG-Jamendo dataset (919h) [Bogdanov et al., 2019]. For the environmental
sound we use FSD50K (108 hours) [Fonseca et al., 2021] and AudioSet (4989 hours) [Gemmeke
et al., 2017]. We used AudioSet only for the research that is described in the publication and for the
benefit of replicability. For evaluation, we also use samples from an internal music dataset.""",,~16k hours,Likely,,,2.00,,,,,,48.0,around 2 days,NVIDIA V100,,,,,"Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",2024-04-01 09:52,Anonymous,,0,,,,,,MultiBand Diffusion,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Transformer local-attention (NesT-B),Vision,,2021-05-26," Apache-2.0 license
https://github.com/google-research/nested-transformer",Open source,,Open source,https://arxiv.org/abs/2105.12723v4,"Google Cloud,Google Research",24057600000000000000.00,"17.9 GFLOPS per forward pass
300 epochs
1.28M training examples
3.5 f_to_b pass ratio
(From Imagenet paper-data, Besiroglu et al., forthcoming) ","Multinational,Multinational","Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Arık, Tomas Pfister",Highly cited,,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",ImageNet-1k,,1280000,,,,,5734.00,90100000.00,"Table A2, NesT-B is the largest size.",,,,,,,Self-supervised learning,$39.51,,Industry,"Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8× faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL.",2024-04-16 11:49,Robi Rahman,,,,,,,,Transformer local-attention (NesT-B),,,,"Industry,Industry",,,,,"Industry,Industry",,,
"LSTM (Hebbian, Cache, MbPA)",Language,,2018-03-27,,,,,https://arxiv.org/abs/1803.10049,"DeepMind,University College London (UCL)",24000000000000000000.00,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap",SOTA improvement,"""We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.""",Fast Parametric Learning with Activation Memorization,,,4300000000,"Omniglot: 32k images
Wikitext-103: ""Over 100 million tokens""
Guttenberg: 175,181,505 tokens
GigaWord v5: 4B tokens",,,,46.00,45200000.00,,90.00,,,144.0,6 days,NVIDIA P100,,,,,,2024-04-26 15:39,Robi Rahman,"""LSTM (Hebbian, Cache, MbPA)""",,,,,8,,"""LSTM (Hebbian, Cache, MbPA)""",,,,"Industry,Academia",,,,,"Industry,Academia",,,
DeLight,Language,,2020-08-03,MIT code: https://github.com/sacmehta/delight,Unreleased,,Open source,https://arxiv.org/abs/2008.00623,"University of Washington,Allen Institute for AI,Facebook AI Research",24000000000000000000.00,,"United States of America,United States of America,United States of America","Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",SOTA improvement,"""Comparison with state-of-the-art methods on machine translation corpora. DeLighT delivers
similar or better performance than state-of-the-art models with fewer parameters.""",DeLighT: Deep and Light-weight Transformer,WikiText-103,,,,,,,98.00,99000000.00,,62.14,,,,,,,,,,,2024-04-24 15:56,Robi Rahman,DeLight,,,,,,,DeLight,,,,"Academia,Research collective,Industry",,,,,"Academia,Research collective,Industry",,,
"MSRA (C, PReLU)",Vision,Image classification,2015-02-06,,,,,https://arxiv.org/abs/1502.01852,Microsoft Research,23974030080000000000.00,"""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",United States of America,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1280000,"""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",,,,20078.00,87048800.00,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

",,,,588.0,,,,$2166.22,,Industry,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Detic,Vision,"Object detection,Image classification",2022-01-07,Apache,Open source,Open source,Open source,https://arxiv.org/abs/2201.02605,"Meta AI,University of Texas at Austin",23439974400000000000.00,"28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization rate
for Swin-B model from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'","United States of America,United States of America",Detecting Twenty-thousand Classes using Image-level Supervision,SOTA improvement,"""On open-vocabulary COCO, our method outperforms the previous state-of-the-art OVR-CNN [ 72 ] by 5 point with the same detector and data""",Detecting Twenty-thousand Classes using Image-level Supervision,"ImageNet21k,Conceptual Captions (CC3M)","table above section 5.1
""We evaluate Detic on the large-vocabulary object detection dataset LVIS [18 ]""
""Image-supervised data. We use two sources of image-supervised data: ImageNet-
21K [10] and Conceptual Captions """,16900000,"14M + 1.5M + 1.2M + 100K + 100K = 16900000.0
table above section 5.1",Speculative,,,367.00,88000000.00,"from https://github.com/microsoft/Swin-Transformer Swin-B have 88M, 
from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'",,,,24.0,"from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100
GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'",NVIDIA V100,,,,," Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \url{this https URL}. ",2024-05-01 09:13,Bartosz Podkanowicz,,,,,,32,,Detic,,,,"Industry,Academia",,,,768,"Industry,Academia",,,
KataGo,Games,Go,2019-02-27,permissive license: https://github.com/lightvector/KataGo/blob/master/LICENSE,Open source,Unreleased,Open source,https://arxiv.org/abs/1902.10565,Jane Street,23200000000000000000.00,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""
14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",Multinational,David J. Wu,SOTA improvement,Better than ELF OpenGo while using 1/50th the compute.,Accelerating Self-Play Learning in Go,,"Self-play: ""In total, KataGo’s main run lasted for 19 days using a maximum of 28 V100 GPUs at any time (averaging 26-27) and generated about 241 million training samples across 4.2 million games.""",241000000,241 million training samples across 4.2 million games,Speculative,,,75.00,2500000.00,https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.,,,,456.0,27 processors for 19 days,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,,,Industry,"By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.",2024-05-01 09:13,Anonymous,,,,,,,,KataGo,,,,Industry,,,,,Industry,,,
UniRep,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2019-03-26,"creative commons non-commercial for weights, GNU General Public License for code. data is UniRef50, which has a commercial license

https://github.com/churchlab/UniRep",Open access (non-commercial),Open source,Open access (restricted use),https://www.nature.com/articles/s41592-019-0598-1,Harvard University,22000000000000000000.00,"""Training was performed using data parallelism on four Nvidia K80 GPUs (mLSTM-1,900) or two Nvidia K-40s (4× mLSTM-256, 4× mLSTM-64). The mLSTM-1,900 model was trained for ~770,000 weight updates, or ~3.5 weeks wall clock time, corresponding to ~1 epoch."" [Methods - Unsupervised training dataset]

Assuming 30% utilization rate and single-precision performance

Estimate: 3.5 weeks * 7 days/week * 24 hours/day * 60 min/hour * 60 sec/min * 4 GPUs *8.73e12 FLOP/sec * 0.3",United States of America,"Ethan C. Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi & George M. Church  ",,,Unified rational protein engineering with sequence-based deep representation learning,UniRef50,"""we use a recurrent neural network (RNN) to learn statistical representations of proteins from ~24 million UniRef50 (ref. 22) sequences""",,~24M protein sequences,Likely,,,812.00,18200000.00,"""1,900-dimensional single-layer multiplicative LSTM (~18.2 million parameters)""",1.00,,,588.0,,NVIDIA Tesla K80,Self-supervised learning,,,Academia,"Rational protein engineering requires a holistic understanding of protein function. Here, we apply deep learning to unlabeled amino-acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich and structurally, evolutionarily and biophysically grounded. We show that the simplest models built on top of this unified representation (UniRep) are broadly applicable and generalize to unseen regions of sequence space. Our data-driven approach predicts the stability of natural and de novo designed proteins, and the quantitative function of molecularly diverse mutants, competitively with the state-of-the-art methods. UniRep further enables two orders of magnitude efficiency improvement in a protein engineering task. UniRep is a versatile summary of fundamental protein features that can be applied across protein engineering informatics.",2024-04-01 17:09,Anonymous,,,,,,4,,,,,,Academia,,,,2352,Academia,,,
ResNet-152 + ObjectNet,Vision,Object recognition,2019-09-06,,Unreleased,Open access (restricted use),Unreleased,https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf,Massachusetts Institute of Technology (MIT),19400000000000000000.00,"3-5 days of training (say, 4.5), 50 teraFLOP/second at 50% utilization rate (reported) = 1.94E19",United States of America,"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz",Highly cited,,Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,ObjectNet,,50000,"In total, 95,824 images were collected from 5,982 workers out of which 50,000 images were retained
after validation and included in the dataset",,,,2393.00,38000000.00,,,,,,,,,$50.79,,Academia,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",2024-04-18 15:00,Robi Rahman,,,,,,,,ObjectNet,,,,Academia,,,,,Academia,,,
GPT,Language,,2018-06-01,"MIT
https://github.com/openai/finetune-transformer-lm/blob/master/LICENSE",Open source,Open source,,https://openai.com/blog/language-unsupervised/,OpenAI,17578125000000000000.00,"COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE

""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""
",United States of America,"A Radford, K Narasimhan, T Salimans, I Sutskever",Highly cited,,Improving Language Understanding by Generative Pre-Training,"""BookCorpus (BooksCorpus, Toronto Book Corpus)""","""We use the BooksCorpus dataset [71] for training the language model""",1000000000,"""BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).""
https://paperswithcode.com/dataset/bookcorpus

BookCorpus seems to have about 5000MB of content
source: https://huggingface.co/datasets/bookcorpusopen

Assuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.

So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens",,,,8064.00,117000000.00,"""The model had 117M parameters in total.""

source: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2",,120000000000.00,"https://github.com/amirgholami/ai_and_memory_wall estimates 9.6e+10. The ELECTRA paper (https://arxiv.org/pdf/2003.10555.pdf) Table 1 reports 3.0E+10 FLOP for GPT. But: ""Infer FLOPs assumes a single length-128 input"". If we instead assume 512 tokens as per GPT's training process, then I think the calculation would be 4x larger, i.e. 1.2E+11. This is closer to the estimate of 9.6E+10 in the link.",720.0,"""1 month on 8 GPUs."" from the reference link",NVIDIA Quadro P600,Self-supervised learning,$68.72,,Industry,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",2024-04-01 09:53,Robi Rahman,,,,,,8,,GPT,,,,Industry,,,,,Industry,,,
DeepStack,Games,Poker,2017-01-06,,,,,https://arxiv.org/abs/1701.01724,"University of Alberta,Charles University,Czech Technical University",14463360000000000000.00,"The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.

From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack’s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""

Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).

But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.

Calculation:
6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.","Canada,Czechia,Czechia","Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling",SOTA improvement,"first human-competitive poker AI, confirmed by website: https://www.deepstack.ai/",DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,,,10000000,"""The turn network was trained by solving 10 million randomly generated poker turn
games. These turn games used randomly generated ranges, public cards, and a random pot
size (10).""",Speculative,,,797.00,2500000.00,"Figure 3, p.9

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,218.0,from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs,,,$0.00,,Academia,,2024-05-01 09:06,Robi Rahman,,,,,,20,,DeepStack,,,,"Academia,Academia,Academia",,,,4368,"Academia,Academia,Academia",,,
DistilBERT,Language,Text autocompletion,2019-10-02,"code: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation

weights: https://huggingface.co/distilbert/distilbert-base-uncased

repo license is apache: https://github.com/huggingface/transformers/blob/main/LICENSE

Wikipedia is open, BookCorpus is not",Open source,,Open source,https://arxiv.org/abs/1910.01108,Hugging Face,12441600000000000000.00,"Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.

1.6e13*8*60**2*90*0.3 = 1.2e19",Multinational,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",Highly cited,,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","Wikipedia,""BookCorpus (BooksCorpus, Toronto Book Corpus)""","Section 3: We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015].",,,,,,5240.00,66000000.00,Table 3,,,,,,,,,,Industry,,2024-04-19 16:02,Robi Rahman,,,,,,,,DistilBERT,,,,Industry,,,,,Industry,,,
ResNet-152 (ImageNet),Vision,Image classification,2015-12-10,,,,,https://arxiv.org/abs/1512.03385,Microsoft,12100000000000000000.00,"(11.4 *10^9) mult-adds per forward pass
2 FLOPS/ mult-add
3.5 for forward & backward pass
1.2 * 10^6 examples in dataset
128 epochs

Source:x",United States of America,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,Deep Residual Learning for Image Recognition,ILSVRC 2012 subset of ImageNet,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1280000,"""We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images""",,,,156882.00,60000000.00,Taken from https://arxiv.org/abs/1605.07146,,22600000000.00,Table 1,,,,,$92.03,,Industry,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",2024-04-22 07:40,Robi Rahman,,,,,,,,ResNet-152 (ImageNet),,,,Industry,,,,,Industry,,,
Memformer (4 encoder + 16 decoder),Language,,2020-10-14,,Unreleased,,Unreleased,https://arxiv.org/abs/2010.06891,"UC Davis,Westlake University,Facebook AI",12000000000000000000.00,,"United States of America,China,United States of America","Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, Zhou Yu",,,Memformer: A Memory-Augmented Transformer for Sequence Modeling,WikiText-103,,,,,,,25.00,76200000.00,,11.93,,,,,,,,,,"Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",2024-05-22 12:30,Robi Rahman,Memformer (4 encoder + 16 decoder),,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
SRU++ Large,Language,,2021-02-24,"[the repo doesn't look very helpful and detailed, I wasn't able to find the models there though authors claim it]

MIT license

Our code, experimental setup and models are available
at https://github.com/asappresearch/sru.",Open source,,Open source,https://arxiv.org/abs/2102.12459,ASAPP,11000000000000000000.00,,United States of America,Tao Lei,SOTA improvement,"""our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. """,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,"WikiText-103,enwik8,One Billion Word benchmark",,,,,,,38.00,234000000.00,,34.08,,,,,,,,,,,2024-04-22 12:17,Robi Rahman,SRU++ Large,,,,,,,SRU++ Large,,,,Industry,,,,,Industry,,,
DITTO,Language,,2022-06-06,"open code
https://github.com/Jxu-Thu/DITTO",Unreleased,Open source,Open source,https://arxiv.org/abs/2206.02369,"Tsinghua University,Apple,Westlake University,Chinese University of Hong Kong (CUHK)",11000000000000000000.00,,"China,United States of America,China,Hong Kong","Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",SOTA improvement,"Achieves SOTA on CNN/DailyMail by fine-tuning and improving on BART-large, which is SOTA",Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,WikiText-103,,,,,,,41.00,750000000.00,,7.16,,,,,,,,,,,2024-05-01 09:22,Robi Rahman,DITTO,,,,,,,DITTO,,,,"Academia,Industry,Academia,Academia",,,,,"Academia,Industry,Academia,Academia",,,
Transformer + Simple Recurrent Unit,Language,Translation,2018-09-17,,,,,https://arxiv.org/abs/1709.02755v5,"ASAPP,Cornell University,Google,Princeton University",11000000000000000000.00,"""We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtained
using 8 GPUs in parallel, which provide a large effective batch size during training. To approximate
the setup, we update the model parameters every 5×5120 tokens and use 16,000 warm-up steps
following OpenNMT suggestions. We train each
model for 40 epochs (250,000 steps), and perform
3 independent trials for each model configuration.
A single run takes about 3.5 days with a Tesla V100 GPU.""

125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19","United States of America,United States of America,United States of America,United States of America","Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, Yoav Artzi",SOTA improvement,"""We use the state-of-the-art Transformer
model of Vaswani et al. (2017) as our base architecture... When SRU is incorporated into the architecture,
both the 4-layer and 5-layer model outperform the
Transformer base model""",Simple Recurrent Units for Highly Parallelizable Recurrence,WMT English-German,"""We train translation models on the WMT English→German dataset, a standard
benchmark for translation systems (Peitz et al.,
2014; Li et al., 2014; Jean et al., 2015). The
dataset consists of 4.5 million sentence pairs""",,,Confident,,,293.00,90000000.00,"5-layer model, Table 3",40.00,,,,,NVIDIA V100,,,,,"Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.",2024-05-21 04:07,Anonymous,,0,,,,8,,Transformer + Simple Recurrent Unit,,,,"Industry,Academia,Industry,Academia",,,,,"Industry,Academia,Industry,Academia",,,
Transformer-XL (257M),Language,,2019-01-09,"Apache 2.0
https://github.com/kimiyoung/transformer-xl?tab=Apache-2.0-1-ov-file#readme",Open source,Open source,Open source,https://arxiv.org/abs/1901.02860,"Carnegie Mellon University (CMU),Google Brain",10900000000000000000.00,,"United States of America,United States of America","Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",Highly cited,,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,WikiText-103,,,,,,,3155.00,257000000.00,"Transformer-XL Large, Table 1",,,,,,,,,,,,2024-04-30 13:07,Robi Rahman,Transformer-XL Large,,,,,,,Transformer-XL Large,,,,"Academia,Industry",,,,,"Academia,Industry",,,
EquiDock,Biology,Proteins,2021-11-15,,Open source,,,https://arxiv.org/abs/2111.07786,"Massachusetts Institute of Technology (MIT),ETH Zurich,Tencent",10800000000000000000.00,"Training details here:

https://docs.nvidia.com/bionemo-framework/latest/models/equidock.html

32 A100s can do 30 epochs per hour on the DIPS dataset. Equidock was trained on 30 epochs on DIPS and 150 epochs on DB5.5. DIPS is about 100x bigger, so the large majority of compute was DIPS.

32 A100-hours = 312 teraflops * 32 * 3600 * 0.3 
~= 1.08e19","United States of America,Switzerland,Multinational","Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, Andreas Krause",,"""Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.""

did not have best results, per Table 1",Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking,"DIPS,DB5.5","""We leverage the following datasets: Docking Benchmark 5.5 (DB5.5) (Vreven et al.,
2015) and Database of Interacting Protein Structures (DIPS) (Townshend et al., 2019). DB5.5 is
a gold standard dataset in terms of data quality, but contains only 253 structures. DIPS is a larger
protein complex structures dataset mined from the Protein Data Bank (Berman et al., 2000) and
tailored for rigid body docking. Datasets information is given in Appendix D""",,"42k proteins pairs in DIPS, per table 2, and 2k atoms per protein. DB5.5 is much smaller",Likely,,,117.00,,,30.00,,,,,,,,,,"Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.",2024-03-20 09:39,Anonymous,,0,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
Transformer-XL + SIS,Language,,2021-05-03,,Unreleased,,Unreleased,http://proceedings.mlr.press/v139/verma21b/verma21b.pdf,INRIA,10400000000000000000.00,,France,"Sagar Verma, Jean-Christophe Pesquet",,,Sparsifying Networks via Subdifferential Inclusion,,,,,,,,11.00,246000000.00,,,,,,,,,,,,"Sparsifying deep neural networks is of paramount
interest in many areas, especially when those
networks have to be implemented on lowmemory devices. In this article, we propose a new
formulation of the problem of generating sparse
weights for a pre-trained neural network. By
leveraging the properties of standard nonlinear
activation functions, we show that the problem
is equivalent to an approximate subdifferential
inclusion problem. The accuracy of the
approximation controls the sparsity. We show
that the proposed approach is valid for a broad
class of activation functions (ReLU, sigmoid,
softmax). We propose an iterative optimization
algorithm to induce sparsity whose convergence
is guaranteed. Because of the algorithm flexibility,
the sparsity can be ensured from partial training
data in a minibatch manner. To demonstrate
the effectiveness of our method, we perform
experiments on various networks in different
applicative contexts: image classification,
speech recognition, natural language processing,
and time-series forecasting. Project page:
https://sagarverma.github.io/compression",2024-05-09 13:33,Robi Rahman,Transformer-XL + SIS,,,,,,,,,,Transformer-XL + SIS,Academia,,,,,Academia,,,
VALL-E,"Audio,Speech",Speech synthesis,2023-01-05,,Unreleased,,,https://arxiv.org/abs/2301.02111,Microsoft,10100000000000000000.00,"""The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic
tokens per GPU for 800k steps""

353M * 800k * 6k * 6 = 1.01e19

16 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?",United States of America,"Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",SOTA improvement,"""VALL-E significantly outperforms
the state-of-the-art zero-shot TTS system [Casanova et al., 2022b] in terms of speech naturalness and
speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean
option score (SMOS) improvement on LibriSpeech""",Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,LibriLight,"""60K hours of English speech with over 7000 unique speakers.""",820800000,"60k hours
~13,680 words/hour * 60,000 = 820800000 words
https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",Speculative,,,241.00,353000000.00,"""Both the AR model and the NAR model have the same transformer architecture with 12
layers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1""

Ben's script says that's 353M parameters, using n_block 12, d_model 1024, d_ff 4096, encoder only False

https://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py",,,,,,NVIDIA V100,Supervised,,,,"We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See this https URL for demos of our work.",2024-05-01 09:22,Anonymous,,0,,,,,,VALL-E,,,,Industry,checked,,,,Industry,,,
ProBERTa,Biology,Proteins,2020-09-01,,,,,https://dl.acm.org/doi/10.1145/3388440.3412467,"University of Illinois Urbana-Champaign (UIUC),Reed College",9720000000000000000.00,"""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""
4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18","United States of America,United States of America","Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz",SOTA improvement,"""Furthermore, we used embeddings from PRoBERTa for a fundamentally different problem, PPI prediction, using two different
datasets generated from the HIPPIE database and found that with
sufficient data, it substantially outperforms the current state-of-theart method in the conservative scenario.""","Transforming the Language of Life: Transformer Neural
Networks for Protein Prediction Tasks",UniProtKB/Swiss-Prot,"""Pre-training data: We use UniProtKB/Swiss-Prot (450K unique sequences with a mean tokenized length of 129.6 tokens), a collection of experimentally annotated and reviewed amino acid sequences""

Fine tuning uses a subset of 313,214 sequences which have annotated labels.",58320000,"450k sequences * 129.6 tokens per sequence = 58,320,000 tokens",Confident,,,97.00,44000000.00,"""In total, our model has approximately 44M trainable parameters.""",,93400000000000.00,2NC = 2 * (129.6 tokens/example * 8192 examples/minibatch) * 44M parameters = 9.34e13,18.0,,NVIDIA V100,Self-supervised learning,,,Academia,"The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the-art approaches for protein family classification while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",2024-05-16 17:32,Anonymous,,,,,,4,,ProBERTa,,,,"Academia,Academia",,,,,"Academia,Academia",,,
DARK,Biology,"Protein generation,Proteins",2022-01-28,,,,,https://www.biorxiv.org/content/10.1101/2022.01.27.478087v1.full,University College London (UCL),9700000000000000000.00,"""Rounding up to the nearest day, if we were to re-perform DARK from nothing to having a trained DARK3 it would take 12 days when parallelized across ten V100 GPUS. Of that time, model training constitutes just over 3 days and only requires 1 GPU""

3 * 24 * 3600 * 125 teraFLOP/s * 0.3 (utilization) = 9.7e18",United Kingdom of Great Britain and Northern Ireland,"Lewis Moffat, Shaun M. Kandathil, David T. Jones",,,Design in the DARK: Learning Deep Generative Models for De Novo Protein Design,,"Iteratively trained by generating synthetic data: ""To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences""",,,Speculative,,,25.00,,"""DARK3, uses a Transformer decoder with 12 layers, 12 heads, and a feedforward dimension of 768.""",,,,,,NVIDIA V100,Unsupervised,,,,"The design of novel protein sequences is providing paths towards the development of novel therapeutics and materials. At the forefront is the challenging field of de novo protein design, which looks to design protein sequences unlike those found in nature using general design methodologies. In this work, we develop a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures. To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences. The resulting model generalizes where models trained on natural sequences struggle and greatly improves on the efficiency of comparable sampling-based approaches. We further show how it can generate high quality candidates for de novo design problems and aid in the development of further novel design methods, in all, providing another step, amongst others, towards truly automated and intelligent protein design.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
MedBERT,Medicine,"Medical diagnosis,Text classification,Prediction of prolonged length of stay in hospital,prediction of heart failure among patients with diabetes (DHF),prediction of onset of pancreatic cancer (PaCa)",2021-05-20,"Apache 2

https://github.com/ZhiGroup/Med-BERT

Initially we really hoped to share our models but unfortunately, the pre-trained models are no longer sharable. According to SBMI Data Service Office: ""Under the terms of our contracts with data vendors, we are not permitted to share any of the data utilized in our publications, as well as large models derived from those data.""",Unreleased,,Open source,https://www.nature.com/articles/s41746-021-00455-y,"Peng Cheng Laboratory,University of Texas at Houston",9470000000000000000.00,"flops = (1) * (3.13e13) * (24*7 * 3600) * (0.5) = 9.47e18
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
I assume higher utilization rate, because only 1 GPU is used.
Citation from the text:
""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11

Note that public code appears not to make use of the tensor core speed up, thus I use 3.13e13 FLOP/sec","China,United States of America","Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi",SOTA improvement,"""This work is the first demonstration of significantly boosted
performance over state-of-the-art methods on multiple
clinical tasks with phenotyped cohorts.""",Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction,Cerner Health Facts,"page 3 data source
""We extracted our cohorts from two databases: Cerner Health
Facts® (version 2017) (Cerner) and Truven Health MarketScan®
(Truven)""
""Our pretraining cohort for Med-BERT is consisting of 28 million
patients extracted from Cerner""""",,"data about 28M patients
""Our pretraining cohort for Med-BERT is consisting of 28 million
patients extracted from Cerner""",Likely,,,407.00,17000000.00,"17M from ""This is possibly due to the fact that the untrained Med-BERT is an over-parameterized model (around 17 million parameters) with a huge
number of configurations, so it might overfit to the training data""",,,,168.0,"""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11",NVIDIA Tesla V100 DGXS 32 GB,,,,Industry,"Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21–6.14% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.",2024-05-17 18:08,Bartosz Podkanowicz,,,,,,1,,MedBERT,,,,"Academia,Academia",,,,168,"Academia,Academia",,,
VGG16,Vision,Image classification,2014-09-04,,,,,https://arxiv.org/abs/1409.1556,University of Oxford,9253440000000000000.00,"2.5 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with
four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""



",United Kingdom of Great Britain and Northern Ireland,Karen Simonyan; Andrew Zisserman,Highly cited,,Very Deep Convolutional Networks for Large-Scale Image Recognition,ILSVRC 2012 subset of ImageNet,,1300000,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,,,87284.00,138000000.00,"Source: Table 2
https://arxiv.org/abs/1409.1556",,15300000000.00,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,NVIDIA GTX Titan Black,,$82.80,,Academia,,2024-04-01 09:30,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Transformer + GFM,Language,,2022-12-01,,Unreleased,,Unreleased,https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf,Nanjing University,8039999999999999000.00,,China,"Hao Yu, Jianxin Wu",,,"""Compressing Transformers: Features Are Low-Rank, but Weights Are Not""",WikiText-103,"""We also evaluate our approach in the WikiText-103 (Merity et al. 2017) dataset. WikiText-103 is composed of shuffled Wikipedia articles where the context carries across sentences.""",,,,,,0.00,185000000.00,,,,,,,,,,,,"Transformer and its variants achieve excellent results in various computer vision and natural language processing tasks,
but high computational costs and reliance on large training
datasets restrict their deployment in resource-constrained settings. Low-rank approximation of model weights has been
effective in compressing CNN models, but its application
to transformers has been less explored and is less effective. Existing methods require the complete dataset to finetune compressed models, which are both time-consuming and
data-hungry. This paper reveals that the features (i.e., activations) are low-rank, but model weights are surprisingly not
low-rank. Hence, AAFM is proposed, which adaptively determines the compressed model structure and locally compresses each linear layer’s output features rather than the
model weights. A second stage, GFM, optimizes the entire
compressed network holistically. Both AAFM and GFM only
use few training samples without labels, that is, they are fewshot, unsupervised, fast and effective. For example, with only
2K images without labels, 33% of the parameters are removed
in DeiT-B with 18.8% relative throughput increase, but only a
0.23% accuracy loss for ImageNet recognition. The proposed
methods are successfully applied to the language modeling
task in NLP, too. Besides, the few-shot compressed models
generalize well in downstream tasks.
",2024-05-21 06:22,Robi Rahman,Transformer + GFM,,,,,,,,,,,Academia,,,,,Academia,,,
CODEFUSION (Python),Language,Code generation,2023-10-26,,,,,https://arxiv.org/abs/2310.17680,"Microsoft,Microsoft Research",7920000000000000000.00,"V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/

11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP","United States of America,United States of America","Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen",SOTA improvement,"See Table 1, SOTA in Python code generation",CODEFUSION: A Pre-trained Diffusion Model for Code Generation,,,4390400,"Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens",Confident,,,9.00,75000000.00,Table 1,,,,11.0,"""The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.""",NVIDIA Tesla V100 SXM2 32 GB,Self-supervised learning,,,Industry,"Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",2024-05-01 09:22,Anonymous,,,,,,,,CODEFUSION (Python),,,,"Industry,Industry",,,,,"Industry,Industry",,,
AlphaX-1,Vision,"Neural architecture search for computer vision,Image classification,Object detection",2019-10-02,"code, no license specified: https://github.com/linnanwang/AlphaX-NASBench101",Unreleased,Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/1903.11059,"Facebook AI Research,Brown University",7600000000000000000.00,,"United States of America,United States of America","Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1",SOTA improvement,"""In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency""",AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,"ImageNet,COCO","12800000 + 200000=1480000
I assume they used 1,281,167 training images when referred to Imagenet and 200 000 when referred to MS COCO

""We set up the ImageNet training using
the standard mobile configuration with the input image size
of (224 × 224)[45]. More details are available in the appendix. AlphaX sampled 1000 networks, and we selected
the top 20 networks in the pre-training to fine-tune another
530 epochs. 

We use AlphaX-1 model pre-trained on ImageNet
dataset. The training dataset is MSCOCO for object
detection[15] which contains 90 classes of objects. Each
image is scaled to 300 × 300 in RGB channels. We trained
the model with 200k iterations with 0.04 initial learning rate
and the batch size is set to 24. We applied the exponential learning rate decay schedule with the 0.95 decay factor. Our
model uses momentum optimizer with momentum rate set
to 0.9. We also use the L2 weight decay for training. We
process each image with random horizontal flip and random
crop[22]. We set the matched threshold to 0.5, which means
only the probability of an object over 0.5 is effective to appear on the image. We use 8000 subsets of validation images in MSCOCO validation set and report the mean average precision (mAP) as computed with the standard COCO metric library[16].""
",1480000,,,,,84.00,579000000.00,"Table 3: multiadds for AlphaX-1 579M, parameters 5.4M",,,,,,NVIDIA Geforce GTX 1080 Ti,,$24.10,,Industry,,2024-04-19 16:05,Robi Rahman,,,,,,,,AlphaX-1,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Transformer,Language,Translation,2017-06-12,,Unreleased,Unreleased,Unreleased,https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,"Google Research,Google Brain",7424524800000000000.00,"""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""

source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

NVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance

source: https://www.nvidia.com/en-gb/data-center/tesla-p100/

We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" article

source: https://openai.com/blog/ai-and-compute/","Multinational,United States of America","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin","Highly cited,Historical significance",The original transformer,Attention Is All You Need,"WMT English-German,WMT14",,1400000000,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary ""

In total, this is 40.5 million sentence-pairs. Assuming each sentence pair is 15-20 words in each language, this is 1.2-1.6 billion words.",Likely,,,87298.00,213000000.00,"This page suggests the transformer has 213M parameters.

""Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.""

https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html",,54000000000.00,"Source: rados dataset (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",84.0,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).",NVIDIA P100,Self-supervised learning,$111.17,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,8,,Transformer,,,,"Industry,Industry",,,,672,"Industry,Industry",,,
NLM,Language,,2021-09-09,MIT for code: https://github.com/jxhe/efficient-knnlm,Unreleased,,Open source,https://arxiv.org/abs/2109.04212,"Carnegie Mellon University (CMU),UC San Diego",7360000000000000000.00,,"United States of America,United States of America","Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick",,,Efficient Nearest Neighbor Language Models,WikiText-103,,,,,,,70.00,515000000.00,,,,,,,,,,,,,2024-05-07 08:34,Robi Rahman,NLM,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
FAIRSEQ Adaptive Inputs,Language,,2019-04-01,"don't see this model in the repo, though it's hard to tell. Repo is MIT-licensed

https://github.com/facebookresearch/fairseq",Unreleased,Unreleased,Open source,https://arxiv.org/abs/1904.01038,"Facebook AI Research,Google Brain",7300000000000000000.00,,"United States of America,United States of America","Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli",Highly cited,,"""fairseq: A Fast, Extensible Toolkit for Sequence Modeling""",WikiText-103,,,,,,,2709.00,247000000.00,,,,,,,,,,,,,2024-04-17 05:50,Robi Rahman,FAIRSEQ Adaptive Inputs,,,,,,,FAIRSEQ Adaptive Inputs,,,,"Industry,Industry",,,,,"Industry,Industry",,,
Transformer-XL Large + Phrase Induction,Language,,2019-06-04,"code license, BSD-3: https://github.com/luohongyin/PILM?tab=BSD-3-Clause-1-ov-file#readme",Unreleased,Open source,Open source,https://arxiv.org/abs/1906.01702,"Massachusetts Institute of Technology (MIT),University of Illinois Urbana-Champaign (UIUC)",7300000000000000000.00,,"United States of America,United States of America","Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",SOTA improvement,"""We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset""","""Improving Neural Language Models by Segmenting, Attending, and Predicting the Future""",WikiText-103,,,,,,,12.00,257000000.00,,1.00,,,,,,,,,,,2024-04-17 15:31,Robi Rahman,Transformer-XL Large + Phrase Induction,,,,,,,Transformer-XL Large + Phrase Induction,,,,"Academia,Academia",,,,,"Academia,Academia",,,
MemSizer,Language,,2022-03-23,"code, no license: https://github.com/jcyk/memsizer ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2203.12644,"Meta AI,Chinese University of Hong Kong (CUHK)",7300000000000000000.00,,"United States of America,Hong Kong","Yizhe Zhang, Deng Cai",,,Linearizing Transformer with Key-Value Memory,WikiText-103,,,,,,,3.00,357000000.00,,,,,,,,,,,,,2024-05-06 14:16,Robi Rahman,MemSizer,,,,,,,,,,MemSizer,"Industry,Academia",,,,,"Industry,Academia",,,
base LM+GNN+kNN,Language,,2021-10-17,"MIT License

https://github.com/ShannonAI/GNN-LM",Open source,,Open source,https://arxiv.org/abs/2110.08743,"Shannon.AI,Nanjing University,Nanyang Technological University,Zhejiang University",7300000000000000000.00,,"China,China,Singapore,China","Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",SOTA improvement,,GNN-LM: Language Modeling based on Global Contexts via GNN,WikiText-103,,,,,,,32.00,274000000.00,,,,,,,,,,,,,2024-04-29 09:56,Robi Rahman,base LM+GNN+kNN,,,,,,,base LM+GNN+kNN,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
Base LM + kNN LM + Continuous Cache,Language,,2019-11-01,"Training code, MIT: https://github.com/urvashik/knnlm",Unreleased,,Open source,https://arxiv.org/abs/1911.00172,"Stanford University,Facebook AI Research",7300000000000000000.00,,"United States of America,United States of America","Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",SOTA improvement,"""GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103""",Generalization through Memorization: Nearest Neighbor Language Models,WikiText-103,,,,,,,556.00,247000000.00,,200.00,,,,,,,,,,,2024-05-01 09:05,Robi Rahman,Base LM + kNN LM + Continuous Cache,,,,,,,Base LM + kNN LM + Continuous Cache,,,,"Academia,Industry",,,,,"Academia,Industry",,,
RFA-GATE-Gaussian-Stateful Big,Language,,2021-03-03,,Unreleased,,Unreleased,https://arxiv.org/abs/2103.02143,"University of Washington,DeepMind,Allen Institute for AI,Hebrew University of Jerusalem,The University of Hong Kong",7140000000000000000.00,,"United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America,Israel,Hong Kong","Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong",,,Random Feature Attention,WikiText-103,,,,,,,260.00,242000000.00,,47.72,,,,,,,,,,,2024-05-09 16:43,Robi Rahman,RFA-GATE-Gaussian-Stateful Big,,,,,,,,,,,"Academia,Industry,Research collective,Academia,Academia",,,,,"Academia,Industry,Research collective,Academia,Academia",,,
MGK 4 heads (medium),Language,,2021-10-16,commercial license: https://github.com/minhtannguyen/transformer-mgk/blob/main/LICENSE ,Unreleased,,Open source,https://arxiv.org/abs/2110.08678,"FPT Software AI Center,University of California Los Angeles (UCLA),VinUniversity,Deezer Research,Rice University,University of Texas at Austin",6670000000000000000.00,,"Viet Nam,United States of America,Viet Nam,France,United States of America,United States of America","Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",,,Improving Transformers with Probabilistic Attention Keys,WikiText-103,,,,,,,20.00,90000000.00,,120.00,,,,,,,,,,"Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.",2024-05-21 09:45,Robi Rahman,MGK 4 heads (medium),,,,,,,,,,,"Industry,Academia,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia,Academia",,,
Transformer-XL DeFINE (141M),Language,,2019-11-27,,Unreleased,,Unreleased,https://arxiv.org/abs/1911.12385,"University of Washington,Allen Institute for AI",6200000000000000000.00,,"United States of America,United States of America","Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",SOTA improvement,"""Compared to state-of-the-art methods including adaptive input representations,
this technique results in a 6% to 20% drop in perplexity""",DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,"WikiText-103,Penn TreeBank",,,,,,,21.00,141000000.00,,20.00,,,,,,,,,,,2024-04-22 14:10,Robi Rahman,Transformer-XL DeFINE (141M),,,,,,,Transformer-XL DeFINE (141M),,,,"Academia,Research collective",,,,,"Academia,Research collective",,,
Transformer Large + HCP,Language,,2022-03-21,"code, no license: https://github.com/richardbaihe/robustLM ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2203.10692,"University of Waterloo,Microsoft Research",6060000000000000000.00,,"Canada,United States of America","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",,other model in this paper has better performance,Better Language Model with Hypernym Class Prediction,WikiText-103,,,,,,,8.00,257000000.00,,38.18,,,,,,,,,,,2024-05-06 14:20,Robi Rahman,Transformer Large + HCP,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Subformer (122M),Language,,2021-01-01,"code, but not for language modeling: https://github.com/machelreid/subformer/blob/master/README.md ",Unreleased,,Unreleased,https://arxiv.org/abs/2101.00234,"National Institute of Advanced Industrial Science and Technology (AIST),The University of Tokyo",5300000000000000000.00,,"Japan,Japan","Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",,,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,WikiText-103,,,,,,,33.00,122000000.00,,70.29,,,,,,,,,,,2024-05-09 17:11,Robi Rahman,Subformer (122M),,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Mono3D++,3D modeling,3D segmentation,2019-01-11,,,,,https://arxiv.org/abs/1901.03446,"University of California Los Angeles (UCLA),Megvii Inc",4856060160000000000.00,"(4) * (6.691 * 10**12) * (168* 3600) * (0.3) = 
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = 

training time - about one week = 168 hours from
""It takes about one week to train the 2D bounding box net-work, and two hours for the orientation/3D scale network
on KITTI with 4 TITAN-X GPUs. The landmark detector is
trained on Pascal3D. The training process for the monocu-
lar depth estimation network is unsupervised using KITTI
stereo-pairs, which takes around 5 to 12 hours depending
on the amount of data available. ""

gpu flops - FP32 (float) 6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632","United States of America,China","Tong He, Stefano Soatto",,,Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors,KITTI,"""t takes about one week to train the 2D bounding box net-
work, and two hours for the orientation/3D scale network
on KITTI with 4 TITAN-X GPUs. The landmark detector is
trained on Pascal3D. The training process for the monocu-
lar depth estimation network is unsupervised using KITTI
stereo-pairs, which takes around 5 to 12 hours depending
on the amount of data available. ""


""We evaluate our method on the KITTI object detection
benchmark. This dataset contains 7, 481 training images """,7481,"""We evaluate our method on the KITTI object detection benchmark. This dataset contains 7, 481 training images """,Likely,,,124.00,,,,,,168.0,"""about one week"" from section 3.4 ",NVIDIA GTX Titan X,,,,,"We present a method to infer 3D pose and shape of vehicles from a single image. To tackle this ill-posed problem, we optimize two-scale projection consistency between the generated 3D hypotheses and their 2D pseudo-measurements. Specifically, we use a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose. To reduce its sensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse representation which improves robustness. We also integrate three task priors, including unsupervised monocular depth, a ground plane constraint as well as vehicle shape priors, with forward projection errors into an overall energy function. ",2024-02-15 13:32,Anonymous,,,,,,4,,,,,,"Academia,Industry",,,,672,"Academia,Industry",,,
Tensorized Transformer (257M),Language,,2019-06-24,"code, no license: https://github.com/szhangtju/The-compression-of-Transformer",Unreleased,Open source,Open access (non-commercial),https://arxiv.org/abs/1906.09777,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",4760000000000000000.00,,"China,China,China","Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",SOTA improvement,"""Table 2: Results and compression with state-of-the-art results on PTB and WikiText-103""",A Tensorized Transformer for Language Modeling,WikiText-103,,,,,,,133.00,257000000.00,,30.00,,,,,,,,,,,2024-05-01 09:06,Robi Rahman,Tensorized Transformer (257M),,,,,,,Tensorized Transformer (257M),,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
HyenaDNA,Biology,general-purpose protein or nucleotide language model (pLM/nLM),2023-06-27,,,,,https://arxiv.org/abs/2306.15794,"Stanford University,Harvard University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal",4490000000000000000.00,"8 Nvidia A100 (80GB) GPUs, ~300 minutes (figure 3.2)

Assuming 40% utilization

Estimate: 78 TFLOP/s * 8 GPUs * (300*60) s * 0.4 = 4.49e18 FLOPs","United States of America,United States of America,Canada,Canada","Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher Ré",SOTA improvement,"""On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.1 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer identification.""",HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution,Human Reference Genome,See footnote 1,,,Confident,,,56.00,1600000.00,See footnote 1,,,,,,NVIDIA A100,,,,,"Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points.",2024-05-22 12:25,Anonymous,,,,,,8,,HyenaDNA,,,,"Academia,Academia,Academia,Academia",,,,,"Academia,Academia,Academia,Academia",,,
Hanabi 4 player,Games,Hanabi,2019-02-01,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/1902.00506,"DeepMind,University of Oxford,Carnegie Mellon University (CMU),Google Brain",4300000000000000000.00,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",,Historical significance,Adapted some SOTA RL algorithms to a new task that posed research challenges,The Hanabi Challenge: A New Frontier for AI Research,,,,,,,,229.00,764000.00,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,,,,,,,,$78.32,"7 days on V100 –> 7 * 24 * $0.55 = $92.40
Adjust to 2020 dollars: $78.32",Industry,,2024-05-02 14:47,Robi Rahman,,,,,,,,Hanabi 4 player,,,,"Industry,Academia,Academia,Industry",,,,,"Industry,Academia,Academia,Industry",,,
ProteinGAN,Biology,"Proteins,Protein generation",2021-03-04,,,,,https://www.nature.com/articles/s42256-021-00310-5,"Vilnius University,Chalmers University of Technology",4300000000000000000.00,"2.5 million steps, batch size 64, 210 hours on NVIDIA Tesla P100 system","Lithuania,Sweden","Donatas Repecka, Vykintas Jauniskis, Laurynas Karpus, Elzbieta Rembeza, Irmantas Rokaitis, Jan Zrimec, Simona Poviloniene, Audrius Laurynenas, Sandra Viknander, Wissam Abuajwa, Otto Savolainen, Rolandas Meskys, Martin K. M. Engqvist & Aleksej Zelezniak",,,Expanding functional protein sequence spaces using generative adversarial networks,UniProtKB,,,16.7k sequences from UniProt,Likely,,,,60000000.00,"""The final architecture of the network comprised 45 layers with over 60 million trainable parameters""",,,,210.0,,NVIDIA P100,Unsupervised,,,,"De novo protein design for catalysis of any desired chemical reaction is a long-standing goal in protein engineering because of the broad spectrum of technological, scientific and medical applications. However, mapping protein sequence to protein function is currently neither computationally nor experimentally tangible. Here, we develop ProteinGAN, a self-attention-based variant of the generative adversarial network that is able to ‘learn’ natural protein sequence diversity and enables the generation of functional protein sequences. ProteinGAN learns the evolutionary relationships of protein sequences directly from the complex multidimensional amino-acid sequence space and creates new, highly diverse sequence variants with natural-like physical properties. Using malate dehydrogenase (MDH) as a template enzyme, we show that 24% (13 out of 55 tested) of the ProteinGAN-generated and experimentally tested sequences are soluble and display MDH catalytic activity in the tested conditions in vitro, including a highly mutated variant of 106 amino-acid substitutions. ProteinGAN therefore demonstrates the potential of artificial intelligence to rapidly generate highly diverse functional proteins within the allowed biological constraints of the sequence space.",2024-02-02 16:45,Anonymous,,,,,,1,,,,,,"Academia,Academia",,,,210,"Academia,Academia",,,
DLRM-2020,Recommendation,,2019-05-31,MIT. seems to just be code and not weights: https://github.com/facebookresearch/dlrm,Unreleased,,Open source,https://arxiv.org/abs/1906.00091,Facebook AI,4000000000000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",United States of America,"M Naumov, D Mudigere, HJM Shi, J Huang",SOTA improvement,"""In this paper, we develop a state-of-the-art deep learning recommendation model
(DLRM)""",Deep Learning Recommendation Model for Personalization and Recommendation Systems,,,,,,,,550.00,100000000000.00,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,$14.60,,Industry,,2024-05-01 09:05,Robi Rahman,,,,,,,,DLRM-2020,,,,Industry,,,,,Industry,,,
Big-Little Net (vision),Vision,Object recognition,2018-07-10,"apache for code/weights: 
https://github.com/IBM/BigLittleNet",Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/1807.03848,IBM,3936768000000000000.00,number of epochs (appendix A1) times flops per inference (from table 2) times dataset size times 3 (to account for backpropagation),United States of America,"Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris",,,BIG-LITTLE NET: AN EFFICIENT MULTI-SCALE FEATURE REPRESENTATION FOR VISUAL AND SPEECH RECOGNITION,ImageNet,section 4.1,1280000,size of ImageNet,Likely,,,97.00,77360000.00,Table 2 - fifth row,110.00,9320000000.00,,,,,,,,,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet.",2024-04-04 14:42,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Linear Transformer (large),Language,,2021-02-22,"code, Apache license: https://github.com/IDSIA/lmtool-fwp/tree/master/example_scripts/2021_linear_transformers_are_secretly_fwps ",Unreleased,,Open source,https://arxiv.org/abs/2102.11174,IDSIA,3890000000000000000.00,,Switzerland,"Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",,,Linear Transformers Are Secretly Fast Weight Programmers,WikiText-103,,,,,,,128.00,90000000.00,,70.00,,,,,,,,,,,2024-05-09 16:54,Robi Rahman,Linear Transformer (large),,,,,,,,,,,Academia,,,,,Academia,,,
DNA Fine-Tuned Language Model (DFLM),Biology,general-purpose protein or nucleotide language model (pLM/nLM),2023-01-02,"MIT for code. not sure there are weights here?

https://github.com/Deep-Bioinfo/DFLM",Unreleased,Open source,Open source,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751352,Tongji University,3500000000000000000.00,"""The pre-training of Human genome language model is
resource-intensive (about 7-10 days on 2 NVIDIA TITAN X GPU).""

Assuming FP32 and 30% utilization

Estimate: (10*24*3600) s * 6.7e12 FLOP/s * 2 * 0.3 = 3.5e18",China,"Ying He , Qinhu Zhang , Siguo Wang , Zhanheng Chen , Zhen Cui ,
Zhen-Hao Guo, and De-Shuang Huang",,,Predicting the Sequence Specificities of DNA-Binding Proteins by DNA Fine-Tuned Language Model With Decaying Learning Rates,GRCH38/hg38,"""DFLM mainly used two types of data sets, the unlabeled large-scale Genome Reference Consortium Human Build 38(GRCh38/hg38) [1] and about 500 labeled ChIP-seq data set [2]. The unlabeled large-scale genome data set hg38 was used to train a general Human Genome language model. The ChIP-seq data sets were used to train the DBP fine-tuning language model and classification model.""",,,Likely,,,1.00,,,,,,,,NVIDIA TITAN Xp,,,,,"DNA-binding proteins (DBPs) play vital roles in the regulation of biological systems. Although there are already many deep learning methods for predicting the sequence specificities of DBPs, they face two challenges as follows. Classic deep learning methods for DBPs prediction usually fail to capture the dependencies between genomic sequences since their commonly used one-hot codes are mutually orthogonal. Besides, these methods usually perform poorly when samples are inadequate. To address these two challenges, we developed a novel language model for mining DBPs using human genomic data and ChIP-seq datasets with decaying learning rates, named DNA Fine-tuned Language Model (DFLM). It can capture the dependencies between genome sequences based on the context of human genomic data and then fine-tune the features of DBPs tasks using different ChIP-seq datasets. First, we compared DFLM with the existing widely used methods on 69 datasets and we achieved excellent performance. Moreover, we conducted comparative experiments on complex DBPs and small datasets. The results show that DFLM still achieved a significant improvement. Finally, through visualization analysis of one-hot encoding and DFLM, we found that one-hot encoding completely cut off the dependencies of DNA sequences themselves, while DFLM using language models can well represent the dependency of DNA sequences. Source code are available at: https://github.com/Deep-Bioinfo/DFLM",2024-05-22 12:38,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
BIDAF,Language,Question answering,2016-11-05,apache 2.0: https://github.com/allenai/bi-att-flow,Open source,Open source,Open source,https://arxiv.org/abs/1611.01603v6,"University of Washington,Allen Institute for AI",3468614400000000000.00,"flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =

citation from the section about cloze test experiments ""The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4"" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).
flops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632","United States of America,United States of America","Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi","Highly cited,SOTA improvement","""Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. """,Bidirectional Attention Flow for Machine Comprehension,"SQuAD,DMQA,GloVe","""In a cloze test, the reader is asked to fill in words that have been removed from a passage,
for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-
sive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)
examples from CNN and DailyMail news articles, respectively. """,47160000,"""In a cloze test, the reader is asked to fill in words that have been removed from a passage,
for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-
sive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)
examples from CNN and DailyMail news articles, respectively. ""
assuming 40 words per example we get around 47160000 words (SQuAD have around 40 words per example - so I think it should be similar case for this dataset)",Likely,,,2246.00,2600000.00,"There are two similar models described in sections ""Models details""
citation from the paper about model for SQuAD ""The model has about 2.6 million parameters""
citation about model for cloze test
""The model architecture used for this task is very similar to that for SQuAD (Sec-
tion 4) with only a few small changes to adapt it to the cloze test. ""
",8.00,,,60.0,see compute notes,NVIDIA GTX Titan X,,,,,"Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. ",2024-04-04 16:14,Anonymous,,,,,,8,,BIDAF,,,,"Academia,Research collective",,,,480,"Academia,Research collective",,,
CLUE,Recommendation,,2022-11-22,,,,,https://arxiv.org/abs/2111.11294,"Naver Clova,Naver AI Lab",3456000000000000000.00,"using grpah reader on Figure 2 b for the upper right point: 
0.04 PF-days * 10^15 FLOPS/s* 3600s*24h=3456000000000000000

counting operations: =160000000.00*50743870312*6=4.871411549952 × 10^19

""The computation (PF-days) is calculated as 6 × # of parameters × batch size × # of training steps × sequence length divided by one PF-day = 8.64 × 10^19. We train all models for 100,000 steps.""

","Korea (Republic of),Korea (Republic of)","Kyuyong Shin, Hanock Kwak, Su Young Kim, Max Nihlen Ramstrom, Jisu Jeong, Jung-Woo Ha, Kyung-Min Kim",,,Scaling Law for Recommendation Models: Towards General-purpose User Representations,,,50743870312,"Pre-training:
We construct a sufficiently large-scale dataset with more
than 50B (50741643225) behavior tokens collected over 2 years from search
engine and e-commerce platform.
Number of behavior tokens 50,741,643,225 (Appendix 1)
Downstream:
Books: We collect 1,298,489 review logs of 100,000 unique
users and 504,572 unique books.
Clothing: We collect 928,598 review logs of 100,000 unique
users and 314,943 unique clothing, shoes, and jewelry.

50741643225+1298489+928598=50743870312",Likely,256,"CLUE is trained with 160M parameters, sequence length (128), and batch size (256).",,160000000.00,160m,8.00,,,168.0,"We shuffle the dataset at every epoch and train the model for
8 epochs, where the transfer performance begins to plateau.
The total training time takes 7 days on 64 V100 GPUs. 

",NVIDIA V100,,,,,"Recent advancement of large-scale pretrained models such as BERT, GPT-3, CLIP, and Gopher, has shown astonishing achievements across various task domains. Unlike vision recognition and language models, studies on general-purpose user representation at scale still remain underexplored. Here we explore the possibility of general-purpose user representation learning by training a universal user encoder at large scales. We demonstrate that the scaling law is present in user representation learning areas, where the training error scales as a power-law with the amount of computation. Our Contrastive Learning User Encoder (CLUE), optimizes task-agnostic objectives, and the resulting user embeddings stretch our expectation of what is possible to do in various downstream tasks. CLUE also shows great transferability to other domains and companies, as performances on an online experiment shows significant improvements in Click-Through-Rate (CTR). Furthermore, we also investigate how the model performance is influenced by the scale factors, such as training data size, model capacity, sequence length, and batch size. Finally, we discuss the broader impacts of CLUE in general.",2024-05-16 08:09,Natalia Martemianova,,,,,,64,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
SPPNet,Vision,Image classification,2014-06-18,,,,,https://arxiv.org/abs/1406.4729,"Microsoft,Xi’an Jiaotong University,University of Science and Technology of China",3411072000000000000.00,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation","United States of America,China,China","Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,ImageNet-1k,,1280000,"Section 3.1: ""We train the networks on the 1000-category training
set of ImageNet 2012.""",,,,9436.00,,,,,,672.0,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",NVIDIA GeForce GTX TITAN,,$65.07,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Multilingual-E5-large,Language,Semantic embedding,2023-06-30,"weights: https://github.com/microsoft/unilm/blob/master/e5/README.md

license (MIT):
https://github.com/microsoft/unilm/blob/master/LICENSE",Open source,,Unreleased,"https://arxiv.org/abs/2402.05672
https://huggingface.co/intfloat/multilingual-e5-large",Microsoft,3370752000000000000.00,"6ND = 6*560000000*(1000000000+1600000*2 epochs) = 3.370752e+18

confidence 'likely"" because pre-training epochs are unknown",United States of America,"Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei",,,Multilingual E5 Text Embeddings: A Technical Report,"mC4,Wikipedia,Stack Exchange,xP3,CC-News",table 2 - multiple data,1000160000,"Pre-training: Table 1 around 1B text pairs in different languages
Fine-tuning: 1.6M 

total: 1000000000+160000 = 1000160000 text pairs ",Likely,,,,560000000.00,560M from https://huggingface.co/intfloat/multilingual-e5-large,,,,,,,,,,,"This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 ",2024-05-23 05:57,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
DiffQ Transformer (16L),Language,,2021-04-20,training code (non-commercial): https://github.com/facebookresearch/diffq/blob/main/examples/FAIRSEQ_README.md,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2104.09987,Meta AI,3360000000000000000.00,,United States of America,"Alexandre Défossez, Yossi Adi, Gabriel Synnaeve",,,Differentiable Model Compression via Pseudo Quantization Noise,WikiText-103,,,,,,,28.00,257000000.00,,,,,,,,,,,,,2024-05-07 15:00,Robi Rahman,DiffQ Transformer (16L),,,,,,,,,,,Industry,,,,,Industry,,,
PermuteFormer,Language,,2021-09-06,"no specific license 

https://github.com/cpcp1998/PermuteFormer",Unreleased,,Open source,https://arxiv.org/abs/2109.02377,Peking University,3100000000000000000.00,,China,Peng Chen,SOTA improvement,"""Results show that
PermuteFormer uniformly improves the performance of Performer, accelerates convergence, and
achieves state-of-the-art on some tasks.""",PermuteFormer: Efficient Relative Position Encoding for Long Sequences,WikiText-103,,,,,,,17.00,33000000.00,,30.00,,,,,,,,,,,2024-05-01 09:13,Robi Rahman,PermuteFormer,,,,,,,PermuteFormer,,,,Academia,,,,,Academia,,,
Shortformer,Language,,2020-12-31,"code, MIT: https://github.com/ofirpress/shortformer ",Unreleased,,Open source,https://arxiv.org/abs/2012.15832,"University of Washington,Facebook AI Research,Allen Institute for AI",3040000000000000000.00,,"United States of America,United States of America,United States of America","Ofir Press, Noah A. Smith, Mike Lewis",,,Shortformer: Better Language Modeling using Shorter Inputs,WikiText-103,,,,,,,67.00,24000000.00,,205.00,,,,,,,,,,,2024-05-10 10:52,Robi Rahman,Shortformer,,,,,,,,,,,"Academia,Industry,Research collective",,,,,"Academia,Industry,Research collective",,,
RQ-Transformer (LSUN-cat dataset),Vision,Text-to-image,2022-03-03,,,,,https://arxiv.org/abs/2203.01941,"Kakao,POSTECH",2911334400000000000.00,"flops = (4) * (3120 * 10**9) * (9*24 * 3600) * (0.3) = 2911334400000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""
we provide details for LSUN-cat with largest compute","Korea (Republic of),Korea (Republic of)","Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han",,,Autoregressive Image Generation using Residual Quantization,LSUN,"Table 5 
we provide details for LSUN-cat with largest compute",1657266,size of LSUN-cat,Wrong,,,107.00,612000000.00,"612M from Table 5 - LSUN-cat row
we provide details for LSUN-cat with largest compute",,,,216.0,"9 days for LSUN-cat
""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,NVIDIA A100,,,,,"For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256×256 image as 8×8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images. ",2024-05-10 12:54,Anonymous,,1,,,,4,,,,,,"Industry,Academia",,,,864,"Industry,Academia",,,
RQ-Transformer (1.4B params ImageNet dataset),Vision,Text-to-image,2022-03-03,,,,,https://arxiv.org/abs/2203.01941,"Kakao,POSTECH",2911334400000000000.00,"flops = (8) * (3120 * 10**9) * (4.5*24 * 3600) * (0.3) = 2911334400000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""
we provide details for LSUN-cat with largest compute","Korea (Republic of),Korea (Republic of)","Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han",,,Autoregressive Image Generation using Residual Quantization,ImageNet,"Table 5 
",1200000,size of ImageNet,Wrong,,,107.00,1388000000.00,"1388M from Table 5 - ImageNet rows
",,,,108.0,"4.5  days for ImageNet 
for 1.4B model
""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,NVIDIA A100,,,,,"For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256×256 image as 8×8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images. ",2024-05-10 12:54,Bartosz Podkanowicz,,1,,,,8,,,,,,"Industry,Academia",,,,864,"Industry,Academia",,,
NAS+ESS (156M),Language,,2020-05-06,,Unreleased,,Unreleased,https://arxiv.org/abs/2005.02593,"Northeastern University (China),Chinese Academy of Sciences,NiuTrans Research,Kingsoft",2890000000000000000.00,,"China,China,China,China","Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",SOTA improvement,"""Our ESS method
achieves state-of-the-art result on the PTB task""",Learning Architectures from an Extended Search Space for Language Modeling,Penn TreeBank,,,,,,,12.00,156000000.00,,30.00,,,,,,,,,,,2024-04-23 15:57,Robi Rahman,NAS+ESS (156M),,,,,,,NAS+ESS (156M),,,,"Academia,Academia",,,,,"Academia,Academia",,,
TrellisNet,Language,,2018-10-15,,,,,https://arxiv.org/abs/1810.06682,"Carnegie Mellon University (CMU),Bosch Center for Artificial Intelligence,Intel Labs",2780000000000000000.00,,"United States of America,Germany,Multinational","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",SOTA improvement,"""Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling
tasks""",Trellis Networks for Sequence Modeling,WikiText-103,,,,,,,132.00,180000000.00,,25.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,TrellisNet,,,,,,,TrellisNet,,,,"Academia,Industry,Industry",,,,,"Academia,Industry,Industry",,,
Cross-lingual alignment,Language,Translation,2019-04-04,"MIT license
https://github.com/TalSchuster/CrossLingualContextualEmb",Open source,,Open source,https://arxiv.org/abs/1902.09492,"Tel Aviv University,Massachusetts Institute of Technology (MIT)",2560000000000000000.00,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute: 7 GPU-days

0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h
= 2.56E+18","Israel,United States of America","Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.",SOTA improvement,"""our method consistently outperforms the previous state-of-the-art on 6 tested languages""","Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.","Wikipedia,CoNLL2017",,,,,,,189.00,,,,3660000000000.00,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute (Estimate): 0.00001 GPU Days


0.4 * 1.06E+13 FLOP/s * 0.00001 days * 24h/day * 3600s/h
= 3.66E+12



",,,NVIDIA GeForce GTX 1080 Ti,,$7.83,,Academia,,2024-04-15 06:15,Robi Rahman,,,ELMo,,,,,Cross-lingual alignment,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Decoupled weight decay regularization,Vision,Image classification,2019-01-04,license: https://github.com/loshchil/AdamW-and-SGDW/blob/master/LICENSE,Open source,Open source,Open source,https://arxiv.org/abs/1711.05101,University of Freiburg,2470000000000000000.00,"From author communication

Per image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs = 2.47E+18 FLOPs",Germany,Ilya Loshchilov and Frank Hutter,Highly cited,,Decoupled weight decay regularization.,CIFAR-10,,50000,,,,,2061.00,36500000.00,"From author communication

WideResNet 28-10 models with 36.5 million parameters (3.65E+07)",,1730000000.00,"From author communication

Best estimate: 1.73E+09",,,,,$8.07,,Academia,,2024-04-15 15:37,Robi Rahman,,,,,,,,Decoupled weight decay regularization,,,,Academia,,,,,Academia,,,
MMLSTM,Language,,2019-12-05,,Unreleased,,Unreleased,http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf,"Beijing University of Posts and Telecommunications,University of West London",2320000000000000000.00,,"China,United Kingdom of Great Britain and Northern Ireland","Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",SOTA improvement,"""In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets""",Major–Minor Long Short-Term Memory for Word-Level Language Model,WikiText-103,,,,,,,14.00,75000000.00,,50.00,,,,,,,,,,,2024-04-22 14:30,Robi Rahman,MMLSTM,,,,,,,MMLSTM,,,,"Academia,Academia",,,,,"Academia,Academia",,,
EGRU (WT2),Language,,2022-06-13,Apache 2.0: https://github.com/Efficient-Scalable-Machine-Learning/EvNN,Unreleased,,Open source,https://arxiv.org/abs/2206.06178v3,"Ruhr University Bochum,Technische Universität Dresden,University of London",2310000000000000000.00,,"Germany,Germany,United Kingdom of Great Britain and Northern Ireland","Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",,SOTA for recurrent networks but not in general,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,WikiText-2,,,,,,,8.00,74000000.00,,2500.00,,,,,,,,,,,2024-04-30 16:45,Robi Rahman,EGRU (WT2),,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
DensePhrases,Language,Question answering,2020-12-23,Apache 2.0: https://github.com/princeton-nlp/DensePhrases,Open source,Open source,Open source,https://arxiv.org/abs/2012.12624v3,"Korea University,Princeton University",2099520000000000000.00," flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

model of GPU from appendix B (Titan Xp)
number of GPUs from table in appendix A
flops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948","Korea (Republic of),United States of America","Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen",SOTA improvement,"from abstract ""our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. """,Learning Dense Representations of Phrases at Scale,"SQuAD,NQ (Natural Questions)","from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""",58000000,"from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""
assuming 40 words per question we get around ~ 58M",Speculative,,,101.00,,may be possible to estimate from batch size (8) and maximum memory of GPUs (96GB),4.00,,,20.0,appendix A row 3,NVIDIA TITAN Xp,,,,,"Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks. ",2024-04-25 12:07,Anonymous,,,,,,8,,DensePhrases,,,,"Academia,Academia",,,,160,"Academia,Academia",,,
RetinaNet-R101,Vision,Object detection,2017-08-07,,,,,https://arxiv.org/abs/1708.02002,Facebook AI Research,2065392000000000000.00,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",United States of America,"TY Lin, P Goyal, R Girshick, K He, P Dollar",Highly cited,,Focal loss for dense object detection,COCO,,135000,trainval135k split,,,,16437.00,53000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,127000000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,35.0,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",NVIDIA M40,,,,Industry,,2024-03-11 16:36,Robi Rahman,,,,,,8,,RetinaNet-R101,,,,Industry,,,,280,Industry,,,
LMRec,"Language,Recommendation",Language modelling/generation,2023-05-13,,,,,https://arxiv.org/abs/2212.03760,"NAVER,Naver AI Lab",1978200000000000000.00,210000000*1570000000*6=1.9782e+18,"Korea (Republic of),Korea (Republic of)","Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyung-Min Kim, Jung-Woo Ha, Sang-Woo Lee",,,Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning,Amazon Review Data,"We use three in-house datasets in order to assess our approach on various applications and add three public datasets that are predominantly evaluated in recommendation communities. The in-house datasets are built from
services of an online booking service (OBS), an online travel agency (OTA), and e-commerce platmform (ECOMM). For public datasets, we select two
categories “Industrial and Scientific” (Scientific) and “Prime Pantry” (Pantry) from Amazon review datatsets (Ni et al., 2019) which are two completely
different service domains.",1570000000,"Table 3
 1222700000 (in-house dataset) + 347300000 (piblic) = 1,570,000,000 tokens",Likely,1024,table 9,,210000000.00,210 million (Figure 4),,,,,,,,,,,"Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.",2024-05-20 03:28,Natalia Martemianova,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
Tensor-Transformer(1core)+PN (WT103),Language,,2020-03-17,copyleft license: https://github.com/sIncerass/powernorm/blob/master/LICENSE,Open source,,Open source,https://arxiv.org/abs/2003.07845,UC Berkeley,1580000000000000000.00,,United States of America,"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",SOTA improvement,"""The results are reported in Table 1. In the first section of
rows, we report state-of-the-art results for these two tasks with comparable model sizes""",PowerNorm: Rethinking Batch Normalization in Transformers,WikiText-103,,,,,,,60.00,85300000.00,,30.00,,,,,,,,,,,2024-04-23 10:11,Robi Rahman,Tensor-Transformer(1core)+PN (WT103),,,,,,,Tensor-Transformer(1core)+PN (WT103),,,,Academia,,,,,Academia,,,
6-Layer-Tensor-Transformer+AdaHessian,Language,,2020-06-01,MIT for code. doesn't have training script for WT103 but looks fairly adaptable: https://github.com/amirgholami/ADAHESSIAN ,Unreleased,,Open source,https://arxiv.org/abs/2006.00719,"""NERSC, Lawrence Berkeley National Laboratory"",UC Berkeley",1580000000000000000.00,,"United States of America,United States of America","Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",,"""We show that AdaHessian achieves new state-of-the-art results by a large margin as compared
to other adaptive optimization methods, including variants of Adam""",ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,WikiText-103,,,,,,,186.00,85300000.00,,30.00,,,,,,,,,,,2024-05-10 16:20,Robi Rahman,6-Layer-Tensor-Transformer+AdaHessian,,,,,,,,,,,"Government,Academia",,,,,"Government,Academia",,,
GoogLeNet / InceptionV1,Vision,Image classification,2015-06-07,,,,,https://arxiv.org/abs/1409.4842,"Google,University of Michigan,University of North Carolina",1557140125176000000.00,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,"United States of America,United States of America,United States of America","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",Highly cited,,Going deeper with convolutions,"ILSVRC 2014 subset of ImageNet,ImageNet",,1200000,"""The ILSVRC 2014 classification challenge involves the
task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing
[...]
We participated in the challenge with no external data
used for training.
""",,,,39194.00,6797700.00,Computed summing the parameters on table 1 of section 5,,,,,,,,$14.16,,Industry,,2024-04-15 06:40,Robi Rahman,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
AlexNet + coordinating filters,Vision,Image classification,2017-03-28,license: https://github.com/wenwei202/caffe/blob/master/LICENSE,Open source,,Open source,https://arxiv.org/abs/1703.09746,"University of Pittsburgh,Duke University",1557140125176000000.00,"The paper reimplements ResNet-20, AlexNet and GoogLeNet. The largest from these models is AlexNet. (data for these models taken form this db)
training of AlexNet taken 470000000000000000.00 FLOPs
training of GoogLeNet taken 1557140125176000000.00 FLOPs
max(1557140125176000000, 470000000000000000) = 1557140125176000000
source ""The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. ""","United States of America,United States of America","Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li",,,Coordinating Filters for Faster Deep Neural Networks,"ImageNet,CIFAR-10,MNIST","from ""5.1"" Our experiments are performed in Caffe [13] using CIFAR-10 [15] and ILSVRC-2012 ImageNet [3].",1200000,size of ImageNet,Likely,,,172.00,60000000.00,"The paper reimplements ResNet-20, AlexNet and GoogLeNet. The largest from these models is AlexNet. (data for these models taken form this db)
AlexNet has 60000000.00 params.
GoogLeNet has 6797700.00 params.
So max(60000000, 6797700) = 60000000
source ""The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. """,,,,,,,,,,,"Very large-scale Deep Neural Networks (DNNs) have achieved remarkable successes in a large variety of computer vision tasks. However, the high computation intensity of DNNs makes it challenging to deploy these models on resource-limited systems. Some studies used low-rank approaches that approximate the filters by low-rank basis to accelerate the testing. Those works directly decomposed the pre-trained DNNs by Low-Rank Approximations (LRA). How to train DNNs toward lower-rank space for more efficient DNNs, however, remains as an open area. To solve the issue, in this work, we propose Force Regularization, which uses attractive forces to enforce filters so as to coordinate more weight information into lower-rank space. We mathematically and empirically verify that after applying our technique, standard LRA methods can reconstruct filters using much lower basis and thus result in faster DNNs. The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. Moreover, Force Regularization better initializes the low-rank DNNs such that the fine-tuning can converge faster toward higher accuracy. The obtained lower-rank DNNs can be further sparsified, proving that Force Regularization can be integrated with state-of-the-art sparsity-based acceleration methods. Source code is available in this https URL",2024-04-04 16:04,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RNNsearch-50*,Language,Translation,2014-09-01,,,,,https://arxiv.org/abs/1409.0473,"Jacobs University Bremen,University of Montreal / Université de Montréal",1555200000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU","Germany,Canada","D Bahdanau, K Cho, Y Bengio",Highly cited,,Neural Machine Translation by Jointly Learning to Align and Translate,WMT'14 + selection,,348000000,"[WORDS]
""WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",,,,24770.00,,,,,,,,,,$81.48,,Academia,,2024-05-22 12:23,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RQ-Transformer (3.8B params ImageNet dataset),Vision,Text-to-image,2022-03-03,,,,,https://arxiv.org/abs/2203.01941,"Kakao,POSTECH",1455667200000000000.00,"flops = (4) * (312 * 10**9) * (4.5*24 * 3600) * (0.3) = 1455667200000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to train RQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""
we provide details for LSUN-cat with largest compute","Korea (Republic of),Korea (Republic of)","Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han",,,Autoregressive Image Generation using Residual Quantization,ImageNet,"Table 5 
",1200000,size of ImageNet,Wrong,,,107.00,1388000000.00,"3822M from Table 5 - ImageNet rows
",,,,108.0,"4.5  days for ImageNet 
for 3.8B model
""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,NVIDIA A100,,,,,"For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256×256 image as 8×8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images. ",2024-05-10 12:54,Bartosz Podkanowicz,,,,,,4,,,,,,"Industry,Academia",,,,432,"Industry,Academia",,,
A.X (Adot) 18B,"Language,Speech",Chat,2021-11-15,,,,Unreleased,https://aclanthology.org/2023.acl-industry.68.pdf,SK Telecom,1350000000000000000.00,'=18000000000*12500000 (unsure about training dataset size->speculative confidence level)*6=1350000000000000000=1.35 × 10^18,Korea (Republic of),"Deuksin Kwon, Sunwoo Lee, Ki Hyun Kim, Seojin Lee, Taeyoon Kim, Eric Davis",,,"WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue",AI-HUB,"we construct a Korean Multi-Session Personalized Dialogue dataset, which we refer to as MSPD. 

Alongside the MSPD, we incorporate a variety of informal dialogue datasets, referred to as Dcasual, to train a more balanced model capable of generating high-quality daily, knowledgebased, empathetic, and personalized conversations.
Dcasual consists of a comprehensive collection of approximately 12.5 million utterances. We use carefully-curated Korean dialogue datasets available online1, developed by National Information Society Agency (NIA), as well as crowdsourced conversational datasets, including Korean versions
of PersonaChat, EmpatheticDialogues, and Wizard of Wikipedia (Dinan et al., 2018; Rashkin et al., 2018; Zhang et al., 2018).

The final training dataset was assembled by blending different conversational datasets, with specific blending weights.",,,Speculative,,,,18000000000.00,,,,,,"All experiments are conducted on SKT’s proprietary supercomputer, Titan, equipped with NVIDIA A100 SXM4 80GB GPUs.",NVIDIA A100 SXM4 80 GB,,,,,"This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative
persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.",2024-05-16 14:19,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,,,
TransE,Language,Entity embedding,2013-12-05,,,,,https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,"Universite de Technologie de Compiègne – CNRS,Google",1340928000000000000.00,"8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate","France,United States of America","Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",Highly cited,,Translating Embeddings for Modeling Multi- relational Data,,,17000000,"""it can be successfully trained on a large scale data set with 1M
entities, 25k relationships and more than 17M training samples""",Speculative,,,7039.00,942000000.00,"Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.

They studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.

FB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.",,,,,,,,$17.58,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Hide and Seek,Games,Hide and seek,2019-09-17,,,,,https://openai.com/blog/emergent-tool-use/,OpenAI,1150000000000000000.00,"1.6e6 * 2 * 120e9 * 3 ~= 1.15e18 FLOP. We assume the single convolution on the lidar input is negligible, and the rest of the model (which consists of MLPs, self-attention and LSTM) has roughly 1 multiply-add per parameter. The following source has a similar estimate but does not adjust for the full number of episodes: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",United States of America,"B Baker, I Kanitscheider, T Markov, Y Wu",,,Emergent Tool Use From Multi-Agent Autocurricula,,,120000000000,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters, requires 132.3 million episodes (31.7 billion frames) over 34 hours of training to reach stage 4 of the skill progression, i.e. ramp defense."" But the full number of episodes, e.g. Figure 1, is 500 million. 500 / 132.3 * 31.7B ~= 120B.",,,,538.00,1600000.00,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters,..."" pg. 7 of the Hide and Seek paper, stored in ""RL papers""

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,,,,,$0.80,,Industry,,2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Delta RNN (+ full context),Language,,2021-06-11,"training code for Delta RNN: https://github.com/IDSIA/recurrent-fwp/tree/master/language_modeling 

apache 2 license: https://github.com/IDSIA/recurrent-fwp/blob/master/language_modeling/LICENSE",Unreleased,,Open source,https://proceedings.neurips.cc/paper/2021/file/3f9e3767ef3b10a0de4c256d7ef9805d-Paper.pdf,"IDSIA,SUPSI,King Abdullah University of Science and Technology / KAUST",1100000000000000100.00,,"Switzerland,Switzerland,Saudi Arabia","Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber",,,Going Beyond Linear Transformers with Recurrent Fast Weight Programmers,WikiText-103,,,,,,,42.00,44600000.00,,40.00,,,,,,,,,,"Transformers with linearised attention (“linear Transformers”) have demonstrated
the practical scalability and effectiveness of outer product-based Fast Weight
Programmers (FWPs) from the ’90s. However, the original FWP formulation is
more general than the one of linear Transformers: a slow neural network (NN)
continually reprograms the weights of a fast NN with arbitrary architecture. In
existing linear Transformers, both NNs are feedforward and consist of a single
layer. Here we explore new variations by adding recurrence to the slow and fast
nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic
tasks (code execution and sequential ListOps), Wikitext-103 language models,
and on the Atari 2600 2D game environment. Our models exhibit properties of
Transformers and RNNs. In the reinforcement learning setting, we report large
improvements over LSTM in several Atari games. Our code is public.1
",2024-05-09 13:35,Robi Rahman,Delta RNN (+ full context),,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
UDSMProt,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2019-09-04,BSD license,Open source,,Open source,https://www.biorxiv.org/content/10.1101/704874v2.full.pdf,"""Fraunhofer Heinrich Hertz Institute
""",890000000000000000.00,"170k sequences, each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.

170k * 1024 * 30 * 28.3 * 6 = 8.9e17.",Germany,"Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek",SOTA improvement,"""The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them.""",UDSMProt: Universal Deep Sequence Models for Protein Classification,"SwissProt,a subset of UniProtKB",,,560K proteins,Likely,,,,28303800.00,"Python code:  
# Given LSTM parameters
emb_sz = 400  # embedding size, typically equal to the input size for the first layer
nh = 1150     # number of hidden units
nl = 3        # number of layers

# The formula for a single LSTM layer parameters is:
# P = 4 * ((input_dim + hidden_dim) * hidden_dim + hidden_dim)

# First layer parameters (input_dim is the embedding size)
first_layer_params = 4 * ((emb_sz + nh) * nh + nh)

# For subsequent layers, input_dim is equal to hidden_dim (nh)
subsequent_layer_params = 4 * ((nh + nh) * nh + nh)

# Total parameters for all layers
total_params = first_layer_params + (nl - 1) * subsequent_layer_params

print(total_params)",30.00,,,,,,Self-supervised learning,,,,"Motivation: Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification tasks are tailored to single classi- fication tasks and rely on handcrafted features such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple finetuning step.
Results: We put forward a universal deep sequence model that is pretrained on unlabeled protein se- quences from Swiss-Prot and finetuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics.",2024-04-01 16:50,Anonymous,,,,,,,,UDSMProt,,,,Research collective,,,,,Research collective,,,
"DEQ-Transformer (Medium, Adaptive Embedding)",Language,,2019-09-03,"code and weights, MIT: https://github.com/locuslab/deq/tree/master/DEQ-Sequence ",Open source,,Open source,https://arxiv.org/abs/1909.01377,Carnegie Mellon University (CMU),816000000000000000.00,,United States of America,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,Deep Equilibrium Models,,,,,,,,500.00,110000000.00,,12.00,,,,,,,,,,,2024-05-22 11:57,Robi Rahman,"""DEQ-Transformer (Medium, Adaptive Embedding)""",,,,,,,,,,,Academia,,,,,Academia,,,
BITTERS,Vision,Image captioning,2023-10-01,,,,,https://arxiv.org/abs/2211.06774,LG,780150000000000000.00,"WaveVAE: 
""We train the model for 10 epochs with batch size 3840.""
""We reduced the number of parameters to 25 million in total""

25000*100000000*6*10=1.5e+14

BiART:
""We train our model for 2 epochs in total with batch size 1280""
""BITTERS has 650 million parameters in total""

650000000*100000000*6*2=7.8e+17

7.8e+17+1.5e+14=7.8015e+17",Korea (Republic of),"Taehoon Kim, Mark Marsden, Pyunghwan Ahn, Sangyun Kim, Sihaeng Lee, Alessandra Sala, Seung Hwan Kim",,,Large-Scale Bidirectional Training for Zero-Shot Image Captioning,ShutterStock,"All images are random sampled from Shutterstock’s1
image catalog. Images are 500px on the longest side with a
single ground truth caption provided. A list of keywords is
also included for each image to enable keyword extraction
model training. All captions and keywords are in English
and were moderated for hate speech, slurs, and expletives",100000000,"we train BITTERS using a new, quality-controlled 100 million image dataset which we will refer to as Text Image Pairs 100 Million (TIP100M)",Likely,,,,650000000.00, BITTERS has 650 million parameters in total.,,,,,,,,,,,"When trained on large-scale datasets, image captioning models can understand the content of images from a general domain but often fail to generate accurate, detailed captions. To improve performance, pretraining-and-finetuning has been a key strategy for image captioning. However, we find that large-scale bidirectional training between image and text enables zero-shot image captioning. In this paper, we introduce Bidirectional Image Text Training in largER Scale, BITTERS, an efficient training and inference framework for zero-shot image captioning. We also propose a new evaluation benchmark which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero-shot captioning accuracy and societal bias. We additionally provide an efficient finetuning approach for keyword extraction. We show that careful selection of large-scale training set and model architecture is the key to achieving zero-shot image captioning.",2024-05-20 06:44,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
AWD-FWM (WT2),Language,,2020-11-16,"code, repo license is MIT: https://github.com/ischlag/Fast-Weight-Memory-public/tree/main/language-modelling/fwm ",Unreleased,,Open source,https://arxiv.org/abs/2011.07831,"IDSIA,Microsoft Research",739000000000000000.00,,"Switzerland,United States of America","Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",,,Learning Associative Inference Using Fast Weight Memory,WikiText-2,,,,,,,29.00,37000000.00,,1600.00,,,,,,,,,,,2024-05-10 15:17,Robi Rahman,AWD-FWM (WT2),,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),Language,,2018-08-30,,,,,https://arxiv.org/abs/1808.10143,"NTT Communication Science Laboratories,Tohoku University",693000000000000000.00,,"Japan,Japan","Sho Takase, Jun Suzuki, Masaaki Nagata",SOTA improvement,"""The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets""",Direct Output Connection for a High-Rank Language Model,WikiText-2,,,,,,,36.00,185000000.00,,300.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),,,,,,,(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),,,,"Industry,Academia",,,,,"Industry,Academia",,,
Self-Attention and Convolutional Layers,Vision,Image classification,2019-11-08,"Apache 2.0 for code: https://github.com/epfml/attention-cnn

data is CIFAR which doesn't have a clear license",Unreleased,Open access (non-commercial),Open source,https://arxiv.org/abs/1911.03584,Ecole Polytechnique F´ed´erale de Lausanne (EPFL),675000000000000000.00,"(15e9) * (300) * (50000) * 3 = 675000000000000000
(inference compute) * (epochs) * (dataset size) * (constant to account for backpropagation)
 
epochs from appendix B table 2
inference compute from table 1
",Switzerland,"Jean-Baptiste Cordonnier, Andreas Loukas & Martin Jaggi",,,On the Relationship between Self-Attention and Convolutional Layers,CIFAR-10,"Figure 2 in the paper
and line 29 from https://github.com/epfml/attention-cnn/blob/master/train.py (access date 30.01.2024)",50000,size of CIFAR-10,Likely,,,550.00,29500000.00,from Table 1,300.00,15000000000.00,from Table 1,,,,,,,Academia,"Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available. ",2024-04-04 13:01,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Unsupervised High-level Feature Learner,Vision,Image classification,2012-07-12,,,,,https://arxiv.org/abs/1112.6209,Google,600000000000000000.00,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",United States of America,"Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng","Highly cited,SOTA improvement","""we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art""",Building High-level Features Using Large Scale Unsupervised Learning,,10 million 200x200 images extracted from Youtube videos,10000000,10 million 200x200 images extracted from Youtube videos,Likely,,,2909.00,1000000000.00,"""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""",,,,72.0,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,,Unsupervised,,Hardware not reported,Industry,"We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
CT-MoS (WT2),Language,,2020-12-25,,Unreleased,,Unreleased,https://arxiv.org/abs/2012.13575,"Google,National Tsing Hua University",562000000000000000.00,,"United States of America,Taiwan","Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",SOTA improvement,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",Contextual Temperature for Language Modeling,WikiText-2,,,,,,,8.00,45000000.00,,1000.00,,,,,,,,,,,2024-05-01 09:05,Robi Rahman,CT-MoS (WT2),,,,,,,CT-MoS (WT2),,,,"Industry,Academia",,,,,"Industry,Academia",,,
CT-MoS + DynamicEval (WT2),Language,,2020-12-25,,Unreleased,,Unreleased,https://arxiv.org/abs/2012.13575,"National Tsing Hua University,Google",562000000000000000.00,,"Taiwan,United States of America","Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",,"other model in the paper (without DynamicEval) was better, per Table 2",Contextual Temperature for Language Modeling,WikiText-2,,,,,,,8.00,45000000.00,,1000.00,,,,,,,,,,"Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",2024-05-10 11:08,Robi Rahman,CT-MoS + DynamicEval (WT2),,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Visualizing CNNs,Vision,Image classification,2013-11-12,,,,,https://arxiv.org/abs/1311.2901,New York University (NYU),532000000000000000.00,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute",United States of America,"MD Zeiler, R Fergus",Highly cited,,Visualizing and Understanding Convolutional Networks,,,,,,,,14562.00,,,,,,,,NVIDIA GeForce GTX 580,,$9.02,,Academia,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
GANs,Image generation,Image generation,2014-06-10,,,,,https://arxiv.org/abs/1406.2661,University of Montreal / Université de Montréal,518400000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix

""Less than 0.006 pfs-days""
(86400*10^15*0.006)

Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",Canada,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",Highly cited,,Generative Adversarial Networks,CIFAR-10,,60000,"""We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].""

MNIST has 60k images 
https://en.wikipedia.org/wiki/MNIST_database

TFD seems to have 2925 examples (?)
https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdf

CIFAR-10 has 60k images
https://www.cs.toronto.edu/~kriz/cifar.html

",Speculative,,,36870.00,,The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.,,,,,,,,$6.09,,Academia,,2024-05-22 12:23,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),Language,,2017-08-29,,,,,https://arxiv.org/abs/1708.08863,Ben-Gurion University of the Negev,474000000000000000.00,,United States of America,"Ziv Aharoni, Gal Rattner, Haim Permuter",SOTA improvement,"""Our GL-LSTM model
overcame the state-of-the-art results with only two layers and 19M parameters, and further improved
the state-of-the-art results with the third layer phase""",Gradual Learning of Recurrent Neural Networks,WikiText-2,,,,,,,4.00,38000000.00,,1000.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),,,,,,,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),,,,Academia,,,,,Academia,,,
AlexNet,Vision,Image classification,2012-09-30,,,,,https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,University of Toronto,470000000000000000.00,"1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/",Canada,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton","Highly cited,Historical significance",,ImageNet Classification with Deep Convolutional Neural Networks,ImageNet,,1200000,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""",,,,106842.00,60000000.00,"""Our neural network architecture has 60 million parameters.""",,,,,,NVIDIA GeForce GTX 580,,$8.00,,Academia,,2024-04-23 18:20,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
TransformerXL + spectrum control,Language,,2020-03-11,,Unreleased,,Unreleased,https://openreview.net/forum?id=ByxY8CNtvr,"University of California Los Angeles (UCLA),JD.com",459999999999999940.00,,"United States of America,China","Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",SOTA improvement,"""We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model""",Improving Neural Language Generation with Spectrum Control,WikiText-103,,,,,,,66.00,151000000.00,,250.00,,,,,,,,,,,2024-05-01 09:05,Robi Rahman,TransformerXL + spectrum control,,,,,,,TransformerXL + spectrum control,,,,"Academia,Industry",,,,,"Academia,Industry",,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",Language,,2017-11-10,,,,,https://arxiv.org/abs/1711.03953,Carnegie Mellon University (CMU),437000000000000000.00,,United States of America,"Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",SOTA improvement,"""Experimental results confirm that the
proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on
the test set of Penn Treebank and WikiText-2""",Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,,,,,,,,358.00,35000000.00,,1000.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)""",,,,,,,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)""",,,,Academia,,,,,Academia,,,
FMMformer (2-kernel fast weight + Band20),Language,,2021-08-05,,Unreleased,,Unreleased,https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf,"University of California Los Angeles (UCLA),University of Utah",430000000000000000.00,,"United States of America,United States of America","Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang",,,FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention,,,,,,,,28.00,40000000.00,,,,,,,,,,,,"We propose FMMformers, a class of efficient and flexible transformers inspired
by the celebrated fast multipole method (FMM) for accelerating interacting particle simulation. FMM decomposes particle-particle interaction into near-field and
far-field components and then performs direct and coarse-grained computation,
respectively. Similarly, FMMformers decompose the attention into near-field and
far-field attention, modeling the near-field attention by a banded matrix and the
far-field attention by a low-rank matrix. Computing the attention matrix for FMMformers requires linear complexity in computational time and memory footprint
with respect to the sequence length. In contrast, standard transformers suffer from
quadratic complexity. We analyze and validate the advantage of FMMformers
over the standard transformer on the Long Range Arena and language modeling
benchmarks. FMMformers can even outperform the standard transformer in terms
of accuracy by a significant margin. For instance, FMMformers achieve an average
classification accuracy of 60.74% over the five Long Range Arena tasks, which is
significantly better than the standard transformer’s average accuracy of 58.70%.
",2024-05-09 13:42,Robi Rahman,FMMformer (2-kernel fast weight + Band20),,,,,,,,,,FMMformer (2-kernel fast weight + Band20),"Academia,Academia",,,,,"Academia,Academia",,,
Big-Little Net (speech),Speech,Speech recognition,2018-07-10,"apache for code/weights: 
https://github.com/IBM/BigLittleNet",Open source,,Open source,https://arxiv.org/abs/1807.03848,IBM,429004800000000000.00,980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000,United States of America,"Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris",SOTA improvement,"""Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.""",Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,"Switchboard,Fisher","""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""",27360000,"""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""

2000h * 13680 words per hour = 27360000

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",Speculative,,,83.00,3320000.00,table 3,16.00,980000000.00,table 3,,,,,,,,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet.",2024-05-01 09:13,Anonymous,,,,,,,,Big-Little Net (speech),,,,Industry,,,,,Industry,,,
AWD-LSTM-DRILL + dynamic evaluation† (WT2),Language,,2019-05-14,"copyleft license (restricts derivative works to be open)

https://github.com/idiap/drill?tab=GPL-3.0-1-ov-file#readme",Open source,Open source,Open access (restricted use),https://arxiv.org/abs/1905.05513,IDIAP,424000000000000000.00,,Switzerland,"Nikolaos Pappas, James Henderson",SOTA improvement,"""our models improve over the state-of-the-art by +1.6 perplexity on PennTreebank and by +3.9 perplexity on
Wikitext-2""",Deep Residual Output Layers for Neural Language Generation,WikiText-2,,,,,,,7.00,34000000.00,,1000.00,,,,,,,,,,,2024-04-16 15:37,Robi Rahman,AWD-LSTM-DRILL + dynamic evaluation† (WT2),,,,,,,AWD-LSTM-DRILL + dynamic evaluation† (WT2),,,,Academia,,,,,Academia,,,
Transformer-XL + PowerSGD + L-Greco,Language,,2022-10-31,"Apache 2. looks like just code: https://github.com/LGrCo/L-GreCo/tree/master/Transformer-XL 

",Unreleased,,Open source,https://arxiv.org/abs/2210.17357,"Institute of Science and Technology Austria (ISTA),Neural Magic",414000000000000000.00,,"Austria,United States of America","Mohammadreza Alimohammadi, Ilia Markov, Elias Frantar, Dan Alistarh",,,L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression,WikiText-103,,,,,,,4.00,,,,,,,,,,,,,,2024-04-30 12:12,Robi Rahman,Transformer-XL + PowerSGD + L-Greco,,,,,,,,,,TransformerXL + PowerSGD + L-Greco,"Academia,Industry",,,,,"Academia,Industry",,,
QRNN,Language,,2018-02-01,,,,,https://mlsys.org/Conferences/doc/2018/50.pdf,Salesforce Research,360000000000000000.00,,United States of America,"Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher",SOTA improvement,"""we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs""",Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours,WikiText-103,,,,,,,4.00,135000000.00,,14.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,QRNN,,,,,,,QRNN,,,,Industry,,,,,Industry,,,
AWD-LSTM + MoS + Partial Shuffled,Language,,2019-06-10,"code and weights. no license provided:
https://github.com/ChengyueGongR/advsoft",Open access (non-commercial),Open source,Open access (non-commercial),https://arxiv.org/abs/1906.03805,University of Texas at Austin,328000000000000000.00,,United States of America,"Dilin Wang, Chengyue Gong, Qiang Liu",SOTA improvement,"""our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively""",Improving Neural Language Modeling via Adversarial Training,WikiText-2,,,,,,,102.00,35000000.00,,750.00,,,,,,,,,,,2024-04-17 15:31,Robi Rahman,AWD-LSTM + MoS + Partial Shuffled,,,,,,,AWD-LSTM + MoS + Partial Shuffled,,,,Academia,,,,,Academia,,,
GPT-2+Active-SGD (WT 103),Language,,2023-01-24,,Unreleased,,Unreleased,https://arxiv.org/abs/2301.10133,University of Montreal / Université de Montréal,310000000000000000.00,,Canada,"Davood Wadi, Marc Fredette, Sylvain Senecal",,,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,WikiText-103,,,,,,,0.00,124000000.00,,200.00,,,,,,,,,,,2024-04-25 13:37,Robi Rahman,GPT-2+Active-SGD,,,,,,,,,,,Academia,,,,,Academia,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Language,,2017-08-07,,,,,https://arxiv.org/abs/1708.02182,Salesforce Research,309000000000000000.00,,United States of America,"Stephen Merity, Nitish Shirish Keskar, Richard Socher","Highly cited,SOTA improvement","""we achieve an
even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.""",Regularizing and Optimizing LSTM Language Models,WikiText-2,,,,,,,1176.00,33000000.00,,750.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),,,,,,,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),,,,Industry,,,,,Industry,,,
Dropout (ImageNet),Vision,Image classification,2012-06-03,http://www.cs.toronto.edu/~nitish/dropout,,,Open source,https://arxiv.org/abs/1207.0580,University of Toronto,273196800000000000.00,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",Canada,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,Improving neural networks by preventing co-adaptation of feature detectors,ImageNet,,1000000,"In 2010, a subset of 1000 classes
with roughly 1000 examples per class was the basis of an object recognition competition...",,,,7171.00,,"""We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with “max-pooling” layer followed by two globally
connected layers and a final 1000-way softmax layer""",,,,96.0,4 days with dropout; 2 days without dropout,NVIDIA GeForce GTX 580,,,,Academia,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2024-05-09 12:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
ULM-FiT,Language,Text classification,2018-01-18,https://nlp.fast.ai/category/classification.html,Open source,,Open source,https://arxiv.org/abs/1801.06146,"University of San Francisco,Insight Centre NUI Galway,Fast.ai",272538000000000000.00,=103000000*441000000*6=2.72538e+17,"United States of America,Ireland","Jeremy Howard, Sebastian Ruder",Highly cited,,Universal Language Model Fine-tuning for Text Classification,"IMDb,Yelp,Trec-6,DBpedia,AG news,WikiText-103",,103000000,"We pretrain the language model on Wikitext-103
(Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.

Fine-tuning datasets:
TREC-6 Question 5.5k
IMDb Sentiment 25k
Yelp-bi Sentiment 560k
Yelp-full Sentiment 650k
AG Topic 120k
DBpedia Topic 560k

560+120+650+560+25+5.5=1920.5k = 1920500",Speculative,,,1940.00,441000000.00,https://files.fast.ai/models/wt103/?C=S;O=D,,,,,,,,,,Industry,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",2024-05-09 13:16,Robi Rahman,,,AWD-LSTM,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
4 layer QRNN (h=2500),Language,,2018-03-22,,,,,https://arxiv.org/abs/1803.08240,Salesforce Research,240000000000000000.00,,United States of America,"Stephen Merity, Nitish Shirish Keskar, Richard Socher",SOTA improvement,"""QRNNs achieve stateof-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103)
datasets, respectively""",An Analysis of Neural Language Modeling at Multiple Scales,WikiText-103,,,,,,,183.00,26000000.00,,14.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,4 layer QRNN (h=2500),,,,,,,4 layer QRNN (h=2500),,,,Industry,,,,,Industry,,,
DeiT,Vision,Image classification,2021-01-15,"https://github.com/facebookresearch/deit

Apache-2.0 license",Open source,,Open source,https://arxiv.org/abs/2012.12877,"Meta AI,Sorbonne University",198144000000000000.00,"2*86000000 parameters*3*1280000 training examples*300 epochs=1.98144e+17 FLOPs

compute [FLOP] = training time [s] × # of GPUs/TPUs × peak FLOP/s × utilization rate

53h*3600*125000000000000 peak FLOP/s*0.3=7.155e+18

","United States of America,France","Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou",Highly cited,,Training data-efficient image transformers & distillation through attention,ImageNet,,1280000,,Likely,,,5493.00,86000000.00,(DeiT-B),300.00,,,53.0,"A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.
In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets having a similar number of parameters and efficiency. It uses Imagenet as the sole training set.",NVIDIA V100,,,,,"Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.
In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.
More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",2024-04-11 14:50,Natalia Martemianova,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
ADP-FAIRSEQ + NGRAMRES,Language,Language modelling,2018-09-28,"https://github.com/ghrua/NgramRes
(no specific license)",,,Open source,https://arxiv.org/abs/2210.14431,"Nara Institute of Science and Technology,Chinese University of Hong Kong (CUHK),Tsinghua University",149682000000000000.00,"6NC = 6 * 247000000 * 101000000 = 1.49682e+17
(no information about epochs -> speculative confidence)","Japan,Hong Kong,China","Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",,,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,WikiText-103,,101000000,"""The training set contains around 101M tokens.""",Speculative,,,,247000000.00,247M (Table 2),,,,,,,,,,,"N-gram language models (LM) have been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an n-gram LM and the real-data distribution. The combination of n-gram and neural LMs not only allows the neural part to focus on the deeper understanding of language but also provides a flexible way to customize an LM by switching the underlying n-gram model without changing the neural model. Experimental results on three typical language tasks (i.e., language modeling, machine translation, and summarization) demonstrate that our approach attains additional performance gains over popular standalone neural models consistently. We also show that our approach allows for effective domain adaptation by simply switching to a domain-specific n-gram model, without any extra training. Our code is released at this https URL.",2024-05-23 04:06,Robi Rahman,ADP-FAIRSEQ,1,Transformer (Adaptive Input Embeddings),,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Part-of-sentence tagging model,Language,Part-of-speech tagging,2016-05-29,,,,,https://arxiv.org/abs/1603.01354,Carnegie Mellon University (CMU),145411200000000000.00,"12 hours of training for POS tagging
GeForce GTX TITAN X GPU
0.33 utilization rate
",United States of America,"Xuehe Ma, Eduard Hovy",Highly cited,,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"WSJ,Penn TreeBank",WSJ subset of Penn Treebank,912344,Table 2,Confident,,,3193.00,,Architecture described in Table 1,50.00,,,12.0,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",NVIDIA GeForce GTX TITAN X,,$0.97,,Academia,,2024-03-11 16:13,Robi Rahman,,,,,,1,,Part-of-sentence tagging model,,,,Academia,,,,12,Academia,,,
ONLSTM-SYD,Language,,2020-05-12,"BSD-3 license: 

https://github.com/wenyudu/SDLM ",Unreleased,,Open source,https://arxiv.org/abs/2005.05864,"Westlake University,Institute for Advanced Study,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),CIFAR AI Research,University of Montreal / Université de Montréal",138999999999999980.00,,"China,United States of America,Canada,Canada,Canada,Canada","Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang",,,Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach,Penn TreeBank,,,,,,,15.00,25000000.00,,1000.00,,,,,,,,,,,2024-05-22 13:23,Robi Rahman,ONLSTM-SYD,,,,,,,,,,,"Academia,Academia,Academia,Academia,Research collective,Academia",,,,,"Academia,Academia,Academia,Academia,Research collective,Academia",,,
Mitosis,"Vision,Medicine",,2013-09-22,,,,,https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51,IDSIA,137000000000000000.00,"""Training each network requires one day of computation with an optimized GPU
implementation""

Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get

3600*24*1.58E+12 = 1.37E+17 FLOP",Switzerland,"Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber",Highly cited,ICPR 2012 mitosis detection competition winner,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,,,1000000,"The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.

The final dataset has 1M instances

""We build the actual training set, composed by 1 million instances, which includes
all mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampled
from non-mitosis pixels by assigning to each pixel p a weight D(p).""",,,,1457.00,37230.00,Sum numbers of weights in Table 1.b,,,,,,,,$2.00,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Dropout-LSTM+Noise(Bernoulli) (WT2),Language,,2018-05-03,,,,,https://arxiv.org/abs/1805.01500,"Columbia University,New York University (NYU),Princeton University",127000000000000000.00,,"United States of America,United States of America,United States of America","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",SOTA improvement,"this is the best model in this paper per Table 4
""On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset""",Noisin: Unbiased Regularization for Recurrent Neural Networks,,,,,,,,26.00,51000000.00,,200.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,Dropout-LSTM+Noise(Bernoulli) (WT2),,,,,,,Dropout-LSTM+Noise(Bernoulli) (WT2),,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
LSTM+Noise(Beta),Language,,2018-05-03,,,,,https://arxiv.org/abs/1805.01500,"Columbia University,New York University (NYU),Princeton University",127000000000000000.00,,"United States of America,United States of America,United States of America","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",,not the best model in this paper,Noisin: Unbiased Regularization for Recurrent Neural Networks,,,,,,,,26.00,51000000.00,,200.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LSTM+Noise(Beta),,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Mogrifier RLSTM (WT2),Language,,2022-11-03,,Unreleased,,,https://arxiv.org/abs/2211.01848,"DeepMind,University College London (UCL)",109000000000000000.00,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",Gábor Melis,SOTA improvement,"""On top of these improvements, the RLSTM
outperformed the LSTM by a small margin, and we established a new state of the art on both datasets""",Circling Back to Recurrent Models of Language,,,,,,,,0.00,35000000.00,,250.00,,,,,,,,,,,2024-03-28 17:41,Robi Rahman,Mogrifier RLSTM (WT2),,,,,,,Mogrifier RLSTM (WT2),,,,"Industry,Academia",,,,,"Industry,Academia",,,
Cube-Space AutoEncoder,"Vision,Search",Visual puzzles,2020-04-27,,,,,https://arxiv.org/abs/2004.12850,MIT-IBM Watson AI Lab,106608960000000000.00,"(1) * (4113 * 10**9) * (24 * 3600) * (0.3) = 106608960000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = 

""For each domain, we searched for 100 iterations (≈15min/iter, 24 hours total) on a Tesla K80""
4.113 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-k80.c2616


",United States of America,"Masataro Asai, Christian Muise",,,Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS),,"""Finally, we tested Mandrill 15-puzzle, a significantly more
challenging 4x4 variant of the sliding tile puzzle (Figure 1).
We trained the network with more hyperparameter tuning it-
erations (300) and a larger training set (50000). We gener-
ated l = 14, 21 instances (20 each) and ran the system (Ta-
ble 2, bottom right). """,50000,"""Finally, we tested Mandrill 15-puzzle, a significantly more
challenging 4x4 variant of the sliding tile puzzle (Figure 1).
We trained the network with more hyperparameter tuning it-
erations (300) and a larger training set (50000). We gener-
ated l = 14, 21 instances (20 each) and ran the system (Ta-
ble 2, bottom right). """,Likely,,,56.00,,It may be possible to estimate from section 4.,,,,24.0,"""For each domain, we searched for 100 iterations (≈15min/iter, 24 hours total) on a Tesla K80""",NVIDIA Tesla K80,,,,,"We achieved a new milestone in the difficult task of enabling agents to learn about their environment autonomously. Our neuro-symbolic architecture is trained end-to-end to produce a succinct and effective discrete state transition model from images alone. Our target representation (the Planning Domain Definition Language) is already in a form that off-the-shelf solvers can consume, and opens the door to the rich array of modern heuristic search capabilities. We demonstrate how the sophisticated innate prior we place on the learning process significantly reduces the complexity of the learned representation, and reveals a connection to the graph-theoretic notion of ""cube-like graphs"", thus opening the door to a deeper understanding of the ideal properties for learned symbolic representations. We show that the powerful domain-independent heuristics allow our system to solve visual 15-Puzzle instances which are beyond the reach of blind search, without resorting to the Reinforcement Learning approach that requires a huge amount of training on the domain-dependent reward information.",2024-02-15 13:32,Anonymous,,,,,,1,,,,,,Academia,,,,24,Academia,,,
Fractional Max-Pooling,Vision,Image classification,2014-12-18,,,,,https://arxiv.org/abs/1412.6071v4,University of Warwick,100000000000000000.00,"For the 12M param model, training required ""18 hours on a GeForce GTX 780"". So would be somewhat larger for 27M.

4 TFLOPS * 18 * 3600 * 0.4 = 1e17",United Kingdom of Great Britain and Northern Ireland,Benjamin Graham,SOTA improvement,"""for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.""",Fractional Max-Pooling,CIFAR-100,,,,Likely,,,672.00,27000000.00,27M weights in largest CIFAR-100 model,250.00,,,18.0,,NVIDIA GeForce GTX 780,,,,,"Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Fraternal dropout + AWD-LSTM 3-layer (WT2),Language,,2017-10-31,,,,,https://arxiv.org/abs/1711.00066,"Jagiellonian University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal",98500000000000000.00,,"Poland,Canada,Canada","Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",SOTA improvement,"""We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets – Penn Treebank and Wikitext-2""",Fraternal Dropout,WikiText-2,,,,,,,55.00,34000000.00,,520.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,Fraternal dropout + AWD-LSTM 3-layer (WT2),,,,,,,Fraternal dropout + AWD-LSTM 3-layer (WT2),,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Named Entity Recognition model,Language,Named entity recognition,2016-05-29,,,,,https://arxiv.org/abs/1603.01354,Carnegie Mellon University (CMU),96940800000000000.00,"8 hours of training for NER
GeForce GTX TITAN X GPU
0.33 utilization rate
",United States of America,"Xuezhe Ma, Eduard Hovy",Highly cited,,Layer Normalization,CoNLL2003,Table 2,204567,Table 2. 204567 tokens,Confident,,,3100.00,,Architecture in Table 1,50.00,,,8.0,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",NVIDIA GeForce GTX TITAN X,,$0.63,,Academia,"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets or two sequence labeling tasks — Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets — 97.55% accuracy for POS tagging and 91.21% F1 for NER.
",2024-03-11 16:13,Robi Rahman,,,,,,1,,Named Entity Recognition model,,,,Academia,,,,8,Academia,,,
DNCON2,Biology,"Proteins,Protein folding prediction",2018-05-01,license: https://github.com/multicom-toolbox/DNCON2?tab=GPL-3.0-1-ov-file#readme,Open source,Open source,Open source,https://academic.oup.com/bioinformatics/article/34/9/1466/4708303?login=false,University of Missouri,95000000000000000.00,"""Our training was conducted on Tesla K20 Nvidia GPUs each having 5 GB of GPU memory, on which, training one model took around 12 h.""

""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""

Assumptions:
peakFLOP rate 3.52e12FLOP/s (from: https://www.techpowerup.com/gpu-specs/tesla-k20c.c564)
30% utilization rate
1 GPU

Estimate 1: ""training one model took around 12h"" => unclear how many GPUs
(12 *3600) s * 3.52e12 FLOP/s * 0.3 = 4.5e16 FLOP

Estimate 2: ""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""
(1600 epochs * 2 min/epoch * 60 s/min) * 3.52e12 FLOP/s * 0.3 =  2e17 FLOP

Geometric mean: 9.5e16",United States of America,"Badri Adhikari, Jie Hou, Jianlin Cheng",,,DNCON2: improved protein contact prediction using two-level deep convolutional neural networks,PDB (Protein Data Bank),"""We used the original DNCON dataset consisting of 1426 proteins having length between 30 and 300 residues curated before the CASP10 experiment to train and test DNCON2. The protein structures in the dataset were obtained from the Protein Data Bank (PDB)""",1426,"""Our raw feature files for all 1426 training proteins""",Likely,,,173.00,,,1600.00,,,12.0,"""training one model took around 12 h""",,,,,,"Motivation
Significant improvements in the prediction of protein residue–residue contacts are observed in the recent years. These contacts, predicted using a variety of coevolution-based and machine learning methods, are the key contributors to the recent progress in ab initio protein structure prediction, as demonstrated in the recent CASP experiments. Continuing the development of new methods to reliably predict contact maps is essential to further improve ab initio structure prediction.

Results
In this paper we discuss DNCON2, an improved protein contact map predictor based on two-level deep convolutional neural networks. It consists of six convolutional neural networks—the first five predict contacts at 6, 7.5, 8, 8.5 and 10 Å distance thresholds, and the last one uses these five predictions as additional features to predict final contact maps. On the free-modeling datasets in CASP10, 11 and 12 experiments, DNCON2 achieves mean precisions of 35, 50 and 53.4%, respectively, higher than 30.6% by MetaPSICOV on CASP10 dataset, 34% by MetaPSICOV on CASP11 dataset and 46.3% by Raptor-X on CASP12 dataset, when top L/5 long-range contacts are evaluated. We attribute the improved performance of DNCON2 to the inclusion of short- and medium-range contacts into training, two-level approach to prediction, use of the state-of-the-art optimization and activation functions, and a novel deep learning architecture that allows each filter in a convolutional layer to access all the input features of a protein of arbitrary length.",2024-05-21 09:26,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Large regularized LSTM,Language,,2014-09-08,,,,,https://arxiv.org/abs/1409.2329,"New York University (NYU),Google Brain",91000000000000000.00,,"United States of America,United States of America","Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals",Highly cited,,Recurrent Neural Network Regularization,Penn TreeBank,,,,,,,3224.00,66000000.00,,55.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Large regularized LSTM,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
aLSTM(depth-2)+RecurrentPolicy (WT2),Language,,2018-05-22,,,,,https://arxiv.org/abs/1805.08574,"University of Manchester,Alan Turing Institute",75900000000000000.00,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",SOTA improvement,"""Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs""",Breaking the Activation Function Bottleneck through Adaptive Parameterization,,,,,,,,12.00,32000000.00,,190.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,aLSTM(depth-2)+RecurrentPolicy (WT2),,,,,,,aLSTM(depth-2)+RecurrentPolicy (WT2),,,,"Academia,Government",,,,,"Academia,Government",,,
genCNN + dyn eval,Language,,2015-03-17,,,,,https://aclanthology.org/P15-1151/,"Chinese Academy of Sciences,Huawei Noah's Ark Lab,Dublin City University",73000000000000000.00,,"China,China,Ireland","Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu",SOTA improvement,"""genCNN outperforms the state-ofthe-arts with big margins.""",genCNN: A Convolutional Architecture for Word Sequence Prediction,Penn TreeBank,,,,,,,33.00,8000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,genCNN + dyn eval,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
SmooCT,Games,,2014-07-01,,,,,https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7,University College London (UCL),69000000000000000.00,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""

Assume an Intel i7 so 400e9 FLOP/s.
6.9e16 = 400e9*60*60*48",United Kingdom of Great Britain and Northern Ireland,"Johannes Heinrich, David Silver",SOTA improvement,First RL system to achieve superhuman level at Poker Limit Texas Hold Em,Self-Play Monte-Carlo Tree Search in Computer Poker,,,12000000000,"""Each three-player agentwas trained for about 12 billion episodes""

An episode seems to be a round of betting.",,,,16.00,,,,,,48.0,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Pluribus,Games,Poker,2019-07-11,,Unreleased,Unreleased,Unreleased,https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf,Facebook AI Research,66000000000000000.00,"Trained in 8 days on a 64 core CPU
https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/

""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""

Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.

 https://epochai.org/blog/estimating-training-compute
8 days, 64 cores, 5e9 FLOP/s, 30% utilization",United States of America,"Noam Brown, Tuomas Sandholm",SOTA improvement,"first to beat humans at multiplayer poker: ""Developing a superhuman AI for multiplayer poker was the widely,recognized main remaining milestone. In this paper we describe Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas hold’em poker, the most commonly played poker format in the world.""",Superhuman AI for multiplayer poker,,,,,,,,583.00,,,,,,,,,,,,,,2024-05-01 09:06,Robi Rahman,,,,,,,,Pluribus,,,,Industry,,,,,Industry,,,
Attend-Infer-Repeat,Vision,Object recognition,2016-08-12,,,,,https://arxiv.org/abs/1603.08575,Google DeepMind,64488960000000000.00,(peak FLOPs for GPU - 1244 GFLOPs) times (training time - 3600 * 48 second) * (0.3 assumed utilization rate) ,Multinational,"SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton",,,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",MNIST,images of multiple MNIST digits,60000,60000 MNIST images,Likely,,,560.00,82130304.00,from appendix E: The convolutional neural network uses a 64×(5×5)-64×(5×5)-64×(5×5)-512 architecture.,,21000000000.00,"Executed on Nvidia Quadro K4000 GPU (1244 gFLOPs), took 17 milliseconds per image of three digits. 1244*0.017 = 21 gFLOP",48.0,"48 hours for MNIST model, 72 hours for 3D scenes model",NVIDIA Quadro K4000,,,,,"We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects – counting, locating and classifying the elements of a scene – without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We
further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
R-FCN,Vision,Object detection,2016-06-21,,,,,https://arxiv.org/abs/1605.06409,"Tsinghua University,Microsoft Research",61492939793999990.00,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/
9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)
83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)

They used a Nvidia K40 GPU and report training time/image in seconds (table 3)

Assumed a 0.33 util rate","China,United States of America","Jifeng Dai, Y. Li, Kaiming He, and Jian Sun",Highly cited,,R-fcn: Object detection via region-based fully convolutional networks.,"PASCAL VOC 2007,PASCAL VOC 2012,COCO",,94427,,,,,5154.00,,,,,,,,,,$5.51,,Industry,,2024-04-18 07:21,Robi Rahman,,,,,,,,R-FCN,,,,"Academia,Industry",,,,,"Academia,Industry",,,
KN5 LM + RNN 400/10 (WSJ),Speech,Transcription,2010-09-26,,,,,https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html,"Brno University of Technology,Johns Hopkins University",61440000000000000.00,"""Convergence is usually
achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network","Czechia,United States of America","T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur",Highly cited,,Recurrent neural network based language model,WSJ,,6400000,"The training corpus consists of 37M words from NYT section of English Gigaword. As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models (300K sentences) - it takes several weeks to train the most complex models",,,,5665.00,80000000.00,"""- size of vector x is equal to
size of vocabulary V (this can be in practice 30 000 − 200 000)
plus size of context layer. Size of context (hidden) layer s is
usually 30 − 500 hidden units.""

""In further experiments, we denote modified Kneser-Ney
smoothed 5-gram as KN5. Configurations of neural network
LMs, such as RNN 90/2, indicate that the hidden layer size is
90 and threshold for merging words to rare token is 2.""",,160000000.00,Roughly twice the number of parameters,,,,,$2.03,,Academia,,2024-04-04 12:18,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ADAM (CIFAR-10),Vision,Image classification,2014-12-22,,,,,https://arxiv.org/abs/1412.6980,"University of Amsterdam,OpenAI,University of Toronto",60480000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix

less than 0.0007 pfs-days (86400*10^15*0.0007)","Netherlands,United States of America,Canada","DP Kingma, J Ba",Highly cited,,Adam: A Method for Stochastic Optimization,,,,,,,,131973.00,,CIFAR-10 with c64-c64-c128-1000 architecture,,,,,,,,$0.60,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Swift,Robotics,Helicopter driving,2023-08-30,,Unreleased,,,https://www.nature.com/articles/s41586-023-06419-4,Intel Labs,53370000000000000.00,"Policies are trained for a total of 1 × 108 environment interactions, which takes 50 min on a workstation (i9 12900K, RTX 3090, 32 GB RAM DDR5). Fine-tuning is performed for 2 × 107 environment interactions.

35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",Multinational,"Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen Koltun, Davide Scaramuzza ",SOTA improvement,"""Our work marks the first time, to our knowledge, that an autonomous mobile robot achieved world-champion-level performance in a real-world competitive sport.""",Champion-level drone racing using deep reinforcement learning,,,,,Likely,,,72.00,56804.00,"The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.
(31+1)*128+(128+1)*128+(128+1)*4 = 21124

Gate detector is a 6 layer U-net with 
8*(3^3*3+1) + 16*(3^2*8+1) + 16*(3^2*16+1) + 16*(5^2*16+1) + 16*(7^2*16+1) + 16*(7^2*16+1) = 35680

35680 + 21124 = 56804",,,,0.8,"50 minutes (training details, page 8)",NVIDIA GeForce RTX 3090,Reinforcement learning,,,Industry,"First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.",2024-05-01 09:22,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
SPN-4+KN5,Language,,2014-01-01,,,,,https://spn.cs.washington.edu/papers/is14.pdf,"Singapore University of Technology & Design,DSO National Laboratories",44000000000000000.00,,"Singapore,Singapore","W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",SOTA improvement,"""Our empirical comparisons with
six previous language models indicate that our SPN has superior
performance""",Language modeling with sum-product networks,Penn TreeBank,,,,,,,102.00,5000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,SPN-4+KN5,,,,,,,,,,,"Academia,Government",,,,,"Academia,Government",,,
Word2Vec (large),Language,Semantic embedding,2013-10-16,,,,,https://arxiv.org/abs/1310.4546,Google,38880000000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix.

""less than 0.00045 pfs days""
(86400*10^15*0.00045)",United States of America,"T Mikolov, I Sutskever, K Chen, GS Corrado",Highly cited,,Distributed Representations of Words and Phrases and their Compositionality,,,33000000000,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,31189.00,692000000.00,"""To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context.""",,,,,,,,$0.55,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Compress-LSTM (66M),Language,,2019-02-06,,,,,"https://arxiv.org/abs/1902.02380#:~:text=Compression%20of%20Recurrent%20Neural%20Networks%20for%20Efficient%20Language%20Modeling,-Artem%20M.&text=Recurrent%20neural%20networks%20have%20proved,real%2Dtime%20offline%20mobile%20applications.","Samsung R&D Institute Russia,National Research University Higher School of Economics",33100000000000000.00,,"Russia,Russia","Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",,,Compression of Recurrent Neural Networks for Efficient Language Modeling,,,,,,,,37.00,66000000.00,,90.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Compress-LSTM (66M),,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
GPT2+CoreLM+Fine-Tuning,Language,,2021-11-04,,Unreleased,,Unreleased,https://arxiv.org/abs/2111.02687,Aristotle University of Thessaloniki,31700000000000000.00,,Greece,"Nikolaos Stylianou, Ioannis Vlahavas",,,CoreLM: Coreference-aware Language Model Fine-Tuning,,,,,,,,2.00,132000000.00,,10.00,,,,,,,,,,,2024-05-06 15:21,Robi Rahman,GPT2+CoreLM+Fine-Tuning,,,,,,,,,,,Academia,,,,,Academia,,,
LSTM-3-layer+Gadam,Language,,2020-03-02,,Unreleased,,Unreleased,https://arxiv.org/abs/2003.01247,"University of Oxford,University of Bristol,University of Cambridge",26800000000000000.00,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Diego Granziol, Xingchen Wan, Samuel Albanie, Stephen Roberts",,,Iterative Averaging in the Quest for Best Test Error,,,,,,,,5.00,24000000.00,,200.00,,,,,,,,,,,2024-05-16 16:25,Robi Rahman,LSTM-3-layer+Gadam,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
LSTM(large)+Sememe+cell,Language,,2019-10-20,code for WT/PTB. no license. https://github.com/thunlp/SememeRNN/blob/master/LM/main.py ,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1910.08910,"Tsinghua University,Beijing University of Posts and Telecommunications,Huawei Noah's Ark Lab",24000000000000000.00,,"China,China,China","Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",,,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,WikiText-2,,,,,,,19.00,48000000.00,,40.00,,,,,,,,,,"Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at this https URL.",2024-05-21 05:10,Robi Rahman,LSTM(large)+Sememe+cell,,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
RNNLM + Dynamic KL Regularization (WT2),Language,,2018-01-01,,,,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,Northwestern University,21900000000000000.00,,United States of America,"Thanapon Noraset, David Demeter, Doug Downey",,,Controlling Global Statistics in Recurrent Neural Network Text Generation,,,,,,,,6.00,87600000.00,,20.00,,,,,,,,,,"Recurrent neural network language models (RNNLMs) are an essential component for many language generation tasks such as machine translation, summarization, and automated conversation. Often, we would like to subject the text generated by the RNNLM to constraints, in order to overcome systemic errors (e.g. word repetition) or achieve application-specific goals (e.g. more positive sentiment). In this paper, we present a method for training RNNLMs to simultaneously optimize likelihood and follow a given set of statistical constraints on text generation.  The problem is challenging because the statistical constraints are defined over aggregate model behavior, rather than model parameters, meaning that a straightforward parameter regularization approach is insufficient.  We solve this problem using a dynamic regularizer that updates as training proceeds, based on the generative behavior of the RNNLMs.  Our experiments show that the dynamic regularizer outperforms both generic training and a static regularization baseline.  The approach is successful at improving word-level repetition statistics by a factor of four in RNNLMs on a definition modeling task.  It also improves model perplexity when the statistical constraints are $n$-gram statistics taken from a large corpus.",2024-05-21 05:05,Robi Rahman,RNNLM + Dynamic KL Regularization (WT2),,,,,,,,,,,Academia,,,,,Academia,,,
VD-LSTM+REAL Large,Language,,2016-11-04,,,,,https://arxiv.org/abs/1611.01462,"Salesforce Research,Stanford University",21300000000000000.00,,"United States of America,United States of America","Hakan Inan, Khashayar Khosravi, Richard Socher",SOTA improvement,"""Our framework leads to state of the art performance on the Penn Treebank""",Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,Penn TreeBank,,,,,,,397.00,51000000.00,,75.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,VD-LSTM+REAL Large,,,,,,,VD-LSTM+REAL Large,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Transformer-C,Language,,2021-04-08,"permissive license, BSD-3: https://github.com/SimengSun/revisit-nplm",Unreleased,,Open source,https://arxiv.org/abs/2104.03474,University of Massachusetts Amherst,21000000000000000.00,,United States of America,"Simeng Sun, Mohit Iyyer",,,Revisiting Simple Neural Probabilistic Language Models,WikiText-103,,,,,,,10.00,148000000.00,,19.88,,,,,,,,,,,2024-05-08 11:32,Robi Rahman,Transformer-C,,,,,,,,,,,Academia,,,,,Academia,,,
ENAS,Language,,2018-02-09,,,,,https://arxiv.org/abs/1802.03268,"Google Brain,Carnegie Mellon University (CMU),Stanford University",20099999999999996.00,,"United States of America,United States of America,United States of America","Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean",Highly cited,,Efficient Neural Architecture Search via Parameter Sharing,Penn TreeBank,,,,,,,2760.00,24000000.00,,150.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,ENAS,,,,,,,ENAS,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
GPT-2 (fine-tuned with HYDRA),Language,,2021-10-16,,Unreleased,,Unreleased,https://arxiv.org/abs/2110.08633,UC San Diego,19200000000000000.00,,United States of America,"Kabir Nagrecha, Arun Kumar",,,Hydra: A System for Large Multi-Model Deep Learning,WikiText-2,,,,,,,4.00,1540000000.00,,1.00,,,,,,,,,,,2024-05-07 08:29,Robi Rahman,GPT-2 (fine-tuned with HYDRA),,,,,,,,,,,Academia,,,,,Academia,,,
Zoneout + Variational LSTM (WT2),Language,,2016-09-26,,,,,https://arxiv.org/abs/1609.07843,"MetaMind Inc,Salesforce",16800000000000000.00,,"United States of America,United States of America","Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",Highly cited,,Pointer Sentinel Mixture Models,WikiText-2,,,,,,,1829.00,21000000.00,,64.00,,,,,,,,,,,2024-04-01 09:35,Robi Rahman,Zoneout + Variational LSTM (WT2),,,,,,,Zoneout + Variational LSTM (WT2),,,,"Industry,Industry",,,,,"Industry,Industry",,,
rTop-k(distributed setting),Language,,2020-05-21,,Unreleased,,Unreleased,https://arxiv.org/abs/2005.10761,Stanford University,14600000000000000.00,,United States of America,"Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur",,,rTop-k: A Statistical Estimation Approach to Distributed SGD,,,,,,,,46.00,69000000.00,,38.00,,,,,,,,,,,2024-05-10 16:31,Robi Rahman,rTop-k(distributed setting),,,,,,,,,,,Academia,,,,,Academia,,,
RNTN,Language,Sentiment classification,2013-10-01,,,,,https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600,Stanford University,14220000000000000.00,"""The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.""
",United States of America,"R. Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, Christopher Potts",Highly cited,,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,,"""we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences""
Average FP32 FLOPS for 2013: 1.58E+12 (https://epochai.org/blog/estimating-training-compute)
Assuming utilization of 0.5
Compute estimate: 0.5 * 5*60*60 * 1.58e12=1.422e16=14220000000000000
",155063,"""The sentences in the treebank were split into a train (8544), dev (1101) and test splits (2210)""
Training data: 215154*(8544/11855)=155063
",Unverified,,,,,,,,,5.0,The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.,,,,,,"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",2024-05-22 03:57,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
Selfish-RNN (SNT-ASGD) Stacked LSTMs,Language,,2021-01-22,code and weights (stacked LSTM). no clear license https://github.com/Shiweiliuiiiiiii/Selfish-RNN ,Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2101.09048,"Eindhoven University of Technology,University of Twente",14000000000000000.00,,"Netherlands,Netherlands","Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",,"SOTA for sparse networks, but presumably not in general",Selfish Sparse RNN Training,Penn TreeBank,,,,,,,36.00,25200000.00,,100.00,,,,,,,,,,,2024-05-09 16:59,Robi Rahman,Selfish-RNN (SNT-ASGD) Stacked LSTMs,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Densely Connected LSTM + Var. Dropout,Language,,2017-07-19,,,,,https://arxiv.org/abs/1707.06130,Ghent University,12800000000000000.00,,Belgium,"Fréderic Godin, Joni Dambre, Wesley De Neve",,,Improving Language Modeling using Densely Connected Recurrent Neural Networks,,,,,,,,7.00,23000000.00,,100.00,,,,,,,,,,"In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.",2024-05-21 05:07,Robi Rahman,Densely Connected LSTM + Var. Dropout,,,,,,,,,,,Academia,,,,,Academia,,,
DARTS,Language,,2018-06-24,,,,,https://arxiv.org/abs/1806.09055,"DeepMind,Carnegie Mellon University (CMU)",11000000000000000.00,,"United Kingdom of Great Britain and Northern Ireland,United States of America","Hanxiao Liu, Karen Simonyan, Yiming Yang",Highly cited,,DARTS: Differentiable Architecture Search,WikiText-2,,,,,,,3990.00,33000000.00,,300.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,DARTS,,,,,,,DARTS,,,,"Industry,Academia",,,,,"Industry,Academia",,,
EI-REHN-1000D,Language,,2017-08-14,,,,,https://arxiv.org/abs/1708.04116,Korea Advanced Institute of Science and Technology (KAIST),10600000000000000.00,,Korea (Republic of),"Hyunsin Park, Chang D. Yoo",SOTA improvement,"""The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.""",Early Improving Recurrent Elastic Highway Network,,,,,,,,6.00,19000000.00,,100.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,EI-REHN-1000D,,,,,,,EI-REHN-1000D,,,,Academia,,,,,Academia,,,
Neural Architecture Search with base 8 and shared embeddings,Language,,2016-11-05,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/1611.01578,Google Brain,10500000000000000.00,,United States of America,"Barret Zoph, Quoc V. Le",Highly cited,,Neural Architecture Search with Reinforcement Learning,Penn TreeBank,,,,,,,4781.00,54000000.00,,35.00,,,,,,,,,,,2024-04-01 09:02,Robi Rahman,Neural Architecture Search with base 8 and shared embeddings,,,,,,,Neural Architecture Search with base 8 and shared embeddings,,,,Industry,,,,,Industry,,,
R-Transformer,Language,,2019-07-12,"code, no clear license: https://github.com/DSE-MSU/R-transformer/tree/master/language_word ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1907.05572,"Michigan State University,TAL Education Group (Xueersi)",8400000000000000.00,,"United States of America,China","Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang",,,R-Transformer: Recurrent Neural Network Enhanced Transformer,Penn TreeBank,,,,,,,93.00,15800000.00,,100.00,,,,,,,,,,,2024-05-22 16:33,Robi Rahman,R-Transformer,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Pointer Sentinel-LSTM (medium),Language,,2016-09-26,,,,,https://arxiv.org/abs/1609.07843,"MetaMind Inc,Salesforce",7490000000000000.00,,"United States of America,United States of America","Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher","Highly cited,SOTA improvement","""Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM""",Pointer Sentinel Mixture Models,Penn TreeBank,,,,,,,1829.00,21000000.00,,64.00,,,,,,,,,,,2024-04-01 09:35,Robi Rahman,Pointer Sentinel-LSTM (medium),,,,,,,Pointer Sentinel-LSTM (medium),,,,"Industry,Industry",,,,,"Industry,Industry",,,
Dropout (MNIST),Vision,Character recognition,2012-06-03,http://www.cs.toronto.edu/~nitish/dropout,,,Open source,https://arxiv.org/abs/1207.0580,University of Toronto,6039370800000000.00,"Num mul-add / forward pass
2 FLOPs / mult-add
3 total mult-add / fp mult-add
3000 epochs
60000 training samples",Canada,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,Improving neural networks by preventing co-adaptation of feature detectors,MNIST,,60000,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,7171.00,5592010.00,,,11184020.00,"mult-add / fp
* 2 FLOPs / mult-add",,,NVIDIA GeForce GTX 580,,$0.10,,Academia,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2024-05-09 12:47,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
"Variational (untied weights, MC) LSTM (Large)",Language,,2015-12-16,,,,,https://arxiv.org/abs/1512.05287?context=stat,University of Cambridge,5620000000000000.00,,United Kingdom of Great Britain and Northern Ireland,"Yarin Gal, Zoubin Ghahramani","Highly cited,SOTA improvement","""The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity)""",A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,,,,,,,,1838.00,66000000.00,,16.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,"""Variational (untied weights, MC) LSTM (Large)""",,,,,,,"""Variational (untied weights, MC) LSTM (Large)""",,,,Academia,,,,,Academia,,,
Dropout (CIFAR),Vision,Character recognition,2012-06-03,http://www.cs.toronto.edu/~nitish/dropout,,,Open source,https://arxiv.org/abs/1207.0580,University of Toronto,4268700000000000.00,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p17
1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",Canada,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,Improving neural networks by preventing co-adaptation of feature detectors,CIFAR-10,,60000,,,,,7171.00,,,,,,1.5,90 minutes,NVIDIA GeForce GTX 580,,,,Academia,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2024-05-09 12:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
RNN+weight noise+dynamic eval,Language,,2013-08-04,,,,,https://arxiv.org/abs/1308.0850,University of Toronto,4210000000000000.00,,Canada,Alex Graves,Highly cited,,Generating Sequences With Recurrent Neural Networks,IAM Online Handwriting Database (IAM-OnDB),,,,,,,4734.00,54000000.00,,14.00,,,,,,,,,,"This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",2024-05-09 12:08,Robi Rahman,RNN+weight noise+dynamic eval,,,,,,,,,,,Academia,,,,,Academia,,,
MCDNN (MNIST),Vision,Character recognition,2012-02-13,,,,,https://arxiv.org/abs/1202.2745v1,IDSIA,3726979200000000.00,"Num of multiply-adds per forward pass
2 FLOPs/mult-add
3 (fp+bp FLOPs / fp FLOPs)
800 epochs
60.000 training size
35 networks

""Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed""",Switzerland,"D Ciregan, U Meier, J Schmidhuber",Highly cited,,Multi-column Deep Neural Networks for Image Classification,MNIST,,60000,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,4828.00,1994300.00,"""We train five DNN columns per normalization, resulting in a total of 35 columns for the entire MCDNN.
[Each DNN has an architecture] 1x29x29-20C4-MP2-40C5-MP3-150N-10N DNN""",,25881800.00,"Num mult-add per fp per network
35 networks
2 FLOPs/mult-add",,,,,$0.08,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
VD-RHN,Language,,2016-07-12,,,,,https://arxiv.org/abs/1607.03474,"ETH Zurich,IDSIA",3570000000000000.00,,"Switzerland,Switzerland","Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",SOTA improvement,"""On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.""",Recurrent Highway Networks,Penn TreeBank,,,,,,,493.00,32000000.00,,20.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,VD-RHN,,,,,,,VD-RHN,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RNN 500/10 + RT09 LM (NIST RT05),Speech,Transcription,2010-09-26,,,,,https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html,"Brno University of Technology,Johns Hopkins University",3414636000000000.00,"""Convergence is usually achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network","Czechia,United States of America","T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur",Highly cited,,Recurrent neural network based language model,NIST RT05,,5400000,"""Table 4: Comparison of very large back-off LMs and RNN LMs
trained only on limited in-domain data (5.4M words).""",,,,5665.00,5269500.00,"""Size of context (hidden) layer s is usually 30 − 500 hidden units.""

""The acoustic HMMs are based on cross-word tied-states triphones trained discriminatively using MPE criteria. Feature extraction use 13 Mel-PLP’s features with deltas, double and triple deltas reduced by HLDA to 39-dimension feature vector""

10k words vocabulary

(39+500)*500 + 500*10000
",,10539000.00,Roughly twice the number of parameters,,,,,$0.11,,Academia,,2024-04-04 12:18,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ISS,Language,,2017-09-15,,,,,https://arxiv.org/abs/1709.05027,"Duke University,Microsoft",3400000000000000.00,,"United States of America,United States of America","Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li",SOTA improvement,"""Moreover, ISS learning can find a
smaller RHN model with width 726, meanwhile improve the state-of-the-art perplexity as shown by the second entry in Table 2.""",Learning Intrinsic Sparse Structures within Long Short-Term Memory,,,,,,,,146.00,11100000.00,,55.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,ISS,,,,,,,ISS,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Search-Proven Best LSTM,Language,,2015-07-06,,,,,https://proceedings.mlr.press/v37/jozefowicz15.pdf,"Google,New York University (NYU),Facebook",3340000000000000.00,,"United States of America,United States of America,United States of America","R. Józefowicz, Wojciech Zaremba, Ilya Sutskever",Highly cited,,An Empirical Exploration of Recurrent Network Architectures,,,,,,,,2207.00,20000000.00,,30.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Search-Proven Best LSTM,,,,,,,,,,,"Industry,Academia,Industry",,,,,"Industry,Academia,Industry",,,
AWD-LSTM+WT+Cache+IOG (WT2),Language,,2017-09-26,,,,,https://arxiv.org/abs/1709.08907,NTT Communication Science Laboratories,3310000000000000.00,,Japan,"Sho Takase, Jun Suzuki, Masaaki Nagata",SOTA improvement,"""IOG achieves comparable scores to the state-of-the-art on the Penn Treebank
dataset and outperforms the WikiText-2 dataset""",Input-to-Output Gate to Improve RNN Language Models,,,,,,,,7.00,53000000.00,,5.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,AWD-LSTM+WT+Cache+IOG (WT2),,,,,,,AWD-LSTM+WT+Cache+IOG (WT2),,,,Industry,,,,,Industry,,,
RNS-RNN,Language,,2021-09-05,"code here, unclear license:
https://github.com/bdusell/nondeterministic-stack-rnn",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2109.01982,University of Notre Dame,3150000000000000.00,,United States of America,"Brian DuSell, David Chiang",,,Learning Hierarchical Structures with Differentiable Nondeterministic Stacks,Penn TreeBank,,,,,,,10.00,5660000.00,,100.00,,,,,,,,,,,2024-05-07 08:36,Robi Rahman,RNS-RNN,,,,,,,,,,,Academia,,,,,Academia,,,
"GCRN-M1, dropout",Language,,2016-12-22,,,,,https://arxiv.org/abs/1612.07659,Ecole Polytechnique F´ed´erale de Lausanne (EPFL),3040000000000000.00,,Switzerland,"Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, Xavier Bresson",,,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,,,,,,,,674.00,42000000.00,,13.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,"""GCRN-M1, dropout""",,,,,,,,,,,Academia,,,,,Academia,,,
LSTM-Char-Large,Language,,2015-08-26,,,,,https://arxiv.org/abs/1508.06615,"Harvard University,New York University (NYU)",2650000000000000.00,,"United States of America,United States of America","Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush",Highly cited,,Character-Aware Neural Language Models,Penn TreeBank,,,,,,,2033.00,19000000.00,,25.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LSTM-Char-Large,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
DQN,Games,Atari,2013-12-19,,,,,https://arxiv.org/abs/1312.5602,DeepMind,2300000000000000.00,"Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*3*16*8*8 = 1.23M add-multiplies
Second layer: 9*9*16*32*4*4 = 0.66M add-multiplies
Third layer: 9*9*32*256 = 0.66M add-mutliplies
Total ~ 2.55M add-multiplies
2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2.3 PF = 2.7e-5 pfs-days

",United Kingdom of Great Britain and Northern Ireland,"V Mnih, K Kavukcuoglu, D Silver, A Graves",Highly cited,,Playing Atari with Deep Reinforcement Learning,,,,,,,,10232.00,836096.00,"""The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""",,,,,,,,$0.04,,Industry,,2024-04-01 09:45,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Multi-cell LSTM,Language,,2018-11-15,,,,,https://arxiv.org/abs/1811.06477,University of Hyderabad,2009999999999999.75,,India,"Thomas Cherian, Akshay Badola, Vineet Padmanabhan",SOTA improvement,"""The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup""",Multi-cell LSTM Based Neural Language Model,,,,,,,,6.00,7200000.00,,50.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,Multi-cell LSTM,,,,,,,Multi-cell LSTM,,,,Academia,,,,,Academia,,,
Fine-tuned-AWD-LSTM-DOC(fin),Language,,2018-11-12,,,,,https://arxiv.org/abs/1811.04623,Samsung R&D Institute Russia,1920000000000000.00,"From section 4.3, 90 epochs total on ~1.07B tokens.
2 * 23M parameters * 3 * 1.067e9 tokens * (60+30) epochs  = 7.6e19 FLOP",Russia,"Vadim Popov, Mikhail Kudinov",SOTA improvement,"""The novel approach that we propose allows us to reach state-of-theart quality on Penn Treebank: perplexity decreases from 52.4 to 52.1.""",Fine-tuning of Language Models with Discriminator,,,1066666667,"Large scale experiment: 4GB of text * 200M words/GB * (0.75 words/token)^-1 = 1,066,666,667 tokens",Speculative,,,2.00,35000000.00,"Model used for language experiments comes from https://aclanthology.org/D18-1489/ and https://arxiv.org/abs/1711.03953 

See Table 7 (Proposed method) in first link for model used in Penn Treebank experiment, and Table 2 (Ours) in second link for model used on WikiText-2 experiment.

Parameter count for ""large scale experiment"" in section 4.3, a single-layer LSTM with 500 hidden units, seems difficult to accurately estimate. Output dimensionality uses differentiated softmax, varying from 16-150.",15.00,,,,,,,,,,,2024-05-05 19:47,Robi Rahman,Fine-tuned-AWD-LSTM-DOC(fin),,,,,,,Fine-tuned-AWD-LSTM-DOC(fin),,,,Industry,,,,,Industry,,,
2-layer-LSTM+Deep-Gradient-Compression,Language,,2017-12-05,,,,,https://arxiv.org/abs/1712.01887,"Tsinghua University,Stanford University,NVIDIA",1340000000000000.00,,"China,United States of America,United States of America","Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally",Highly cited,,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,,,,,,,,1270.00,6020000.00,,40.00,,,,,,,,,,,2024-03-11 16:13,Robi Rahman,2-layer-LSTM+Deep-Gradient-Compression,,,,,,,2-layer-LSTM+Deep-Gradient-Compression,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
NPLM,Language,Text autocompletion,2003-03-15,,,,,https://dl.acm.org/doi/10.5555/944919.944966,University of Montreal / Université de Montréal,1303898760000000.00,"""For example, consider the following architecture used in the experiments on the AP (Associated
Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order
of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

The first 800,000 words were used for training... reducing the vocabulary size to |V| = 16,383

convergence of the stochastic gradient ascent procedure was obtained after around 10
to 20 epochs for the Brown corpus

NOTE: there are two corpuses. The one represented in this calculation is the Brown one, which got a better improvement over sota",Canada,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin",Highly cited,,A Neural Probabilistic Language Model,Brown corpus,,1000000,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,,,7627.00,11904264.00,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""",,21731646.00,,,,,,,,Academia,,2024-05-22 12:24,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
LSTM+NeuralCache,Language,,2018-09-24,,,,,https://arxiv.org/abs/1809.08826,"KU Leuven,ESAT - PSI,Apple",1020000000000000.00,,"Belgium,Belgium,United States of America","Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq",SOTA improvement,"""We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs"" 
... 

""we observe that neural cache models
consistently outperform regular cache models on this dataset.""",Information-Weighted Neural Cache Language Models for ASR,,,,,,,,3.00,2100000.00,,39.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LSTM+NeuralCache,,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
GPU DBNs,Other,,2009-06-15,,,,,http://www.machinelearning.org/archive/icml2009/papers/218.pdf,Stanford University,1000000000000000.00,https://www.getguesstimate.com/models/19602,United States of America,"R Raina, A Madhavan, AY Ng",Highly cited,,Large-scale Deep Unsupervised Learning using Graphics Processors,,,1000000,"Table 2 shows the running time for processing 1 million
examples for RBMs of varying size",,,,1032.00,100000000.00,"""For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.""",,,,,,,,$0.06,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,GPU DBNs,Academia,,,,,Academia,,,
LBL,Language,,2012-06-27,,,,,https://arxiv.org/abs/1206.6426,University College London (UCL),501999999999999.94,,United Kingdom of Great Britain and Northern Ireland,"Andriy Mnih, Yee Whye Teh",,,A Fast and Simple Algorithm for Training Neural Probabilistic Language Models,,,,,,,,835.00,2000000.00,,45.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LBL,,,,,,,,,,,Academia,,,,,Academia,,,
Image generation,Vision,Image clustering,2013-12-20,,,,,https://arxiv.org/abs/1312.6114,University of Amsterdam,475200000000000.00,"From https://openai.com/blog/ai-and-compute/ Appendix

""less than 0.0000055 pfs-days""
(86400*10^15*0.0000055)",Netherlands,"DP Kingma, M Welling",Highly cited,,Auto-Encoding Variational Bayes,MNIST,,60000,"""We trained generative models of images from the MNIST and Frey Face datasets""

MNIST has 60k images
https://en.wikipedia.org/wiki/MNIST_database

Frey Face has 2k images
https://cs.nyu.edu/~roweis/data.html",,,,21760.00,,,,,,,,,,$0.01,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Feedforward NN,Vision,Digit recognition,2010-05-13,,,,,https://proceedings.mlr.press/v9/glorot10a.html,University of Montreal / Université de Montréal,350000000000000.00,"Roughly two times the number of parameters for ops per forward pass. 

So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",Canada,"X Glorot, Y Bengio",Highly cited,,Understanding the difficulty of training deep feedforward neural networks,MNIST,,,,,,,15848.00,7082000.00,"pg250 of the paper, section 2.3: 
""We optimized feedforward neural networks with one to
five hidden layers, with one thousand hidden units per
layer""

Input is a flattened 32x32 image, which corresponds to an input vector of length 3072

Output is a number from 0-9, so 10 neurons

No. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000
",,14000000.00,Roughly twice the no. of params,,,,,$0.01,,Academia,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
DiffStk-MRNN,Language,,2020-04-04,,Unreleased,,Unreleased,https://arxiv.org/abs/2004.07623,"The Pennsylvania State University,Rochester Institute of Technology",282000000000000.00,,"United States of America,United States of America","Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles",,,Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack,,,,,,,,13.00,1010000.00,,50.00,,,,,,,,,,,2024-05-16 14:45,Robi Rahman,DiffStk-MRNN,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
AFP+FPI (PTB),Language,,2021-06-04,,Unreleased,,Unreleased,https://arxiv.org/abs/2106.02417,University of Sheffield,227000000000000.00,,United Kingdom of Great Britain and Northern Ireland,"Zhengxiong Wang, Anton Ragni",,,Approximate Fixed-Points in Recurrent Neural Networks,Penn TreeBank,,,,,,,2.00,2040000.00,,20.00,,,,,,,,,,,2024-05-07 10:04,Robi Rahman,AFP+FPI (PTB),,,,,,,,,,,Academia,,,,,Academia,,,
bRSM + cache,Language,,2019-12-02,GNU (copyleft) license for code: https://github.com/numenta/nupic.research/tree/master/projects/rsm ,Unreleased,,Open source,https://arxiv.org/abs/1912.01116,"Numenta,Incubator 491",213000000000000.00,,United States of America,"Jeremy Gordon, David Rawlinson, Subutai Ahmad",,,Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling,Penn TreeBank,,,,,,,4.00,2550000.00,,15.00,,,,,,,,,,,2024-05-22 11:27,Robi Rahman,bRSM + cache,,,,,,,,,,,Industry,,,,,Industry,,,
Layer Normalization: Handwriting sequence generation,Language,,2016-07-21,,,,,https://arxiv.org/abs/1607.06450,University of Toronto,189261660000000.00,=8525300*3700000*6=1.8926166e+14,Canada,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",,,Layer Normalization,IAM Online Handwriting Database (IAM-OnDB),,8525300,"12179*700=8525300

There are, in total, 12179 handwriting line sequences. The input
string is typically more than 25 characters and the average handwriting line has a length around 700.",Speculative,,,,3700000.00," The total number of
weights was increased to approximately 3.7M. ",,,,,,,,,,,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",2024-05-09 12:06,Natalia Martemianova,,,RNN+weight noise+dynamic eval,,,,,,,,,Academia,,,,,Academia,,,
life2vec,Medicine,Mortality prediction,2023-06-05,License: Not for public use or distribution,Unreleased,,,https://arxiv.org/abs/2306.03009,"Technical University of Denmark,University of Copenhagen",163905134400000.00,"=6*3252086*8400000=1.639051344 × 10^14

I am not sure about datasize thus Speculative confidence level","Denmark,Denmark","Germans Savcisens, Tina Eliassi-Rad, Lars Kai Hansen, Laust Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, Sune Lehmann",,,Using Sequences of Life-events to Predict Human Lives,"Labour Market Account (AMRUN),National Patient Registry (LPR)","We work with the Labour Market Account (AMRUN) [11] and the National Patient Registry (LPR) datasets [13, 40]. Within the Labour Market Account dataset are event data for every resident of Denmark. For Danish residents who have been in contact with secondary of health care services,
primarily hospitals, the events are accounted in the National Patient Registry. We limit ourselves to data recorded in the period from 2008 until the end of 2015. Datasets are pseudonymized prior to our work by de-identifying addresses, Central Person Register numbers (CPRs), and names. Data is
stored within Statistics Denmark, and all access/use of data is logged.",3252086,The total number of residents in the filtered dataset is 3 252 086.,Speculative,,,,8400000.00,Appendix B: 8.4m,,,,,,,,,,,"Over the past decade, machine learning has revolutionized computers' ability to analyze text through flexible computational models. Due to their structural similarity to written language, transformer-based architectures have also shown promise as tools to make sense of a range of multi-variate sequences from protein-structures, music, electronic health records to weather-forecasts. We can also represent human lives in a way that shares this structural similarity to language. From one perspective, lives are simply sequences of events: People are born, visit the pediatrician, start school, move to a new location, get married, and so on. Here, we exploit this similarity to adapt innovations from natural language processing to examine the evolution and predictability of human lives based on detailed event sequences. We do this by drawing on arguably the most comprehensive registry data in existence, available for an entire nation of more than six million individuals across decades. Our data include information about life-events related to health, education, occupation, income, address, and working hours, recorded with day-to-day resolution. We create embeddings of life-events in a single vector space showing that this embedding space is robust and highly structured. Our models allow us to predict diverse outcomes ranging from early mortality to personality nuances, outperforming state-of-the-art models by a wide margin. Using methods for interpreting deep learning models, we probe the algorithm to understand the factors that enable our predictions. Our framework allows researchers to identify new potential mechanisms that impact life outcomes and associated possibilities for personalized interventions.",2024-05-08 06:27,Natalia Martemianova,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
6-layer MLP (MNIST),Vision,Character recognition,2010-03-01,,,,,https://arxiv.org/abs/1003.0358,"IDSIA,University of Lugano,SUPSI",130788000000000.00,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""

I assume that the number of passes per epoch is 60k, the training set size.","Switzerland,Switzerland,Switzerland","Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber",Highly cited,,Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition,MNIST,,60000,"""MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images""",,,,1264.00,12110000.00,Table 1,,,,,,,,$0.01,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Image Classification with the Fisher Vector: Theory and Practice,Vision,Image classification,2013-06-12,,,,,https://hal.inria.fr/hal-00830491v2/document,"Universidad Nacional de Cordoba,Inteligent Systems Lab Amsterdam,University of Amsterdam,LEAR Team,INRIA,Xerox Research Centre Europe (XRCE)",90842400000000.00,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec 
https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003","Argentina,Netherlands,Netherlands,France,France,France","orge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek",Highly cited,,Image Classification with the Fisher Vector: Theory and Practice,ImageNet,,,,,,,1707.00,,,,,,2.0,,,,$0.00,,,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia,Academia,Industry",,,,,"Academia,Academia,Academia,Academia,Industry",,,
Decision tree (classification),Vision,Face recognition,2001-12-08,,,,,https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf,"Mitsubishi Electric Research Labs,Compaq CRL",63000000000000.00,"
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs","United States of America,United States of America","P. Viola, M. Jones",Highly cited,,Rapid object detection using a boosted cascade of simple features,,They scraped the dataset personally for training,14460,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,,,,23449.00,120000000.00,"From table 1, it looks like the number of weights depends on the dataset size, which in this case is 2*4916 faces+9544 non-faces = 19376, and multiplies that by the number of filters T = 6061, so no. of params = 1.2e8 (Note:I think ""features"" = ""filters"" in this paper)",,67000000.00,"The inference compute depends on the image - because the algorithm works via pass/fail conditions of the decision tree, I think the compute varies a lot (e.g. if the first image fails then little compute is needed). They claim to take about 0.067s to classify an image using a 700MHz Pentium III processor - I'm not sure about how many FLOPs this required but an estimate is 1e9 FLOPs, which works out to 6.7e7 FLOPs for inference",,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
BiLSTM for Speech,Speech,Speech recognition,2005-08-01,,,,,https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206,"IDSIA,Technical University of Munich",24124575958774.88,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,"Switzerland,Germany","A Graves, J Schmidhuber",Highly cited,,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,TIMIT,,36960,"https://catalog.ldc.upenn.edu/LDC93s1
One sample utterance has around 10 words

3696 utterances * 10 words = around 37k words",,,,4281.00,152061.00,"""The hidden layer sizes were chosen to ensure that all networks had roughly the same number of weights W (≈100,000). However, for the MLPs the network grew with the time-window size, and W varied between 22,061 and 152,061.""",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
LSTM,Language,Language modelling,1997-11-15,,,,,https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext,Technical University of Munich,21008000000000.00,"""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN",Germany,Sepp Hochreiter ; Jurgen Schmidhuber,Highly cited,,Long short-term memory,,,1273000,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping
criterion.

This applies to experiment 5 (multiplication)

Sequences have random lengths, on the order of 100-1000 (table 7 )",,,,74586.00,10504.00,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf",,42016.00,"Appendix A.1
""LSTM's update complexity per time is [...] K + 2KH + KC + 2KSC + H I + C I + 4CSI steps [...] where K is the number of output units, C is the number of memory cell blocks, S > 0 is the size of the memory cell blocks, H is the number of hidden units, I is the (maximal) number of units forward-connected to memory cells, gate units and hidden units""

""W = KH + KCS + CSI + 2C is the number of weights""

So the update complexity is roughly twice the number of weights.

The authors take 1 FMA = 1 step, so this is roughly 4*W FLOP",,,,,,,Academia,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
TD-Gammon,Games,Backgammon,1992-05-01,,,,,https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,IBM,18232157622832.70,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,United States of America,G Tesauro,Highly cited,,Practical Issues in Temporal Difference Learning,,,6300000,"""This network was trained
for over 300,000 training games""

Each backgammon game has an avg of around 21 movements
https://www.bkgm.com/rgb/rgb.cgi?view+712",,,,1344.00,25000.00,"""The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.""",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DMPFold,Biology,"Proteins,Protein folding prediction",2018-11-29,license: https://github.com/psipred/DMPfold?tab=GPL-3.0-1-ov-file#readme,Open source,Open source,Open source,https://www.nature.com/articles/s41467-019-11994-0,University College London (UCL),12000000000000.00,"""Training of all models was performed using the Adam optimiser for 75
epochs with default parameters""

""The training set here was based on the same 6729 protein chains, ≤500 residues in length, with non-redundancy at the 25% sequence identity level and no unresolved main chain atoms""

Estimate = 2 * 3.8e6 * 3 * 6729 * 75 ~ 1.2e13 FLOP",United Kingdom of Great Britain and Northern Ireland,"Joe G. Greener, Shaun M. Kandathil and David T. Jones",,,Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints,PSICOV150,"First published in Jones et al. 2012: https://academic.oup.com/bioinformatics/article/28/2/184/198108

""All other aspects of training, including data augmentation procedures were out as
previously described for DMP. The training set here was based on the same 6729 protein chains, ≤500 residues in length""

In DMP publication: (https://academic.oup.com/bioinformatics/article/34/19/3308/4987145?login=false)
""We assessed the mean precision achieved by DeepCov on the now standard PSICOV150 set of proteins and alignments, described in Jones et al. (2012).""",6729,"""The training set here was based on the same 6729 protein chains""",Likely,,,174.00,3800000.00,"Based on Fig. 9:

Distance predictor network: [Total = 17684]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: 2*64*18 = 16384 parameters (a shift and scale parameter for every InstanceNorm2D layer)
(3) Conv2D 1x1: 64*20 + 20 = 1300 parameters
(4) Softmax: 0 parameters

Hydrogen bond predictor network: [Total = 3691073]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: (2*64*2 + 5*5*64*64*2)*18 = 3 691 008 parameters
(3) Conv2D 1x1: 64*1 + 1 = 65 parameters
(4) Sigmoid: 0 parameters

Torsion angles and errors network [Total = 115459]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: 2*64*18 = 16384 parameters (a shift and scale parameter for every InstanceNorm2D layer)
(3) BLSTM: 4×2×(N+M)×M where M is 128 (hidden units in BLSTM layer) and N is 64 (input dimensionality) = 98304
(4) Conv1D 1x1: 256*3 + 3 = 771 parameters

Estimate total parameters = 3.8e6 parameters

[See Section 'Additional constraint types and iterative predictions': ""a bidirectional recurrent LSTM layer with 128 hidden units (BLSTM in Fig. 9c), which embeds each row of the final 2-D 64-channel feature map in a single 256-D vector (concatenation of 128-D final timestep output states of the forward and reverse direction LSTM passes)""]",75.00,,,,,,,,,,"The inapplicability of amino acid covariation methods to small protein families has limited their use for structural annotation of whole genomes. Recently, deep learning has shown promise in allowing accurate residue-residue contact prediction even for shallow sequence alignments. Here we introduce DMPfold, which uses deep learning to predict inter-atomic distance bounds, the main chain hydrogen bond network, and torsion angles, which it uses to build models in an iterative fashion. DMPfold produces more accurate models than two popular methods for a test set of CASP12 domains, and works just as well for transmembrane proteins. Applied to all Pfam domains without known structures, confident models for 25% of these so-called dark families were produced in under a week on a small 200 core cluster. DMPfold provides models for 16% of human proteome UniProt entries without structures, generates accurate models with fewer than 100 sequences in some cases, and is freely available.",2024-04-04 14:40,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
SPIDER2,Biology,"Protein folding prediction,Proteins",2016-10-28,"some kind of download, unclear license

http://zhouyq-lab.szbl.ac.cn/download/",Open access (non-commercial),,,https://link.springer.com/protocol/10.1007/978-1-4939-6406-2_6,"Griffith University,University of Iowa,Dezhou University",8540000000000.00,"120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. Each input takes 17 residues.","Australia,United States of America,China","Yuedong Yang, Rhys Heffernan, Kuldip Paliwal, James Lyons, Abdollah Dehzangi, Alok Sharma, Jihua Wang, Abdul Sattar, and Yaoqi Zhou",SOTA improvement,"The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. ","SPIDER2: A Package to Predict Secondary Structure, Accessible Surface Area, and Main-Chain Torsional Angles by Deep Neural Networks",Unspecified,,,"5,789 nonredundant, high resolution structure",Likely,,,,116112.00,Total Parameters = (459 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 116112,120.00,,,,,,Supervised,,,,"Predicting one-dimensional structure properties has played an important role to improve prediction of protein three-dimensional structures and functions. The most commonly predicted properties are secondary structure and accessible surface area (ASA) representing local and nonlocal structural characteristics, respectively. Secondary structure prediction is further complemented by prediction of continuous main-chain torsional angles. Here we describe a newly developed method SPIDER2 that utilizes three iterations of deep learning neural networks to improve the prediction accuracy of several structural properties simultaneously. For an independent test set of 1199 proteins SPIDER2 achieves 82 % accuracy for secondary structure prediction, 0.76 for the correlation coefficient between predicted and actual solvent accessible surface area, 19° and 30° for mean absolute errors of backbone φ and ψ angles, respectively, and 8° and 32° for mean absolute errors of Cα-based θ and τ angles, respectively. The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. The method is implemented, as a webserver along with a standalone package that are available in our website: http://sparks-lab.org.",2024-05-22 12:48,Anonymous,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
LeNet-5,Vision,Character recognition,1998-11-01,,,,,http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf,AT&T,2810937600000.00,"""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs",United States of America,"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner","Historical significance,Highly cited",,Gradient-based Learning Applied to Document Recognition,MNIST,,60000,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,,,46482.00,60000.00,"""[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing""",,780816.00,,,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
RNN for speech,Speech,Speech synthesis,1998-05-15,,,,,https://ieeexplore.ieee.org/abstract/document/668817,National Chiao Tung University,226690156032.02,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,Taiwan,"SH Chen, SH Hwang, YR Wang",,,An RNN-based prosodic information synthesizer for Mandarin text-to-speech,,,14096,"The data base was divided into two parts: a training set and an open test set. These two sets consisted of 28 191 and 7051 syllables,
respectively.

Of the top 10,000 Chinese words, 15% have 1 syllable, 78% have 2 syllables, and 7% have more than two syllables. Assuming 2 syllables per word, the training set is around 14100 words.",,,,231.00,7512.00,"""The RNN generated a total of eigt output prosodic parameters. [...] The numbers of nodes in the first and second hidden layers were determined empirically and set to be 35 and 30, respectively""

Figure 1 contains an overview of the architecture.

Layer 1: (102 + 35 + 1)*35 parameters
Layer 2: (43 + 35 + 1)*30 parameters
Output layer: (30+8+1)*8 parameters",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Handwritten Digit Recognition System,Vision,Digit recognition,1989-01-01,,,,,https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6,AT&T,181440000000.00,"1.4e6 * 3 * 24 * 60* 60 * 0.5 = 181440000000 = 1.81e11
""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""
Sparcstation 1 has an estimated compute of 1.4MFLOPS (source: https://ieeexplore.ieee.org/document/63671 )",United States of America,"Yann LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, L. Jackel",Historical significance,,Handwritten Digit Recognition with a Back-Propagation Network,,,9840,"""After 30 training passes the error rate on training set (7291 handwritten plus 2549
printed digits)""",Unverified,,,,2578.00,"""In summary, the network has 4635 units, 98442 connections, and 2578 independent parameters.“",30.00,,,72.0,"""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""",,,,,,"We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.",2024-05-15 06:30,Lovis Heindrich,,,,,,,,,,,,Industry,,,,,Industry,,,
Zip CNN,Vision,Character recognition,1989-12-01,,,,,https://ieeexplore.ieee.org/document/6795724,"AT&T,Bell Laboratories",43372117520.00,"Its a deep CNN so we assume a backward-forward ratio of 2:1

""The network was trained for 23
passes through the training set (167,693 pattern presentations).""","United States of America,United States of America",Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel,Highly cited,,Backpropagation applied to handwritten zip code recognition,Buffalo zips,"""The data base used to train and test the network consists of 9298 segmented numerals digitized from handwritten zip codes
that appeared on U.S. mail passing through the Buffalo, NY post office.
Examples of such images are shown in Figure 1. The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance. One important feature of this data base is that
both the training set and the testing set contain numerous examples that
are ambiguous, unclassifiable, or even misclassified. """,7291,"The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance",,,,9952.00,9760.00,"""In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters""",,129320.00,Roughly twice the number of connections,,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
NetTalk (transcription),Speech,Speech synthesis,1987-06-06,,,,,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,Princeton University,28328002560.00,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,United States of America,"TJ Sejnowski, CR Rosenberg",Highly cited,,Parallel Networks that Learn to Pronounce English Text,,,1024,"We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",,,,2558.00,18629.00,"""The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)""",55.00,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
NetTalk (dictionary),Speech,Speech synthesis,1987-06-06,,,,,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,Princeton University,27664065000.00,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word,United States of America,"TJ Sejnowski, CR Rosenberg",Highly cited,,Parallel Networks that Learn to Pronounce English Text,,,1000,"""A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus""",,,,2558.00,18629.00,"""The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)""",55.00,,,,,,,,,,,2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Invariant image recognition,Vision,Representation learning,1989-06-18,,,,,https://ieeexplore.ieee.org/document/118669,Complutense University of Madrid,27000000000.00,"0.5*6*60*60*2.5e6 = 27000000000 = 2.7e10
Trained for 6h on a SUN-4 (section 4)
Assumed utilization of 0.5
SUN-4 is estimated at 2.5e6 FLOP/s (Nordhaus, 2007)",Spain,"V. Cruz, G. Cristóbal, T. Michaux, S. Barquin",Historical significance,,Invariant image recognition using a multi-network neural model,,,,,Confident,,,,,,,,,6.0,Section 4,,,,,,"A new model which permits visual patterns to be invariant to affine transforms (translations, rotations, and dimensions) is presented. A training multilayer fully connected network of ADALINE neurons is proposed as a preprocessing step for invariant image extraction. A second neural network has been trained by the popular backpropagation algorithm for recovering the real image without distortions. First, the sample invariants are obtained by the preprocessing network. In the second step, the general invariant that includes all the sample invariants is computed. Afterward, the reordered sample invariants are input to a multilayer neural network trained by the backpropagation algorithm. The original image, without distortions, is obtained in the output of this system. Several test images have been computed, and evaluation of the results shows that in the case of images with intrinsic perceptual similarity, the learning procedure leads to a global invariant extraction that requires less computational effort in comparison with an arbitrary training selection. After the training process, this system is able to extract the generalized invariant image from an arbitrary picture recovering the input image without distortions.<<ETX>>",2024-05-22 12:11,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,,,
System 11,Vision,Face detection,1996-06-18,,,,,https://ieeexplore.ieee.org/document/655647,Carnegie Mellon University (CMU),12930000000.00,"Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.

Number of passes (Section 2.1):
* ""Nearly 1,050 face examples were gathered from face databases [...]""
* ""Fifteen face examples are generated for the training set from each original image""

Training loop:
1. ""initial set of nonface images by generating 1,000 random images""
2. Train (presumably on whole set)
3. Run + collect false positives
4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""

""A typical training run selects approximately 8,000 nonface images ""

Selecting 8,000 nonface images implies 8000/250 = 32 loops.

Assuming compute is 3 * N * D, we have
* Loop 1: D = 15*1050 + 1000
* Loop 2: D = 15*1050 + 1000 + 250
* So on.

Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.

Hence compute = 3 * 6452 * 668e3 = 1.3e10.",United States of America,"HA Rowley, S Baluja, T Kanade",Highly cited,,Neural Network-Based Face Detection,,,9050,"""A typical training
run selects approximately 8000 non-face images from the
146,212,178 subimages that are available at all locations
and scales in the training scenery images.""

""Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from each
original image [...]""

""Create an initial set of non-face images by generating
1000 images with random pixel intensities""",,,,6011.00,6452.00,"System 11 is a combination of Network 1 and Network 2

Network 1 has 2095 connections and network 2 has 4357 connections (see table 1)",,12904.00,The connections are linear so roughly twice the number of parameters,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
ALVINN,Driving,,1989-12-01,,,,,https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,Carnegie Mellon University (CMU),10548576000.00,"2 * 36627 * 3 * 40 * 1200 = 10548576000 = 1.05e10
36627 parameters
""After 40 epochs of training on the 1200 simulated road snapshots""",United States of America,DA Pomerleau,Highly cited,,ALVINN: an autonomous land vehicle in a neural network,Road snapshots,,1200,"""Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels""",,,,1863.00,36627.00,"1217*29 + 29*46 =36627
“Each of these 1217 input units is fully connected to the hidden layer of 29 units, which is in turn fully connected to the output layer. The output layer consists of 46 units, divided into two groups.”",40.00,,,,,,,,,Academia,,2024-05-22 08:32,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Speaker-independent vowel classification,Speech,Speech recognition,1989-01-01,,,,,https://www.semanticscholar.org/paper/Performance-Comparisons-Between-Backpropagation-and-Atlas-Cole/e42d2b89fcb4a1a3dfa63408f424f76975ed1e1b,University of Washington,7485696000.00,"2*3040*3*410400=7485696000=7.49e9
“The network was trained on 100 iterations through the 4104 training vectors.”",United States of America,"L. Atlas, R. Cole, J. Connor, M. El-Sharkawi, R. Marks, Y. Muthusamy, E. Barnard",Historical significance,,Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications,,,4104,,Unverified,,,,3040.00,"“The MLP consisted of 64 inputs (the DFf coefficients. each nonnalized between zero and one), a single hidden layer of 40 units, and 12 output units;”",100.00,,,,,,,,,,"Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees.",2024-05-15 06:17,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
,Materials science,Pattern classification,1994-01-01,,,,,https://www.sciencedirect.com/science/article/abs/pii/S0921883108605506,Sapienza Università di Roma,4531200000.00,"Compute estimate: 2*1888*3*400000=4531200000=4.53e9
Training steps: ""In Fig. 6 we report the classification results obtained on the testing set in the 12 and 16 component compressed data after 400000 training iterations""",Italy,"G. Bonifazi, P. Burrascano",Historical significance,,Ceramic powder characterization by multilayer perceptron (MLP) data compression and classification,,,80,"After the pre-processing phase, a training set of 80 patterns and a testing set of 64 patterns were available.",Unverified,,,,1888.00,"Parameters: 100*16 + 16*16 + 16*2 = 1888
Architecture: ""The topology of the classifier was X-Y-Y-2, where X is the number of input components, Y is the number of neurons in each hidden layer and the number of neurons in the output layer is two, which is the number of classes. The two hidden layers were considered to have the same number of nodes for simplification purposes. ""
Input size: ""Each pattern consists of a 10 x 10 pixel sub-image.""
Hidden size: ""Experiments have been made on networks with 6, 9, 12 and 16 hidden nodes. """,5000.00,,,,,,,,,,"A neural network approach for pattern classification has been explored in the present paper as part of the recent resurgence of interest in this area. Our research has focused on how a multilayer feedforward structure performs in the particular problem of particle characterization. The proposed procedure, after suitable data preprocessing, consists of two distinct phases: in the former, a feedforward neural network is used to obtain an image data compression. In the latter, a neural classifier is trained on the compressed data. All the tests have been conducted on a sample constituted by two different typologies of ceramic particles, each characterized by a different microstructure. The sample image of different particles acquired and directly digitalized by scanning electron microscopy has been processed in order to achieve the best conditions to obtain the boundary profile of each particle. The boundary is thus assumed to be representative of the morphological characteristics of the ceramic products. Using the neural approach, a classification accuracy as high as 100% on a training set of 80 sub-images was achieved. These networks correctly classified up to 96.9% of 64 testing patterns not contained in the training set.",2024-05-22 12:58,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
Fuzzy NN,Speech,Speech recognition,1992-09-01,,,,,https://ieeexplore.ieee.org/document/159058,Indian Statistical Institute,1403117760.00,1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,India,"SK Pal, S Mitra",Highly cited,,"Multilayer perceptron, fuzzy sets, and classification",,,436,"""The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds"" and 50% of the dataset was used. 871*0.5 ~= 436",,,,1223.00,1166.00,"Table II: ""he neural network has three hidden layers, with m hidden nodes in each layer"", m = 20, input dim. = 9, output dim. = 6",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Perceptron (1960),Vision,Image classification,1960-03-30,,,,,https://www.semanticscholar.org/paper/Perceptron-Simulation-Experiments-Rosenblatt/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67,Cornell Aeronautical Laboratory,720000000.00,"4000 * 12000 * 15
from the text ""This program uses the IBM 704 computer to simulate per-
ceptual learning, recognition, and spontaneous classification of visual
stimuli in the perceptron,""
from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.
"" For the first system, the computing time averaged about 15 seconds per stimulus cycle, ""
In Fig 10 we see up to 4000 stimuli",United States of America,Frank Rosenblatt,Historical significance,,Perceptron Simulation Experiments,,,5000,"from the text ""The two main simulation programs total about 5000 words each.""",Speculative,,,394.00,1000.00,""" The first program was designed to handle
up to 1000 A units, and a 72 by 72 sensory mosaic. It
was found that this large sensory system presented
stimuli with a fineness of grain considerably better than
the limits of discrimination of a thousand-unit percep-
tron, and at the same time, required an excessive
amount of time for stimulus transformations, since each
illuminated point in the stimulus must be transformed
individually into its image point.""",,,,,,,,,,,"An experimental simulation program, which has been in progress at the Cornell Aeronautical Laboratory since 1957, is described. This program uses the IBM 704 computer to simulate perceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron, a theoretical brain model which has been described elsewhere. The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study ""forced"" and ""spontaneous"" learning of pattern discriminations.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Pandemonium (morse),Language,Morse translation,1959-02-01,,,,,https://aitopics.org/doc/classics:504E1BAC/,Massachusetts Institute of Technology (MIT),600000000.00,"The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",United States of America,OG Selfridge,Highly cited,,Pandemonium: A Paradigm for Learning,,,,??? Might need to make a guesstimate here.,Speculative,,,1453.00,,"The paper mentions 11 function types. Unclear how many times they are called (number of ""demons"" in their Pandemonium implementation).",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Samuel Neural Checkers,Games,Checkers,1959-07-01,,,,,https://ieeexplore.ieee.org/abstract/document/5392560,IBM,428400000.00,"""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""

""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""

""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""

source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html

""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""

""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""",United States of America,Arthur L. Samuel,Highly cited,,Some studies in machine learning using the game of checkers,,,53000,"Based on number of board positions

At the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much larger
number of positions by means of the culling techniques
described. While this is still far from the number which
would tax the listing and searching procedures used in
the program, rough estimates, based on the frequency
with which the saved boards are utilized during normal
play (these figures being tabulated automatically), indicate that a library tape containing at least 20 times the
present number of board positions would be needed to
improve the midgame play significantly. At the present
rate of acquisition of new positions this would require
an inordinate amount of play and, consequently, of
machine time.",,,,4509.00,16.00,"""with 16 terms for generalization learning""

""Mention has been made several times of the procedure
for replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different
terms (in addition to the piece-advantage term), 16 of
these being included in the scoring polynomial at anyone
time and the remaining 22 being kept in reserve.""",,,,,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ASE+ACE,Robotics,Pole balancing,1983-09-01,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077,University of Massachusetts Amherst,324000000.00,"324 * 2 * 500000 = 324000000 = 3.24e8. The calculation assumes ""compute per forward pass"" = ""number of parameters"" = ""compute per backward pass"". Their model only has a single layer and is trained with simple update rules instead of gradient descent. Training details are described in Section IX.",United States of America,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson","Highly cited,Historical significance",,Neuronlike adaptive elements that can solve difficult learning control problems,,,500000,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",Unverified,,,4296.00,324.00,"The system consists of two parts: ACE and ASE, each with 162 weights (=324 parameters). Found in Figures 2 and 3.",,,,2.8,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",,,,,Academia,"It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.",2024-05-09 02:59,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
,Speech,Speech recognition,1988-08-01,,,,,https://aaai.org/papers/00734-aaai88-130-data-driven-execution-of-multi-layered-networks-for-automatic-speech-recognition/,McGill University,296425000.00,"“For an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650”, “Learning and recognition were performed on a VAX 8650.”
Dataset: 70*10*2=1400 (Train) 10*10*2=200 (Test)
“The ten words of the El set were pronounced twice by 80 speakers (40 males and 40 females)”
“The data from 70 speakers were used as a training set while the data from the remaining 10 speakers (6 males and 4 females) were used for the test”
VAX 8650 FLOPS  = 1.67E+06 (Nordhaus)
Training time: 317ms * 1400  * 0.8 = 355040ms = 355s
Estimate: 0.5 * 1.67e6 * 355 = 296425000 = 2.96e8",Canada,"Renato De Mori, Yoshua Bengio, Régis Cardin",Historical significance,,Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition,,,1400,,Unverified,,,,10000.00,"“For an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650”",,,,0.1,,,,,,,"A set of Multi-Layered Networks (MLN) for Automatic Speech Recognition (ASR) is proposed. Such a set allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small in order to allow significant generalization during learning with a reasonable training set size. Subsets of networks can be executed depending on preconditions based on descriptions of the time evolution of signal energies allowing spectral properties that are significant in different acoustic situations to be learned.
Preliminary experiments on speaker-independent recognition of the letters of the E-set are reported. Voices from 70 speakers were used for learning. Voices of 10 new speakers were used for test. An overall error rate of 9.5% was obtained in the test showing that results better than those previously reported can be achieved.",2024-05-15 06:41,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
Neocognitron,Vision,Character recognition,1980-04-01,,,,,https://link.springer.com/article/10.1007/BF00344251,NHK Broadcasting Science Research Laboratories,228115200.00,"""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds
",Japan,"K Fukushima, S Miyake",Highly cited,,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,,,5,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",,,,5782.00,1140576.00,"""The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Back-propagation,Mathematics,Triplet completion,1986-10-01,,,,,https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769,"UC San Diego,Carnegie Mellon University (CMU)",124416000.00,"We assume that the number of mult-adds per pass is equal to the number of parameters.

""We trained the network for 1500 sweeps""

There are 12*12 possible pairs of people, so we assume that is the dataset size","United States of America,United States of America","Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",Highly cited,,Learning representations by back-propagating errors,,,144,"There are 12*12 possible pairs of people, so we assume that is the dataset size",,,,25301.00,144.00,Figure 4 includes a representation of the weights learned by the people to relationship network,,288.00,We assume that the number of mult-adds is equal to the number of parameters.,,,,Unsupervised,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Innervator,Mathematics,Pattern classification,1989-01-01,,,,,https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms,"Stanford University,California Institute of Technology",120000000.00,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,"United States of America,United States of America","Geoffrey Miller, Peter Todd, and Shailesh Hegde",Highly cited,,Designing neural networks using genetic algorithms,,,4,,,,,1132.00,10.00,Each net has 5 units,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
LTE speaker verification system,Speech,Speech recognition,1966-11-01,,,,,https://pubs.aip.org/asa/jasa/article-abstract/40/5/966/754180/Experimental-Studies-in-Speaker-Verification-Using?redirectedFrom=fulltext,IBM,105917060.00,"1st and 2nd level system are trained separately, multiple versions of both are trained, I chose the largest clearly described training runs.

1st level LTE compute: 2*1810*28700=103894000=1.04e8
1st level steps: 28700 (""Only 287 samples were selected to train the 10 LTE's. The same algorithm was used as that used with the 100-class gain. Two LTE's
converged before 100 training passes."")

2nd level LTE compute: 2*251*4030=2023060=2e6
2nd level steps: 4030 (31 epochs, 130 training examples, see Table 3)

Total compute: 103894000+2023060=105917060=1.06e8 (assuming no backward pass since they didn't use backpropagation)
",United States of America,K. P. Li; J. E. Dammann; W. D. Chapman,Historical significance,,"Experimental Studies in Speaker Verification, Using an Adaptive System",,,417,"Split between both systems, 287 for 1st level, 130 for 2nd level.",Unverified,,,,2061.00,"2 connected systems, 1st level LTE and 2nd level LTE.
1st Level: 1810 parameters (""Thus, every 20 msec after the beginning of the utterance, the 15 filter amplitudes were each represented by a 12-bit code, resulting in a 180-bit time sample of the spectrum for that interval. Each time sample was fed to the first-level LTE's, which reduced it to a 10-bit code"")
2nd Level: 251 parameters (""This resulted in a 250-bit input pattern to the second level for the first half-second of each utterance. Each 250-bit pattern was then classified by the LTE into one of two classes"")",131.00,,,,,,,,,,"This paper describes an investigation of the capability of a two‐level adaptive linear threshold element (LTE) system to perform speaker discriminations. The study also includes an investigation of discriminating a speaker from an unknown population. The problem has been confined to the verification of an utterance as that of an expected informant. The environment of the experiments is discussed, and the experimental system is described. At the first level LTE, four different kinds of training have been developed for effective transformation and data reduction. At the second‐level LTE, different training conditions and different decision processes are investigated and evaluated. Over 90% accuracy is obtained in separating a known speaker from impostors.",2024-05-22 13:02,Lovis Heindrich,,,,,,,,,,,,Industry,,,,,Industry,,,
Cancer drug mechanism prediction,Medicine,Drug discovery,1992-10-16,,,,,https://pubmed.ncbi.nlm.nih.gov/1411538/,National Cancer Institute,53460000.00,"2*594*3*15000=53460000=5.35e7
“The extent of training was 15,000 presentations“",United States of America,"John N. Weinstein, Kurt W. Kohn, Michael R. Grever, Vellarkad N.
Viswanadhan, Lawrence V. Rubinstein, Anne P. Monks, Dominic A. Scudiero, Lester
Welch, Antonis D. Koutsoukos, August J. Chiausa, Kenneth D. Paull",Historical significance,,Neural computing in cancer drug development: predicting mechanism of action,,,,,Unverified,,,,594.00,"“The network shown has 60 input PEs, one for each cell line, and 6 output PEs“
“Neural networks with three to nine hidden layer PEs used”
9*60 + 6*9 = 594",,,,,,,,,,,"Described here are neural networks capable of predicting a drug's mechanism of action from its pattern of activity against a panel of 60 malignant cell lines in the National Cancer Institute's drug screening program. Given six possible classes of mechanism, the network misses the correct category for only 12 out of 141 agents (8.5 percent), whereas linear discriminant analysis, a standard statistical technique, misses 20 out of 141 (14.2 percent). The success of the neural net indicates several things. (i) The cell line response patterns are rich in information about mechanism. (ii) Appropriately designed neural networks can make effective use of that information. (iii) Trained networks can be used to classify prospectively the more than 10,000 agents per year tested by the screening program. Related networks, in combination with classical statistical tools, will help in a variety of ways to move new anticancer agents through the pipeline from in vitro studies to clinical application.",2024-05-22 12:55,Lovis Heindrich,,,,,,,,,,,,Government,,,,,Government,,,
Print Recognition Logic,Vision,Character recognition,1963-01-01,,,,,https://ieeexplore.ieee.org/document/5392331,IBM,22500000.00,"0.5*2.5*60*60*5000 = 22500000 = 2.25e7
Assumed utilization of 0.5
Trained for 2-3h on an IBM 7090 (from Introduction)
Estimated IBM 7090 at 5000 FLOP/s based on multiplications per second (Nordhaus, 2007)
Note: the Nordhaus estimate is very different from Wikipedia's estimate of 100000 FLOP/s, which cites a PowerPoint as source.",United States of America,"L. Kamentsky, Chao-Ning Liu",Historical significance,,Computer-Automated Design of Multifont Print Recognition Logic,,,,,Speculative,,,,,,,,,2.5,2-3h (from Introduction),,,,,,"A computer program has been written to design character recognition logic based on the processing of data samples. This program consists of two subroutines: (1) to search for logic circuits having certain constraints on hardware design, and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. An executive routine is used to apply these subroutines to select a complete logic with a given performance and complexity. This logic consists of 39 to 96 and gates connected to a shift register and a table look-up or resistance network comparison system.",2024-05-09 13:21,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Cognitron,Other,,1975-09-01,,,,,https://link.springer.com/article/10.1007%2FBF00342633,Biological Cybernetics,5760000.00,"Backward to forward ratio: 1 to 1 as weight updates are calculated from local activation patterns instead of gradient descent.
Total compute estimate: 100*2*4*288*25 = 5760000 = 5.76e6
",Japan,Kunihiko Fukushima,Historical significance,Precursor of the Neocognitron,Cognitron: a self-organizing multilayered neural network,,,5,5 examples presented for (at least) 20 cycles = 100 training steps,,,,791.00,28800.00,"4 layers, 288 neurons per layer, weights connect each neuron to only 25 neurons in the previous layer = 4*288*25 parameters",20.00,,,,,,,,,Industry,,2024-05-09 03:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Credibilty Network,Vision,"Character recognition,Image classification",1999-07-01,,Unreleased,,Unreleased,https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf,"University College London (UCL),University of Toronto",5443200.00,=6*324*2800,"United Kingdom of Great Britain and Northern Ireland,Canada","Geoffrey E. Hinton, Zoubin Ghahramani, Vee Whye Tah",,,Learning to Parse Images,CEDAR CDROM-1,"700*4 classes = 2800 
The data used is a set of 4400 images of single digits from the classes 2, 3, 4 and 5 derived from the CEDAR CDROM 1 database [17]. Each image has size 16x16. The size of the credibility network is 256-64-4. The 64 middle layer units are meant to encode low level features, while each of the 4 top level units are meant to encode a digit class. We used 700 images of single digits from each class· to train the network. ",2800,,Speculative,,,,324.00,"The size of the credibility network is 256-64-4. The 64 middle layer units are meant to encode low level features, while each of the 4 top level units are meant to encode a digit class",,,,,,,,,,,"We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained. ",2024-05-01 01:08,Natalia Martemianova,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Linear Decision Functions,Mathematics,Binary classification,1962-06-01,,,,,https://ieeexplore.ieee.org/document/4066882?denied=,Bell Laboratories,1559250.00,"0.5*45*35*1980 = 1559250 = 1.56e6
Trained using IBM punched cards, computation took 45 * 35s for all 10 digits (Section Estimating the Linear Decision Function).
Multiplications per second estimate based on publication year: 1.98e3 (regression on Nordhaus data).
Assumed utilization of 0.5",United States of America,W. Highleyman,"Historical significance,Highly cited",,"Linear Decision Functions, with Application to Pattern Recognition",,,500,"""Fifty different people were asked, resulting in a sample size of 50 for each of the ten pattern classes. """,Speculative,,,,,,,,,0.4,"""Forty-five hyperplanes are required in the complete linear decision function""
""About 35 seconds, on the average, was required to determine a hyperplane, given an initial position.""",,,,,,"Many pattern recognition machines may be considered to consist of two principal parts, a receptor and a categorizer. The receptor makes certain measurements on the unknown pattern to be recognized; the categorizer determines from these measurements the particular allowable pattern class to which the unknown pattern belongs. This paper is concerned with the study of a particular class of categorizers, the linear decision function. The optimum linear decision function is the best linear approximation to the optimum decision function in the following sense: 1) ""Optimum"" is taken to mean minimum loss (which includes minimum error systems). 2) ""Linear"" is taken to mean that each pair of pattern classes is separated by one and only one hyperplane in the measurement space. This class of categorizers is of practical interest for two reasons: 1) It can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available. 2) Its implementation is quite simple and inexpensive. Various properties of linear decision functions are discussed. One such property is that a linear decision function is guaranteed to perform at least as well as a minimum distance categorizer. Procedures are then developed for the estimation (or design) of the optimum linear decision function based upon an appropriate sampling from the pattern classes to be categorized.",2024-05-20 10:06,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Heuristic Reinforcement Learning,Robotics,,1965-10-01,,,,,https://ieeexplore.ieee.org/document/1098193,Purdue University,1080000.00,"Figure 10 shows their largest system is trained for 3h and was trained on an analog IBM 1620 that was simulated on a digital IBM 1710.
Nordhaus, 2007 lists the IBM 1620 at 200 multiplications per second and doesn’t contain the 1710
Flops estimate: 0.5 * 3 * 60 * 60 * 200 = 1080000 = 1.08e6
Assumed utilization of 0.5
",United States of America,"M. Waltz, K. Fu","Historical significance,Highly cited",,A heuristic approach to reinforcement learning control systems,,,,,Speculative,,,,,,,,,3.0,Figure 10,,,,,,This paper describes a learning control system using a reinforcement technique. The controller is capable of controlling a plant that may be nonlinear and nonstationary. The only a priori information required by the controller is the order of the plant. The approach is to design a controller which partitions the control measurement space into sets called control situations and then learns the best control choice for each control situation. The control measurements are those indicating the state of the plant and environment. The learning is accomplished by reinforcement of the probability of choosing a particular control choice for a given control situation. The system was stimulated on an IBM 1710-GEDA hybrid computer facility. Experimental results obtained from the simulation are presented.,2024-05-09 13:19,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,,,
Perceptron Mark I,Other,Binary classification,1957-01-01,,,,,https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf,"Cornell Aeronautical Laboratory,Cornell University",694894.94,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,"United States of America,United States of America",F Rosenblatt,"Historical significance,Highly cited",First modern neural network ,The Perceptron—a perceiving and recognizing automaton,,,6,Appendix II describes an experiment with 6 stimulus patterns,,,,1610.00,1000.00,"""Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.""

source: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning

The Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ADALINE,Vision,Pattern recognition,1960-06-30,,,,,https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf,Stanford University,9900.00,"""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size",United States of America,Widrow and Hoff,Highly cited,,Adaptive switching circuits,,,100,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""

https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",,,,6329.00,17.00,"""The machine's total experience is stored in the values of the weights a0,...,a16""",,33.00,We have 16 weights and a bias parameter. So 16 multadds and an add. The result is then thresholded to produce a binary output.,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Theseus,Robotics,Maze solving,1950-07-02,,,,,https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/,Bell Laboratories,40.00,"The ""training"" consists on the mouse running around and checking each wall.",United States of America,Claude Shannon,Historical significance,,Mighty Mouse,,,40,Each wall Theseus bumps into is a datapoint,,,,0.00,40.00,"The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ResNet-50 Billion-scale,Vision,Image classification,2019-05-02,non-commercial for weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models,Open access (non-commercial),Unreleased,Unreleased,https://arxiv.org/abs/1905.00546,Facebook AI,,,United States of America,"I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, Dhruv Mahajan",Highly cited,,Billion-scale semi-supervised learning for image classification,YFCC-100M,"""The following web-scale datasets are used for
semi-supervised learning experiments involving an unlabeled dataset U.
• YFCC-100M [38] is a publicly available dataset of about
90 million images from Flickr website with associated
tags. After removing duplicates, we use this data for
most experiments and ablations.
• IG-1B-Targeted: Following [27], we collected a dataset
of 1B public images with associated hashtags from a
social media website. We consider images tagged with
at least one of the 1500 hashtags associated with one of
the 1000 ImageNet-1k classes.""",1090000000,"1 billion + 90 million, per above",,,,411.00,25000000.00,25M parameters vanilla ResNet50,,,,,,,,,,Industry,,2024-05-01 09:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
WGAN-GP,Image generation,Image generation,2017-03-31,,,,,https://arxiv.org/abs/1704.00028,"Courant Institute of Mathematical Sciences,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",,,"United States of America,Canada","Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",Highly cited,,Improved Training of Wasserstein GANs,,,,,Unknown,,,8201.00,,,,,,,,,,,,Academia,,2024-05-22 13:19,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
BellKor 2009,Recommendation,Movie ratings,2009-08-01,,,,,https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf,AT&T,,,United States of America,Y Koren,Historical significance,Introduced new algorithms; won Netflix Grand Prize,The BellKor Solution to the Netflix Grand Prize,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,509.00,,,,,,,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Refined Part Pooling,Vision,Person retrieval,2018-01-09,,,,,https://arxiv.org/abs/1711.09349,"Tsinghua University,University of Technology Sydney,University of Texas at San Antonio",,,"China,Australia,United States of America","Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang",Highly cited,,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),,,,,Unknown,,,1887.00,,,,,,,,,,,,Academia,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
ShuffleNet v1,Vision,"Object detection,Image classification",2017-07-03,,,,,https://arxiv.org/abs/1707.01083,Megvii Inc,,,China,"X Zhang, X Zhou, M Lin, J Sun",Highly cited,,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,,,,,,,,5460.00,2430000.00,,,140000000.00,"Table 4 (ShuffleNet 1x, g=8)

https://arxiv.org/abs/1707.01083",,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Multiresolution CNN,Video,Video classification,2014-06-23,,,,,https://ieeexplore.ieee.org/document/6909619,"Google,Stanford University",,,"United States of America,United States of America","A Karpathy, G Toderici, S Shetty, T Leung",Highly cited,,Large-Scale Video Classification with Convolutional Neural Networks,,,50000000,"""We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.""

So 5e+7 datapoints and 10 epochs.",,,,6040.00,126125568.00,"""Using shorthand notation, the full [single frame] architecture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f ×f, applied to the input with stride s""

Two such single-frame architectures are concatenated as shown in figure 2

""Since the input is only of half the
spatial size as the full-frame models, we take out the last
pooling layer to ensure that both streams still terminate in a
layer of size 7×7×256. ""

We assume the input are T=10 frames with C=3 color channels each

2*(256*(10*3*5*5+1) + 384*(256*3*3+1) + 384*(384*3*3+1) + 256*(384*3*3+1)) + (2*7*7*256 + 1)*4096 + (4096+1)*4096



",,,,,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
NeuMF (Pinterest),Recommendation,Collaborative filtering,2017-08-16,,,,,https://arxiv.org/abs/1708.05031,"Shandong University,Texas A&M,National University of Singapore,Columbia University",,,"China,United States of America,Singapore,United States of America","X He, L Liao, H Zhang, L Nie, X Hu",Highly cited,,Neural Collaborative Filtering,,,,,Unknown,,,4768.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia,Academia",,,,,"Academia,Academia,Academia,Academia",,,
Statement Curriculum Learning,Language,Automated theorem proving,2022-03-02,,,,,https://arxiv.org/abs/2202.01344,OpenAI,,Probably below 1e23 FLOP given the small model size.,United States of America,"Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever ",SOTA improvement,"""by applying this expert iteration to a manually curated set
of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving
multiple challenging problems drawn from high school olympiads.""",Formal Mathematics Statement Curriculum Learning,"Common Crawl,WebMath","300 billion tokens from Common Crawl
72 billion tokens (220 GB) of code from WebMath
25000 theorems from mathlib
327 math problems from competitions and textbooks

The model was also trained on its own self-generated proofs",275000000000,"Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB.",,,,63.00,774000000.00,,,,,,,,,,,Industry,,2024-05-01 09:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
RetinaNet-R50,Vision,Object detection,2017-08-07,,,,,https://arxiv.org/abs/1708.02002,Facebook AI Research,,,United States of America,"TY Lin, P Goyal, R Girshick, K He",Highly cited,,Focal loss for dense object detection,,,,,,,,16437.00,34000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,97000000000.00,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Boss (DARPA Urban Challenge),Driving,Self-driving car,2008-07-23,,,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255,Carnegie Mellon University (CMU),,,United States of America,"Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William “Red” Whittaker,Ziv Wolkowicki, and Jason Ziglar",Highly cited,,Autonomous driving in urban environments: Boss and the Urban Challenge,,,,,Unknown,,,1840.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
HMM Word Alignment,Language,Word alignment,1996-08-05,,,,,https://dl.acm.org/doi/10.3115/993268.993313,University of Erlangen - Nuremburg,,,Germany,"Stephan Vogel, Hermann Ney, Christoph Tillmann",Highly cited,,HMM-Based Word Alignment in Statistical Translation,,,442316,"[WORDS]
Table 1.
I take the sum of all words. Maybe it would be better to use only the sum of English or German words?",,,,1099.00,,,,,,,,,Supervised,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Maximum Entropy Models for machine translation,Language,Translation,2002-07-06,,,,,https://aclanthology.org/P02-1038/,"University of Southern California,RWTH Aachen",,,"United States of America,Germany",Franz Josef Och and Hermann Ney,Highly cited,,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,,,519523,"[WORDS]
Table 1",,,,1413.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
XLMR-XXL,Language,,2021-08-17,,,,,https://arxiv.org/abs/2105.00572,Facebook AI Research,,,United States of America,"Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",SOTA improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",Larger-Scale Transformers for Multilingual Masked Language Modeling,CC100,,125250000000,"""We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.""

1 token ~ 0.7 words",,,,79.00,10700000000.00,"Section 2.1:
"" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)""",,,,,,,,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
SACHS,Biology,,2005-04-22,,,,,https://science.sciencemag.org/content/308/5721/523.long,"Massachusetts Institute of Technology (MIT),Stanford University",,,"United States of America,United States of America","K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger and G. P. Nolan",Highly cited,,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.,,,5400,"I think? 

"" The truncated singlecell data set (420 data points) shows a large
(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). ""

Seems potentially wrong by maybe 20%. Might need to add 1200.",,,,1682.00,178.00,From https://www.bnlearn.com/bnrepository/,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
SimCLR,Vision,Image completion,2020-02-13,Apache 2.0: https://github.com/google-research/simclr,Open source,,Open source,https://arxiv.org/abs/2002.05709,Google Brain,,,United States of America,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",Highly cited,,A Simple Framework for Contrastive Learning of Visual Representations,ImageNet,"""Dataset and Metrics. Most of our study for unsupervised
pretraining (learning encoder network f without labels)
is done using the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015). Some additional pretraining experiments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be
found in Appendix B.9.""",,,,,,13067.00,375000000.00,source: https://openai.com/blog/image-gpt/,1000.00,,,,,Google TPU v3,,,,Industry,,2024-04-23 09:55,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Polarity Classifier,Language,,2009-09-01,,,,,https://aclanthology.org/J09-3003.pdf,"University of Edinburgh,University of Pittsburgh",,,"United Kingdom of Great Britain and Northern Ireland,United States of America","Theresa Wilson, Janyce Wiebe, Paul Hoffmann.",,,Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis,,,11112,"Section 3.3 reveals there are 11,112 sentences. Since this is phrase-level sentiment analysis sentences seem like the best unit",,,,866.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
MatrixFac for Recommenders,Recommendation,,2009-08-07,,,,,https://ieeexplore.ieee.org/document/5197422,"Yahoo Research,AT&T",,,"United States of America,United States of America","Yehuda Koren, Robert Bell, and Chris Volinsky",Highly cited,,Matrix factorization techniques for recommender systems,Netflix Prize,,100480507,,,,,8913.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
Domain Adaptation,Vision,Object recognition,2011-11-06,,,,,http://ftp.idiap.ch/pub/courses/EE-700/material/05-12-2012/2011_ICCV_DomainAdaptation.pdf,University of Maryland,,,United States of America,"Raghuraman Gopalan, Ruonan Li, Rama Chellappa",Highly cited,,Domain Adaptation for Object Recognition: An Unsupervised Approach,Dataset introduced in 'Adapting Visual Category Models to New Domains',,4652,"Dataset introduced in 'Adapting Visual Category Models to New
Domains'",,,,1061.00,15260.00,"Did not take into account initial image feature extraction, only novel stuff.

1. Perform PCA on the feature matrices from both domains. Learnable parameters are projection matrices.
= 800 (# features) x 200 (reduced dimension) x 2 (once per subdomain)

2. Perform partial least squares regression. Learnable parameters are

Matrix P with dimensions 200 (# features) x 30 (dimension of latent space)
Matrix Q with dimensions 1 (# responses) x 30 (dimension of latent space)
Projection matrix of X onto latent space:  200 (# features) x 30 (dimension of latent space)
Projection matrix of Y onto latent space:  1 (# responses) x 30 (dimension of latent space)
",,,,,,,Supervised,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Theseus 6/768,Language,Text autocompletion,2020-02-07,Apache 2.0: https://github.com/JetRunner/BERT-of-Theseus,Open source,,Open source,https://arxiv.org/abs/2002.02925,"UC San Diego,Beihang University,Microsoft",,,"United States of America,China,United States of America","Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou",SOTA improvement,"""Our approach outperforms existing knowledge distillation approaches on GLUE benchmark""",BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,GLUE,"fine-tuned on training sets from GLUE benchmark:

""We test our approach under a task-specific compression setting (Sun et al., 2019; Turc et al., 2019)
instead of a pretraining compression setting (Sanh
et al., 2019; Sun et al., 2020). That is to say, we use
no external unlabeled corpus but only the training set of each task in GLUE to compress the
model. """,,,,,,177.00,66000000.00,"66M, Table 1",,11300000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,NVIDIA V100,,,,Industry,,2024-05-01 09:05,Robi Rahman,,,BERT-Large,2700000000000000000,"Actually BERT-base, 110M params. Up to 20 V100-hours depending on task. 

125 trillion * 20 * 3600 * 0.3 (utilization assumption) = 2.7e18",,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
DeepNet,Language,Language modelling,2022-03-01,,,,,https://arxiv.org/abs/2203.00555,Microsoft Research,,"They show results on par with the original Transformer, so probably less than 2.3e19 FLOP.",United States of America,"Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei",SOTA improvement,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points""","DeepNet: Scaling Transformers to 1,000 Layers",,,12000000000,""" The final data consists of 102 languages, 1932 directions, and
12B sentence pairs.""",,,,104.00,3200000000.00,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction""

EDIT 05/05/2022: The 12B model was presented in an earlier paper. This paper presents a 3.2B model",,,,,,,,,,Industry,,2024-04-03 12:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
TFE SVM,Vision,Character recognition,2006-02-02,,,,,https://hal.archives-ouvertes.fr/hal-00018426/en,"Centre de Recherche en Automatique de Nancy (CRAN),CENPARMI",,,"France,Canada","Fabian Lauer, Ching Y Suen, Gerard Bloch",SOTA improvement,best at affine-transformed digits in table 4,A trainable feature extractor for handwritten digit recognition,,,,,Unknown,,,365.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,TFE SVM,"Academia,Academia",,,,,"Academia,Academia",,,
SimCLRv2,Vision,,2020-10-26,,,,,https://arxiv.org/abs/2006.10029,Google Brain,,,United States of America,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton",Highly cited,,Big self- supervised models are strong semi-supervised learners.,,,1280000,,,,,1795.00,795000000.00,"From author communication

We trained different model sizes (from 24M to 795M), and they're summarized in Table 1 of the paper (https://arxiv.org/pdf/2006.10029.pdf).",,,,,,,,,,Industry,,2024-04-01 09:45,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Multi-cause Binary Clustering,Other,,1995-01-01,,,,,https://ieeexplore.ieee.org/document/6795568,Xerox,,,United States of America,Eric Saund,,,A Multiple Cause Mixture Model for Unsupervised Learning,,,,,Unknown,,,176.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Regularized SVD for Collaborative Filtering,Recommendation,,2007-08-12,,,,,https://www.semanticscholar.org/paper/Improving-regularized-singular-value-decomposition-Paterek/f732d0f69fe4e84a95c32706b28b9e4ef1753c61,Warsaw University,,,Poland,A Paterek,Highly cited,,Improving regularized singular value decomposition for collaborative filtering,Netflix Prize,,100480507,,,,,1117.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Bayesian automated hyperparameter tuning,Other,,2012-12-02,,,,,https://arxiv.org/abs/1206.2944,"University of Toronto,University of Sherbrooke,Harvard University",,,"Canada,Canada,United States of America","J Snoek, H Larochelle, RP Adams",Highly cited,,Practical Bayesian optimization of machine learning algorithms,,,,,Unknown,,,6735.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
DenseNet-264,Vision,Image classification,2016-08-25,,,,,https://arxiv.org/abs/1608.06993,"Tsinghua University,Facebook AI Research,Cornell University",,,"China,United States of America,United States of America","G Huang, Z Liu, L Van Der Maaten",Highly cited,,Densely Connected Convolutional Networks,,,,,,,,30607.00,34000000.00,,,,,,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
GLEE,Games,Tic Tac Toe,1968-07-01,,,,,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,Michie and Chambers,Historical significance,,Boxes: An Experiment in Adaptive Control,,,,,Unknown,,,590.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Wide Residual Network,Vision,Image classification,2016-09-19,,,,,https://arxiv.org/abs/1605.07146,Université Paris-Est,,,France,"Sergey Zagoruyko, Nikos Komodakis",Highly cited,,Wide Residual Networks,,,,,Unknown,,,6791.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Bidirectional RNN,Speech,Speech recognition,1997-11-01,,,,,https://ieeexplore.ieee.org/document/650093,Advanced Telecommunications Research Institute,,,Japan,"M. Schuster, KK Paliwal",Highly cited,,Bidirectional recurrent neural networks,TIMIT,,73920,"""the training data set consisting of 3696 sentences
from 462 speakers""

Assuming avg sentence length of 20 words

3696 * 20 total words",,,,7297.00,13000.00,"Page 7: ""The structures of all networks are adjusted so that
each of them has about the same number of free parameters
(approximately 13 000 here""",,,,,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Word Representations,Language,,2010-06-01,,,,,https://aclanthology.org/P10-1040.pdf,"University of Montreal / Université de Montréal,University of Illinois Urbana-Champaign (UIUC)",,,"Canada,United States of America","Joseph Turian, Lev-Arie Ratinov, Yoshua Bengio",Highly cited,,Word Representations: A Simple and General Method for Semi-Supervised Learning,,,37000000,"Section 6: ""After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences""",,,,2510.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
EfficientNet-L2,Vision,Image classification,2019-05-28,Apache license: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet,Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/1905.11946,Google,,,United States of America,"M Tan, Q Le",Highly cited,,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,ImageNet,,,,,,,12815.00,480000000.00,,,390000000.00,"table 2: using efficientnet_b0
",,,,,,,Industry,,2024-04-16 16:17,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Sparse coding model for V1 receptive fields,Vision,,1997-12-01,,,,,https://www.sciencedirect.com/science/article/pii/S0042698997001697,"UC Davis,Cornell University",,,"United States of America,United States of America","Bruno A. Olshausen, David J. Field",Highly cited,,Sparse coding with an overcomplete basis set: A strategy employed by V1?,,,10,"In Simulation Methods: ""The data for training were taken from ten 512 × 512
pixel images of natural surroundings""",,,,4257.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Internal functionality of visual invariants,Vision,,1979-05-02,,,,,https://link.springer.com/article/10.1007/BF00337644,Utrecht University,,,Netherlands,Koenderink & van Doom,Historical significance,,The internal representation of solid shape with respect to vision,,,,??? Seemingly no info,Unknown,,,981.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
LMS,Other,,1960-06-30,,,,,https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=547230,Stanford University,,,United States of America,Widrow and Hoff,Highly cited,,Adaptive switching circuits (technical report),,,6,,,,,6329.00,17.00,,,,,,,,,,,Academia,,2024-04-26 18:15,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
TensorReasoner,Language,,2013-12-01,,,,,https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html,Stanford University,,,United States of America,"R Socher, D Chen, CD Manning, A Ng",Highly cited,,Reasoning With Neural Tensor Networks for Knowledge Base Completion,,,,,Unknown,,,1847.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Denoising Autoencoders,Other,,2008-07-05,,,,,https://dl.acm.org/doi/10.1145/1390156.1390294,University of Montreal / Université de Montréal,,,Canada,"Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol",Highly cited,,Extracting and Composing Robust Features with Denoising Autoencoders,,,,,Unknown,,,6794.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Elastic weight consolidation,"Vision,Games",,2016-12-02,,,,,https://arxiv.org/abs/1612.00796,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"J Kirkpatrick, R Pascanu",Highly cited,,Overcoming catastrophic forgetting in neural networks,,,,,Unknown,,,5246.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
SearchFusion,Vision,Object detection,2013-04-02,,,,,https://link.springer.com/article/10.1007/s11263-013-0620-5,"University of Trento,University of Amsterdam",,,"Italy,Netherlands","JRR Uijlings, KEA Van De Sande, T Gevers",Highly cited,,Selective search for object recognition,,,,,Unknown,,,5642.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
GroupLens,Recommendation,,1994-10-22,,,,,https://dl.acm.org/doi/10.1145/192844.192905,Massachusetts Institute of Technology (MIT),,,United States of America,"Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl",Highly cited,,GroupLens: an Open Architecture for Collaborative Filtering of Netnews,,,100000000,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,7733.00,,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Generative BST,Language,,2021-03-05,,,,,https://arxiv.org/abs/2004.13637,Facebook AI Research,,"Unclear - no mention of GPUs used, or training time, and the architecture is terribly complicated",United States of America,"Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",SOTA improvement,"Abstract:
""Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.""",Recipes for building an open-domain chatbot,,"Section 6:
Pushshfit.io Reddit, ConvAI 2, Wizard of Wikipedia",,,,,,843.00,9400000000.00,"Abstract:
""We build variants of these recipes with 90M, 2.7B and 9.4B parameter models""",,,,,,,,,,Industry,,2024-05-01 09:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
BPE,Language,Translation,2015-08-31,,,,,https://arxiv.org/abs/1508.07909,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,"R Sennrich, B Haddow, A Birch",Highly cited,,Neural Machine Translation of Rare Words with Subword Units,WMT'15,,37500000,"[WORDS]
""We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens""

100M tokens, around half will be in English, 0.75 words per token

",,,,6666.00,,,,,,,,,,,,Academia,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Inflated 3D ConvNet,3D modeling,Action recognition,2017-06-01,,,,,https://arxiv.org/abs/1705.07750,"DeepMind,University of Oxford",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Joao Carreira, Andrew Zisserman",Highly cited,,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",,,,,Unknown,,,6542.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Stacked Denoising Autoencoders,Other,,2010-01-03,,,,,https://www.jmlr.org/papers/v11/vincent10a.html,"University of Montreal / Université de Montréal,University of Toronto",,,"Canada,Canada","P Vincent, H Larochelle, I Lajoie, Y Bengio",Highly cited,,Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,,,,,Unknown,,,6689.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RBM-tuning,Other,,2010-08-02,,,,,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_32,University of Toronto,,,Canada,GE Hinton,Highly cited,,A practical guide to training restricted boltzmann machines,,,,,Unknown,,,3335.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
ConvNet similarity metric,Vision,Face verification,2005-06-20,,,,,https://ieeexplore.ieee.org/document/1467314,New York University (NYU),,,United States of America,"S Chopra, R Hadsell, Y LeCun",Highly cited,,"Learning a similarity metric discriminatively, with application to face verification",,,140000,"The actual training set that was used contained
140,000 image pairs that were evenly split between genuine
and impostor.",,,,3846.00,,,,,,,,,,,,Academia,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
GPipe (Transformer),Language,Translation,2018-11-16,,,,,https://arxiv.org/abs/1811.06965,Google,,,United States of America,"Y Huang, Y Cheng, A Bapna, O Firat","Highly cited,SOTA improvement","""We train a single 6-billion-parameter,
128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.""",GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,,,20000000000,"[WORDS]

Section 5: ""We use a
corpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language""

10^9 sentences * 20 words per sentence",,,,1218.00,6000000000.00,Section 5: ,,,,,,,Self-supervised learning,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Word2Vec (small),Language,Semantic embedding,2013-10-16,,,,,https://arxiv.org/abs/1310.4546,Google,,,United States of America,"T Mikolov, I Sutskever, K Chen, GS Corrado",Highly cited,,Distributed Representations of Words and Phrases and their Compositionality,,,692000,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,31189.00,207600000.00,"""We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K [...] Starting with the same news data as in the previous experiments, we first constructed the phrase based training corpus and then we trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5.""",,,,,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Conditional probability machines,Other,,1956-07-01,,,,,https://www.moma.org/collection/works/illustratedbooks/16252?locale=es,Princeton University,,,United States of America,AM Uttley,Historical significance,,Conditional probability machines,,,,,Unknown,,,84.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Textual Imager,Vision,Object recognition,2013-01-16,,,,,https://arxiv.org/abs/1301.3666,Stanford University,,,United States of America,"R Socher, M Ganjoo, H Sridhar, O Bastani",Highly cited,,Zero-Shot Learning Through Cross-Modal Transfer,,,,,Unknown,,,1387.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MLP as Bayesian Approximator,Other,,1990-12-01,,,,,https://ieeexplore.ieee.org/abstract/document/80266,Air Force Institute of Technology,,,United States of America,D.W. Ruck & S.K. Rogers & M. Kabrisky & M.E. Oxley & B.W. Suter,Highly cited,,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,,,,,Unknown,,,1046.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Adaptive Broom Balancer,Games,Pole balancing,1988-07-24,,,,,https://ieeexplore.ieee.org/document/23982,Stanford University,,,United States of America,"VV Tolat, B Widrow",,,An Adaptive “Broom Balancer” with Visual Inputs,,,,,,,,80.00,110.00,Figure 3,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Restricted Bolzmann machines,Recommendation,,2007-06-20,,,,,https://dl.acm.org/doi/abs/10.1145/1273496.1273596?casa_token=cfdkH2x12MwAAAAA:sEUzfllIGyPcOfzgUoDPHlpC1ukfCAo8ewocBXWBswIIF9eS5HdFo30nOtfmIV8gm-XpBpQJJ5zYVO8,University of Toronto,,,Canada,"Russ Salukhutdinov, Andriy Mnih, GE Hinton",Highly cited,,Restricted Boltzmann machines for collaborative filtering,Netflix Prize,,100480507,"The training data set consists of 100,480,507
ratings",,,,2140.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
DANet,Vision,Semantic segmentation,2019-04-21,MIT for code and weights: https://github.com/junfu1115/DANet/,Open source,Open access (non-commercial),Open source,https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html,Chinese Academy of Sciences,,,China,"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu",Highly cited,,Dual Attention Network for Scene Segmentation,"Cityscapes,COCO-Stuff,PASCAL-Context",,,,Unknown,,,4105.00,,,,,,,,,,,,Industry,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
λ-WASP,Language,,2007-06-01,,,,,https://www.aclweb.org/anthology/P07-1121/,UT Austin,,,United States of America,"YW Wong, R Mooney",SOTA improvement,"""The resulting parser is shown to be the bestperforming system so far in a database query domain""",Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,,,792,"""Table 1 summarizes the results at the end of the learning curves (792 training examples for λWASP, WASP and SCISSOR, 600 for Z&C)""",,,,383.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
GPipe (Amoeba),Vision,Image classification,2018-11-16,,,,,https://arxiv.org/abs/1811.06965,Google,,,United States of America,"Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen",Highly cited,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,ImageNet,,1281167,Table 4,,,,1218.00,557000000.00,Section 4,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Chinese - English translation,Language,Translation,2018-03-01,,,,,https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/,Microsoft,,,United States of America,"H Hassan, A Aue, C Chen, V Chowdhary",SOTA improvement,"""We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations""",Achieving Human Parity on Automatic Chinese to English News Translation,,,,,Unknown,,,575.00,,,,,,,,,Self-supervised learning,,,Industry,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Greedy layer-wise DNN training,Other,,2006-12-04,,,,,https://dl.acm.org/doi/10.5555/2976456.2976476,University of Montreal / Université de Montréal,,,Canada,"Y Bengio, P Lamblin, D Popovici",Highly cited,,Greedy layer-wise training of deep networks,,,,,Unknown,,,5605.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Grover-Mega,Language,,2019-05-29,,,,,https://arxiv.org/abs/1905.12616,University of Washington,,,United States of America,"R Zellers, A Holtzman, H Rashkin, Y Bisk",,,Defending Against Neural Fake News,,,,,,,,786.00,1500000000.00,,,,,,,,,,,Industry,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Hopfield network,Other,Sequence memorization,1982-04-01,,,,,https://www.pnas.org/doi/10.1073/pnas.79.8.2554,California Institute of Technology,,,United States of America,JJ Hopfield,Highly cited,,Neural networks and physical systems with emergent collective computational abilities,,,,,,,,23315.00,9900.00,"My understanding is that the biggest Hopfield networks they studied had N=100 units. 

Each unit has 99 synapses Tij from each other unit, for a total of 100*99 parameters",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
SqueezeNet,Vision,Image classification,2016-02-24,,,,,https://arxiv.org/abs/1602.07360,"DeepScale,UC Berkeley,Stanford University",,,"United States of America,United States of America,United States of America","Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer",Highly cited,,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,,,,,,,,6489.00,1200000.00,"The paper says ""SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.""

AlexNet has 60 million parameters.",,,,,,,,,,Industry,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
MS-CNN,Vision,Object detection,2016-09-17,,,,,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22,"IBM,UC San Diego",,,"United States of America,United States of America","Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos",Highly cited,,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,,,,,Unknown,,,1418.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Inception-ResNet-V2,Vision,Image classification,2016-02-23,,,,,https://arxiv.org/abs/1602.07261,Google,,,United States of America,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",Highly cited,,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",,,,,,,,12187.00,56000000.00,,,2638000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
YouTube Video Recommendation System,Recommendation,,2010-09-26,,,,,https://dl.acm.org/doi/10.1145/1864708.1864770,Google,,,United States of America,"J Davidson, B Liebald, J Liu, P Nandy",Highly cited,,The YouTube Video Recommendation System,,,10000000000,"""We currently handle millions of users
and tens of billions of activity events with a total footprint
of several terabytes of data""

If 10M users each watch 1000 videos, that's 10B visualizations, which matches their ""activity events"" count.",,,,1109.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
PNAS-net,Vision,Image classification,2017-12-02,,,,,https://arxiv.org/abs/1712.00559,"Johns Hopkins University,Google AI,Stanford University",,,"United States of America,Multinational,United States of America","C Liu, B Zoph, M Neumann, J Shlens",Highly cited,,Progressive Neural Architecture Search,,,,,,,,1819.00,86000000.00,,,,,,,,,,,Industry,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
ProgressiveGAN,Vision,Image generation,2017-10-27,,,,,https://arxiv.org/abs/1710.10196,NVIDIA,,,United States of America,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",Highly cited,,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",,,,,Unknown,,,6089.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
RL mapping instructions (troubleshooting),Language,Instruction interpretation,2009-08-02,,,,,https://aclanthology.org/P09-1010/,Massachusetts Institute of Technology (MIT),,,United States of America,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",,,Reinforcement Learning for Mapping Instructions to Actions,Windows Help and Support,,1327,"Shown at beginning of section 7
Total number of documents is 128, average number of actions per document is 10.37",,,,308.00,133140.00,"""We use a policy gradient
algorithm to estimate the parameters of a log-linear model for action selection [...] In total, there are 4,438 features [in the Windows domain]. [...]  This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are 27.14 choices per action in the Windows domain""",,,,,,,,,,Academia,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Network in Network,Vision,Image classification,2013-12-16,,,,,https://arxiv.org/abs/1312.4400,National University of Singapore,,,Singapore,"M Lin, Q Chen, S Yan",Highly cited,,Network In Network,,,,,Unknown,,,5825.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Maxout Networks,Vision,Image classification,2013-02-18,,,,,https://arxiv.org/abs/1302.4389,University of Montreal / Université de Montréal,,,Canada," Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",Highly cited,,Maxout Networks ,,,,,Unknown,,,2576.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Hiero,Language,Translation,2005-06-01,,,,,https://aclanthology.org/P05-1033/,University of Maryland,,,United States of America,David Chiang,Highly cited,,A Hierarchical Phrase-Based Model for Statistical Machine Translation,,,171400000,"[WORDS]
155M words dataset for the language model plus (7.2+9.2)M words for the translation model?",,,,1487.00,120000000.00,"Very unsure, but the paper mentions 
""We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules"" 
and 
""For our experiments we used the following features, analogous to Pharaoh’s default feature set:
• P(γ | α) and P(α | γ), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
• the lexical weights Pw(γ | α) and Pw(α | γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;
2
• a phrase penalty exp(1), which allows the
model to learn a preference for longer or
shorter derivations, analogous to Koehn’sphrase penalty (Koehn, 2003).""

Suggesting 24M rules * 5 features per rule (?)",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
DnCNN,"Vision,Image generation",Image super-resolution,2017-02-01,,,,,https://ieeexplore.ieee.org/abstract/document/7839189,"Harbin Institute of Technology,Hong Kong Polytechnic University,ULSee Inc.,Xi’an Jiaotong University",,,"China,Hong Kong,China,China","Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",Highly cited,,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,,,,,Unknown,,,5737.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Industry,Academia",,,,,"Academia,Academia,Industry,Academia",,,
YOLO,Vision,Object detection,2015-06-08,,,,,https://arxiv.org/abs/1506.02640,"University of Washington,Allen Institute for AI,Facebook AI Research",,,"United States of America,United States of America,United States of America","Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",Highly cited,,"You Only Look Once: Unified, Real-Time Object Detection",,,,,,,,28534.00,271684800.00,"Calculation based on figure 3 of the paper:
7 * 7 * 3 * 64 + 3 * 3 * 64 * 192 + 1 * 1 * 192 * 128 + 3 * 3 * 128 * 256 + 1 * 1 * 256 * 256 + 3 * 3 * 256 * 512 + 4 * (1 * 1 * 512 * 256 + 3 * 3 * 256 * 512) + 1 * 1 * 512 * 512 + 3 * 3 * 512 * 1024 + 2 * (1 * 1 * 1024 * 512 + 3 * 3 * 512 * 1024) + 4 * (3 * 3 * 1024 * 1024) + 7 * 7 * 1024 * 4096 + 4096 * 7 * 7 * 30",,,,,,,,,,Industry,,2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,"Academia,Research collective,Industry",,,,,"Academia,Research collective,Industry",,,
3D city reconstruction,3D modeling,3D reconstruction,2009-09-29,,,,,https://grail.cs.washington.edu/rome/,"University of Washington,Microsoft Research,Cornell University",,,"United States of America,United States of America,United States of America","Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski",Highly cited,,Building Rome in a Day,,,,,Unknown,,,2203.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
MoCo,"Vision,Image generation",Image completion,2019-11-13,"non-commercial. the released models seem to be trained on ImageNet, not Instagram

https://github.com/facebookresearch/moco",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/1911.05722,Facebook AI,,,United States of America,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick",Highly cited,,Momentum Contrast for Unsupervised Visual Representation Learning,"ImageNet,Instagram-1B","""We study unsupervised training performed in:
ImageNet-1M (IN-1M): This is the ImageNet [11] training set that has ∼1.28 million images in 1000 classes (often
called ImageNet-1K; we count the image number instead,
as classes are not exploited by unsupervised learning). This
dataset is well-balanced in its class distribution, and its images generally contain iconic view of objects.
Instagram-1B (IG-1B): Following [44], this is a dataset
of ∼1 billion (940M) public images from Instagram. The
images are from ∼1500 hashtags [44] that are related to the
ImageNet categories. This dataset is relatively uncurated
comparing to IN-1M, and has a long-tailed, unbalanced
distribution of real-world data. This dataset contains both
iconic objects and scene-level images.""",,,,,,8909.00,375000000.00,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,Industry,,2024-04-22 13:25,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Time-delay neural networks,Speech,,1989-03-03,,,,,https://ieeexplore.ieee.org/abstract/document/21701,"Advanced Telecommunications Research Institute,Carnegie Mellon University (CMU)",,,"Japan,United States of America","A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang",Highly cited,,Phoneme recognition using time-delay neural networks,,,,,Unknown,,,3445.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
BLSTM for handwriting (2),Vision,Character recognition,2007-12-03,,,,,https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html,"University of Bern,IDSIA,Technical University of Munich",,,"Switzerland,Switzerland,Germany","Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández",SOTA improvement,"""In experiments on an unconstrained
online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.""",Unconstrained online handwriting recognition with recurrent neural networks,,,,,,,,341.00,100881.00,"For the raw input representation,
there were 4 input units and a total of 100,881 weights",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
HRA,Games,Ms Pacman,2017-06-13,,,,,https://arxiv.org/abs/1706.04208,"Maluuba,Microsoft",,,"Canada,United States of America","H Van Seijen, M Fatemi, J Romoff, R Laroche",SOTA improvement,"""With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also significantly outperforms the human score, convincingly demonstrating the strength of HRA.""",Hybrid Reward Architecture for Reinforcement Learning,,,,,Unknown,,,228.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
Go-explore,Games,Atari,2020-04-27,non-commercial code: https://github.com/uber-research/go-explore/blob/master/LICENSE,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2004.12919,"Uber AI,OpenAI",,,"United States of America,United States of America","A Ecoffet, J Huizinga, J Lehman, KO Stanley, J Clune",SOTA improvement,"""GoExplore solves all heretofore unsolved Atari games (meaning those for which algorithms could not previously
outperform humans when evaluated following current community standards for Atari3) and surpasses the state
of the art on all hard-exploration games""","First return, then explore",,,,,Unknown,,,273.00,,,,,,,,,,,,Industry,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
Advantage Learning,Games,Atari,2015-12-15,,,,,http://arxiv.org/abs/1512.04860v1,Google DeepMind,,,Multinational,"MG Bellemare, G Ostrovski, A Guez",SOTA improvement,,Increasing the Action Gap: New Operators for Reinforcement Learning,,,,,Unknown,,,144.00,,,,,,,,,,,,Industry,"This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.",2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DIABETES,Medicine,Medical diagnosis,1991-06-24,,,,,https://link.springer.com/chapter/10.1007/978-3-642-48650-0_19,"Aalborg University,University of London",,,"Denmark,United Kingdom of Great Britain and Northern Ireland","S. Andreassen, R. Hovorka, J. Benn, K. G. Olesen, and E. R. Carson",,,A Model-based Approach to Insulin Adjustment,,,,,,,,132.00,429409.00,From https://www.bnlearn.com/bnrepository/,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Cascaded LNet-ANet,Vision,,2014-11-28,,,,,https://arxiv.org/abs/1411.7766,Chinese University of Hong Kong (CUHK),,,Hong Kong,"Z Liu, P Luo, X Wang, X Tang",Highly cited,,Deep Learning Face Attributes in the Wild,,,,,Unknown,,,6902.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
BigChaos 2008,Recommendation,Movie ratings,2008-11-25,,,,,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf,AT&T,,,United States of America,"A Töscher, M Jahrer",Historical significance,Winners of the 2008 Netflix Price,The BigChaos Solution to the Netflix Prize 2008,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,35.00,,,,,,,,,,,,Industry,,2024-04-04 12:19,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DrLIM,Other,Image embedding,2006-06-17,,,,,https://ieeexplore.ieee.org/document/1640964,New York University (NYU),,,United States of America,R. Hadsell; S. Chopra; Y. LeCun,Highly cited,,Dimensionality Reduction by Learning an Invariant Mapping,,,217470,"""The dataset was split into 660 training images and a 312
test images. The result of training on all 10989 similar pairs
and 206481 dissimilar pairs is a 3-dimensional manifold in
the shape of a cylinder (see figure 8).""

206481 + 10989 = 217470",,,,4511.00,37097.00,Architecture described in figure 3,,,,,,,,,,Academia,,2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MEB,Search,,2021-09-04,,,,,https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/,Microsoft,,,United States of America,"W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi",Significant use,"""MEB is running in production for 100 percent of Bing searches, in all regions and languages.""",Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance,,,,"""MEB uses three years of search logs from Bing as training data."" TODO convert",,,,26.00,135000000000.00,See paper title,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ResNeXt-101 Billion-scale,Vision,Image classification,2019-05-02,"non-commercial for weights: 
https://github.com/facebookresearch/semi-supervised-ImageNet1K-models",Open access (non-commercial),Unreleased,Unreleased,https://arxiv.org/abs/1905.00546,Facebook AI,,,United States of America,"IZ Yalniz, H Jégou, K Chen, M Paluri",SOTA improvement,"""We demonstrate the performance of our method on popular classification benchmarks for both images and videos and significantly outperforms the state of the art.""",Billion-scale semi-supervised learning for image classification,YFCC-100M,,,,,,,411.00,193000000.00,,,,,,,,,,,Industry,,2024-05-01 09:06,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
SNARC,Robotics,Maze solving,1952-01-08,,,,,https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator,Harvard University,,,United States of America,Marvin Minsky,Historical significance,,A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement,,,,,,,,33.00,40.00,"The link below seems to suggest the SNARC had 40 cells, each with a dial that acts as a configurable weight.

https://www.webofstories.com/play/marvin.minsky/137",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
SENet (ImageNet),Vision,Image classification,2017-09-05,,,,,https://arxiv.org/abs/1709.01507,"Chinese Academy of Sciences,University of Oxford",,,"China,United Kingdom of Great Britain and Northern Ireland","Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu",Highly cited,,Squeeze-and-Excitation Networks,ImageNet,,,,,,,20005.00,28100000.00,Table 16,,3870000000.00,Table 16,,,,,,,Academia,,2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Mid-level Features,Vision,Object recognition,2010-06-13,,,,,https://ieeexplore.ieee.org/document/5539963,"INRIA,Ecole Normale Supèrieure,New York University (NYU)",,,"France,France,United States of America","YL Boureau, F Bach, Y LeCun, J Ponce",Highly cited,,Learning mid-level features for recognition,,,,,Unknown,,,1314.00,,This is extracting low-level SIFT features then max-pooling them and using in a linear SVM. The training compute could be estimated loosely for the SVM part.,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
data2vec (language),Language,,2022-01-20,,,,,https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,Meta AI,,,United States of America,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",SOTA improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""","Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","""BookCorpus (BooksCorpus, Toronto Book Corpus)"",English Wikipedia",,3300000000,"Section 5.3: ""we
adopt the same training setup as BERT (Devlin et al., 2019)
by pre-training on the Books Corpus (Zhu et al., 2015) and
English Wikipedia data over 1M updates and a batch size
of 256 sequences.""",,,,563.00,705134592.00,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,,,,,Self-supervised learning,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Symmetric Residual Encoder-Decoder Net,"Vision,Image generation",Image super-resolution,2016-03-30,,,,,https://arxiv.org/abs/1603.09056v2,"Nanjing University,University of Adelaide",,,"China,Australia","Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang",Highly cited,,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,,,,,Unknown,,,1184.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
SVD in recommender systems,Recommendation,,2000-07-14,,,,,http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf,University of Minnesota,,,United States of America,"B Sarwar, G Karypis, J Konstan, J Riedl",Highly cited,,Application of Dimensionality Reduction in Recommender System -- A Case Study,,,,,Unknown,,,2126.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
LRCN,Video,Video description,2014-11-07,,,,,https://arxiv.org/abs/1411.4389,"UT Austin,University of Massachusetts Lowell,UC Berkeley",,,"United States of America,United States of America,United States of America","Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell",Highly cited,,Long-term Recurrent Convolutional Networks for Visual Recognition and Description,TaCoS,"Largest model is for image captioning:
Pretrained with ILSVRC 2021 (1.2M images)
Trained on 40k video-sentence pairs from TaCoS",40000,,,,,5683.00,142552000.00,"1st model: CaffeNet fc6 feature extractor (4096-length vectors) -> LSTM with 1024 hidden units

2nd model: CaffeNet fc6 feature extractor (4096-length vectors) -> 2 layer LSTM with 1000 hidden units

3rd mode: Like the second, but has encoder and decoder LSTMs (both with 2 layers)

AlexNet (close relative to CaffeNet) has 61M params.

LSTM RNN number of parameters is given by L*(n*m + n^2 + n) where L:= Number of layers, n:= hidden units, m:= input vector length
",,,,,,,Reinforcement learning,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
RL mapping instructions (games),Language,Instruction interpretation,2009-08-01,,,,,https://aclanthology.org/P09-1010/,Massachusetts Institute of Technology (MIT),,,United States of America,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",,,Reinforcement Learning for Mapping Instructions to Actions,Windows Help and Support,,293,"Shown at beginning of section 7
Total number of documents is 50, average number of actions per document is 5.86

source: https://en.wikipedia.org/wiki/Netflix_Prize",,,,308.00,80940.00,"""We use a policy gradient
algorithm to estimate the parameters of a log-linear model for action selection [...] In total, there are 8,094 features [in the Crossblock domain]. [...]  This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are [...] 9.78 [actions] in the Crossblock
domain""",,,,,,,,,,Academia,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Optimized Single-layer Net,Vision,Image classification,2011-04-11,,,,,http://proceedings.mlr.press/v15/coates11a.html,"University of Michigan,Stanford University",,,"United States of America,United States of America","A Coates, A Ng, H Lee",Highly cited,,An analysis of single-layer networks in unsupervised feature learning,,,,,Unknown,,,3434.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ResNeXt-50,Vision,Image classification,2016-11-16,,,,,https://arxiv.org/abs/1611.05431,"UC San Diego,Facebook",,,"United States of America,United States of America","Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He",Highly cited,,Aggregated Residual Transformations for Deep Neural Networks,,,,,,,,8780.00,25000000.00,"""If you’re thinking about ResNets, yes, they are related. ResNeXt-50 has 25M parameters (ResNet-50 has 25.5M).""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,8400000000.00,"Rados  (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
ReLU (LFW),Vision,Face recognition,2010-06-15,,,,,https://dl.acm.org/doi/10.5555/3104322.3104425,University of Toronto,,,Canada,"Nair, V., Hinton, G. E.",Highly cited,,Rectified linear units improve restricted boltzmann machines,,,,,Unknown,,,16092.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Stanley (DARPA Grand Challenge 2),Driving,Self-driving car,2006-01-01,,,,,http://robots.stanford.edu/papers/thrun.stanley05.pdf,Stanford University,,,United States of America,"S Thrun, M Montemerlo, H Dahlkamp",Highly cited,,Stanley: The Robot that Won the DARPA Grand Challenge,,,,,Unknown,,,2561.00,,"""Our  approach  and  the underlying  probabilistic  Markov  model  possess  anumber  of  unknown  parameters.  These  parameters include the height threshold, the statistical acceptance  probability  threshold,  and  various  Markov chain error parameters the noise covariances of theprocess noise and the measurement noise. Stanley uses a discriminative learning algorithm for  locally  optimizing  these  parameters.""",,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
CRF-RNN,Vision,Image segmentation,2015-02-11,,,,,https://arxiv.org/abs/1502.03240,"University of Oxford,Stanford University,Baidu",,,"United Kingdom of Great Britain and Northern Ireland,United States of America,China","Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr",Highly cited,,Conditional Random Fields as Recurrent Neural Networks,,,,,Unknown,,,2661.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
Cloob,"Multimodal,Language,Vision",Image captioning,2021-10-21,,,,,https://arxiv.org/abs/2110.11316,"Johannes Kepler University,HERE Technologies,Institute of Advanced Research in Artificial Intelligence",,,"Austria,Switzerland,Austria","Andreas Fürst ∗Elisabeth Rumetshofer ∗Johannes Lehner,Viet Tran,Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto-Nemling, Sepp Hochreiter",,"improves on CLIP, which was SOTA at the time 1.5 years ago. but this paper doesn't claim to be SOTA, so I think it's unlikely to be so.",CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP,,,15000000,"[Image-text pairs]
""To be comparable to the CLIP results, we use the same subset of 15 million samples from the YFCC100M dataset""",,,,71.00,,,,,,,,,,,,Industry,"CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from an explaining away problem, that is, it focuses on one or few features, while neglecting other relevant features. This problem is caused by insufficiently extracting the covariance structure in the original multi-modal data. We suggest to use modern Hopfield networks to tackle the problem of explaining away. Their retrieved embeddings have an enriched covariance structure derived from co-occurrences of features in the stored embeddings. However, modern Hopfield networks increase the saturation effect of the InfoNCE objective which hampers learning. We propose to use the InfoLOOB objective to mitigate this saturation effect. We introduce the novel ""Contrastive Leave One Out Boost"" (CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.",2024-05-13 09:32,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Image-to-image cGAN,Vision,,2016-11-21,,,,,https://arxiv.org/abs/1611.07004,UC Berkeley,,,United States of America,"P Isola, JY Zhu, T Zhou",Highly cited,,Image-to-Image Translation with Conditional Adversarial Networks,,,,,Unknown,,,16736.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Sequence-based pattern recognition,Vision,Character recognition,1955-03-01,,,,,https://dl.acm.org/doi/10.1145/1455292.1455310,Massachusetts Institute of Technology (MIT),,,United States of America,O. G. Selfridge,Historical significance,,Pattern recognition and modern computers,,,,,Unknown,,,290.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Semantic Taxonomy Induction,Language,,2006-07-07,,,,,https://www.aclweb.org/anthology/P06-1101/,Stanford University,,,United States of America,"Rion Snow, Dan Jurafsky, and Andrew Y. Ng",,,Semantic Taxonomy Induction from Heterogenous Evidence,,,850750,"[Classification task]

The labeled training set is
constructed by labeling the collected feature vectors as positive “known hypernym” or negative
“known non-hypernym” examples using WordNet
2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairs
were labeled as negative training examples.

800,828 + 49,922 = 850750",,,,571.00,100.00,"The main learning algorithm is a logistic classifier. The input is a matrix M, where the rows are pairs of words, and the columns (variables) are counts of occurrences of synthetic dependency paths between those two words.

Since there are on the order of 10~100 different types of syntactic relationships, this is the number of length-1 paths, and thus the number of parameters if only length-1 paths are used.

However, if the length of the paths considered is longer (say, 5), then the parameters would be on the order of (10~100)^5. It's not clear to me which is the case",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Recursive sentiment autoencoder,Language,,2011-07-01,,,,,https://aclanthology.org/D11-1014/,Stanford University,,,United States of America,"R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning",Highly cited,,Semi-supervised recursive autoencoders for predicting sentiment distributions,,,,"They use several datasets for self-supervised and supervised learning
",Unknown,,,1477.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
NTM,Other,,2014-12-10,,,,,https://arxiv.org/abs/1410.5401,Google DeepMind,,,Multinational,"Alex Graves, Greg Wayne, Ivo Danihelka",Highly cited,,Neural Turing Machines,,,,,Unknown,,,2150.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
fastText,Language,,2016-07-06,,,,,https://arxiv.org/abs/1607.01759,Facebook AI Research,,,United States of America,"A Joulin, E Grave, P Bojanowski, T Mikolov",Highly cited,,Bag of Tricks for Efficient Text Classification,,,,,Unknown,,,4118.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Deconvolutional Network,Vision,,2010-06-13,,,,,https://ieeexplore.ieee.org/document/5539957,New York University (NYU),,,United States of America,"Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus",Highly cited,,Deconvolutional Networks,,,,,Unknown,,,1516.00,,,,,Inference time of the largest model was 55s on Caltech 101 images.,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
RefineNet,Vision,Object detection,2016-11-20,,,,,https://arxiv.org/abs/1611.06612v3,"University of Adelaide,Australian Centre for Robotic Vision",,,"Australia,Australia","Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",Highly cited,,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,,,,,Unknown,,,2567.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
BaGuaLu,"Multimodal,Language,Vision","Language modelling,Image classification",2022-03-28,,,,,https://dl.acm.org/doi/abs/10.1145/3503221.3508417,"Tsinghua University,Zhejiang Lab,Beijing Academy of Artificial Intelligence,Alibaba",,"The 174T parameter system was not trained, the paper simply demonstrated they were able to train it for a few iterations.

Calculations below give some rough estimates of FLOP for full training.

From Table 5, sustained performance was 230 PFLOPS. Assuming they ran a 2-month training run, C=230e12*60**2*24*8=1.6e20 FLOP.

Using C=6ND with D=17.5B tokens and N=173.9T parameters, we get C=1.83e+25 FLOP. However, it's an MoE with 96e3 experts.

If we assume that scaling was perfect, then C=1.8e25/93e3=1.9e20 FLOP.

If we assume that they got similar utilisation to the Switch transformer, i.e. 0.10% of params active at a time, then C=1.8e25 * 0.001=1.8e22 FLOP.","China,China,China,China","Zixuan Ma, Jiaao He, Jiezhong Qiu, Huanqi Cao, Yuanwei Wang, Zhenbo Sun, Liyan Zheng, Haojie Wang, Shizhi Tang, Tianyu Zheng, Junyang Lin, Guanyu Feng, Zeqiang Huang, Jie Gao, Aohan Zeng, Jianwei Zhang, Runxin Zhong, Tianhui Shi, Sha Liu, Weimin Zheng, Jie Tang, Hongxia Yang, Xin Liu, Jidong Zhai, Wenguang Chen",,,BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores,M6-Corpus,"""The data are collected from different sources, including encyclopedias, ecommerce platforms, and other crawled web pages. The detailed statistics of the final processed dataset are reported in Table 4, where ""#Img"" refers to the number of distinct images, ""#Tok"" to the number of distinct tokens... after
the images transformed to features, the final product was a dataset of size 16 TB.""",13200000000,"17.5B tokens (in English, this is approximately 13.1B words, but the conversion may be different in Chinese) and 60.5M images.",,,,28.00,173900000000000.00,"Table 3, MoDa-174T has 173.9 trillion parameters",,,,,,,Self-supervised learning,,,Industry,"Large-scale pretrained AI models have shown state-of-the-art accuracy in a series of important applications. As the size of pretrained AI models grows dramatically each year in an effort to achieve higher accuracy, training such models requires massive computing and memory capabilities, which accelerates the convergence of AI and HPC. However, there are still gaps in deploying AI applications on HPC systems, which need application and system co-design based on specific hardware features.

To this end, this paper proposes BaGuaLu1, the first work targeting training brain scale models on an entire exascale supercomputer, the New Generation Sunway Supercomputer. By combining hardware-specific intra-node optimization and hybrid parallel strategies, BaGuaLu enables decent performance and scalability on unprecedentedly large models. The evaluation shows that BaGuaLu can train 14.5-trillion-parameter models with a performance of over 1 EFLOPS using mixed-precision and has the capability to train 174-trillion-parameter models, which rivals the number of synapses in a human brain.",2024-05-13 09:17,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
Culturome,Language,,2010-12-16,,,,,https://science.sciencemag.org/content/331/6014/176,Harvard University,,,United States of America,"JB Michel, YK Shen, AP Aiden, A Veres, MK Gray",Highly cited,,Quantitative Analysis of Culture Using Millions of Digitized Books,,,,,Unknown,,,2588.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Stacked hourglass network,Vision,Pose estimation,2016-09-17,,,,,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29,University of Michigan,,,United States of America,"Alejandro Newell, Kaiyu Yang, Jia Deng",Highly cited,,Stacked Hourglass Networks for Human Pose Estimation,,,,,Unknown,,,4511.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
PointNet,3D modeling,3D segmentation,2016-12-02,,,,,https://arxiv.org/abs/1612.00593,Stanford University,,,United States of America,"CR Qi, H Su, K Mo, LJ Guibas",Highly cited,,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,,,,,Unknown,,,10993.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Jurassic-X,Language,,2022-05-03,,,,,https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system,AI21 Labs,,,Israel,"Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz",,,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",,,,,,,,42.00,7000000000.00,,,,,,,,,,,Industry,,2024-04-03 11:50,Robi Rahman,,,Jurassic-1-Jumbo,,,,,,,,,Industry,,,,,Industry,,,
Learning deep architectures,Other,,2009-11-15,,,,,https://www.nowpublishers.com/article/Details/MAL-006,"University of Montreal / Université de Montréal,Microsoft Research",,,"Canada,United States of America",Y Bengio,Highly cited,,Learning deep architectures for AI,,,,,Unknown,,,9782.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
BellKor 2007,Recommendation,Movie ratings,2009-09-21,,,,,https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba,AT&T,,,United States of America,"RM Bell, Y Koren, C Volinsky",SOTA improvement,Won Netflix prize,The BellKor solution to the Netflix Prize,Netflix Prize,,100480507,"The training data set consists of 100,480,507
ratings",,,,241.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Two-stream ConvNets for action recognition,Video,Video classification,2014-06-09,,,,,https://arxiv.org/abs/1406.2199,University of Oxford,,,United Kingdom of Great Britain and Northern Ireland,"Karen Simonyan, Andrew Zisserman",Highly cited,,Two-Stream Convolutional Networks for Action Recognition in Videos,,,,,Unknown,,,6891.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MuZero VP9,Video,Video compression,2022-02-14,,,,,https://arxiv.org/abs/2202.06626,DeepMind,,"Definitely < 1e23 FLOP: ""All experiments in this paper are run using Google Cloud TPUs [Google, 2018]. The learner processes use two 3rd generation TPUs, and the actor processes use four 2nd generation TPUs for 15 hours.""",United Kingdom of Great Britain and Northern Ireland,"Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Jackson Broshear, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Timothy Mann",,,MuZero with Self-competition for Rate Control in VP9 Video Compression,,,,,Unknown,,,32.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Gradient Boosting Machine,Other,,2001-10-01,,,,,https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full,Stanford University,,,United States of America,Jerome H. Friedman,Highly cited,,Greedy function approximation: A gradient boosting machine,,,,,Unknown,,,17891.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
CICERO,Games,Diplomacy,2022-11-22,"creative commons (non comm) for model weights, MIT for code

https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights",Open access (non-commercial),,Open source,https://www.science.org/doi/10.1126/science.ade9097,Meta AI,,,United States of America,"Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra",SOTA improvement,"""We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy""",Human-level play in the game of Diplomacy by combining language models with strategic reasoning,WebDiplomacy,,,"""We obtained a dataset of 125,261 games of Diplomacy played online at webDiplomacy.net. Of these, 40,408 games contained dialogue, with a total of 12,901,662 messages exchanged between players. Player accounts were de-identified and automated redaction of personally identifiable information (PII) was performed by webDiplomacy. We refer to this dataset hereafter as WebDiplomacy .""",Unknown,,,195.00,,"""We took R2C2 (22) as our base model – a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).""",,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Rotation,"Image generation,Vision",Image completion,2018-03-21,,,,,https://arxiv.org/abs/1803.07728,École des Ponts ParisTech,,,France,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",Highly cited,,Unsupervised Representation Learning by Predicting Image Rotations,,,,,,,,2843.00,86000000.00,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,Academia,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Wu Dao 2.0,"Multimodal,Language,Vision,Image generation","Image captioning,Chat,Image generation",2021-05-31,,API access,,,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,Beijing Academy of Artificial Intelligence,,"It's a mixture-of-experts model, so 1.75 trillion params likely overstates how much compute was required:

""The parameter scale of Enlightenment 2.0 reached a record-breaking 1.75 trillion. According to reports, the new generation FastMoE technology is the key to the realization of the ""Trillion Model"" cornerstone of Enlightenment 2.0.""

",China,,,,China's gigantic multi-modal AI is no one-trick pony,WuDao Corpora,"WuDao Corpora, as of version 2.0, was a large dataset constructed for training Wu Dao 2.0. It contains 3 terabytes of text scraped from web data, 90 terabytes of graphical data (incorporating 630 million text/image pairs), and 181 gigabytes of Chinese dialogue (incorporating 1.4 billion dialogue rounds).
https://en.wikipedia.org/wiki/Wu_Dao#WuDao_Corpora",,,,,,0.00,1750000000000.00,"""It's been trained on 1.75 trillion parameters""",,,,,,,,,,Industry,"Sporting 1.75 trillion parameters, Wu Dao 2.0 is roughly ten times the size of Open AI's GPT-3.",2024-05-13 09:43,Robi Rahman,,,,,,,,,,,,Academia,checked,,,,Academia,,,
DeepFace,Vision,Face verification,2014-06-23,,,,,https://ieeexplore.ieee.org/document/6909616,"Tel Aviv University,Facebook",,,"Israel,United States of America","Y Taigman, M Yang, MA Ranzato",Highly cited,,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,,,,,Unknown,,,5833.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
LIRA,Vision,Character recognition,2004-07-30,,,,,https://www.sciencedirect.com/science/article/abs/pii/S0262885604000721,Instituto de Ciencias Aplicadas y Technologia,,The coding time was 20 h and the training time was 45 h.,Mexico,"E Kussul, T Baidyk",,,Improved method of handwritten digit recognition tested on MNIST database,,,10000,,,,,188.00,100000.00,"""For the first modification of the Rosenblatt perceptron 10 neurons were included into the R-layer. [...] The number of the A-layer neurons was 256,000"" The relation between the S-layer and A-layer is hardcoded",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
STeLLA,Robotics,,1963-06-01,,,,,https://www.sciencedirect.com/science/article/pii/S1474667017696824?via%3Dihub,University of Canterbury,,,New Zealand,J.H. Andreae and Peter L. Joyce,,,STeLLA: A Scheme for a Learning Machine,,,,,Unknown,,,34.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
CapsNet (MultiMNIST),Vision,Character recognition,2017-10-26,,,,,https://arxiv.org/abs/1710.09829,Google Brain,,,United States of America,"S Sabour, N Frosst, GE Hinton",Highly cited,,Dynamic Routing Between Capsules,,,,,,,,4035.00,11360000.00,"""This model has 24.56M parameters which is 2 times more parameters
than CapsNet with 11.36M parameters.""",,,,,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
LDA,Language,Document classification,2003-02-02,,,,,https://jmlr.org/papers/volume3/blei03a/blei03a.pdf,Stanford University,,,United States of America,"David M. Blei, Andrew Y. Ng, Michael I. Jordan",Highly cited,,Latent Dirichlet Allocation,,,,Multiple experiments with different tasks and datasets,Unknown,,,38724.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
PointNet++,3D modeling,3D segmentation,2017-06-07,,,,,https://arxiv.org/abs/1706.02413,Stanford University,,,United States of America,"Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas",Highly cited,,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,,,,,Unknown,,,8118.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Thumbs Up?,Language,Sentiment classification,2002-05-28,,,,,https://arxiv.org/abs/cs/0205070,"Cornell University,IBM",,,"United States of America,United States of America","Bo Pang, Lillian Lee, Shivakumar Vaithyanathan",Highly cited,,Thumbs up? Sentiment Classification using Machine Learning Techniques,IMDb,,2053,"yielding a corpus of 752 negative and
1301 positive reviews",,,,10656.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Cutout-regularized net,Vision,Image classification,2017-08-15,,,,,https://arxiv.org/abs/1708.04552,"University of Guelph,Vector Institute,CIFAR AI Research",,,"Canada,Canada,Canada"," Terrance DeVries, Graham W. Taylor",Highly cited,,Improved Regularization of Convolutional Neural Networks with Cutout,,,,,Unknown,,,3063.00,,,,,,,,,,,https://www.yuzeh.com/data/agz-cost.html,Industry,,2024-05-22 13:15,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Research collective",,,,,"Academia,Academia,Research collective",,,
FixRes ResNeXt-101 WSL,Vision,Image classification,2019-06-14,code/weights with non-commercial license: https://github.com/facebookresearch/FixRes?tab=License-1-ov-file#readme,Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/1906.06423,Facebook AI,,,United States of America,"H Touvron, A Vedaldi, M Douze, H Jégou",SOTA improvement,"""To the best of our knowledge our ResNeXt-101 32x48d surpasses all other models available in the literature""",Fixing the train-test resolution discrepancy,ImageNet,,940000000,"""Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)""",,,,405.00,829000000.00,,,,,,,,,,"https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4#:~:text=According%20to%20the%20analysis%20by,Source%3A%20DeepMind.",Industry,,2024-04-17 15:34,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Character-enriched word2vec,Language,,2016-07-15,,,,,https://arxiv.org/abs/1607.04606,Facebook AI Research,,,United States of America,"P Bojanowski, E Grave, A Joulin",Highly cited,,Enriching Word Vectors with Subword Information,,,,,Unknown,,,8845.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
InstructGPT,Language,,2022-01-27,,,,,https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf,OpenAI,,,United States of America,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike","Historical significance,Highly cited",,Training language models to follow instructions with human feedback,,,374000033207,"Table 6 - describes **number of prompts**

26584 + 6623 = 33207

This is added to GPT-3 dataset size.",,,,5461.00,175000000000.00,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,,,,Self-supervised learning,,,Industry,,2024-05-06 03:33,Robi Rahman,,,GPT-3 175B (davinci),,,,,,,,,Industry,checked,,,,Industry,,,
Spectrally Normalized GAN,Image generation,Image generation,2018-02-16,,,,,https://arxiv.org/abs/1802.05957,"Preferred Networks Inc,Ritsumeikan University,National Institute of Informatics",,,"Japan,Japan,Japan","Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida",Highly cited,,Spectral Normalization for Generative Adversarial Networks,,,,,Unknown,,,3884.00,,,,,,,,,,,,Industry,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
DeepLabV3,Vision,Semantic segmentation,2017-06-17,,,,,https://arxiv.org/abs/1706.05587,Google,,,United States of America,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",Highly cited,,Rethinking Atrous Convolution for Semantic Image Segmentation,,,,,Unknown,,,6912.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
BLSTM for handwriting (1),Vision,,2007-09-23,,,,,https://people.idsia.ch//~juergen/icdar_2007.pdf,"University of Bern,IDSIA,Technical University of Munich",,,"Switzerland,Switzerland,Germany","M Liwicki, A Graves, S Fernàndez",SOTA improvement,,A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks,,,,,Unknown,,,287.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Deep LSTM for video classification,Video,Video,2015-05-01,,,,,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html,"University of Texas at Austin,Google",,,"United States of America,United States of America","Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici",Highly cited,,Beyond Short Snippets: Deep Networks for Video Classification,,,,,Unknown,,,2260.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
SRN-Encoded Grammatical Structures,Language,,1991-09-01,,,,,https://dl.acm.org/doi/10.1007/BF00114844,UC San Diego,,,United States of America,J. L. Elman,Highly cited,,"Distributed representations, simple recurrent networks, and grammatical structure",,,177805,4 training sets of 10k sentences each. Total number of words calculated by multiplying 10k and the avg. number of words per sentence in the training set.,,,,1717.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Spatial Pyramid Matching,Vision,Image classification,2006-06-17,,,,,https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf,"INRIA,Ecole Normale,University of Illinois Urbana-Champaign (UIUC)",,,"France,France,United States of America","S Lazebnik, C Schmid, J Ponce",Highly cited,,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,,,,,Unknown,,,9807.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Multi-scale Dilated CNN,Vision,Image segmentation,2015-11-23,,,,,https://arxiv.org/abs/1511.07122,"Princeton University,Intel Labs",,,"United States of America,Multinational","Fisher Yu, Vladlen Koltun",Highly cited,,Multi-Scale Context Aggregation by Dilated Convolutions,,,,,Unknown,,,7479.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
HOGWILD!,Other,,2011-11-11,,,,,https://arxiv.org/abs/1106.5730,University of Wisconsin Madison,,,United States of America,"F Niu, B Recht, C Ré, SJ Wright",Highly cited,,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,,,,,Unknown,,,2157.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Adaptive Subgrad,Other,,2011-10-03,,,,,https://dl.acm.org/doi/10.5555/1953048.2021068,"Technion - Israel Institute of Technology,Google,UC Berkeley",,,"Israel,United States of America,United States of America","J Duchi, E Hazan, Y Singer",Highly cited,,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,,,,,Unknown,,,9616.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
GAN-Advancer,Vision,Image classification,2016-12-05,,,,,https://dl.acm.org/doi/10.5555/3157096.3157346,OpenAI,,,United States of America,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen",Highly cited,,Improved Techniques for Training GANs,,,,,Unknown,,,7645.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Fast R-CNN,Vision,Object detection,2015-04-30,,,,,https://arxiv.org/abs/1504.08083,Microsoft Research,,,United States of America,R Girshick,Highly cited,,Fast R-CNN,,,,,Unknown,,,21241.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
PAPA,Other,Binary classification,1961-09-01,,,,,https://www.semanticscholar.org/paper/Further-experiments-with-PAPA-Gamba-Gamberini/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2,University of Genoa,,,Italy,"A Gamba, L Gamberini, G Palmieri, R Sanna",,,Further experiments with PAPA,,,,,Unknown,,,24.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
GloVe (6B),Language,Semantic embedding,2014-01-01,,,,,https://nlp.stanford.edu/projects/glove/,Stanford University,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

Details of dual 2.1GHz Intel Xeon E5-2658 machine:
https://www.intel.com/content/www/us/en/products/sku/61428/intel-xeon-processor-e52658-20m-2-10-ghz-8-0-gts-intel-qpi/specifications.html",United States of America,"J Pennington, R Socher, CD Manning",Highly cited,,GloVe: Global Vectors for Word Representation,Gigaword5 + Wikipedia2014,,6000000000,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",,,,29074.00,120000000.00,400k vocab * 300 vector dimensions,,,Embeddings are precalculated,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,,Academia,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Wide & Deep,Recommendation,,2016-06-24,,,,,https://arxiv.org/abs/1606.07792,Google,,,United States of America,"HT Cheng, L Koc, J Harmsen, T Shaked",Highly cited,,Wide & Deep Learning for Recommender Systems,,,,,Unknown,,,3034.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
CLIP (ResNet-50),"Multimodal,Vision,Language,Video","Zero-shot image classification,Character recognition,Video description",2021-01-05,,,,,https://arxiv.org/abs/2103.00020,OpenAI,,,United States of America,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever","Highly cited,SOTA improvement",,Learning Transferable Visual Models From Natural Language Supervision,,Custom image-text pairs from the internet,400000000,,,,,12732.00,88600000.00,"Image encoder
~ResNet-50 (from paper)
25.6M params

Text encoder
~Transformer (from paper)
63M params",,7000000.00,Figure 10 https://arxiv.org/pdf/2103.00020.pdf,,,,,,,Industry,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",2024-05-13 10:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
MADALINE III,Other,,1990-09-01,,,,,https://ieeexplore.ieee.org/document/58323,Stanford University,,,United States of America,"B Widrow, M. A. Lehr",Highly cited,,"30 years of adaptive neural networks: perceptron, madaline, and backpropagation",,,,,Unknown,,,3013.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Mask R-CNN,Vision,Image segmentation,2017-03-30,,,,,https://arxiv.org/abs/1703.06870,Facebook AI Research,,,United States of America,"Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick",Highly cited,,Mask R-CNN,COCO,,,,Unknown,,,21790.00,,,,,,,"Training with
ResNet-50-FPN on COCO trainval35k takes 32 hours
in our synchronized 8-GPU implementation (0.72s per 16-
image mini-batch), and 44 hours with ResNet-101-FPN",,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Differentiable neural computer,Other,,2016-10-12,,,,,https://www.nature.com/articles/nature20101,Google DeepMind,,,Multinational,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis",Highly cited,,Hybrid computing using a neural network with dynamic external memory,,,,,Unknown,,,1444.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Statistical Shape Constellations,Vision,Image classification,2003-01-01,,,,,https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf,California Institute of Technology,,,United States of America,"M. Weber, M. Welling, and P. Perona",Historical significance,,Unsupervised Learning of Models for Recognition,,,,,Unknown,,,949.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
EfficientDet,Vision,Object detection,2020-07-27,"Repo is Apache 2.0:

https://github.com/google/automl/tree/master/efficientdet",Open source,,Open source,https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html,Google Brain,,,United States of America,"Mingxing Tan, Ruoming Pang, Quoc V. Le","Highly cited,SOTA improvement","""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",EfficientDet: Scalable and Efficient Object Detection,COCO 2017,,,,,,,4701.00,77000000.00,"""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,410000000000.00,"""In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,,,,,,Industry,,2024-04-24 14:39,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Samuel Neural Checkers II,Games,Checkers,1967-11-01,,,,,https://www.cs.virginia.edu/~evans/greatworks/samuel.pdf,University of Geneva,,,Switzerland,"Palmieri, G. and R. Sanna",,,Some studies in machine learning using the game of checkers. Part II,,,,,,,,747.00,40.00,"""The total number of parameters used at any one time has been varied from a very few to as many as 40""",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Probabilistic modeling for object recognition,Vision,Face recognition,1998-06-23,,,,,https://ieeexplore.ieee.org/document/698586,Carnegie Mellon University (CMU),,,United States of America,"H Schneiderman, T Kanade",,,Probabilistic modeling of local appearance and spatial relationships for object recognition,,,120472,"Section 5.1: ""We formed training sets from 991 faces images and 1,552
non-face images.""
""For each face image we generated
120 synthetic variations""",,,,602.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MT-DNN,Language,,2019-01-31,MIT for code/weights: https://github.com/namisan/mt-dnn,Open source,,Open source,https://arxiv.org/abs/1901.11504,Microsoft,,,United States of America,"X Liu, P He, W Chen, J Gao","Highly cited,SOTA improvement","""MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement)""",Multi-Task Deep Neural Networks for Natural Language Understanding,"GLUE,SciTail","GLUE, SNLI, and SciTail ",,,,,,1217.00,330000000.00,,,,,,,,,,,Industry,,2024-04-15 16:30,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Random Decision Forests,Other,,1995-08-14,,,,,https://ieeexplore.ieee.org/document/598994,"AT&T,Bell Laboratories",,,"United States of America,United States of America",TK Ho,Highly cited,,Random decision forests,MNIST,,60000,The images are from the 1992 NIST (National Institute of Standards and Technology) Competition,,,,4678.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
AmoebaNet-A (F=190),Vision,Image classification,2018-02-05,,,,,https://arxiv.org/abs/1802.01548,Google Brain,,,United States of America,"E Real, A Aggarwal, Y Huang, QV Le",Highly cited,,Regularized Evolution for Image Classifier Architecture Search,,,,,,,,2596.00,87000000.00,Table 2,,,,,,,,,,Industry,,2024-04-01 09:45,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
GRUs,Language,,2014-06-03,,,,,https://arxiv.org/abs/1406.1078,"University of Montreal / Université de Montréal,Jacobs University,University of Maine",,,"Canada,Germany,United States of America","K Cho, B Van Merriënboer, C Gulcehre",Highly cited,,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,,,,,Unknown,,,20326.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
BatchNorm,Other,,2015-06-15,,,,,https://arxiv.org/abs/1502.03167,Google,,,United States of America,"S Ioffe, C Szegedy",Highly cited,,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,,,,,Unknown,,,38974.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Optimized Multi-Scale Edge Detection,Vision,,1986-11-01,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851,Massachusetts Institute of Technology (MIT),,,United States of America,John Canny,Highly cited,,A Computational Approach To Edge Detection,,,,,Unknown,,,37931.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
CURL,Games,Atari,2020-04-08,"MIT: https://github.com/MishaLaskin/curl

",Open source,,Open source,https://arxiv.org/abs/2004.04136v4,UC Berkeley,,,United States of America,"A Srinivas, M Laskin, P Abbeel",SOTA improvement,,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,,"RL on Atari:

""We measure the data-efficiency and performance of our
method and baselines at 100k and 500k environment steps
on DMControl and 100k interaction steps (400k environment steps with action repeat of 4) on Atari, which we will
henceforth refer to as DMControl100k, DMControl500k
and Atari100k for clarity. While Atari100k benchmark has been common practice when investigating data-efficiency
on Atari (Kaiser et al., 2019; van Hasselt et al., 2019; Kielak,
2020), the DMControl benchmark was set at 500k environment steps because state-based RL approaches asymptotic
performance on many environments at this point, and 100k
steps to measure the speed of initial learning. A broader
motivation is that while RL algorithms can achieve superhuman performance on Atari games, they are still far less
efficient than a human learner. Training for 100-500k environment steps corresponds to a few hours of human time.""",,,,,,848.00,907264.00,,,,,,,,,,,Academia,,2024-05-01 09:06,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Peephole LSTM,Mathematics,Periodic function approximation,2000-07-26,,,,,https://ieeexplore.ieee.org/document/861302,IDSIA,,,Switzerland,F.A. Gers; J. Schmidhuber,,,Recurrent nets that time and count,,,64970000,See Table 2,,,,630.00,17.00,"""In absence of the 3 peephole connections there are 14 adjustable weights""",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
TSN,Video,Action recognition,2016-09-17,,,,,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2,"ETH Zurich,Shenzhen Institute of Advanced Technology,Chinese University of Hong Kong (CUHK)",,,"Switzerland,China,Hong Kong","Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",Highly cited,,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,,,,,Unknown,,,3365.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Photo-Geometric Autoencoder,3D modeling,,2019-11-25,MIT license: https://github.com/elliottwu/unsup3d,Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/1911.11130,University of Oxford,,,United Kingdom of Great Britain and Northern Ireland,"Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
",SOTA improvement,"""Our model outperforms a
current state-of-the-art 3D reconstruction method that uses 2D keypoint supervision""",Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,"CelebA,3DFAW,BFM","""We test our method on three human face
datasets: CelebA [35], 3DFAW [21, 27, 73, 69] and
BFM [47]""",,,Unknown,,,267.00,,,,,,,,,,,,Academia,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Deep Boltzmann Machines,Other,,2009-04-16,,,,,https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine,University of Toronto,,,Canada,"Ruslan Salakhutdinov, Geoffrey Hinton",Highly cited,,Deep Boltzmann Machines,,,,,Unknown,,,2666.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
ELMo,Language,,2018-02-01,,,,,https://arxiv.org/abs/1802.05365,"University of Washington,Allen Institute for AI",,3300e12 - https://github.com/amirgholami/ai_and_memory_wall,"United States of America,United States of America","ME Peters, M Neumann, M Iyyer, M Gardner",Highly cited,,Deep contextualized word representations,,,,,,,,10628.00,94000000.00,,,26000000000.00,"Rados dataset (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,Self-supervised learning,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,"Academia,Research collective",,,,,"Academia,Research collective",,,
CNN Best Practices,Vision,Character recognition,2003-08-06,,,,,https://ieeexplore.ieee.org/document/1227801,Microsoft Research,,,United States of America,"PY Simard, D Steinkraus, JC Platt",Highly cited,,Best practices for convolutional neural networks applied to visual document analysis,MNIST,,50000,,,,,3065.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DBN for NLP,Language,Text classification,2014-02-11,,,,,https://ieeexplore.ieee.org/document/6737243,"Microsoft,University of Toronto",,,"United States of America,Canada","R Sarikaya, GE Hinton, A Deoras",,,Application of Deep Belief Networks for Natural Language Understanding,,,178000,The training data has 27K automatically transcribed utterances amounting to 178K words.,,,,445.00,1021535.00,"Assuming 1000 input features, 35 classes and 3 hidden layers of 500 units each",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
NASNet-A,Vision,Image classification,2017-07-21,,,,,https://arxiv.org/abs/1707.07012,Google Brain,,,United States of America,"B Zoph, V Vasudevan, J Shlens",Highly cited,,Learning Transferable Architectures for Scalable Image Recognition,,,,,,,,4949.00,89000000.00,,,,,,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ResNet-1001,Vision,Image classification,2016-09-17,,,,,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,Microsoft,,"""On CIFAR, ResNet-1001 takes about 27 h to train on 2 GPUs""",United States of America,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,Identity Mappings in Deep Residual Networks,"CIFAR-10,CIFAR-100",,,,,,,8927.00,10200000.00,,,,,,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ViT-Base/32,Vision,Image representation,2020-10-22,,,,,https://arxiv.org/abs/2010.11929,Google Brain,,,United States of America,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",Highly cited,,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,,,,,,,,20003.00,86000000.00,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ShuffleNet v2,Vision,,2018-06-30,,,,,https://arxiv.org/abs/1807.11164,"Tsinghua University,Megvii Inc",,,"China,China","N Ma, X Zhang, HT Zheng",Highly cited,,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,,,,,,,,3750.00,2280000.00,,,300000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:52,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Conv-DBN,Vision,,2009-06-14,,,,,https://dl.acm.org/doi/10.1145/1553374.1553453,Stanford University,,,United States of America,"H Lee, R Grosse, R Ranganath, AY Ng",Highly cited,,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,,,,,Unknown,,,2964.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Max-Margin Markov Networks,"Vision,Language",,2004-03-01,,,,,https://papers.nips.cc/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf,Stanford University,,,United States of America,"B. Taskar, C. Guestrin, and D. Koller",Highly cited,,Max-margin markov networks,,,600,"The data set is divided into 10 folds of ∼ 600 training and ∼ 5500 testing examples.
The accuracy results, ... are averages over the 10 folds",,,,1764.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Youtube recommendation model,Recommendation,,2016-09-15,,,,,https://research.google/pubs/pub45530/,Google,,,United States of America,"Paul Covington, Jay Adams, and Emre Sargin",Highly cited,,Deep Neural Networks for YouTube Recommendations,,,,,Unknown,,,2604.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DMN,Language,,2016-06-20,,,,,https://arxiv.org/abs/1506.07285,Salesforce,,,United States of America,"Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher",Highly cited,,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,,,,,Unknown,,,1187.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Immediate trihead,Language,,2001-07-06,,,,,https://dl.acm.org/doi/10.3115/1073012.1073029,Brown University,,,United States of America,E Charniak,SOTA improvement,"""The perplexity for both of these models significantly improve
upon the trigram model base-line as
well as the best previous grammar based language model""",Immediate-Head Parsing for Language Models,,,,,Unknown,,,422.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
PSPNet,Vision,Image segmentation,2017-07-21,,,,,https://ieeexplore.ieee.org/document/8100143,Chinese University of Hong Kong (CUHK),,,Hong Kong,"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",Highly cited,,Pyramid Scene Parsing Network,,,,,Unknown,,,9696.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
CapsNet (MNIST),Vision,Character recognition,2017-10-26,,,,,https://arxiv.org/abs/1710.09829,Google Brain,,"It should be feasible to estimate this from the information in the paper, but it would require carefully checking the FLOP involved for capsules.",United States of America,"S Sabour, N Frosst, GE Hinton",Highly cited,,Dynamic Routing Between Capsules,MNIST,,60000,Section 5: The dataset has 60K and 10K images for training and testing respectively.,,,,4035.00,8200000.00,"""In terms of number of parameters the baseline has 35.4M while CapsNet
has 8.2M parameters and 6.8M parameters without the reconstruction subnetwork""",,,,,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Heuristic problem solving for AI,Other,,1961-01-01,,,,,https://ieeexplore.ieee.org/abstract/document/4066245,Massachusetts Institute of Technology (MIT),,,United States of America,Marvin Minsky,Highly cited,,Steps Toward Artificial Intelligence,,,,,Unknown,,,2430.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Social and content-based classification,Recommendation,Recommender system,1998-07-01,,,,,https://www.aaai.org/Papers/AAAI/1998/AAAI98-101.pdf,"AT&T,Bell Laboratories,Rutgers University",,,"United States of America,United States of America,United States of America","C Basu, H Hirsh, W Cohen",Highly cited,,Recommendation as Classification: Using Social and Content-based Information in Recommendation,,,45000,"""Our data set consists of more than 45,000 movie rat-
ings collected from approximately 260 users.""",,,,1564.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Industry,Academia",,,,,"Industry,Industry,Academia",,,
LRSO-GAN,Vision,Person re-identification,2017-10-22,,,,,https://arxiv.org/abs/1701.07717,University of Technology Sydney,,,Australia,"Zhedong Zheng, Liang Zheng, Yi Yang",Highly cited,,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,,,,,Unknown,,,1685.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
FrameNet role labeling,Language,,2000-09-01,,,,,https://dl.acm.org/doi/10.1162/089120102760275983,University of Rochester,,,United States of America,"Daniel Gildea, Daniel Jurafsky",Highly cited,,Automatic Labeling of Semantic Roles,FrameNet,,50000,"Abstract: ""The system is based on statistical classifiers trained on roughly 50,000 sentences""",,,,2499.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
StarGAN v2,Vision,,2019-12-04,https://github.com/clovaai/stargan-v2?tab=readme-ov-file non-commercial,Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/1912.01865,"NAVER,Yonsei University,Swiss Federal Institute of Technology",,,"Korea (Republic of),Korea (Republic of),Switzerland","Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha","Highly cited,SOTA improvement","""Votes from AMT workers for the most preferred method
regarding visual quality and style reflection (%). StarGAN v2 outperforms the baselines with remarkable margins in all aspects.""",StarGAN v2: Diverse Image Synthesis for Multiple Domains,"CelebA,AFHQ","""Datasets. We evaluate StarGAN v2 on CelebA-HQ [21] and
our new AFHQ dataset (Appendix A)""",,,Unknown,,,1318.00,,,,,,,,,,,,Industry,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Markov-driven POS tagger,Language,Part-of-speech tagging,1994-06-01,,,,,https://dl.acm.org/doi/10.5555/972525.972526,EURECOM,,,France,Bernard Merialdo,,,Tagging English Text with a Probabilistic Model,,,1000000,"""We use the ""treebank"" data described in Beale (1988). It contains 42,186 sentences (about one million words) from the Associated Press.""
https://www.aclweb.org/anthology/J94-2001.pdf",,,,788.00,2447124.00,"""The total number of free parameters is then:
(Nw - 1).NT + (NT - 1).NT.NT.""
Where:
Nw= Vocabulary size
NT = Number of tags

""In the treebank 159 different tags are used. These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix). The results quoted in this paper all refer to this smaller system""
So NT = 76

https://www.aclweb.org/anthology/J94-2001/

There is no direct reference to Nw, but the data is from ""Lexicon and grammar in probabilistic tagging of written English."" which says

""(the new CLAWS lexicón has almost 26,500 entries)""
So tentatively Nw=26500

https://dl.acm.org/doi/10.3115/982023.982049",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Sparse Energy-Based Model,Vision,Character recognition,2006-12-04,,,,,https://papers.nips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html,New York University (NYU),,,United States of America,"M Ranzato, C Poultney, S Chopra, Y Cun",Highly cited,,Efficient Learning of Sparse Representations with an Energy-Based Model,MNIST,,60000,,,,,1601.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
DeepLab,Vision,Image segmentation,2014-12-22,,,,,https://arxiv.org/abs/1412.7062,"Google,University of California Los Angeles (UCLA)",,,"United States of America,United States of America","Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",Highly cited,,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,,,,,Unknown,,,4391.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
MobileBERT,Language,Text autocompletion,2020-04-06,,,,,https://arxiv.org/abs/2004.02984,"Carnegie Mellon University (CMU),Google Brain",,,"United States of America,United States of America","Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou",,,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,,,,,,,,620.00,25300000.00,Rados,,5360000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
RCAN,"Image generation,Vision",Image super-resolution,2018-07-08,,,,,https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html,Northeastern University,,,United States of America," Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu",Highly cited,,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,,,,,Unknown,,,3383.00,,,,,,,,,,,,Academia,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Perceiver IO (optical flow),"Multimodal,Language,Vision","Language modelling/generation,Image captioning",2020-02-08,,Unreleased,,Unreleased,https://arxiv.org/abs/2107.14795,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,
Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira",SOTA improvement,"""Perceiver IO... achieves state-of-the-art performance on Sintel optical flow estimation""",Perceiver IO: A General Architecture for Structured Inputs & Outputs,AutoFlow,"""In all cases, we train on the AutoFlow dataset (Sun et al., 2021), which consists of 400, 000 image
pairs, for 480 epochs using a cosine learning rate schedule which starts at a learning rate of 4e-4.
We use a batch size of 512. We use the LAMB (You et al., 2021) optimizer.""",,,,,,397.00,27900000.00,"Optical flow model (SOTA) was 27.9M params. There are other, larger models described in this paper, e.g. for language.

""For the pixel- and patch-based models, total computational
complexity for a forward pass on a 368 × 496 image is roughly 987 billion FLOPs, and there are
roughly 27.9 million parameters.""",,,,,,,,,,Industry,"A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",2024-05-13 10:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Error Propagation,Other,,1986-01-03,,,,,https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf,"UC San Diego,Carnegie Mellon University (CMU)",,,"United States of America,United States of America","D. E. Rumelhart, G. E. Hinton, and R. J. Williams",Highly cited,,Learning internal representations by error propagation,,,,,Unknown,,,27322.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
OverFeat,Vision,Image classification,2013-12-21,,,,,https://arxiv.org/abs/1312.6229,New York University (NYU),,,United States of America,"Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",Highly cited,,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",,,,,Unknown,,,5148.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MobileNetV2,Vision,"Object detection,Image classification,Image segmentation",2018-06-18,,,,,https://ieeexplore.ieee.org/document/8578572,Google,,,United States of America,"M Sandler, A Howard, M Zhu",Highly cited,,MobileNetV2: Inverted Residuals and Linear Bottlenecks,,,,,,,,14573.00,3400000.00,Rados,,600000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Multi-task Cascaded CNN,Vision,Face detection,2016-08-26,,,,,https://arxiv.org/abs/1604.02878,"Chinese Academy of Sciences,Chinese University of Hong Kong (CUHK)",,,"China,Hong Kong","Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao",Highly cited,,Joint Face Detection and Alignment using Multitask cascaded convolutional networks,,,,,Unknown,,,4282.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ESRGAN,"Vision,Image generation",Image super-resolution,2018-09-01,,,,,https://arxiv.org/abs/1809.00219,"Chinese University of Hong Kong (CUHK),Chinese Academy of Sciences,Nanyang Technological University",,,"Hong Kong,China,Singapore","Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang",Highly cited,,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,,,,,Unknown,,,2816.00,,,,,,,,,,,,Academia,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
MetaMimic,Games,,2018-10-11,,,,,https://arxiv.org/abs/1810.05017,Google,,,United States of America,"Tom Le Paine, Sergio Gomez",SOTA improvement,"""By retaining and taking advantage of all its experiences,
MetaMimic also substantially outperforms the state-of-the-art D4PG RL agent, when D4PG
uses only the current task experiences.""",One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,,,,,,,,26.00,22000000.00,"""This representational demand motivates the introduction of high-capacity deep neural networks. We found the architecture, shown in Figure 3, with residual connections, 20 convolution layers with 512 channels
for a total of 22 million parameters, and instance normalization to drastically improve performance, as shown in Figure 6 of the Experiments section.""",,,,,,,Reinforcement learning,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Stacked Semisuperviser Autoencoders,Language,Document representation,2008-07-15,,,,,https://dl.acm.org/doi/10.1145/1390156.1390256,"New York University (NYU),Microsoft",,,"United States of America,United States of America","MA Ranzato, M Szummer",,,Semisupervised learning of compact document representations with deep networks,,,66087,"""The 20 Newsgroups dataset contains 18845
postings taken from the Usenet newsgroup collection.
Documents are partitioned into 20 topics. The dataset
is split into 11314 training documents and 7531 test
documents. Training and test articles are separated in
time. Reuters has a predefined ModApte split of the
data into 11413 training documents and 4024 test doc-
uments. Documents belong to one of 91 topics. The
Ohsumed dataset has 34389 documents with 30689
words and each document might be assigned to more
than one topic, for a total of 23 topics. The dataset is
split into training and test by randomly selecting the
67% and the 33% of the data""

total # documents = 11314 + 11413 + 34389*0.6

I'm using #documents here since the task is document representation. Using #words would increase the size by ~3 OOMs",,,,243.00,3000000.00,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Histograms of Oriented Gradients,Vision,,2005-06-25,,,,,https://ieeexplore.ieee.org/document/1467360,INRIA,,,France,"N Dalal, B Triggs",Highly cited,,Histograms of oriented gradients for human detection,,,1805," we produced a new and significantly more
challenging data set, ‘INRIA’, containing 1805 64×128 im-
ages",,,,36578.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Perceptron for Large Margin Classification,Vision,Character recognition,1999-12-01,,,,,https://link.springer.com/article/10.1023/A:1007662407062,"UC San Diego,Shannon Laboratory,AT&T",,,"United States of America,United States of America,United States of America",Yoav Freund & Robert E. Schapire,Highly cited,,Large Margin Classification Using the Perceptron Algorithm,MNIST,,60000,"""The dataset consists of 60,000 training examples and 10,000 test examples.""",,,,1731.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Phrase-based translation,Language,Translation,2003-05-01,,,,,https://dl.acm.org/doi/10.3115/1073445.1073462,University of Southern California,,,United States of America,"Philipp Koehn, Franz Josef Och, Daniel Marcu",Highly cited,,Statistical Phrase-Based Translation,,,20000000,"[WORDS]
""We used the freely available Europarl corpus to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.""

""These results are consistent
over training corpus sizes from 10,000 sentence pairs to
320,000 sentence pairs. ""

So 20 million words or 320k sentence pairs.",,,,4270.00,9178890.00,"There are various components to the system:

- Translation probability model phi
- The distortion probability distribution d
- A langage model p_LM
- A length factor w

Several translation probability models are considered. The most performant one is the AP word alignment model. The sentence length preferred by the authors is 3 words maximum. In the biggest corpus considered (320k phrase pairs) it produces a phrase translation probability table of 1996k entries.

The distortion probability model d is taken from  (Marcu and Wong, 2002).

The distortion probability model must have ~10 parameters at most

The language model p_LM is a back off trigram model from (Seymore and Rosenfeld,1997). AFAIK the cutoff used is not specified. Based on the example on section 4.3 of (Seymore and Rosefeld, 1997), a trigram probability model has about 3866964 + 2674322 + 641604 parameters.

""For each possible phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase. As language model probability we use the unigram probability for the first word, the bigram probability for the second, and the trigram probability for all following words""

The length factor w is an additional single parameter.

""In order to calibrate the output length, we introduce a
factor w for each generated English word in addition to
the trigram language model ""

In summary, the parameter count seems to be dominated by the trigram language model and the word alignment phrase translation model. ",,,"""With our decoder, translating 1755 sentence of length 5-15 words
takes about 10 minutes on a 2 GHz Linux system.""",,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Phenaki,Video,Video generation,2022-10-05,,,,,https://arxiv.org/abs/2210.02399,"University College London (UCL),University of Michigan,Google Brain",,,"United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America","Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan",SOTA improvement,"""To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts""",Phenaki: Variable Length Video Generation From Open Domain Textual Description,,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,215.00,1800000000.00,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,,,,Self-supervised learning,,,,,2024-05-01 09:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Industry",checked,,,,"Academia,Academia,Industry",,,
Big Transfer (BiT-L),Vision,Image classification,2019-12-24,,Unreleased,,Unreleased,https://arxiv.org/abs/1912.11370,Google Brain,,,United States of America,"A Kolesnikov, L Beyer, X Zhai, J Puigcerver, J Yung",SOTA improvement,"""We transfer BiT to many diverse tasks... These tasks include ImageNet’s ILSVRC-2012 [10], CIFAR-10/100 [27], Oxford-IIIT Pet [41], Oxford
Flowers-102 [39] (including few-shot variants), and the 1000-sample VTAB-1k benchmark [66], which consists of 19 diverse datasets. BiT-L attains state-ofthe-art performance on many of these tasks",Large scale learning of general visual representations for transfer,JFT-300M,"""We train networks on three different scales of datasets. The largest, BiT-L
is trained on the JFT-300M dataset [51], which contains 300 M noisily labelled images""",,,,,,1003.00,928000000.00,,40.00,,,,,Google TPU v3,,,,Industry,,2024-05-01 09:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Megatron-LM (1T),Language,Text autocompletion,2021-04-09,,,,,https://arxiv.org/abs/2104.04473,"Microsoft Research,NVIDIA,Stanford University",,"NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation.

We calculate below the FLOP required for a full training, but we do not populate it in the Training Compute column.

“For the 1 trillion parameter model, we assume that 450 billion tokens are needed for end-to-end training. With 3072 A100 GPUs, we can achieve a per-GPU throughput of 163 teraFLOP/s, and end-to-end training time of 84 days. We believe these training times (using a reasonable number of GPUs) are practical.”

Table 1 gives a utilisation rate of 52%

Plugging this into the calculator: https://epochai.org/blog/estimating-training-compute
84 days, 3072 GPUs, NVIDIA A100, FP16, 52% utilisation rate --> 3.6e24 FLOP","United States of America,United States of America,United States of America","Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia",,Improved SOTA efficiency at distributed training.,Efficient Large-Scale Language Model Training on GPU Clusters,,Dataset information not provided.,,,Likely,,,292.00,1000000000000.00,"[NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation]

""Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.""",,,,,,NVIDIA A100,Self-supervised learning,,,Industry,"Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress.
In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.",2024-04-01 09:35,Robi Rahman,,,,,,,,,,,,"Industry,Industry,Academia",,,,,"Industry,Industry,Academia",,,
BPL,Image generation,Image generation,2015-12-11,,,,,https://science.sciencemag.org/content/350/6266/1332/,"University of Toronto,New York University (NYU),Massachusetts Institute of Technology (MIT)",,,"Canada,United States of America,United States of America","BM Lake, R Salakhutdinov, JB Tenenbaum",Highly cited,,Human-level concept learning through probabilistic program induction,,,,,Unknown,,,2708.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Learning past tenses,Language,Verb conjugation,1986-01-03,,,,,https://www.semanticscholar.org/paper/On-learning-the-past-tenses-of-English-verbs%3A-rules-Rumelhart-McClelland/4fa569625b5ab35e955a8d5be11a4aa9f59ca424,Stanford University,,,United States of America,"Rumelhart, D. E., & McClelland, J. L",,,Learning the past tenses of English verbs: Implicit rules or parallel distributed processing?,,,,,,,,318.00,211600.00,"Source: https://files.eric.ed.gov/fulltext/ED267419.pdf
p.9: network architecture is given, with two layers of hidden units. The hidden units are  called “Wickelfeature representation”. The “modifiable connections” are only between the hidden units. p.19: “All in all then, we used only 460 of the 1,210 possible Wickelfeatures. Using this representation, a verb is represented by a pattern of activation over a set of 460 Wickelfeature units.""",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
VQGAN + CLIP,Image generation,Text-to-image,2020-12-17,,,,,https://arxiv.org/abs/2012.09841,Heidelberg University,,,Germany,"Patrick Esser, Robin Rombach, Björn Ommer","Highly cited,SOTA improvement",,Taming Transformers for High-Resolution Image Synthesis,,,,I'm confused - I guess they pretrained on several different datasets? I think the model is also able to do zero-shot learning,Unknown,,,1583.00,,,,,,,,,,,,Academia,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Enhanced Neighborhood-Based Filtering,Recommendation,,2007-10-28,,,,,https://ieeexplore.ieee.org/abstract/document/4470228,AT&T,,,United States of America,"RM Bell, Y Koren",SOTA improvement,"""We evaluate these methods on the Netflix dataset, where they deliver significantly better results than the commercial Netflix Cinematch recommender system.""",Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,,,,,Unknown,,,687.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
TD(0),Other,,1977-08-01,,,,,https://www.sciencedirect.com/science/article/pii/S0019995877903540,University of Essex,,,United Kingdom of Great Britain and Northern Ireland,Ian Witten,Historical significance,,An adaptive optimal controller for discrete-time Markov environments,,,,??? Seemingly no info,Unknown,,,269.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Semi-Supervised Embedding for DL,Other,,2008-07-05,,,,,https://dl.acm.org/doi/10.1145/1390156.1390303,"Google,NUANCE Communications,IDIAP,University of Illinois Urbana-Champaign (UIUC)",,,"United States of America,United States of America,Switzerland,United States of America","Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert",Highly cited,,Deep Learning via Semi-Supervised Embedding,,,,,Unknown,,,1087.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Industry,Industry,Academia,Academia",,,,,"Industry,Industry,Academia,Academia",,,
Universal approximation via Feedforward Networks,Other,,1989-03-09,,,,,https://www.sciencedirect.com/science/article/abs/pii/0893608089900208,"UC San Diego,Technische Universität Wien",,,"United States of America,Austria",Kurt Hornik & Maxwell Stinchcombe & Halbert White,Highly cited,,Multilayer feedforward networks are universal approximators,,,,,Unknown,,,21663.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Transformer ELMo,Language,Language modelling,2019-01-01,,Unreleased,,Unreleased,https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59,"Allen Institute for AI,University of Washington",,,"United States of America,United States of America","ME Peters, M Neumann, L Zettlemoyer, W Yih",SOTA improvement,"""Our model is the Reconciled Span Parser (RSP; Joshi et al., 2018), which, using ELMo representations, achieved state of the art performance for this
task. As shown in Table 2, the LSTM based models demonstrate the best performance with a 0.2% and 1.0% improvement over the Transformer and CNN models, respectively""",Dissecting Contextual Word Embeddings: Architecture and Representation,,More info on this is extractable with some time,,,,,,368.00,56000000.00,,,,,,,,,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,,,,,"Research collective,Academia",,,,,"Research collective,Academia",,,
NLP from scratch,Language,,2011-11-08,,,,,https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf,"NEC Laboratories,Princeton University",,,"United States of America,United States of America","Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa",Highly cited,,Natural Language Processing (Almost) from Scratch,,,852000000,"""Section 4 leverages large unlabeled data sets (∼ 852 million words)""",,,,7640.00,5000000.00,"""The capacity of our network architectures lies mainly in the word lookup table, which contains 50 × 100,000 parameters to train. [...] most of the trainable parameters are located in the lookup tables.""",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
A3C FF hs,Games,Atari,2016-02-04,,,,,http://arxiv.org/abs/1602.01783v2,"Google,University of Montreal / Université de Montréal",,,"United States of America,Canada","V Mnih, AP Badia, M Mirza, A Graves","Highly cited,SOTA improvement",,Asynchronous Methods for Deep Reinforcement Learning,,,,,Unknown,,,7605.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Motion-Driven 3D Feature Tracking,3D modeling,,1988-07-01,,,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf,Roke Manor Research,,,United Kingdom of Great Britain and Northern Ireland,Harris & Stephens,Highly cited,,A Combined Corner and Edge Detector,,,1500,"""The total number of possible input patterns was 65,536. Training sets of 650 and 1500 patterns picked at random from this total were used.""",,,,19068.00,,"""The simulation studies reported here all involved a 16-bit input pattern. """,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
VGG19,Vision,,2014-09-04,,,,,https://arxiv.org/abs/1409.1556,University of Oxford,,,United Kingdom of Great Britain and Northern Ireland,"K Simonyan, A Zisserman",Highly cited,,Very Deep Convolutional Networks for Large-Scale Image Recognition,ILSVRC 2012 subset of ImageNet,,1300000,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,,,87284.00,144000000.00,"Source: Table 2
https://arxiv.org/abs/1409.1556",,19600000000.00,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,,,,,Academia,,2024-04-01 09:30,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
BART-large,Language,,2019-10-29,"Models:
https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md

MIT license:
https://github.com/facebookresearch/fairseq/blob/main/LICENSE",Open source,Unreleased,Open source,https://arxiv.org/abs/1910.13461,Facebook AI,,,United States of America,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer",Highly cited,,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",Wikipedia,"""All models are of comparable size and are trained for 1M steps
on a combination of books and Wikipedia data""",,,,,,7748.00,406291456.00,"""In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.""

I counted the parameters in the huggingface model
https://huggingface.co/facebook/bart-large/tree/main

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large"")
model = AutoModel.from_pretrained(""facebook/bart-large"")
sum(p.numel() for p in model.parameters() if p.requires_grad)",,,,,,,,,,Industry,,2024-04-19 16:26,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
DALL·E 2,Image generation,Text-to-image,2022-04-06,,,,,https://cdn.openai.com/papers/dall-e-2.pdf,OpenAI,,"Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.

DALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.

So it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.",United States of America,"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen","Highly cited,SOTA improvement",,Hierarchical Text-Conditional Image Generation with CLIP Latents,"CLIP,DALL-E",,650000000,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability""",Confident,,,3802.00,3500000000.00,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""",,,,,,,Self-supervised learning,,,Industry,,2024-04-01 09:30,Robi Rahman,,,,,,,,,,,,Industry,checked,,,,Industry,,,
DBLSTM,Speech,Speech recognition,2013-12-08,,,,,https://ieeexplore.ieee.org/document/6707742,University of Toronto,,,Canada,"A Graves, N Jaitly, A Mohamed",Highly cited,,Hybrid speech recognition with Deep Bidirectional LSTM,,,,,,,,1533.00,29900000.00,"""The DBLSTM network had five bidirectional hidden levels, with 500 LSTM cells in each of the forward and backward
layers, and a size 3385 softmax output layer, giving a total of
29.9M weights.""",,,,,,,,,,Academia,,2024-04-01 09:30,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Empirical evaluation of deep architectures,Other,,2007-06-01,,,,,https://dl.acm.org/doi/10.1145/1273496.1273556,University of Montreal / Université de Montréal,,,Canada,"Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio",Highly cited,,An empirical evaluation of deep architectures on problems with many factors of variation,,,,,Unknown,,,1130.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Diabetic Retinopathy Detection Net,Vision,Image classification,2016-12-13,,,,,https://jamanetwork.com/journals/jama/article-abstract/2588763,"UT Austin,UC Berkeley,Google",,,"United States of America,United States of America,United States of America","V Gulshan, L Peng, M Coram, MC Stumpe, D Wu",Highly cited,,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs,,,,,Unknown,,,3540.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
Deep Deterministic Policy Gradients,Robotics,,2015-09-09,,,,,https://arxiv.org/abs/1509.02971,Google DeepMind,,,Multinational,"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez",Highly cited,,Continuous control with deep reinforcement learning,,,,,Unknown,,,10898.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
BellKor 2008,Recommendation,Movie ratings,2009-08-01,,,,,https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/ProgressPrize2008_BellKor.pdf,AT&T,,,United States of America,"RM Bell, Y Koren, C Volinsky",SOTA improvement,Won Netflix prize,The BellKor 2008 Solution to the Netflix Prize,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,162.00,,,,,,,,,,,,Industry,,2024-04-04 12:19,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Faster R-CNN,Vision,Object detection,2015-06-04,,,,,https://arxiv.org/abs/1506.01497,Microsoft Research,,,United States of America,"S Ren, K He, R Girshick, J Sun",Highly cited,,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,,,,,Unknown,,,51178.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Inception v3,Vision,Image classification,2015-12-02,,,,,https://arxiv.org/abs/1512.00567,"Google,University College London (UCL)",,,"United States of America,United Kingdom of Great Britain and Northern Ireland","Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",Highly cited,,Rethinking the inception architecture for computer vision.,ILSVRC 2012 subset of ImageNet,,1200000,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",,,,23358.00,23626728.00,Table 3 from Xception paper,,114830000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-22 07:40,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Automated WSD via WordNet,Language,Word sense disambiguation,2004-07-01,,,,,https://aclanthology.org/P04-1036/,University of Sussex,,,United Kingdom of Great Britain and Northern Ireland,"D McCarthy, R Koeling, J Weeds",,,Finding Predominant Word Senses in Untagged Text,,,5000,"They do two experiments, one on a dataset of 5.000 tagged words and
another one on two datasets containing a total of around 40 million words, of which they only select 38 unique words and manually annotate the senses?
I think the first one is more representative",,,,475.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
REINFORCE in Stochastic Connectionism,Other,,1992-05-01,,,,,https://dl.acm.org/doi/10.1007/BF00992696,Northeastern University,,,United States of America,R. J. Williams,Highly cited,,Simple statistical gradient-following algorithms for connectionist reinforcement learning,,,,,Unknown,,,7842.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Pragmatic Theory solution (Netflix 2009),Recommendation,Movie ratings,2009-08-01,,,,,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf,Pragmatic Theory Inc.,,"This is an ensemble of many smaller models. Ideally, the training compute of all the sub-models should be added up and recorded here.",Canada,"M Piotte, M Chabbert",SOTA improvement,Netflix grand prize winner (along with two other teams),The Pragmatic Theory solution to the Netflix Grand Prize,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",Likely,,,111.00,,"This is an ensemble of many smaller models. Ideally, the number of parameters of all the sub-models should be added up and recorded here.",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
MetNet,Earth science,Weather prediction,2020-03-24,,Unreleased,,Unreleased,https://arxiv.org/abs/2003.12140,Google,,,United States of America,"Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",SOTA improvement,"""MetNet improves upon the current operational NWP system HRRR for up to 8 hours of lead time""
... 
""Numerical Weather Prediction is the most successful framework to perform medium- and longrange (up to 6 days with high confidence) forecast to date (Bauer et al., 2015).""",MetNet: A Neural Weather Model for Precipitation Forecasting,,"""Precipitation provides a benchmark for a highly varying and densely measured target (Agrawal
et al.). We cast precipitation forecasting as a structured prediction problem where the output comes
in the form of a three-dimensional tensor. Each value of the tensor corresponds to a time and a
location and indicates the corresponding rate of precipitation measured in mm/h. Target precipitation rates are estimated by the Multi Radar Multi Sensor (MRMS) ground based radars as a
function of the returned radar echoes (Zhang et al., 2016). The spatial size obtained from MRMS
is 7000 × 2500 covering the continental United States. Each pixel covers 0.01◦ of longitude and
latitude corresponding to approximately 1 km2
. In addition to MRMS frames, the available input
data include the 16 spectral bands of the optical Geostationary Operational Environmental Satellite
16 (GOES-16). Figure 1 contains examples of MRMS and GOES-16 frames.""",,,Unknown,,,221.00,,,,,,,,,,,,Industry,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Imagen Video,Video,Video generation,2022-10-05,,,,,https://arxiv.org/abs/2210.02303,Google Brain,,,United States of America,"Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans",,,Imagen Video: High Definition Video Generation with Diffusion Models,,,,"We train our models on a combination of an internal dataset consisting of 14 million video-text pairs
and 60 million image-text pairs, and the publicly available LAION-400M image-text dataset.",,,,692.00,11600000000.00,"Figure 6 summarizes the entire cascading pipeline of Imagen Video. In total, we have 1 frozen text
encoder, 1 base video diffusion model, 3 SSR (spatial super-resolution), and 3 TSR (temporal superresolution) models – for a total of 7 video diffusion models, with a total of 11.6B diffusion model
parameters",,,,,,,,,,,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
IBM Model 4,Language,Translation,1999-07-02,,,,,http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf,"University of Southern California,IBM,University of Pennsylvania",,,"United States of America,United States of America,United States of America","Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky",Highly cited,,Statistical machine translation,,,800000,"[WORDS]
See FIgure 6",,,,1921.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Deep Belief Nets,Vision,Character recognition,2006-07-18,,,,,https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf,"University of Toronto,National University of Singapore",,,"Canada,Singapore","GE Hinton, S Osindero, YW Teh",Highly cited,,A fast learning algorithm for deep belief nets,MNIST,,60000,"""The network that performed best on the validation set was
then tested and had an error rate of 1.39%. This network was
then trained on all 60,000 training images8 until its error-rate
on the full training set was as low as its final error-rate had
been on the initial training set of 44,000 images.""",,,,16071.00,1600000.00,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
LSTM with forget gates,Language,,1999-01-02,,,,,https://ieeexplore.ieee.org/document/818041,IDSIA,,,Switzerland,"F. A. Gers, J. Schmidhuber, and F. Cummins",Highly cited,,Learning to forget: Continual prediction with LSTM,,,30000,"Training was stopped after at most 30000
training streams, each of which was ended
when the first prediction error or the
100000th successive input symbol occurred

NOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)",,,,5866.00,276.00,See Table 1,,,,,,,,,,Academia,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Q-learning,"Robotics,Games",,1989-01-01,,,,,http://www.cs.rhul.ac.uk/~chrisw/thesis.html,University of London,,,United Kingdom of Great Britain and Northern Ireland,Christopher Watkins,Highly cited,,Learning from delayed rewards,,,,,Unknown,,,8025.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
EfficientZero,Games,,2021-10-30,,,,,https://arxiv.org/abs/2111.00210,"Tsinghua University,UC Berkeley,Shanghai Qi Zhi institute",,"""Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.""","China,United States of America,China","Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao",SOTA improvement,"""Our method is 176% and 163% better
than the previous SoTA performance, in mean and median human normalized score respectively""",Mastering Atari Games with Limited Data,,,,,Unknown,,,129.00,,,,,,,,,,,,Academia,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
DImensionality Reduction,Vision,Face recognition,2006-07-18,,,,,https://www.cs.toronto.edu/~hinton/science.pdf,University of Toronto,,,Canada,"GE Hinton, RR Salakhutdinov",Highly cited,,Reducing the dimensionality of data with neural networks.,,,70000,"After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA
(Fig. 2B)",,,,15697.00,3800000.00,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
SqueezeBERT,Language,Text autocompletion,2020-06-10,,,,,https://arxiv.org/abs/2006.11316,UC Berkeley,,,United States of America,"Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer",,,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,,,,,,,,106.00,51100000.00,Rados,,7420000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Academia,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Hopfield Networks (2020),"Biology,Vision,Language,Medicine","Drug discovery,Language modelling,Object recognition",2020-07-16,"copyleft-like license, derivative works must retain this license. code here:
https://github.com/ml-jku/hopfield-layers/blob/master/LICENSE",Open source,,Open source,https://arxiv.org/abs/2008.02217,"Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo",,,"Austria,Austria,Norway","Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter",SOTA improvement,"""Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield
layers achieved state-of-the-art on two drug design datasets""",Hopfield Networks is All You Need,"BACE,SIDER","""We test the Hopfield layer HopfieldLayer, on four drug
design datasets. These datasets represent four main areas of modeling tasks in drug design, concretely
to develop accurate models for predicting a) new anti-virals (HIV) by the Drug Therapeutics Program
(DTP) AIDS Antiviral Screen, b) new protein inhibitors, concretely human β-secretase (BACE) inhibitors by Subramanian et al. (2016), c) metabolic effects as blood-brain barrier permeability (BBBP)
(Martins et al., 2012) and d) side effects of a chemical compound from the Side Effect Resource
(SIDER) Kuhn et al. (2016). """,,,Unknown,,,273.00,,,,,,,,,,,,Academia,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
data2vec (speech),Speech,,2022-01-20,,,,,https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,Meta AI,,,United States of America,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",SOTA improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""","Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",LS-960,,13132800,"Section 5.2:
""we pre-train data2vec on the 960
hours of speech audio data from Librispeech (LS-960)""

13,680 words per hour",,,,563.00,705134592.00,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,,,,,Self-supervised learning,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Tagging via Viterbi Decoding,Language,,2002-06-01,,,,,https://dl.acm.org/doi/10.3115/1118693.1118694,AT&T,,,United States of America,Michael Collins,Highly cited,,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,,,,,Unknown,,,2582.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Residual Dense Network,"Vision,Image generation",Image super-resolution,2018-02-24,,,,,https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html,"Northeastern University,University of Rochester",,,"United States of America,United States of America"," Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu",Highly cited,,Residual Dense Network for Image Super-Resolution,,,,,Unknown,,,2744.00,,,,,,,,,,,,Academia,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Ngram corpus,Language,,2012-07-08,,,,,https://aclanthology.org/P12-3029/,Google,,,United States of America,"Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman and Slav Petrov",,,Syntactic Annotations for the Google Books NGram Corpus,,,,,Unknown,,,489.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Learnability theory of language development,Language,,1984-07-01,,,,,https://psycnet.apa.org/record/1985-97439-000,Massachusetts Institute of Technology (MIT),,,United States of America,Steven Pinker,Highly cited,,Language learnability and language development,,,,,Unknown,,,4730.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
DQN-2015,Games,Atari,2015-02-25,,,,,https://www.nature.com/articles/nature14236,Google,,"This should be calculatable, just needs careful reasoning about compute per frame.",United States of America,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis ",Highly cited,,Human-level control through deep reinforcement learning,,,50000000,"Methods: ""we trained for a total of 50 million frames""",,,,23247.00,1693362.00,"""The input to the neural network consists of an 84x84x4 image produced by the preprocess-ing mapw. The first hidden layer convolves 32 filters of 8x8 with stride 4 with theinput image and applies a rectifier nonlinearity. The second hidden layer con-volves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.This is followedby a thirdconvolutional layer thatconvolves 64 filtersof 3x3 withstride 1 followed by a rectifier. The final hidden layer is fully-connected and con-sists of 512 rectifier units. The output layer is a fully-connected linear layer with asingle output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""

Example num params here: https://colab.research.google.com/drive/1Ty6SFYWd7EcKoxJohucL2OdiLR_3oXnI?usp=sharing",,,,,,,,,,Industry,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Inceptionv4,Vision,Image classification,2016-02-23,,,,,https://arxiv.org/abs/1602.07261,Google,,,United States of America,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",Highly cited,,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",,,,,,,,12187.00,43000000.00,"""The folks from Google strike again with Inception-v4, 43M parameters.""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,24600000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Wu Dao - Wen Su,Biology,Proteins,2021-03-01,,,,,https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,Beijing Academy of Artificial Intelligence,,,China,,,,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',,,,,Unknown,,,,,,,,,,,,Self-supervised learning,,,Industry,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
PreTrans-3L-250H,Speech,Speech recognition,2013-03-22,,,,,https://arxiv.org/abs/1303.5778,University of Toronto,,,Canada,"Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton",Highly cited,,Speech Recognition with Deep Recurrent Neural Networks,,,,,,,,8015.00,43000000.00,Table 1,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
NoisyNet-Dueling,Games,Atari,2017-06-30,,,,,https://arxiv.org/abs/1706.10295v3,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"M Fortunato, MG Azar, B Piot, J Menick",SOTA improvement,,Noisy Networks for Exploration,,,,,Unknown,,,790.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Dropout (TIMIT),Speech,Speech recognition,2012-06-03,http://www.cs.toronto.edu/~nitish/dropout,,,Open source,https://arxiv.org/abs/1207.0580,University of Toronto,,,Canada,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,Improving neural networks by preventing co-adaptation of feature detectors,TIMIT,,41620,"4162 utterances, guesstimated avg 10 words per utterance",,,,7171.00,48840185.00,The input to the net is 21 adjacent frames with an advance of 10ms per frame. The neural net has 4 fully-connected hidden layers of 4000 units per layer and 185 “softmax” output units that are subsequently merged into the 39 distinct classes used for the benchmark.,,,,,,NVIDIA GeForce GTX 580,,,,Academia,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2024-05-09 12:35,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
YOLOv2,Vision,Object detection,2016-12-25,,,,,https://arxiv.org/abs/1612.08242,"University of Washington,Allen Institute for AI",,,"United States of America,United States of America","Joseph Redmon, Ali Farhadi",Highly cited,,"YOLO9000: Better, Faster, Stronger",,,,,,,,13096.00,51000000.00,Source: https://resources.wolframcloud.com/NeuralNetRepository/resources/YOLO-V2-Trained-on-MS-COCO-Data_1,,,,,,,,,,Industry,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,"Academia,Research collective",,,,,"Academia,Research collective",,,
Prototypical networks,Vision,Image classification,2017-03-15,,,,,https://arxiv.org/abs/1703.05175,"University of Toronto,Twitter",,,"Canada,United States of America"," Jake Snell, Kevin Swersky, Richard S. Zemel",Highly cited,,Prototypical Networks for Few-shot Learning,,,,,Unknown,,,6395.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
AdClickNet,Other,,2014-08-24,,,,,https://dl.acm.org/doi/10.1145/2648584.2648589,Facebook,,,United States of America,"X He, J Pan, O Jin, T Xu, B Liu, T Xu, Y Shi",,,Practical Lessons from Predicting Clicks on Ads at Facebook,,,,,Unknown,,,820.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
GloVe (32B),Language,Semantic embedding,2014-01-01,,,,,https://nlp.stanford.edu/projects/glove/,Stanford University,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

But we are interested in the 42B token model",United States of America,"J Pennington, R Socher, CD Manning",Highly cited,,GloVe: Global Vectors for Word Representation,Common Crawl,,42000000000,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",,,,29074.00,120000000.00,400k vocab * 300 vector dimensions,,,Embeddings are precalculated,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,,Academia,,2024-04-01 09:28,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MV-RNN,Language,Text classification,2012-07-12,,,,,https://www.aclweb.org/anthology/D12-1110/,Stanford University,,,United States of America,"R. Socher, B. Huval, C. D. Manning, and A. Y. Ng",Highly cited,,Semantic Compositionality through Recursive Matrix-Vector Spaces,,,,,,,,1459.00,3510255.00,"""We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x ∈ Rn with pre-trained 50-dimensional word vectors from the unsupervised model of Collobert and Weston (2008). [...] Every word is also associated with a matrix X.  [...] If the vectors have dimensionality n, then each word’s matrix has dimensionality X ∈ Rn×n.""

""We propose the following combination function which is input dependent:
p = fA,B(a, b) = f(Ba, Ab) = g(W x (Ba Ab)) ,(2)
where A, B are matrices for single words, the global W ∈ Rn×2n is a matrix that maps both transformed words back into the same n-dimensional space.""

""For computing nonterminal phrase matrices, we define the function
P = fM(A, B) = WMA, B, (3)
where WM ∈ Rn×2n, so P ∈ Rn×n just like each input matrix.""

""If every word is represented by an n-dimensional vector and additionally by an n × n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation: A = UV + diag(a), (5)
where U ∈ Rn×r, V ∈ Rr×n, a ∈ Rnand we set the rank for all experiments to r = 3.""

""We train these representations by adding on top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d ∈ RK is a K-dimensional multinomial distribution""

In total there are V*(n+n*r + r*n) + n*2n + n*2n + (n+1)*k parameters, where n is the vector dimension, r is the low-rank decomposition dimension, V is the vocabulary size and k is the number of classes.

In the experiments we have that n=50, r=3, k=? and V=?. I'm guesstimating k=5 and V=10k.",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Deeply-recursive ConvNet,"Vision,Image generation",Image super-resolution,2016-11-11,,,,,https://arxiv.org/abs/1511.04491,Seoul National University,,,Korea (Republic of),"Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",Highly cited,,Deeply-Recursive Convolutional Network for Image Super-Resolution,,,,,Unknown,,,2265.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Kohonen network,Mathematics,Dimensionality reduction,1981-07-25,,,,,https://link.springer.com/article/10.1007/BF00337288,Helsinki University of Technology,,,Finland,T Kohonen,Highly cited,,Self-organized formation of topologically correct feature maps,,,,??? Seemingly no info,,,,11841.00,4096.00,"The input vectors are 3D.
I could not find the grid size, but from the images it looks 8x8.
So the network was 8x8x3 parameters.",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
n-gram LM,Language,,1997-07-01,,,,,https://www.semanticscholar.org/paper/Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87,"University of Cambridge,Carnegie Mellon University (CMU)",,,"United Kingdom of Great Britain and Northern Ireland,United States of America","P Clarkson, R Rosenfeld",,,Statistical language modeling using the CMU-Cambridge toolkit,,,,,Unknown,,,954.00,,,,,,,,,Supervised,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
DeepLab (2017),Vision,Image segmentation,2017-04-27,,,,,https://ieeexplore.ieee.org/abstract/document/7913730,"Johns Hopkins University,Google,University College London (UCL)",,,"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland","Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",Highly cited,,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",,,,,Unknown,,,14934.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Sparse digit recognition SVM,Vision,,2008-11-19,,,,,https://pubmed.ncbi.nlm.nih.gov/19000969/,University of Lubeck,,,Germany,"Kai Labusch, Erhadt Barth, Thomas Martinetz",SOTA improvement,"""Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark""",Simple method for high-performance digit recognition based on sparse coding,,,,,Unknown,,,124.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MENACE,Games,Tic Tac Toe,1963-11-01,,,,,https://academic.oup.com/comjnl/article/6/3/232/360077,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,Donald Michie,,,Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters,,,,,Unknown,,,46.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Bayesian Starcraft,Games,,2011-08-31,,,,,https://ieeexplore.ieee.org/document/6032006,Collège de France,,,France,"G Synnaeve, P Bessiere",,,A Bayesian Model for RTS Units Control applied to StarCraft,,,,,,,,86.00,13125.00,"It's a bayes net, parameters are probabilty tables for probability that X happens in direction i given that we go in direction i. There are 25 directions.",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
ResNet-110 (CIFAR-10),Vision,Image classification,2015-12-10,,,,,https://arxiv.org/abs/1512.03385,Microsoft,,,United States of America,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,Deep Residual Learning for Image Recognition,,,,,,,,156882.00,1700000.00,Table 6,,,,,,,,,,Industry,,2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Make-A-Video,Video,Video generation,2022-09-29,,,,,https://arxiv.org/abs/2209.14792,Meta AI,,,United States of America,"Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",SOTA improvement,,Make-A-Video: Text-to-Video Generation without Text-Video Data,,,,,Unknown,,,649.00,,,,,,,,,Self-supervised learning,,,,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Pattern recognition and reading by machine,Vision,Character recognition,1959-12-01,,,,,https://dl.acm.org/doi/10.1145/1460299.1460326,Sandia Corporation,,,United States of America,"W. W. Bledsoe, I. Browning",Historical significance,,Pattern recognition and reading by machine,,,,,,,,587.00,2625.00,A two bit state is recorded for each of the 75 cell pairs and each of the 25+10 characters recognized.,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ERNIE-ViLG,"Multimodal,Image generation",Vision-language generation,2021-12-31,,,,,https://arxiv.org/abs/2112.15283,Baidu,,,China,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA improvement,"""we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks""",ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation,,,145000000,"To explore the landscape of large-scale pre-training for bidirectional text-image generation,
we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",,,,43.00,10000000000.00,"""To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.""",,,,,,,Self-supervised learning,,,,,2024-05-13 07:39,Robi Rahman,,,,,,,,,,,,Industry,checked,,,,Industry,,,
CTC-Trained LSTM,Speech,Speech recognition,2006-06-25,,,,,https://www.cs.toronto.edu/~graves/icml_2006.pdf,"IDSIA,Technical University of Munich",,,"Switzerland,Germany","Alex Graves, Santiago Fernández, Faustino Gómez, Jürgen Schmidhuber",Highly cited,,Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,TIMIT,,41620,"4162 utterances, guesstimated avg 10 words per utterance",,,,4862.00,114662.00,"""The hidden layers were fully connected to themselves
and the output layer, and fully connected from the input layer. The input layer was size 26, the softmax output layer size 62 (61 phoneme categories plus the blank label), and the total number of weights was
114, 662.""

https://www.cs.toronto.edu/~graves/icml_2006.pdf",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Punish/Reward,Games,Blackjack,1973-09-01,,,,,https://ieeexplore.ieee.org/document/4309272,IEEE,,,Multinational,"Widrow, Gupta, and Maitra",,,Punish/Reward: Learning with a Critic in Adaptive Threshold Systems,,,,??? Seemingly no info,,,,397.00,21.00,"Fig. 1 shows that there is a bias term, while Fig. 5 shows that the input is a sequence of 20 bits, corresponding to 20 weights. So the total number of parameters is 21.",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Sandstorm (DARPA Grand Challenge I),Driving,Self-driving car,2004-06-14,,,,,https://ieeexplore.ieee.org/document/1336386,Carnegie Mellon University (CMU),,,United States of America,William Red L. Whittaker,,,DARPA Grand Challenge Technical Paper,,,,,Unknown,,,66.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
EDSR,"Vision,Image generation",Image super-resolution,2017-06-10,,,,,https://arxiv.org/abs/1707.02921,Seoul National University,,,Korea (Republic of),"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee",Highly cited,,Enhanced Deep Residual Networks for Single Image Super-Resolution,,,,,Unknown,,,4710.00,,,,,,,,,,,,Academia,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
FAST,Video,Corner detection,2006-05-07,,,,,https://link.springer.com/chapter/10.1007/11744023_34,University of Cambridge,,,United Kingdom of Great Britain and Northern Ireland,Edward Rosten and Tom Drummond,Highly cited,,Machine Learning for High-Speed Corner Detection,,,,,Unknown,,,5419.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
NEAT in neuroevolution,Other,,2002-06-01,,,,,https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting,IDSIA,,,Switzerland,"Justin Bayer, Daan Wierstra, Julian Togelius, Jürgen Schmidhuber",Highly cited,,Evolving Neural Networks through Augmenting Topologies ,,,,,Unknown,,,3366.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
M6-10B,"Multimodal,Image generation,Vision,Language","Image generation,Visual question answering",2021-03-01,,,,,https://arxiv.org/abs/2103.00823,"Tsinghua University,Alibaba",,"""We implement M6-100B with around 100 billion parameters
on 128 Nvidia A100s and the speed of pretraining achieves 1440
samples/s (for samples of the sequence length of 272).""

Their response to our email doesn't say enough to tell us what the compute is for this paper, but allows us to determine the compute for the follow-up paper with the M6-10T model (but we knew this already)","China,China","J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",,,M6: A Chinese Multimodal Pretrainer,,,1900000000000,"""1.9TB images and 292GB texts""

TODO: figure out what to do for multimodal pretraining datasets",,,,112.00,10000000000.00,"""We scale the
model size up to 10 billion and 100 billion parameters, and build
the largest pretrained model in Chinese.""",,,,,,,,,,Industry,"In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.",2024-05-13 09:57,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
SemVec,Language,,2013-06-09,,,,,https://www.aclweb.org/anthology/N13-1090/,Microsoft Research,,,United States of America,"T Mikolov, W Yih, G Zweig",Highly cited,,Linguistic Regularities in Continuous Space Word Representations,,,,,Unknown,,,3625.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
WaveNet,Speech,,2016-09-12,,,,,https://arxiv.org/abs/1609.03499,Google DeepMind,,,Multinational,"A Oord, S Dieleman, H Zen, K Simonyan",Highly cited,,WaveNet: A Generative Model for Raw Audio,,,,,Unknown,,,6409.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
M6-100B,"Multimodal,Image generation,Vision,Language","Image generation,Visual question answering",2021-03-01,,,,,https://arxiv.org/abs/2103.00823,"Tsinghua University,Alibaba",,,"China,China","J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",,,M6: A Chinese Multimodal Pretrainer,,,1900000000000,"""1.9TB images and 292GB texts""

TODO: figure out what to do for multimodal pretraining datasets",,,,112.00,100000000000.00,"""We scale the
model size up to 10 billion and 100 billion parameters, and build
the largest pretrained model in Chinese.""",,,,,,,,,,Industry,"In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.",2024-05-13 09:58,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Semantic Hashing,Language,,2008-12-10,,,,,https://www.cs.cmu.edu/~rsalakhu/papers/sdarticle.pdf,University of Toronto,,,Canada,"R Salakhutdinov, G Hinton",Highly cited,,Semantic Hashing,,,310521,Section 4.1,,,,1487.00,2600000.00,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
BigBiGAN,"Vision,Image generation",Image completion,2019-07-04,"model (Apache 2.0 license): https://www.kaggle.com/models/deepmind/bigbigan

they share a notebook but with a broken link",Open source,Open access (non-commercial),Unreleased,https://arxiv.org/abs/1907.02544,Google,,,United States of America,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",SOTA improvement,"""BigBiGAN, an unsupervised learning approach based purely on generative models, achieves state-of-the-art results in image representation learning on ImageNet""",Large Scale Adversarial Representation Learning,ImageNet,"""We train a BigBiGAN on unlabeled ImageNet, freeze its learned
representation, and then train a linear classifier on its outputs, fully supervised using all of the training
set labels""",,,,,,497.00,86000000.00,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,Industry,,2024-05-01 09:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Deeply-supervised nets,Vision,Image classification,2014-09-18,,,,,https://arxiv.org/abs/1409.5185,Microsoft Research,,,United States of America,"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu","Highly cited,SOTA improvement",,Deeply-Supervised Nets,"MNIST,CIFAR-10,CIFAR-100,SVHN (Street View House Numbers)","According to the paper, the Deeply-Supervised Nets (DSN) model was trained and evaluated on these image classification datasets:

MNIST - handwritten digits dataset with 60,000 training images and 10,000 test images.
CIFAR-10 - 60,000 32x32 color images across 10 classes, with 50,000 for training and 10,000 for testing.
CIFAR-100 - similar to CIFAR-10 but with 100 classes and 600 images per class.
SVHN - Street View House Numbers dataset with over 600,000 images of digits for training and 26,000 images for testing.",870000,60000+50000+60000+600000,,,,2509.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Deep rectifier networks,Other,,2011-04-13,,,,,http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf,University of Montreal / Université de Montréal,,,Canada,"X Glorot, A Bordes, Y Bengio",Highly cited,,Deep sparse rectifier neural networks,,,,,Unknown,,,7539.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Cross-Lingual POS Tagger,Language,Part-of-speech tagging,2011-06-19,,,,,https://aclanthology.org/P11-1061/,"Carnegie Mellon University (CMU),Google Research",,,"United States of America,Multinational","Dipanjan Das, Slav Petrov",SOTA improvement,"""Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.""",Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,,,,,Unknown,,,316.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Local Binary Patterns for facial recognition,Vision,,2006-12-01,,,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf,"University of Oulu,IEEE",,,"Finland,Multinational","Timo Ahonen, Abdenour Hadid, and Matti Pietikainen",Highly cited,,Face Description with Local Binary Patterns: Application to Face Recognition,,,,,Unknown,,,5816.00,,"Shallowly investigated, couldn't find much.
",,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
MADALINE II,Mathematics,Pattern classification,1988-07-24,,,,,https://ieeexplore.ieee.org/document/23872,Stanford University,,,United States of America,"Rodney Winter, Bernard Widrow",,,MADALINE RULE II: A Training Algorithm for Neural Networks,,,,,Unknown,,,81.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
data2vec (vision),Vision,,2022-01-20,,,,,https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,Meta AI,,,United States of America,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",SOTA improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""","Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",ImageNet,,1281167,"Section 5.1: 
""we pretrain data2vec on the images of the ImageNet-1K training
set""",,,,563.00,705134592.00,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,,,,,Self-supervised learning,,,Industry,,2024-05-01 09:13,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
R-CNN (T-net),Vision,Object detection,2013-11-11,,,,,https://arxiv.org/abs/1311.2524,UC Berkeley,,,United States of America,"Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",Highly cited,,Rich feature hierarchies for accurate object detection and semantic segmentation,,,,,,,,23149.00,69003872.00,"Computed from architecture description in Caffee

https://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb",,,,,,,,,,Academia,,2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
SRGAN,"Vision,Image generation",Image super-resolution,2017-05-25,,,,,https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html,Twitter,,,United States of America,"Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",Highly cited,,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,,,,,Unknown,,,11032.00,,,,,,,,,,,,Industry,,2024-05-21 13:02,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Graph-based structural reasoning,Vision,,1970-09-01,,,,,https://dspace.mit.edu/handle/1721.1/6884,Massachusetts Institute of Technology (MIT),,,United States of America,Patrick Winston,Highly cited,,Learning Structural Definitions from Examples,,,,,Unknown,,,1805.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MobileNet,Vision,,2017-04-17,,,,,https://arxiv.org/abs/1704.04861,Google,,,United States of America,"AG Howard, M Zhu, B Chen, D Kalenichenko",Highly cited,,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,,,,,,,,16854.00,4200000.00,,,1140000000.00,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Industry,,2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Joint Probability Machine Translation,Language,,2002-06-01,,,,,https://dl.acm.org/doi/10.3115/1118693.1118711,University of Southern California,,,United States of America,Daniel Marcu and William Wong,,,"A Phrase-Based, Joint Probability Model for Statistical Machine Translation",Hansard Corpus,,1073480,"[WORDS]
""To evaluate our system, we trained [...] our joint
probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most
20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132
unique tokens)""",,,,623.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Constituency-Tree LSTM,Language,Semantic embedding,2015-02-28,,,,,https://arxiv.org/abs/1503.00075,"MetaMind Inc,Stanford University",,,"United States of America,United States of America","KS Tai, R Socher, CD Manning",Highly cited,,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,,,,,,,,2967.00,205190.00,"Table 1

https://arxiv.org/abs/1503.00075",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Netflix Recommender System,Recommendation,,2015-12-01,,,,,https://dl.acm.org/doi/pdf/10.1145/2843948,Netflix,,,United States of America,"CA Gomez-Uribe, N Hunt",Highly cited,,"The Netflix Recommender System: Algorithms, Business Value, and Innovation",,,,,Unknown,,,1092.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Trajectory-pooled conv nets,Video,Action recognition,2015-05-19,,,,,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html,"Chinese University of Hong Kong (CUHK),Chinese Academy of Sciences",,,"Hong Kong,China","Limin Wang, Yu Qiao, Xiaoou Tang",Highly cited,,Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors,"ImageNet,UCF101",,,"They pretrain on ImageNet, and use UCF101 for actions. Its paper says ""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data"".",,,,3786.00,9106245.00,"The input layer takes either a single RGB frame (224x224x3) for the spatial stream or a stack of 10 optical flow frames (224x224x20) for the temporal stream.
The first convolutional layer has 96 filters of size 7x7 with stride 2.
This is followed by max pooling with size 3x3 and stride 2.
The second convolutional layer has 256 filters of size 5x5 with stride 2.
After that is another max pooling layer (3x3, stride 2).
The third convolutional layer has 512 filters of size 3x3 with stride 1.
The fourth convolutional layer has 512 filters of size 3x3 with stride 1.
The fifth convolutional layer has 512 filters of size 3x3 with stride 1.
Next is a max pooling layer (3x3, stride 2).
The fully-connected layers have 4096, 2048, and 101 neurons respectively.

(7*7*20+1)*96 + (5*5*20+1)*256 + (3*3*20+1)*512 + (3*3*20+1)*512 + (3*3*20+1)*512 + 2*4096 + (4096+1)*2048 + (2048+1)*101 = 9106245",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
SVM for face detection,Vision,,1997-06-17,,,,,https://ieeexplore.ieee.org/document/609310,Massachusetts Institute of Technology (MIT),,,United States of America,"E. Osuna, R. Freund, F. Girosi",Highly cited,,Training Support Vector Machines: An Application to Face Detection,,,50000,"Section 1: ""The problem that we have to solve involves training a classifier
to discriminate between face and non-face patterns, using a
data set of 50,000points. """,,,,3851.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
TriNet,Video,Person re-identification,2017-11-21,,,,,https://arxiv.org/abs/1703.07737,"Visual Computing Institute,Aachen University",,,"Germany,Germany","Alexander Hermans, Lucas Beyer, Bastian Leibe",Highly cited,,In Defense of the Triplet Loss for Person Re-Identification,,,,,Unknown,,,2817.00,,,,,,,,,,,,Academia,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
GLIDE,Image generation,Image generation,2021-12-20,,,,,https://arxiv.org/abs/2112.10741,OpenAI,,"""Note that GLIDE was
trained with roughly the same training compute as DALL-E
but with a much smaller model (3.5 billion vs. 12 billion
parameters)""",United States of America,"Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam Pamela Mishkin Bob McGrew IlyaSutskever MarkChen",Highly cited,,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,,,250000000,"Section 4:
""We train our model on the same dataset as DALL-E (Ramesh
et al., 2021)""

This paper used 250M image-text pairs
https://arxiv.org/pdf/2102.12092.pdf",,,,1931.00,3500000000.00,"""Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking""",,,,,,,,,,Industry,,2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Naive Bayes,Other,,1974-09-01,,,,,https://www.semanticscholar.org/paper/Pattern-classification-and-scene-analysis-Duda-Hart/b07ce649d6f6eb636872527104b0209d3edc8188,Stanford Research Institute,,,United States of America,Duda and Hart,Highly cited,,Pattern Classification and Scene Analysis,,,,,Unknown,,,23127.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Genetic algorithm,Other,,1954-07-02,,,,,https://link.springer.com/article/10.1007/BF01556771,Institute for Advanced Study,,,United States of America,NA Barricelli,Historical significance,Possibly first computer simulation of a genetic evolution algorithm,Numerical testing of evolution theories,,,,,Unknown,,,266.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
MADALINE I,Other,,1962-07-01,,,,,https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?,Stanford University,,,United States of America,William Combs Ridgway,Historical significance,,An adaptive logic system with generalizing properties,,,,,Unknown,,,75.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
BOXES,Games,Pole balancing,1968-07-01,,,,,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,Michie and Chambers,Historical significance,,Boxes: An Experiment in Adaptive Control,,,,,Unknown,,,590.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Fisher-Boost,Vision,Image classification,2010-09-05,,,,,https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11,Xerox Research Centre Europe (XRCE),,,France,Florent PerronninJorge SánchezThomas Mensink,Highly cited,,Improving the Fisher Kernel for Large-Scale Image Classification,,,,,Unknown,,,3062.00,,,,,,,,,,,,Industry,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Spatiotemporal fusion ConvNet,Video,"Video,Action recognition",2016-06-01,,,,,https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html,"Graz University of Technology,University of Oxford",,,"Austria,United Kingdom of Great Britain and Northern Ireland","Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman",Highly cited,,Convolutional Two-Stream Network Fusion for Video Action Recognition,UCF101,,97200,"[SECONDS OF VIDEO]

They use UCF101, whose paper says
""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data""",,,,2432.00,,,,,,,,,,,,Academia,,2024-04-01 09:53,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Support Vector Machines,Vision,Image classification,1995-09-01,,,,,https://link.springer.com/article/10.1007/BF00994018,"AT&T,Bell Laboratories",,,"United States of America,United States of America","C Cortes, V Vapnik",Highly cited,,Support-Vector Networks,MNIST,,60000,"Section 6.2: ""The large database consists of 60,000 training and 10,000 test patterns""",,,,48968.00,100000000.00,"Section 6.2.2: ""...polynomials
of degree 4 (that have more than 10^8 free parameters)...""
They used 4-degree polynomials for MNIST",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
SpecAugment,Language,Speech recognition,2019-04-18,LibriSpeech is open source,Unreleased,Open source,Unreleased,https://arxiv.org/abs/1904.08779,Google Brain,,,United States of America," Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le",Highly cited,,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,"LibriSpeech,Switchboard,Fisher",,,,Unknown,,,2877.00,,,,,,,,,,,,Industry,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
XLM,Language,,2019-06-01,weights/code non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme,Open access (non-commercial),Unreleased,Open access (non-commercial),https://arxiv.org/abs/1901.07291,Facebook,,,United States of America,"G Lample, A Conneau","Highly cited,SOTA improvement","""On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more
than 4 BLEU""",Cross-lingual Language Model Pretraining,,"subset of Wikipedia: ""We use WikiExtractor2
to extract raw sentences
from Wikipedia dumps and use them as monolingual data for the CLM and MLM objectives.""",,,,,,2342.00,665000000.00,,,,,,,,,,,Industry,,2024-04-17 15:26,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
PDP model for serial order,Speech,Speech synthesis,1986-01-05,,,,,https://www.osti.gov/biblio/6910294,UC San Diego,,,United States of America,"Jordan, M.I.",Highly cited,,Serial order: A parallel distributed processing approach,,,,,Unknown,,,1502.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
RL for helicopter flight,Driving,Helicopter driving,2006-03-09,,,,,https://papers.nips.cc/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html,"UC Berkeley,Stanford University",,,"United States of America,United States of America","H. Kim, Michael Jordan, Shankar Sastry, Andrew Ng",,,Autonomous helicopter flight via reinforcement learning,,,,,Unknown,,,361.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Rational DQN Average,Games,Atari,2021-02-18,,,,,https://openreview.net/forum?id=gnRmI8TatHV,TU Darmstadt,,,Germany,"Q Delfosse, P Schramowski, A Molina",SOTA improvement,,Recurrent Rational Networks,,,,,,,,4.00,1683456.00,See figure 7,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
DeepLabV3+,Vision,Semantic segmentation,2018-02-07,,,,,https://arxiv.org/abs/1802.02611v3,Google,,,United States of America,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",Highly cited,,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,,,,,Unknown,,,10003.00,,,,,,,,,,,,Industry,,2024-05-21 13:03,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Unsupervised Scale-Invariant Learning,Vision,,2003-06-18,,,,,https://ieeexplore.ieee.org/document/1211479,University of Oxford,,,United Kingdom of Great Britain and Northern Ireland,"R Fergus, P Perona, A Zisserman",Highly cited,,Object Class Recognition by Unsupervised Scale-Invariant Learning,,,3500,"See Table 2 and Figure 1.
There are 7 datasets, each with 200-800 of pictures. I pick 500 as the avg number of pictures",,,,2970.00,451.00,"See Table 1
",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Agent57,Games,Atari,2020-03-30,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2003.13350,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"AP Badia, B Piot, S Kapturowski",SOTA improvement,"""We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games""",Agent57: Outperforming the Atari Human Benchmark,,,,,Unknown,,,437.00,,,,,,,,,,,,Industry,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
IBM-5,Language,Translation,1993-06-15,,,,,https://dl.acm.org/doi/10.5555/972470.972474,IBM,,,United States of America,"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer",Highly cited,,The Mathematics of Statistical Machine Translation: Parameter Estimation,Proceedings of the Canadian parliament,,53358600,"""They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.""",,,,5752.00,1658364.00,"The model is initiallized with 2.44E+09 translation probabilities, which are progressively culled until 1,658,364 remain. There are other parameters in the models (eg the fertility probabilities that relate each word in the input to the number of words it will align to) but the parameter count is dominated by the translation probabilities.",,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
MetaLM,"Multimodal,Language,Vision",,2022-06-13,,,,,https://arxiv.org/abs/2206.06336v1,Microsoft Research,,,United States of America,"Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",SOTA improvement,"Abstract: ""Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.""",Language Models are General-Purpose Interfaces,,,,,Unknown,,,77.00,,,,,,,,,Self-supervised learning,,,Industry,,2024-05-21 13:05,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Codex,Language,Code autocompletion,2021-07-07,,,,,https://openai.com/blog/openai-codex/,OpenAI,,"""The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B
consumed a similar amount of compute.""
",United States of America,"Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ","Significant use,Highly cited",,Evaluating Large Language Models Trained on Code,,,31800000000,"""Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.""

1 GB ~ 200M words",,,,2387.00,12000000000.00,"""With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems""",,,,,,,Self-supervised learning,,,Industry,,2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,Industry,checked,,,,Industry,,,
Fully Convolutional Networks,Vision,Image segmentation,2014-11-14,,,,,https://arxiv.org/abs/1411.4038,UC Berkeley,,,United States of America,"J Long, E Shelhamer, T Darrell",Highly cited,,Fully Convolutional Networks for Semantic Segmentation,,,,,Unknown,,,33254.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
"Listen, Attend and Spell",Speech,,2015-08-20,,,,,https://ieeexplore.ieee.org/document/7472621,"Google,Carnegie Mellon University (CMU)",,,"United States of America,United States of America","William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals",Highly cited,,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",,,,,Unknown,,,2061.00,,,,,,,,,,,,Industry,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Multiscale deformable part model,Vision,Object detection,2008-06-23,,,,,https://ieeexplore.ieee.org/abstract/document/4587597,"UC Irvine,University of Chicago,Toyota Technological Institute at Chicago",,,"United States of America,United States of America,United States of America","Pedro Felzenszwalb, David McAllester, Deva Ramanan",Highly cited,,"A discriminatively trained, multiscale, deformable part model",,,,,Unknown,,,3093.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
RNN-SpeedUp,Language,,2011-05-22,,,,,https://ieeexplore.ieee.org/document/5947611,"Brno University of Technology,Johns Hopkins University",,,"Czechia,United States of America","T. Mikolov, S. Kombrink, L. Burget, J. Cernock ˇ y, and S. Khudanpur",Highly cited,,Extensions of recurrent neural network language model,Penn TreeBank,,697500,"Section 3: ""The data used in the following experiments were obtained from
Penn Tree Bank: sections 0-20 were used as training data (about
930K tokens)""

0.75 words per token for English",,,,1546.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
GPU implementation of neural networks,Vision,,2004-06-01,,,,,https://www.sciencedirect.com/science/article/pii/S0031320304000524,Soongsil University,,,Korea (Republic of),"KS Oh, K Jung",,,GPU implementation of neural networks,,,,,Unknown,,,471.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
ReLU (NORB),Vision,Object recognition,2010-06-15,,,,,https://dl.acm.org/doi/10.5555/3104322.3104425,University of Toronto,,,Canada,"Nair, V., Hinton, G. E.",Highly cited,,Rectified linear units improve restricted boltzmann machines,,,291600,"""There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).""",,,,16092.00,16210006.00,"""The stereo-pair images are subsampled from their original resolution of 108 × 108 × 2 to 32 × 32 × 2 to speed up experiments [...]  the architecture
with the best results have 4000 units in the first layer
and 2000 in the second [...] there are 58,320 test
cases (9,720 cases per class) ""

So the architecture has (32*32*2+1)x4000 + (4000+1)*2000 + (2000+1)*58,320/9,720 parameters",,,,,,,,,,Academia,,2024-04-01 09:03,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Self Organizing System,Other,Pattern recognition,1955-03-01,,,,,https://dl.acm.org/doi/10.1145/1455292.1455309,Massachusetts Institute of Technology (MIT),,,United States of America,W. A. Clark and B. G. Farley,Historical significance,,Generalization of pattern recognition in a self-organizing system,,,256,""" The modifier was then
disabled so that no further changes in the net could
occur and all 256 possible input patterns were then presented in turn.""

""For these purposes, 16-element nets (8 input and 8
output) were used because it was desired to exhaust all
possible input patterns, and we were limited to about
2^8 inputs by available time. """,,,,93.00,225.00,Figure 4 contains the learnt weight matrix,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
BigSSL,Speech,Audio speech recognition,2021-01-10,,,,,https://arxiv.org/abs/2109.13226,"Google,Apple",,,"United States of America,United States of America","Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Françoise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu",SOTA improvement,"""In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set""",BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,,,42626880000,"Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hours

Note this involves significant self-training: ""Noisy student training (NST) [23], [41] is a self-training
method where a teacher model generates pseudo-labels for a
large unlabeled dataset, which is in turn used to train a student
model with augmentation.""

1 hour ~ 13,680 words
13680 * 3116000 = 42626880000",,,,128.00,8000000000.00,"""... we study the utility of large models, with the parameter count ranging from 600M to 8B...""",,,,,,,,,,Industry,,2024-05-01 09:06,Robi Rahman,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
Web mining + Decision tree recommender,Recommendation,Recommender system,2002-10-01,,,,,https://reader.elsevier.com/reader/sd/pii/S0957417402000520?token=155B6D1937982D7D0271AFD1CFB034DFD7F3D1DE816B66C025EBC9D0A305BA6DA685DD62989DC05246C794CAC74CDAEF&originRegion=us-east-1&originCreation=20220325235441,Korea Advanced Institute of Science and Technology (KAIST),,,Korea (Republic of),"YH Cho, JK Kim, SH Kim",,,A personalized recommender system based on web usage mining and decision tree induction,,,,,Unknown,,,656.00,,,,,,,,,,,,Academia,,2024-05-21 13:00,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
AMDIM,"Vision,Image generation",Image completion,2019-06-03,MIT: https://github.com/Philip-Bachman/amdim-public,Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/1906.00910,Microsoft Research,,,United States of America,"Philip Bachman, R Devon Hjelm, William Buchwalter",Highly cited,,Learning Representations by Maximizing Mutual Information Across Views,"ImageNet,CIFAR-10","""We evaluate our model using standard datasets: CIFAR10, CIFAR100, STL10 [Coates et al., 2011], ImageNet1 [Russakovsky et al., 2015], and Places205 [Zhou et al., 2014].""",,,,,,1307.00,626000000.00,source: https://openai.com/blog/image-gpt/#rfref13e,,,,,,,,,,Industry,,2024-04-17 15:29,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
ALBERT,Language,,2019-09-26,Apache 2.0: https://github.com/google-research/ALBERT,Open source,,Open source,https://arxiv.org/abs/1909.11942,"Toyota Technological Institute at Chicago,Google Research",,,"United States of America,Multinational","Z Lan, M Chen, S Goodman, K Gimpel",Highly cited,,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"""BookCorpus (BooksCorpus, Toronto Book Corpus)"",Wikipedia",,3300000000,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",,,,5199.00,18000000.00,Section 3.2 of paper,,22500000000.00,"Rados dataset  (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,Google TPU v3,,,,Industry,,2024-04-18 16:01,Robi Rahman,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Convolutional Pose Machines,Vision,Pose estimation,2016-01-30,,,,,https://arxiv.org/abs/1602.00134,Carnegie Mellon University (CMU),,,United States of America,"Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",Highly cited,,Convolutional Pose Machines,,,,,Unknown,,,2544.00,,,,,,,,,,,,Academia,,2024-05-21 13:01,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
HyperNEAT,Games,Atari,2014-03-05,,,,,https://ieeexplore.ieee.org/abstract/document/6756960,University of Texas at Austin,,,United States of America,"M Hausknecht, J Lehman",SOTA improvement,"""Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games""",A Neuroevolution Approach to General Atari Game Playing,,,,,,,,195.00,239712.00,"""The ANN consists of three layers (Fig. 3): a substrate layer inwhich information from the game screen (raw pixels, objects, ornoise) is given as input to the network; a processing layer whichadds a nonlinear internal representation; and a nonlinear outputlayer from which actions are read and conveyed to the Atari em-ulator. Both the input and output layers are fully connected tothe processing layer. The substrate dimensionality of the inputand processinglayers is 810 in the case of the object repre-sentation and 1621 for the pixel and noise representations.3The output layer consists of a 33 substrate mirroring the ninepossible directions of the Atari 2600 joystick and a single noderepresenting thefire button""",,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
Iterative Bootstrapping WSD,Language,,1995-06-26,,,,,https://dl.acm.org/doi/10.3115/981658.981684,University of Pennsylvania,,,United States of America,D Yarowsky,Highly cited,,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,,,460000000,the data were extracted from a 460 million word corpus,,,,2996.00,,,,,,,,,,,,Academia,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Academia,,,,,Academia,,,
CPC v2,"Vision,Image generation",Image completion,2019-05-22,,Unreleased,Open access (non-commercial),Unreleased,https://arxiv.org/abs/1905.09272,"DeepMind,UC Berkeley",,,"United Kingdom of Great Britain and Northern Ireland,United States of America",,SOTA improvement,"""this unsupervised representation substantially improves transfer learning to object detection on the
PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers""",Data-Efficient Image Recognition with Contrastive Predictive Coding,ImageNet,"""In all cases, the dataset of unlabeled images Du we pre-train
on is the full ImageNet ILSVRC 2012 training set""",,,,,,491.00,303000000.00,source: https://openai.com/blog/image-gpt/#rfref25d,,,,,,,,,,Industry,,2024-04-16 16:06,Robi Rahman,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Japanese dialog transformers,Language,,2021-11-09,"research only: 
https://github.com/nttcslab/japanese-dialog-transformers?tab=License-1-ov-file#readme",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/2109.05217,NTT Communication Science Laboratories,,,Japan,"Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro",,,Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems,,,2100000000,"[Pairs of text]

""We obtained 2.1 billion (521 GB) pairs by this method. The average number of utterances in the input context was 2.913, and the average number of characters was 62.3 for the input context and 20.3 for the target utterance""",,,,31.00,1600000000.00,"""We examined the improvement in model size in detail by considering four model sizes: 0.35B, 0.7B, 1.1B, and 1.6B parameters""",,,,,,,,,,Industry,,2024-03-28 10:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
BigChaos OptiBlend,Recommendation,Movie ratings,2009-08-01,,,,,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf,AT&T,,,United States of America,"A Töscher, M Jahrer, RM Bell",SOTA improvement,"won Netflix prize
",The BigChaos Solution to the Netflix Grand Prize,Netflix Prize,,100480507,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,237.00,,,,,,,,,,,,Industry,,2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
PLATO-XL,Language,Language modelling,2021-09-20,"apache 2.0
https://github.com/PaddlePaddle/Knover/blob/develop/LICENSE",Open source,,,https://arxiv.org/abs/2109.09519,Baidu,,,China,"Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zheng-Yu Niu",SOTA improvement,,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,,,,,Likely,,,52.00,11000000000.00,,,,,,,NVIDIA Tesla V100 DGXS 32 GB,,,,Industry,"To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.",2024-05-01 09:13,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Zidong Taichu,"Multimodal,Speech,Vision,Language","Language modelling/generation,Speech recognition,Image captioning",2021-08-11,,,,,https://gitee.com/zidongtaichu/multi-modal-models,Chinese Academy of Sciences,,,China,,Historical significance,"The world’s first image, language, and audio trimodal pre-trained model.",Zidong Ancestral multi-modal large model,,,,,Likely,,,0.00,100000000000.00,,,,,,,,,,,,,2024-05-13 09:39,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
CogVideo,"Multimodal,Video",Video generation,2022-05-29,,,,,https://arxiv.org/abs/2205.15868,"Tsinghua University,Beijing Academy of Artificial Intelligence",,,"China,China","Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang",Historical significance,The world's largest and first opensource large-scale pre-trained text-to-video model.,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,,,,,Likely,,,245.00,9000000000.00,,,,,,,,,,,,"Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",2024-05-13 09:15,Anonymous,,,,,,,,,,,,"Academia,Academia",checked,,,,"Academia,Academia",checked,,
AltCLIP,"Multimodal,Language,Vision,Image generation","Language modelling/generation,Chat,Text-to-image,Image generation",2022-11-12,"https://github.com/FlagAI-Open/FlagAI
Apache-2.0 license",Open source,,Open source,https://arxiv.org/abs/2211.06679,Beijing Academy of Artificial Intelligence,,,China,"Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu",SOTA improvement,"""We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30kCN, COCO-CN and XTD""",AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,,,,,Unknown,,,44.00,,,10.00,,,,,,,,,,"In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at this https URL.",2024-05-21 13:05,Anonymous,,,CLIP (ViT L/14@336px),,,,,CLIP (ViT L/14@336px),,,,Academia,,,,,Academia,,,
ALM 1.0,Language,Language modelling,2022-11-28,,,,,https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,Beijing Academy of Artificial Intelligence,,,China,,SOTA improvement,SOTA results on Arabic-language benchmark ALUE.,ALM 1.0,,,,,Speculative,,,0.00,335000000.00,335M parameters: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,,,,,,,,,,,,2024-04-03 09:14,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
MusicGen,Audio,Audio generation,2023-06-08,,,,,https://arxiv.org/abs/2306.05284,Meta AI,,"We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.

Unclear how many epochs used so FLOP calculation is not feasible.",United States of America,"Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez",SOTA improvement,"""We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark""",Simple and Controllable Music Generation,ShutterStock and Pond5 music data collections,"""We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.""",,"""We train on 30-second audio crops sampled at random from the full track... We use 20K hours of licensed music""

20000 hours * 60 min/hour * 2 inputs/min = 2400000 input sequences

EnCodec is run at 32kHz but after convolutions has a frame rate of 50 Hz, suggesting 2400000 * 30s * 50/s = 3,600,000,000 audio tokens.

Not confident enough in this calculation to add to database.",Likely,,,88.00,3359000000.00,"""We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters""

Uses EnCodec 32kHz (HF version has 59M params) for audio tokenization.",,,,,,,,,,Industry,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",2024-05-06 09:58,Anonymous,,,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Gen-2,Video,Video generation,2023-03-20,,,,,https://research.runwayml.com/gen2,Runway,,,United States of America,Gen-2 authors,SOTA improvement,"Website claims SOTA improvement over Stable Diffusion and Text2Live, paper forthcoming",,,,,,Unknown,,,0.00,,,,,,,,,,,,,,2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Gen-1,Video,Video generation,2023-02-06,,,,,https://arxiv.org/abs/2302.03011,Runway,,,United States of America,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",SOTA improvement,,Structure and Content-Guided Video Synthesis with Diffusion Models,,,,,Unknown,,,251.00,,,,,,,,,,,,,,2024-05-21 13:05,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Walking Minotaur robot,Robotics,,2019-06-19,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/1812.11103,"UC Berkeley,Google Brain",,,"United States of America,United States of America","Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",SOTA improvement,,Learning to Walk via Deep Reinforcement Learning,,,,,Unknown,,,377.00,,,,,,,,,Reinforcement learning,,,Industry,"Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",2024-05-21 13:03,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
RoboCat,Robotics,,2023-06-20,,,,,https://arxiv.org/abs/2306.11706,"Google DeepMind,Google",,,"Multinational,United States of America","Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess",SOTA improvement,,RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation,,"""We use a diverse and large number of datasets for training RoboCat. These include data from agent experience, human demonstrations and self-generated data, on both simulated and real-world robot environments. See Section 3.4 for details on our datasets.""",,,Speculative,,,21.00,1180000000.00,"""Most of the experimental results are based on models with a 1.18B-parameter decoder-only transformer (Vaswani et al., 2017) with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196."" page 8",,,,,,,,,,Industry,"The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",2024-05-01 09:22,Anonymous,,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
ERNIE 3.5,Language,Language modelling,2023-06-27,,,,,http://research.baidu.com/Blog/index-view?id=185,Baidu,,,China,,SOTA improvement,SOTA scores on AGIEval and MMLU. See article in China Science Daily: https://mp.weixin.qq.com/s/QVdkmofRSTgjQ7UOFX7s1g,Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward,,,,,Unknown,,,0.00,,,,,,,,,,,,,,2024-05-21 13:05,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Context-dependent RNN,Language,Language modelling,2012-07-27,,,,,https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf,"Microsoft Research,Brno University of Technology",,,"United States of America,Czechia","Tomas Mikolov, Geoffrey Zweig",SOTA improvement,New SOTA perplexity on PTB,Context Dependent Recurrent Neural Network Language Model,,,,,Unknown,,,707.00,,,,,,,,,,,,Industry,"Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe 
improvements in word-error-rate.",2024-05-21 13:00,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
RNN (SGD+CLR),Audio,,2012-12-14,,,,,https://arxiv.org/abs/1212.0901,University of Montreal / Université de Montréal,,,Canada,"Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu",,,Advances in Optimizing Recurrent Networks,,"""We evaluate our models on the four polyphonic music datasets of varying complexity used in [25]: classical piano music (Pianomidi.de), folk tunes with chords instantiated from ABC notation (Nottingham), orchestral music (MuseData) and the four-part chorales by J.S. Bach (JSB chorales)""",,,Speculative,,,647.00,195600.00,"It uses 400 hidden units (selected via hyperparameter tuning)
The input size is 88 (corresponding to the 88 piano pitches)
It uses rectified linear units, so no activation function parameters
So the number of parameters would be:
Input to hidden weights: 88 * 400 = 35,200
Hidden to hidden weights: 400 * 400 = 160,000
Biases: 400
Total: ~195,600 parameters
Above estimate is by Claude 2. Should be checked manually.",,,,,,,,,,,"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges
with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error",2024-05-22 12:24,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Deep Multitask NLP Network,Language,Language modelling,2008-07-05,,,,,http://icml2008.cs.helsinki.fi/papers/391.pdf,NEC Laboratories,,,United States of America,"Ronan Collobert, Jason Weston","Highly cited,SOTA improvement",,"A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning
","PropBank,Penn TreeBank,Wikipedia","PropBank (1M words) for semantic role labeling task
Penn Treebank (1M words) for part-of-speech tagging and chunking tasks
Stanford NER dataset for named entity recognition task
Wikipedia text (631M words) for unsupervised pretraining",633000000,"Section 7: ""631 million words
from Wikipedia""",Speculative,,,7095.00,1500000.00,"With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",,,,168.0,1 week on 1 computer,,Unsupervised,,,Industry,"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
DLRM-12T,Recommendation,Recommender system,2021-04-12,,,,,https://arxiv.org/abs/2104.05158,"Meta AI,Carnegie Mellon University (CMU)",,No training details provided.,"United States of America,United States of America","Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao",,,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,,No training details provided.,,No training details provided.,Confident,,,93.00,12000000000000.00,They instantiated a 12T-parameter model to show that their hardware setup can train it despite the huge memory requirements.,,,No inference details provided.,,No training details provided.,NVIDIA A100,,,No training details provided.,,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",2024-04-01 09:35,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
CALM,Robotics,,2023-08-06,,,,,https://research.nvidia.com/labs/par/calm/,"NVIDIA,Technion - Israel Institute of Technology",,The total pre-training of the networks involved 5 billion environment steps. The low-level policy operates at 30Hz while the high-level policy operates at 6Hz.,"United States of America,Israel","Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",,,CALM: Conditional Adversarial Latent Models for Directable Virtual Characters,,"160 diverse motion clips totaling 30 minutes, from a motion capture dataset. These include motions like walking, running, sword strikes, etc.",,,Unknown,,,9.00,,,,,,,,NVIDIA A100,Reinforcement learning,,,Industry,"In this work, we present Conditional Adversarial Latent Models (CALM),
an approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control
over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces,
akin to those found in video games.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",checked,,
InternLM,Language,Language modelling,2023-07-06,,,,,https://internlm.org/,"Shanghai AI Lab,SenseTime",,,"China,Hong Kong",,SOTA improvement,"(from Google-translated page) ""In addition to using academic datasets to evaluate InternLM, we also use human examinations to assess its capabilities. InternLM can achieve good scores on examination benchmarks such as MMLU, AGIEval, C-Eval, and GAOKAO-bench that cover different languages and subjects, scoring higher than ChatGPT on multiple benchmarks""",,,,750000000000,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens"" equals approximately 750B words for English, but the tokenizer's conversion ratio may be different for Chinese.",Speculative,,,0.00,100000000000.00,Pre-training a bilingual 100B Foundation model on data with over a trillion tokens,,,,,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,,,,,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",2024-04-03 10:01,Anonymous,,,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Claude,Language,"Language modelling,Chat",2023-03-14,,,,,https://www.anthropic.com/index/introducing-claude,Anthropic,,,United States of America,,"Historical significance,SOTA improvement",,Introducing Claude,,,,,Unknown,,,0.00,,,,,,,,,Reinforcement learning,,,,"Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",2024-05-23 06:35,Anonymous,,,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Stable Diffusion XL,Image generation,"Image generation,Text-to-image",2023-07-04,,,,,https://arxiv.org/abs/2307.01952,Stability AI,,,Multinational,"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach",Significant use,Looks like this is now the main/flagship Stable Diffusion model,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,,,,,Speculative,,,386.00,3400000000.00,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,,,,,,,,Industry,"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL",2024-04-19 10:21,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Perfusion,Image generation,Text-to-image,2023-05-02,,,,,https://arxiv.org/abs/2305.01644,"NVIDIA,Tel Aviv University,Bar-Ilan University",,"Must be <1e23 FLOP, it's trained with a single A100.","United States of America,Israel,Israel","Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon",,Pareto frontier performance but not SOTA,Key-Locked Rank One Editing for Text-to-Image Personalization,,,,,Unknown,,,73.00,,,,,,,,,,,,Industry,"Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that ""locks"" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model, which is five orders of magnitude smaller than the current state of the art. Moreover, it can span different operating points across the Pareto front without additional training. Finally, we show that Perfusion outperforms strong baselines in both qualitative and quantitative terms. Importantly, key-locking leads to novel results compared to traditional approaches, allowing to portray personalized object interactions in unprecedented ways, even in one-shot settings.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Vicuna-13B,Language,,2023-03-30,,Open access (non-commercial),,Open access (non-commercial),https://lmsys.org/blog/2023-03-30-vicuna/,"Large Model Systems Organization,UC Berkeley",,Might be possible to estimate training compute from the training cost. Fine-tuning cost $300.,"United States of America,United States of America","Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing",Historical significance,,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,,"70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations.",,,Speculative,,,0.00,13000000000.00,,,,,,,,,$259.00,"$300 in 2020, adjusted for inflation using BLS.gov inflation calculator",Academia,,2024-05-21 05:57,Anonymous,,,LLaMA-13B,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Robot Parkour,Robotics,,2023-09-12,,,,,https://arxiv.org/abs/2309.05665,"Shanghai Qi Zhi institute,Stanford University,Carnegie Mellon University (CMU),Tsinghua University",,"The paper provides some details on the training time and hardware used:

Each specialized skill policy (climbing, leaping, etc) was pre-trained with soft dynamics constraints for 12 hours using 1 Nvidia RTX 3090 GPU.
The skills were then fine-tuned with hard dynamics constraints for 6 hours each.
The final parkour policy distillation process used 4 computers with 1 RTX 3090 GPU each, training for an unspecified amount of time.
So the total training time was at least 12 + 6 x 5 = 42 hours for the initial skills, plus an additional unknown time for the distillation.

The hardware used was high-end Nvidia RTX 3090 GPUs, which at the time of paper writing would have been top of the line GPUs. Multiple GPUs were used in parallel during the distillation stage.","China,United States of America,United States of America,China","Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao",SOTA improvement,,Robot Parkour Learning,,"Isaac Gym simulated proprioceptive data, images, and actions",,,Likely,,,37.00,500000.00,"Parkour policy details on page 8, table 11.",,,,,,NVIDIA GeForce RTX 3090,,,,,"Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.",2024-05-01 09:22,Anonymous,,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),Language,,2018-08-14,,,,,https://arxiv.org/abs/1808.05908,IBM,,,United States of America,Siddhartha Brahma,SOTA improvement,"""our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.""",Improved Language Modeling by Decoding the Past,WikiText-2,,,,,,,6.00,35000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),,,,,,,,,,,Industry,,,,,Industry,,,
Verbatim Memory Transformer (182M),Language,,2022-10-24,CC BY 4.0 for code: https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,Unreleased,,Open source,https://arxiv.org/abs/2210.13569,"Johns Hopkins University,New York University (NYU)",,,"United States of America,United States of America","Kristijan Armeni, Christopher Honey, Tal Linzen",,,Characterizing Verbatim Short-Term Memory in Neural Language Models,WikiText-103,,,,,,,3.00,182000000.00,Table 3,,,,,,,,,,,,2024-04-30 12:32,Robi Rahman,Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
"MicroNet (Adaptive, Cache)",Language,,2020-01-01,"MIT license, code and weights: https://github.com/mit-han-lab/neurips-micronet ",Open source,,Open source,http://proceedings.mlr.press/v123/yan20a.html?ref=https://githubhelp.com,"Massachusetts Institute of Technology (MIT),Harvard University",,,"United States of America,United States of America","Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han",,,MicroNet for Efficient Language Modeling,WikiText-103,,,,,,,8.00,8300000.00,,,,,,,,,,,,,2024-05-22 11:26,Robi Rahman,"""MicroNet (Adaptive, Cache)""",,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
TSLM+MoS (WT2),Language,,2019-01-31,,,,,https://arxiv.org/abs/1901.11167,"Tianjin University,Beijing Institute of Technology",,,"China,China","Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",,,A Generalized Language Model in Tensor Space,,,,,,,,21.00,9120000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,TSLM+MoS (WT2),,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Transformer-XL + AutoDropout (WT2),Language,,2021-01-05,404: https://github.com/google-research/google-research/tree/master/auto_dropout ,Unreleased,,Unreleased,https://arxiv.org/abs/2101.01761,"Google Brain,Carnegie Mellon University (CMU)",,,"United States of America,United States of America","Hieu Pham, Quoc V. Le",,,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,WikiText-2,,,,,,,45.00,35000000.00,,,,,,,,,,,,,2024-05-09 17:02,Robi Rahman,Transformer-XL + AutoDropout (WT2),,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Stack RNN,Language,,2015-03-03,,,,,https://arxiv.org/abs/1503.01007,Facebook AI Research,,,United States of America,"Armand Joulin, Tomas Mikolov",,,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,,,,,,,,440.00,2010000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Stack RNN,,,,,,,,,,,Industry,,,,,Industry,,,
Gated HORNN (3rd order),Language,,2016-04-30,,,,,https://arxiv.org/abs/1605.00064,York University,,,Canada,"Rohollah Soltani, Hui Jiang",SOTA improvement,"""Both FOFEbased pooling and gated HORNNs have achieved the stateof-the-art performance, i.e., 100 in perplexity on this task.
To the best of our knowledge, this is the best reported performance on PTB under the same training condition.""",Higher Order Recurrent Neural Networks,Penn TreeBank,,,,,,,77.00,8970000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Gated HORNN (3rd order),,,,,,,,,,,Academia,,,,,Academia,,,
Transformer-XL+AdamGapAware(GA),Language,,2019-09-24,,Unreleased,,Unreleased,https://arxiv.org/abs/1909.10802,Technion - Israel Institute of Technology,,,Israel,"Saar Barkai, Ido Hakimi, Assaf Schuster",,,Gap Aware Mitigation of Gradient Staleness,,,,,,,,15.00,257000000.00,,,,,,,,,,,,,2024-05-22 11:45,Robi Rahman,Transformer-XL+AdamGapAware(GA),,,,,,,,,,,Academia,,,,,Academia,,,
Engine-XL(NE),Language,,2021-12-11,"MIT, code and weights: https://github.com/Zhongping-Zhang/ENGINE",Open source,,Open source,https://arxiv.org/abs/2112.05917,Boston University,,,United States of America,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",,,Show and Write: Entity-aware Article Generation with Image Information,WikiText-2,,,,,,,1.00,1500000000.00,,3.00,,,,,,,,,,,2024-05-06 14:48,Robi Rahman,Engine-XL(NE),,GPT-2 (1.5B),,,,,,,,,Academia,,,,,Academia,,,
Quantized ADMM,Language,,2021-11-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2111.14836,"Chinese University of Hong Kong (CUHK),Microsoft",,,"Hong Kong,United States of America","Junhao Xu, Xie Chen, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",,,Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers,,,,,Unknown,,,9.00,,,50.00,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,Quantized ADMM,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Amended-DARTS,Language,,2019-10-25,,Unreleased,,Unreleased,https://arxiv.org/abs/1910.11831,"Tsinghua University,Huawei,Tongji University",,,"China,China,China","Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian",,,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,,,,,,,,48.00,23000000.00,,,,,,,,,,,,,2024-05-22 11:37,Robi Rahman,Amended-DARTS,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Multipop Adaptive Continuous Stack (WT2),Language,,2018-02-15,,,,,https://openreview.net/forum?id=SkFqf0lAZ,"DeepMind,University of Oxford",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",,,Memory Architectures in Recurrent Neural Network Language Models,,,,,,,,63.00,26000000.00,,,,,,,,,,,,"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.",2024-05-21 05:25,Robi Rahman,Multipop Adaptive Continuous Stack (WT2),,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Transformer-XL + RMS dynamic eval,Language,,2019-04-17,Apache for code: https://github.com/benkrause/dynamiceval-transformer,Unreleased,Open source,Open source,https://arxiv.org/abs/1904.08378,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",SOTA improvement,"""By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.""",Dynamic Evaluation of Transformer Language Models,WikiText-103,,,,,,,40.00,257000000.00,,,,,,,,,,,,,2024-04-16 12:35,Robi Rahman,Transformer-XL + RMS dynamic eval,,,,,,,,,,,Academia,,,,,Academia,,,
CryptoGRU,Language,,2020-10-22,"repo here, don't think it has PTB code though https://github.com/bfeng/CryptoGRU ",Unreleased,,Unreleased,https://arxiv.org/abs/2010.11796,Indiana University Bloomington,,,United States of America,"Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox",,improves latency but not accuracy compared to other cryptographic models,CryptoGRU: Low Latency Privacy-Preserving Text Analysis With GRU,Penn TreeBank,,,,Unknown,,,12.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,CryptoGRU,,,,,,,,,,,Academia,,,,,Academia,,,
RNN (SGD+CLR) (PTB),Language,,2012-12-04,,,,,https://arxiv.org/abs/1212.0901,University of Montreal / Université de Montréal,,,Canada,"Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu",,,Advances in Optimizing Recurrent Networks,,,,,,,,665.00,2050000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,RNN (SGD+CLR) (PTB),,,,,,,,,,,Academia,,,,,Academia,,,
RNN + char4-MS-vec,Language,,2019-07-17,,Unreleased,,Unreleased,https://ojs.aaai.org/index.php/AAAI/article/view/4440,"NTT Communication Science Laboratories,Tohoku University",,,"Japan,Japan","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Character n-Gram Embeddings to Improve RNN Language Models,WikiText-103,,,,,,,26.00,226000000.00,,,,,,,,,,,,,2024-05-22 14:39,Robi Rahman,RNN + char4-MS-vec,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
GRU + p-tHSM (pretrain via Brown) (WT103),Language,,2017-08-19,,,,,https://www.ijcai.org/proceedings/2017/271,"Beihang University,University of Montreal / Université de Montréal,Chongqing University",,,"China,Canada,China","Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",,,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,WikiText-103,,,,,,,6.00,206000000.00,,,,,,,,,,,,"Recently, variants of neural networks for computational linguistics have been proposed and successfully applied to neural language modeling and neural machine translation. These neural models can leverage knowledge from massive corpora but they are extremely slow as they predict candidate words from a large vocabulary during training and inference. As an alternative to gradient approximation and softmax with class decomposition, we explore the tree-based hierarchical softmax method and reform its architecture, making it compatible with modern GPUs and introducing a compact tree-based loss function. When combined with several word hierarchical clustering algorithms, improved performance is achieved in language modelling task with intrinsic evaluation criterions on PTB, WikiText-2 and WikiText-103 datasets.",2024-05-10 04:50,Robi Rahman,GRU + p-tHSM (pretrain via Brown) (WT103),,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Temporal Convolutional Attention-based Network(TCAN) (WT2),Language,,2020-02-28,MIT license for code: https://github.com/haohy/TCAN,Unreleased,,Open source,https://arxiv.org/abs/2002.12530,"Nanjing University,Ant Group",,,"China,China","Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",SOTA improvement,"""We improve the state-of-theart results of ... 9.20 on WikiText-2""",Temporal Convolutional Attention-based Network For Sequence Modeling,WikiText-2,,,,,,,33.00,33000000.00,,,,,,,,,,,,,2024-04-23 10:04,Robi Rahman,Temporal Convolutional Attention-based Network(TCAN) (WT2),,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
WD+LR+M,Language,,2021-10-20,"code, no license specified: https://github.com/rmclarke/OptimisingWeightUpdateHyperparameters",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2110.10461,"University of Cambridge,Alan Turing Institute",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Ross M. Clarke, Elre T. Oldewage, José Miguel Hernández-Lobato",,,Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation,Penn TreeBank,,,,Unknown,,,9.00,,,72.00,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,WD+LR+M,,,,,,,,,,,"Academia,Government",,,,,"Academia,Government",,,
Fairseq + UID: variance,Language,,2021-05-15,,Unreleased,,Unreleased,https://arxiv.org/abs/2105.07144,"Google AI,ETH Zurich,University of Cambridge",,,"Multinational,Switzerland,United Kingdom of Great Britain and Northern Ireland","Jason Wei, Clara Meister, Ryan Cotterell",,,A Cognitive Regularizer for Language Modeling,,,,,Unknown,,,17.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,Fairseq + UID: variance,,,,,,,,,,Fairseq + UID: variance,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Transformer-XL + FWL,Language,,2022-12-05,"apache 2 for code:

https://github.com/google-research/google-research/tree/master/fwl
",Unreleased,,Open source,https://arxiv.org/abs/2212.02475,Google Research,,,Multinational,"Kevin Clark, Kelvin Guu, Ming-Wei Chang, Panupong Pasupat, Geoffrey Hinton, Mohammad Norouzi",,,Meta-Learning Fast Weight Language Models,WikiText-103,original Transformer-XL also trained on WikiText-103,103000000,,,,,6.00,257000000.00,"""For Transformer-XL,
we use the large model (257M parameters).""",,,,,,,,,,,,2024-04-30 11:23,Robi Rahman,TransformerXL + FWL,,,,"finetuned from TransformerXL. 

""To reduce the training cost, we add the FWLs on top of the publicly released pre-trained Transformer-XL model
and then fine-tune for 20K steps rather than training it from scratch.""",,,,,,TransformerXL + FWL,Industry,,,,,Industry,,,
H-LSTM+wg+rcp+rcg+wp,Language,,2019-01-30,,,,,https://arxiv.org/abs/1901.10997,"Princeton University,Alibaba",,,"United States of America,China","Hongxu Yin, Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, Niraj K. Jha",,,"""Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM""",,,,,,,,10.00,800000.00,,,,,,,,,,,,,2024-04-17 05:50,Robi Rahman,H-LSTM+wg+rcp+rcg+wp,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Integer Transformer,Language,,2020-09-17,,Unreleased,,Unreleased,https://arxiv.org/abs/2009.08034,"Northeastern University (China),NiuTrans Research,Chinese Academy of Sciences",,,"China,China,China","Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu, Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu",,,"Towards Fully 8-bit Integer Inference for the Transformer Model, Towards Fully 8-bit Integer Inference for the Transformer Model",,,,,,,,74.00,247000000.00,,,,,,,,,,,,,2024-05-10 15:23,Robi Rahman,"Integer Transformer,Integer Transformer",,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
2-layer skip-LSTM + dropout tuning (WT2),Language,,2018-05-23,,,,,https://arxiv.org/abs/1805.09208,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",,,Pushing the bounds of dropout,WikiText-2,,,,,,,14.00,5400000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,2-layer skip-LSTM + dropout tuning (WT2),,,,,,,,,,,Industry,,,,,Industry,,,
LSTM+GraB,Language,,2022-05-22,MIT for code. LSTM script here: https://github.com/EugeneLYC/GraB/blob/main/neurips22/examples/nlp/word_language_model/lstm_wiki.sh,Unreleased,,Open source,https://arxiv.org/abs/2205.10733,Cornell University,,Definitely <1e23 FLOP: all experiments used an NVIDIA GeForce RTX 2080 Ti GPU.,United States of America,"Yucheng Lu, Wentao Guo, Christopher De Sa",,,GraB: Finding Provably Better Data Permutations than Random Reshuffling,,,,,Unknown,,,12.00,,,50.00,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,LSTM+GraB,,,,,,,,,,,Academia,,,,,Academia,,,
PAR Transformer Large,Language,,2020-09-09,,Unreleased,,Unreleased,https://arxiv.org/abs/2009.04534,NVIDIA,,,United States of America,"Swetha Mandava, Szymon Migacz, Alex Fit Florea",,,Pay Attention when Required,WikiText-103,,,,Unknown,,,11.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,PAR Transformer Large,,,,,,,,,,,Industry,,,,,Industry,,,
RSM,Language,,2019-05-28,"code for PTB, Apache 2.0 license: https://github.com/Cerenaut/rsm ",Unreleased,,Open source,https://arxiv.org/abs/1905.11589,Cerenaut,,,Australia,"David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo",,,Learning distant cause and effect using only local and immediate credit assignment,,,,,Unknown,,,3.00,,,,,,,,,,,,,,2024-05-22 17:30,Robi Rahman,RSM,,,,,,,,,,,,,,,,,,,
Grown to Prune Two-layer stacked LSTM,Language,,2020-07-30,,Unreleased,,Unreleased,https://arxiv.org/abs/2007.15353,"University of Chicago,Toyota Technological Institute at Chicago",,,"United States of America,United States of America","Xin Yuan, Pedro Savarese, Michael Maire",,,Growing Efficient Deep Networks by Structured Continuous Sparsification,,,,,Unknown,,,37.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,Grown to Prune Two-layer stacked LSTM,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RHN+HSG(depth=40),Language,,2018-05-23,,,,,https://arxiv.org/abs/1805.09238,Ben-Gurion University,,,Israel,"Ron Shoham, Haim Permuter",,,Highway State Gating for Recurrent Highway Networks: improving information flow through time,,,,,Unknown,,,0.00,,,300.00,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,RHN+HSG(depth=40),,,,,,,,,,,Academia,,,,,Academia,,,
L_UL-seq,Language,,2019-08-12,"code and weights, non-commercial: https://github.com/facebookresearch/unlikelihood_training ",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/1908.04319,"New York University (NYU),Facebook AI Research,CIFAR AI Research",,,"United States of America,United States of America,Canada","Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston",,"""We empirically showed that
unlikelihood training - both at the token and sequence levels - substantially reduced degeneration
according to automatic metrics, and outperformed likelihood-trained models with various decoding
methods according to human evaluation, being superior to the current state-of-the-art approaches.""",Neural Text Generation with Unlikelihood Training,WikiText-103,,,,,,,454.00,247000000.00,,,,,,,,,,,,,2024-05-22 13:33,Robi Rahman,L_UL-seq,,,,,,,,,,,"Academia,Industry,Research collective",,,,,"Academia,Industry,Research collective",,,
DOC + Finetune∗ + Partial Shuffle (WT2),Language,,2019-03-11,,,,,https://arxiv.org/abs/1903.04167,University of Washington,,,United States of America,Ofir Press,,,Partially Shuffling the Training Data to Improve Language Models,WikiText-2,,,,,,,5.00,67300000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,DOC + Finetune∗ + Partial Shuffle (WT2),,,,,,,,,,,Academia,,,,,Academia,,,
retrieval-quality-kNN-LMs,Language,,2022-10-28,MIT for code: https://github.com/iesl/knnlm-retrieval-quality,Unreleased,,Open source,https://arxiv.org/abs/2210.15859,University of Massachusetts Amherst,,,United States of America,"Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, Mohit Iyyer",,,"""You can’t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM""",WikiText-103,,,,,,,10.00,247000000.00,,,,,,,,,,,,,2024-04-30 12:16,Robi Rahman,retrieval-quality-kNN-LMs,,Base LM + kNN LM + Continuous Cache,,"""Wikitext-103 The data is split 103M/217K/245K
tokens for training, validation, and test. We use the
pretrained model from Khandelwal et al. (2020),
and associated 267K word-level vocab.""",,,,,,,Academia,,,,,Academia,,,
SPALM + RelationLM,Language,,2022-01-24,,Unreleased,,Unreleased,https://arxiv.org/abs/2201.09680,"DeepMind,University of Oxford",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Qi Liu, Dani Yogatama, Phil Blunsom",,,Relational Memory-Augmented Language Models,WikiText-103,,,,,,,26.00,124000000.00,,,,,,,,,,,,,2024-05-06 14:35,Robi Rahman,SPALM + RelationLM,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Relational Memory Core,Language,,2018-06-05,,,,,https://arxiv.org/abs/1806.01822,"DeepMind,University College London (UCL)",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",SOTA improvement,"""Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.""",Relational recurrent neural networks,WikiText-103,,,,Unknown,,,235.00,,,,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,Relational Memory Core,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Transformer-XL+AdamP,Language,,2020-06-15,"repo here, looks like code for AdamP but not training code for the model: https://github.com/clovaai/AdamP ",Unreleased,,Unreleased,https://arxiv.org/abs/2006.08217,"Naver AI Lab,Naver Clova",,,"Korea (Republic of),Korea (Republic of)","Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",,,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,,,,,,,,114.00,257000000.00,,,,,,,,,,,,,2024-05-10 15:40,Robi Rahman,Transformer-XL+AdamP,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
LSTM + dynamic eval,Language,,2017-09-21,,,,,https://arxiv.org/abs/1709.07432,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",SOTA improvement,"""Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively""",Dynamic Evaluation of Neural Sequence Models,Penn TreeBank,,,,,,,130.00,50000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LSTM + dynamic eval,,,,,,,,,,,Academia,,,,,Academia,,,
Word-Independent-SRNN+KN5,Language,,2017-03-23,,,,,https://arxiv.org/abs/1703.08068,Saarland University,,,Germany,"Youssef Oualil, Clayton Greenberg, Mittul Singh, Dietrich Klakow",,,Sequential Recurrent Neural Networks for Language Modeling,,,,,,,,7.00,5320000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Word-Independent-SRNN+KN5,,,,,,,,,,,Academia,,,,,Academia,,,
dense-IndRNN+dynamic eval,Language,,2019-10-11,,Unreleased,,Unreleased,https://arxiv.org/abs/1910.06251,"Shandong University,University of Wollongong",,,"China,Australia","Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao",,,Deep Independently Recurrent Neural Network (IndRNN),Penn TreeBank,,,,,,,45.00,44100000.00,,,,,,,,,,,,,2024-05-22 11:44,Robi Rahman,dense-IndRNN+dynamic eval,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
"Mogrifier (d2, MoS2, MC) + dynamic eval",Language,,2019-09-04,Github has dead link: https://github.com/google-deepmind/lamb/blob/master/experiment/mogrifier/README.md,Unreleased,Open source,Unreleased,https://arxiv.org/abs/1909.01792,"DeepMind,University of Oxford",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Gábor Melis, Tomáš Kočiský, Phil Blunsom",SOTA improvement,"""We establish a new state of the art on all datasets with the exception of Enwik8""",Mogrifier LSTM,WikiText-2,,,,,,,109.00,35000000.00,,145.00,,,,,,,,,,,2024-04-18 14:50,Robi Rahman,"""Mogrifier (d2, MoS2, MC) + dynamic eval""",,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),Language,,2020-09-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2009.13774,"Johns Hopkins University,Xiaomi Corp",,,"United States of America,China","Ke Li, Daniel Povey, Sanjeev Khudanpur",,,Neural Language Modeling With Implicit Cache Pointers,,,,,,,,4.00,33000000.00,,,,,,,,,,,,,2024-05-10 15:22,Robi Rahman,Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
SPALM + kNN,Language,,2021-04-26,,Unreleased,,Unreleased,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00371/100688/Adaptive-Semiparametric-Language-Models,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Dani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong",,"paper says ""Our implementation produces results that are in the same range as state-of-the-art numbers, demonstrating the strength of our baselines"" I think this suggests it's close to, but not reaching SOTA?",Adaptive Semiparametric Language Models,,,,,Unknown,,,83.00,,,,,,,,,,,,,"We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states—similar to transformer-XL—and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines.",2024-05-21 13:05,Robi Rahman,SPALM + kNN,,,,,,,,,,SPALM + kNN,Industry,,,,,Industry,,,
1-layer-LSTM,Language,,2020-07-13,,Unreleased,,Unreleased,https://arxiv.org/abs/2007.06389,Harvard University,,,United States of America,"H. T. Kung, Bradley McDanel, Sai Qian Zhang",,,Term Revealing: Furthering Quantization at Run Time on Quantized DNNs,,,,,,,,9.00,86500000.00,,,,,,,,,,,,,2024-05-10 15:38,Robi Rahman,1-layer-LSTM,,,,,,,,,,,Academia,,,,,Academia,,,
E-SPA,Language,,2023-02-20,,Unreleased,,Unreleased,https://arxiv.org/abs/2302.10322,"University of Oxford,DeepMind",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L Smith, Yee Whye Teh",,,Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation,WikiText-103,,,,,,,15.00,243000000.00,,,,,,,,,,,,,2024-04-29 16:04,Robi Rahman,E-SPA,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Scatterbrain,Language,,2021-10-28,code: https://github.com/HazyResearch/fly,Unreleased,,Open source,https://arxiv.org/abs/2110.15343,"Stanford University,Adobe,University at Buffalo",,,"United States of America,United States of America,United States of America","Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré",,,Scatterbrain: Unifying Sparse and Low-rank Attention Approximation,WikiText-103,,,,Unknown,,,77.00,,,30.00,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,Scatterbrain,,,,,,,,,,Scatterbrain,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Byte-mLSTM+emb+WN+VD,Language,,2016-09-26,,,,,https://arxiv.org/abs/1609.07959,"University of Edinburgh,Toyota Technological Institute at Chicago",,,"United Kingdom of Great Britain and Northern Ireland,United States of America","Ben Krause, Liang Lu, Iain Murray, Steve Renals",,,Multiplicative LSTM for sequence modelling,,,,,,,,216.00,46000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,Byte-mLSTM+emb+WN+VD,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
True-Regularization+Finetune,Language,,2019-04-08,,,,,https://arxiv.org/abs/1904.04163,"Mobvoi,Williams College",,,"China,United States of America","Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",,not the best model in this paper,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,,,,,,,,24.00,7000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,True-Regularization+Finetune,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
LSTM-Large+Behaviorial-Gating,Language,,2019-08-31,,Unreleased,,Unreleased,https://arxiv.org/abs/1909.00107,University of Southern California,,,United States of America,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",,,Behavior Gated Language Models,,,,,,,,3.00,67000000.00,,,,,,,,,,,,,2024-05-22 11:59,Robi Rahman,LSTM-Large+Behaviorial-Gating,,,,,,,,,,,Academia,,,,,Academia,,,
LTM,Language,,2019-04-18,,Unreleased,,Unreleased,https://arxiv.org/abs/1904.08936,Murdoch University,,,Australia,"Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel, Hong Xie",,,Language Modeling through Long Term Memory Network,,,,,Unknown,,,19.00,,,,,,,,,,,,,,2024-05-22 17:46,Robi Rahman,LTM,,,,,,,,,,,Academia,,,,,Academia,,,
TF-LM-discourse LSTM (WT2),Language,,2018-05-01,,,,,https://aclanthology.org/L18-1470.pdf,ESAT - PSI,,,Belgium,"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",,,TF-LM: TensorFlow-based Language Modeling Toolkit,,,,,Unknown,,,11.00,,,39.00,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,TF-LM-discourse LSTM (WT2),,,,,,,,,,,Academia,,,,,Academia,,,
Transformer-XL + RMT,Language,,2022-07-14,Apache 2: https://github.com/booydar/LM-RMT,Unreleased,,Open source,https://arxiv.org/abs/2207.06881,"Moscow Institute of Physics and Technology,AIRI Artificial Intelligence Research Institute",,,"Russia,Russia","Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev",,,Recurrent Memory Transformer,WikiText-103,,,,,,,50.00,247000000.00,,,,,,,,,,,,,2024-04-30 13:07,Robi Rahman,Transformer-XL + RMT,,Transformer-XL (257M),,,,,,,,Transformer-XL + RMT,"Academia,Research collective",,,,,"Academia,Research collective",,,
S + I-Attention (3),Language,,2018-06-26,,,,,https://arxiv.org/abs/1806.10090,"National Research University Higher School of Economics,Samsung R&D Institute Russia",,,"Russia,Russia","Artyom Gadetsky, Ilya Yakubovskiy, Dmitry Vetrov",,,Conditional Generators of Words Definitions,Oxford Dictionary,,,,Unknown,,,56.00,,,35.00,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,S + I-Attention (3),,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
TF-LM-discourse LSTM (PTB),Language,,2018-05-01,,,,,https://aclanthology.org/L18-1470.pdf,ESAT - PSI,,,Belgium,"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",,,TF-LM: TensorFlow-based Language Modeling Toolkit,,,,,Unknown,,,11.00,,,39.00,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,TF-LM-discourse LSTM (PTB),,,,,,,,,,,Academia,,,,,Academia,,,
LaMemo,Language,,2022-04-15,"code and weights, MIT: https://github.com/thu-coai/LaMemo",Open source,,Open source,https://arxiv.org/abs/2204.07341,"Tsinghua University,NetEase,Oppo Mobile Telecommunications",,,"China,China,China","Haozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng Hu, Minlie Huang",,,LaMemo: Language Modeling with Look-Ahead Memory,,,,,,,,2.00,151000000.00,Table 1,79.53,,,,,,,,,,"Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory.",2024-05-22 12:12,Robi Rahman,LaMemo,,,,,,,,,,LaMemo,"Academia,Industry,Industry",,,,,"Academia,Industry,Industry",,,
RGC+ASQ (WT2),Language,,2018-08-13,,,,,https://arxiv.org/abs/1808.04357,"Tsinghua University,University of California Los Angeles (UCLA)",,,"China,United States of America","Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",,,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,,,,,,,,28.00,209000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,RGC+ASQ (WT2),,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Char-CNN-BiLSTM,Language,,2019-06-13,,Unreleased,,Unreleased,https://arxiv.org/abs/1906.05678,Capital One,,,United States of America,"Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller",SOTA improvement,"""Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.""",Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise,,,,,Unknown,,,2.00,,,,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,Char-CNN-BiLSTM,,,,,,,,,,,Industry,,,,,Industry,,,
RHN(depth=40),Language,,2018-05-23,,,,,https://arxiv.org/abs/1805.09238,Ben-Gurion University,,,Israel,"Ron Shoham, Haim Permuter",,,Highway State Gating for Recurrent Highway Networks: improving information flow through time,,,,,Unknown,,,0.00,,,300.00,,,,,,,,,,,2024-05-21 13:03,Robi Rahman,RHN(depth=40),,,,,,,,,,,Academia,,,,,Academia,,,
AWD-LSTM,Language,,2017-07-18,,,,,https://arxiv.org/abs/1707.05589,"DeepMind,University of Oxford",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Gábor Melis, Chris Dyer, Phil Blunsom",SOTA improvement,"""We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora""",On the State of the Art of Evaluation in Neural Language Models,WikiText-2,,,,,,,555.00,24000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,AWD-LSTM,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
TCN (148M),Language,,2018-02-15,,,,,https://openreview.net/forum?id=rk8wKk-R-,"Carnegie Mellon University (CMU),Intel Labs",,,"United States of America,Multinational","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,Convolutional Sequence Modeling Revisited,WikiText-103,,,,,,,64.00,148000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,TCN (148M),,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
2nd order FOFE-FNNLM,Language,,2015-05-06,,,,,https://arxiv.org/abs/1505.01504,"University of Science and Technology of China,York University",,,"China,Canada","Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai",,,A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models,Penn TreeBank,,,,,,,18.00,6000000.00,,,,,,,,,,,,,2024-03-27 17:32,Robi Rahman,2nd order FOFE-FNNLM,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
D-LSRC(200)+KN5,Language,,2017-08-22,,,,,https://arxiv.org/abs/1708.06555,Saarland University,,,Germany,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",,,Long-Short Range Context Neural Networks for Language Modeling,Penn TreeBank,,,,,,,19.00,7160000.00,,,,,,,,,,,,"The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.",2024-05-21 05:19,Robi Rahman,D-LSRC(200)+KN5,,,,,,,,,,,Academia,,,,,Academia,,,
Transformer-XL+WN+AdamP,Language,,2020-06-15,"repo here, looks like code for AdamP but not training code for the model: https://github.com/clovaai/AdamP ",Unreleased,,Unreleased,https://arxiv.org/abs/2006.08217,"Naver AI Lab,Naver Clova",,,"Korea (Republic of),Korea (Republic of)","Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",,,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,,,,,,,,114.00,257000000.00,,,,,,,,,,,,,2024-05-10 15:40,Robi Rahman,Transformer-XL+WN+AdamP,,,,,,,,,,,"Industry,Industry",,,,,"Industry,Industry",,,
TPM-LVD,Language,,2022-10-10,,Unreleased,,Unreleased,https://arxiv.org/abs/2210.04398,University of California Los Angeles (UCLA),,,United States of America,"Anji Liu, Honghua Zhang, Guy Van den Broeck",,,Scaling up Probabilistic Circuits by Latent Variable Distillation,WikiText-2,,,,,,,14.00,1120000000.00,,,,,,,,,,,,,2024-04-30 12:44,Robi Rahman,TPM-LVD,,,,,,,,,,,Academia,,,,,Academia,,,
FNetAR Medium,Language,,2021-07-22,,Unreleased,,Unreleased,https://arxiv.org/abs/2107.10932,X-Mechanics,,,United States of America,"Tim Lou, Michael Park, Mohammad Ramezanali, Vincent Tang",,not SOTA per https://paperswithcode.com/sota/language-modelling-on-wikitext-103,FNetAR: Mixing Tokens with Autoregressive Fourier Transforms,WikiText-103,,,,,,,2.00,34300000.00,,,,,,,,,,,,"In this note we examine the autoregressive generalization of the FNet algorithm, in which self-attention layers from the standard Transformer architecture are substituted with a trivial sparse-uniformsampling procedure based on Fourier transforms. Using the Wikitext-103 benchmark, we demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the task of causal language modelingcompared to a Transformer-XL baseline (24.2 ppl) with only half the number self-attention layers,thus providing further evidence for the superfluity of deep neural networks with heavily compoundedattention mechanisms. The autoregressive Fourier transform could likely be used for parameterreduction on most Transformer-based time-series prediction models.",2024-05-21 09:36,Robi Rahman,FNetAR Medium,,,,,,,,,,,Research collective,,,,,Research collective,,,
EN^2AS with performance reward,Language,,2019-07-22,,Unreleased,,Unreleased,https://arxiv.org/abs/1907.09109,"Beijing Institute of Technology,University of Technology Sydney,Monash University",,,"China,Australia,Australia","Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su",SOTA improvement,"""The best architecture obtained by our algorithm with
the same search space achieves the state-of-the-art test error rate of 2.51% on CIFAR-10""",Efficient Novelty-Driven Neural Architecture Search,,,,,,,,1.00,23000000.00,,,,,,,,,,,,,2024-04-18 14:43,Robi Rahman,EN^2AS with performance reward,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
SCRN(Structurally Constrained Recurrent Network),Language,,2014-12-24,,,,,https://arxiv.org/abs/1412.7753,Facebook AI Research,,,United States of America,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",,,Learning Longer Memory in Recurrent Neural Networks,,,,,,,,306.00,26500000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,SCRN(Structurally Constrained Recurrent Network),,,,,,,,,,,Industry,,,,,Industry,,,
top-down frozen classifier,Language,,2021-02-09,,Unreleased,,Unreleased,https://arxiv.org/abs/2102.04697,"University of Edinburgh,Toshiba Cambridge Research Laboratory",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals",SOTA improvement,"""Table 2 demonstrates that, to the best of our knowledge, top-down training results in state-of-the art character error rates for LSTM-based endto-end models on WSJ""",Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers,,,,,Unknown,,,2.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,top-down frozen classifier,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
LSTM-300units,Language,,2012-09-01,,,,,http://www.quaero.org/media/files/bibliographie/sundermeyer_lstm_neural_interspeech2012.pdf,RWTH Aachen,,,Germany,"Martin Sundermeyer, Ralf Schlüter, Hermann Ney",Highly cited,,LSTM Neural Networks for Language Modeling,,,,,,,,2503.00,12000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LSTM-300units,,,,,,,,,,,Academia,,,,,Academia,,,
VRNS-RNN-3-3-5,Language,,2022-10-04,"code here, unclear license:
https://github.com/bdusell/nondeterministic-stack-rnn",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2210.01343,University of Notre Dame,,,United States of America,"Brian DuSell, David Chiang",,,The Surprising Computational Power of Nondeterministic Stack RNNs,Penn TreeBank,,,,,,,2.00,1500000.00,,,,,,,,,,,,,2024-04-25 16:03,Robi Rahman,VRNS-RNN-3-3-5,,,,,,,,,,,Academia,,,,,Academia,,,
Decay RNN,Language,,2020-05-17,repo linked but link is broken,Unreleased,,Unreleased,https://arxiv.org/abs/2005.08199,Indian Institute of Technology Delhi,,,India,"Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal",,,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,Lizen et al 2016,,,,,,,7.00,1400000.00,,,,,,,,,,,,,2024-05-16 14:34,Robi Rahman,Decay RNN,,,,,,,,,,,Academia,,,,,Academia,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",Language,,2018-09-18,,,,,https://arxiv.org/abs/1809.06858,"Peking University,Microsoft Research Asia",,,"China,China","Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",SOTA improvement,"""Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2
and WMT14 English-German datasets.""",FRAGE: Frequency-Agnostic Word Representation,,,,,,,,152.00,35000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)""",,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
LSTM (2018),Language,,2018-03-04,,,,,https://arxiv.org/abs/1803.01271,"Intel Labs,Carnegie Mellon University (CMU)",,,"Multinational,United States of America","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",Highly cited,,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,Penn TreeBank,,,,,,,4024.00,13000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,LSTM (2018),,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
4-gram + 8 DENN,Language,,2014-12-22,,,,,https://arxiv.org/abs/1412.7063,IBM,,,United States of America,"Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran",,,Diverse Embedding Neural Network Language Models,,,,,,,,1.00,16100000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,4-gram + 8 DENN,,,,,,,,,,,Industry,,,,,Industry,,,
Alleviated TOI 10 (WT103),Language,,2019-09-18,BSD 3-Clause License for code: https://github.com/nkcr/overlap-ml ,Unreleased,,Open source,https://arxiv.org/abs/1909.08700,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),Swisscom,University of Freiburg",,,"Switzerland,Switzerland,Germany","Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",,,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,WikiText-103,,,,Unknown,,,0.00,,,1000.00,,,,,,,,,,"In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order information -Alleviated TOI- by iteratively overlapping the token composition of data points. For recurrent networks, we use prime numbers for the batch size to avoid redundancies when building batches from overlapped data points. The proposed method achieved state of the art performance in both text and speech related tasks.",2024-05-22 11:32,Robi Rahman,Alleviated TOI 10 (WT103),,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
GCNN-14,Language,,2016-12-23,,,,,https://arxiv.org/abs/1612.08083,Facebook AI Research,,,United States of America,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier, Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",Highly cited,,"Language Modeling with Gated Convolutional Networks, Language Modeling with Gated Convolutional Networks",WikiText-103,,,,Unknown,,,2176.00,,,35.00,,,,,,,,,,,2024-05-21 13:02,Robi Rahman,"GCNN-14,GCNN-14",,,,,,,,,,,Industry,,,,,Industry,,,
DOT(S)-RNN,Language,,2013-12-20,,,,,https://arxiv.org/abs/1312.6026,"Aalto University,University of Montreal / Université de Montréal",,,"Finland,Canada","Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio",Highly cited,,How to Construct Deep Recurrent Neural Networks,,,,,,,,1255.00,6160000.00,,,,,,,,,,,,,2024-05-22 12:24,Robi Rahman,DOT(S)-RNN,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Adaptive Inputs + LayerDrop,Language,,2019-09-25,"https://github.com/facebookresearch/fairseq/blob/main/examples/layerdrop/README.md

Repo has MIT license",Open source,Open source,Open source,https://arxiv.org/abs/1909.11556,"Facebook AI Research,LORIA",,,"United States of America,France","Angela Fan, Edouard Grave, Armand Joulin",SOTA improvement,"""In neural machine translation on newstest2014, our 12 encoder layer Transformer model with LayerDrop further improves the state of the art, reaching 30.2 BLEU""",Reducing Transformer Depth on Demand with Structured Dropout,WikiText-103,,,,,,,482.00,423000000.00,,,,,,,,,,,,,2024-05-01 09:05,Robi Rahman,Adaptive Inputs + LayerDrop,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Neural cache model (size=2000) (300M),Language,,2016-12-13,,,,,https://arxiv.org/abs/1612.04426,Facebook AI Research,,,United States of America,"Edouard Grave, Armand Joulin, Nicolas Usunier",,,Improving Neural Language Models with a Continuous Cache,WikiText-103,,,,,,,302.00,300000000.00,,,,,,,,,,,,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",2024-05-10 00:37,Robi Rahman,Neural cache model (size=2000) (300M),,,,,,,,,,,Industry,,,,,Industry,,,
RNN+LDA+KN5+cache,Language,,2012-12-01,,,,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,"Microsoft,Brno University of Technology",,,"United States of America,Czechia","Tomas Mikolov, Geoffrey Zweig",SOTA improvement,"""We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art""",Context dependent recurrent neural network language model,Penn TreeBank,,,,,,,716.00,9000000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,RNN+LDA+KN5+cache,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Hyena-3-slim,Language,,2023-02-21,,Unreleased,,Unreleased,https://arxiv.org/abs/2302.10866,"Stanford University,University of Montreal / Université de Montréal,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",,,"United States of America,Canada,Canada","Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré",,,Hyena Hierarchy: Towards Larger Convolutional Language Models,"WikiText-103,The Pile","""Next, we verify the scaling of Hyena on autoregressive language modeling. We evaluate the perplexity on
WikiText103 (Table 4.3) and The Pile (Table 4.4).""",,,,,,100.00,125000000.00,,,,,,,,,,,,,2024-05-22 13:22,Robi Rahman,Hyena-3-slim,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
ADP-FAIRSEQ+NGRAMRES,Language,,2022-10-26,no license specified: https://github.com/ghrua/NgramRes,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2210.14431,"Tsinghua University,Chinese University of Hong Kong (CUHK),Nara Institute of Science and Technology",,,"China,Hong Kong,Japan","Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",,,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,WikiText-103,,,,,,,1.00,247000000.00,Table 2,,,,,,,,,,,"N-gram language models (LM) have been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an n-gram LM and the real-data distribution. The combination of n-gram and neural LMs not only allows the neural part to focus on the deeper understanding of language but also provides a flexible way to customize an LM by switching the underlying n-gram model without changing the neural model. Experimental results on three typical language tasks (i.e., language modeling, machine translation, and summarization) demonstrate that our approach attains additional performance gains over popular standalone neural models consistently. We also show that our approach allows for effective domain adaptation by simply switching to a domain-specific n-gram model, without any extra training. Our code is released at this https URL.",2024-05-21 05:27,Robi Rahman,ADP-FAIRSEQ+NGRAMRES,,,,,,,,,,ADP-FAIRSEQ+NGRAMRES,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
TransfoRNN(d=1024)(2-layer) (WT2),Language,,2021-04-04,,Unreleased,,Unreleased,https://arxiv.org/abs/2104.01572,Lenovo Research,,,China,"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",,,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,,,,,,,,0.00,97600000.00,,,,,,,,,,,,,2024-05-08 11:37,Robi Rahman,TransfoRNN(d=1024)(2-layer) (WT2),,,,,,,,,,,Industry,,,,,Industry,,,
CD-GraB (WT103),Language,,2023-02-02,"training code, Apache 2: https://github.com/GarlGuo/CD-GraB",Unreleased,,Open source,https://arxiv.org/abs/2302.00845,Cornell University,,,United States of America,"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",,,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,WikiText-103,,,,,,,2.00,6846080.00,They provide the model initialization code in Appendix D. Running it  gives the parameter count.,30.00,,,,,,,,,,,2024-04-29 16:11,Robi Rahman,CD-GraB (WT103),,,,,,,,,,,Academia,,,,,Academia,,,
True-Regularization+Finetune+Dynamic-Eval,Language,,2019-04-08,,Unreleased,Open access (non-commercial),Unreleased,https://arxiv.org/abs/1904.04163,"Mobvoi,Williams College",,,"China,United States of America","Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",SOTA improvement,"""In the first experiment, the student model achieves state-of-the-art perplexity results on the Penn Treebank dataset [1] with a model size one third of that of the
previously published best model""",Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,Penn TreeBank,,,,,,,24.00,7000000.00,,,,,,,,,,,,,2024-04-16 10:07,Robi Rahman,True-Regularization+Finetune+Dynamic-Eval,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Transformer LM + MinSen,Language,,2021-11-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2112.11540,Chinese University of Hong Kong (CUHK),,,Hong Kong,"Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",,,Mixed Precision of Quantization of Transformer Language Models for Speech Recognition,,,,,Unknown,,,10.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,Transformer LM + MinSen,,,,,,,,,,,Academia,,,,,Academia,,,
Transformer + Average Attention Network,Language,,2019-01-01,,,,,https://ieeexplore.ieee.org/abstract/document/9067534,University of Electronic Science and Technology of China,,,China,"Jian Guo Zhang, Jian Ping Li, Huang Li",,,Language Modeling with Transformer,WikiText-103,,,,Unknown,,,126.00,,,,,,,,,,,,,"To date, the main method of language modeling is based on recurrent neural networks or convolutional neural networks. We show that two simple models which get inspiration from Transformer [1]. Compare to other attention network, the Transformer which just use self-attention and FFN (feedforward network) are highly efficient in training. We apply the idea which from the mechanism of Transformer to language module. We predict the future elements by the long-term dependence of context words which through the ANN (average attention network) and self-attention mechanism. We test our model on WikiText-103 where our model achieves 22.13 perplexity and on the Google Billion Word benchmark, which we achieve 26.31 perplexity.",2024-05-22 12:17,Robi Rahman,Transformer + Average Attention Network,,,,,,,,,,,Academia,,,,,Academia,,,
TransformerXL-LayerFusion-CA,Language,,2020-07-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2007.14917,"University of Liverpool,University of Southern California",,,"United Kingdom of Great Britain and Northern Ireland,United States of America","James O' Neill, Greg Ver Steeg, Aram Galstyan",,,Compressing Deep Neural Networks via Layer Fusion,,,,,Unknown,,,5.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,TransformerXL-LayerFusion-CA,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
GPT2-LayerFusion-WS,Language,,2020-07-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2007.14917,"University of Liverpool,University of Southern California",,,"United Kingdom of Great Britain and Northern Ireland,United States of America","James O' Neill, Greg Ver Steeg, Aram Galstyan",,,Compressing Deep Neural Networks via Layer Fusion,,,,,Unknown,,,5.00,,,,,,,,,,,,,,2024-05-21 13:05,Robi Rahman,GPT2-LayerFusion-WS,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
LSTM+Adam+Lookahead,Language,,2019-07-19,"may just be optimizer code, not training code: https://github.com/michaelrzhang/lookahead?tab=readme-ov-file ",Unreleased,,Unreleased,https://arxiv.org/abs/1907.08610,University of Toronto,,,Canada,"Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba",,,"""Lookahead Optimizer: k steps forward, 1 step back""",,,,,,,,612.00,7190000.00,,,,,,,,,,,,,2024-05-22 13:35,Robi Rahman,LSTM+Adam+Lookahead,,,,,,,,,,,Academia,,,,,Academia,,,
Routing Transformer,Language,,2020-03-12,"code/weights: https://github.com/google-research/google-research/tree/master/routing_transformer

repo is Apache 2.0: https://github.com/google-research/google-research/blob/master/LICENSE",Open source,,Open source,https://arxiv.org/abs/2003.05997,Google Research,,,Multinational,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",SOTA improvement,"""Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192""",Efficient Content-Based Sparse Attention with Routing Transformers,WikiText-103,,,,,,,436.00,79500000.00,,,,,,,,,,,,,2024-05-01 09:05,Robi Rahman,Routing Transformer,,,,,,,,,,,Industry,,,,,Industry,,,
Stacked-LSTM+PruningNo,Language,,2019-06-17,,Unreleased,,Unreleased,https://arxiv.org/abs/1906.06847,"University of Electronic Science and Technology of China,Chinese University of Hong Kong (CUHK),Peng Cheng Laboratory",,,"China,Hong Kong,China","Liangjian Wen, Xuanyang Zhang, Haoli Bai, Zenglin Xu",,,Structured Pruning of Recurrent Neural Networks through Neuron Selection,,,,,,,,34.00,6160000.00,,,,,,,,,,,,,2024-05-22 16:38,Robi Rahman,Stacked-LSTM+Pruning,,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
WeNet (WT2),Language,,2019-04-08,,,,,https://arxiv.org/abs/1904.03819,Amazon,,,United States of America,"Zhiheng Huang, Bing Xiang",,,WeNet: Weighted Networks for Recurrent Network Architecture Search,WikiText-2,,,,,,,5.00,33000000.00,,6000.00,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,WeNet (WT2),,,,,,,,,,,Industry,,,,,Industry,,,
NMM(LSTM+RNN),Language,,2017-08-23,,,,,https://arxiv.org/abs/1708.06989,Saarland University,,,Germany,"Youssef Oualil, Dietrich Klakow",,,A Neural Network Approach for Mixing Language Models,,,,,,,,10.00,5180000.00,,,,,,,,,,,,,2024-03-07 14:22,Robi Rahman,NMM(LSTM+RNN),,,,,,,,,,,Academia,,,,,Academia,,,
CODA,Language,,2021-05-31,MIT for code: https://github.com/LZhengisme/CODA,Unreleased,,Open source,https://arxiv.org/abs/2105.14850,"The University of Hong Kong,Sun Yat-sen University,Shanghai AI Lab",,,"Hong Kong,China,China","Lin Zheng, Zhiyong Wu, Lingpeng Kong",,,Cascaded Head-colliding Attention,WikiText-103,,,,,,,2.00,247000000.00,,,,,,,,,,,,,2024-05-07 10:53,Robi Rahman,CODA,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Big-Little Net,Vision,Image classification,2018-07-10,,,,,https://arxiv.org/abs/1807.03848,IBM,,,United States of America,"Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris",SOTA improvement,"""On object recognition task, we demonstrated that our approach provides approximately 2× speedup over baselines while
improving accuracy, and the result significantly outperforms the state-of-the-art networks by a large margin in terms of accuracy and FLOPs reduction""",Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,ImageNet,,1280000,,Likely,,,83.00,77360000.00,Table 2,110.00,582500000.00,9320000000 FLOPs per batch of 16 images,,,NVIDIA Tesla K80,,,,,"In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.",2024-05-01 09:13,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
GPT-4V,"Multimodal,Vision,Language","Language modelling,Visual question answering",2023-09-25,,API access,,,https://cdn.openai.com/papers/GPTV_System_Card.pdf,OpenAI,,,United States of America,,Significant use,Incorporated into ChatGPT,GPT-4V(ision) system card,,,,,Unknown,,,0.00,,,,,,,,,Reinforcement learning,,,Industry,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Fisher Kernel GMM,Vision,Image classification,2017-01-01,,,,,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.7388&rep=rep1&type=pdf,Xerox,,2.5 hours on an AMD Opteron 2.4GHz with 4GB RAM,United States of America,"Florent Perronnin, Christopher Dance",Highly cited,,Fisher kernels on visual vocabularies for image categorization,,"in-house image dataset of 19 object/scene categories

",30000,"""Approximately 30K images were available for training and 5K for testing. Both sets were manually multi-labeled""",Likely,,,1915.00,,,,,,2.5,"""With Fisher kernels, the training
cost is reduced down to approximately 2h30.""",,Supervised,,,,"Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to
characterize a signal with a gradient vector derived from a
generative probability model and to subsequently feed this
representation to a discriminative classifier. We propose to
apply this framework to image categorization where the input signals are images and where the underlying generative
model is a visual vocabulary: a Gaussian mixture model
which approximates the distribution of low-level features in
images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Low-Cost Collaborative Network,Vision,Image classification,2017-05-15,,,,,https://arxiv.org/abs/1703.08651,"National University of Singapore,University of Technology Sydney,Qihoo 360 AI Institute",,Seems to suggest 400 epochs were used for CIFAR experiments.,"Singapore,Australia,China","Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan",,,More is Less: A More Complicated Network with Less Inference Complexity,"CIFAR-10,CIFAR-100,ImageNet",,1280000,,Speculative,,,266.00,,,400.00,,,,,,,,,,"In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in
network structure yet with less inference complexity. The
core idea is to equip each original convolutional layer
with another low-cost collaborative layer (LCCL), and the
element-wise multiplication of the ReLU outputs of these
two parallel layers produces the layer-wise output. The
combined layer is potentially more discriminative than the
original convolutional layer, and its inference is faster for
two reasons: 1) the zero cells of the LCCL feature maps will
remain zero after element-wise multiplication, and thus it is
safe to skip the calculation of the corresponding high-cost
convolution in the original convolutional layer; 2) LCCL
is very fast if it is implemented as a 1 × 1 convolution or
only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012
benchmarks show that our proposed network structure can
accelerate the inference process by 32% on average with
negligible performance drop.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
ResNet-200,Vision,Image classification,2016-09-17,,,,,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,Microsoft,,"""ResNet-200 takes about 3 weeks to train on 8 GPUs"". didn't specify which GPU",United States of America,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,Identity Mappings in Deep Residual Networks,ImageNet,,,,Unknown,,,8927.00,,,,,,500.0,"""about 3 weeks""",,,,,,"Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Ferret (13B),"Multimodal,Language,Vision","Object recognition,Language modelling",2023-10-11,"https://github.com/apple/ml-ferret?tab=License-1-ov-file#readme

confusingly, the license page in the repo is permissive and MIT-like, but the README says ""The data, and code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes."" ",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/2310.07704,"Columbia University,Apple",,"Fine-tuned from Vicuna-13B, which we don't have an estimate for. Finetuning cost is ~4e19.

""Training Details. We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vicuna, and the projection layer with LLaVA’s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of 2e − 5 and a batch size of 128. The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19","United States of America,United States of America","Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang",SOTA improvement,"""To evaluate this new capability, we introduce Ferret-Bench, covering three new types of tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.""",Ferret: Refer and Ground Anything Anywhere at Any Granularity,GRIT,"""In order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following, and robust, we collect GRIT, a Ground-and-Refer Instruction-Tuning dataset with 1.1M samples. GRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descriptions, and complex reasoning. It includes both text-in location-out (grounding) and location-in textout (referring) data, as well as data that mixes location and text in both input and output""",,,Likely,,,84.00,13000000000.00,13B,3.00,,,120.0,"""The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""",NVIDIA A100,,,,,"We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.",2024-05-13 08:31,Anonymous,,,Vicuna-13B,40400000000000000000,"""The training takes ~5 days on 8 A100 GPU for a Ferret-13B""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",8,,,,,,"Academia,Industry",,,,960,"Academia,Industry",,,
Show-1,Video,Video generation,2023-09-27,,,,,https://arxiv.org/abs/2309.15818,National University of Singapore,,,Singapore,"David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou",SOTA improvement,"""Our approach achieves state-of-the-art performance on standard benchmarks including UCF-101 and MSR-VTT.""",Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation,WebVid-10M,"""WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. 10.7M video-caption pairs. 52K total video hours.""",,,Unknown,,,,,,,,,,,NVIDIA A100,,,,,"Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,checked,,
SimCSE,Language,Semantic embedding,2022-05-18,,,,,https://arxiv.org/abs/2104.08821,"Princeton University,Tsinghua University",,,"United States of America,China","Tianyu Gao, Xingcheng Yao, Danqi Chen","Highly cited,SOTA improvement",,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,"""Training details. We start from pre-trained checkpoints of BERT (Devlin et al., 2019) (uncased) or RoBERTa (Liu et al., 2019) (cased) and take the [CLS] representation as the sentence embedding (see §6.3 for comparison between different pooling methods). We train unsupervised SimCSE on 106 randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k). More training details can be found in Appendix A""",,,Unknown,,,2079.00,,,,,,,,,,,,Academia,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using ""entailment"" pairs as positives and ""contradiction"" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",2024-05-21 13:05,Anonymous,,,RoBERTa Large,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
ChatGPT (gpt-3.5-turbo),Language,,2022-11-30,,,,,,OpenAI,,,United States of America,,"Historical significance,Significant use",https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/,,,,,,Speculative,,,,20000000000.00,20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,,,,,,,,Industry,,2024-04-03 10:01,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
CodeT5+,Language,"Code generation,Code autocompletion",2023-05-20,https://github.com/salesforce/CodeT5/blob/main/LICENSE.txt,Open source,,,https://arxiv.org/abs/2305.07922,Salesforce,,,United States of America,"Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi",SOTA improvement,"""We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks""",CodeT5+: Open Code Large Language Models for Code Understanding and Generation,,"""We enlarge the pretraining dataset of CodeSearchNet [Husain et al., 2019] with the recently released GitHub Code dataset""",,"""We use the CodeT5 tokenizer to tokenize the multilingual dataset, resulting in 51.5B tokens""",,,,168.00,16000000000.00,"""We implemented a family of CodeT5+ models, with model sizes ranging from 220M to 16B""",10.00,,,,,NVIDIA A100,,,,,"""Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.""",2024-05-01 09:24,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
CodeGen-Mono 16.1B,Language,"Code generation,Code autocompletion",2023-02-27,apache 2.0,Open source,,,https://arxiv.org/abs/2203.13474,Salesforce,,,United States of America,"Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong",,"Not as good as code-davinci-001 or code-davinci-002, per Table 1",CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,"The Pile,Big Query,BigPython","""The family of CODEGEN models is trained sequentially on three datasets: The Pile, BigQuery, and BigPython.""",568200000000,Table 5,Likely,,,420.00,16100000000.00,16.1B parameters,,,,,,Google TPU v4,,,,,"""Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: this https URL.""",2024-05-21 03:25,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Claude 1.3,Language,"Language modelling,Chat",2023-04-18,,,,,https://twitter.com/AnthropicAI/status/1648353600350060545?lang=en,Anthropic,,,United States of America,,,100k context window may have been SOTA at the time.,,,,,,Unknown,,,,,,,,,,,,,,,Industry,,2024-05-23 06:35,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Mistral 7B,Language,"Code generation,Language generation",2023-10-10,apache 2.0,Open source,,,https://arxiv.org/abs/2310.06825,Mistral AI,,,France,"Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",,,Mistral 7B,,,,,Speculative,,,237.00,7000000000.00,,,,,,,,,,,Industry,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",2024-04-03 10:01,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
DALL·E 3,Image generation,Image generation,2023-10-19,https://platform.openai.com/docs/models/dall-e,API access,,,https://cdn.openai.com/papers/dall-e-3.pdf,OpenAI,,,United States of America,"James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",SOTA improvement,,Improving Image Generation with Better Captions,,,,,Unknown,,,243.00,,,,,,,,,,,,Industry,"We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.
Existing text-to-image models struggle to follow detailed image descriptions and
often ignore words or confuse the meaning of prompts. We hypothesize that this
issue stems from noisy and inaccurate image captions in the training dataset. We
address this by training a bespoke image captioner and use it to recaption the
training dataset. We then train several text-to-image models and find that training
on these synthetic captions reliably improves prompt following ability. Finally, we
use these findings to build DALL-E 3: a new text-to-image generation system, and
benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.
",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Neural cache model (size=2000),Language,,2016-12-13,,,,,https://arxiv.org/abs/1612.04426,Facebook AI Research,,,United States of America,"Edouard Grave, Armand Joulin, Nicolas Usunier",,,Improving Neural Language Models with a Continuous Cache,,,,,Unknown,,,,,,,,,,,,,,,,,2024-04-17 06:13,Robi Rahman,Neural cache model (size=2000),1,,,,,,,,,,Industry,,,,,Industry,,,
RNN+LDA,Language,,2012-12-01,,,,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,Microsoft Research,,,United States of America,"Tomas Mikolov, Geoffrey Zweig",,,Context dependent recurrent neural network language model,,,,,Unverified,,,,,,,,,,,,,,,,"Recurrent neural network language models (RNNLMs) have
recently demonstrated state-of-the-art performance across a
variety of tasks. In this paper, we improve their performance
by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual
information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding
text, we achieve a topic-conditioned RNNLM. This approach
has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data
subsets. We report perplexity results on the Penn Treebank
data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition
task, where we observe improvements in word-error-rate.",2024-05-10 04:34,Robi Rahman,RNN+LDA,1,,,,,,,,,,Industry,,,,,Industry,,,
Selfish-RNN (ON-LSTM),Language,,2021-01-22,"code, no clear license https://github.com/Shiweiliuiiiiiii/Selfish-RNN  ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2101.09048,Eindhoven University of Technology,,,Netherlands,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",,,Selfish Sparse RNN Training,Penn TreeBank,,,,Unverified,,,,,,1000.00,,,,,,,,,,"Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at this https URL.",2024-05-10 04:57,Robi Rahman,Selfish-RNN (ON-LSTM),1,,,,,,,,,,Academia,,,,,Academia,,,
TransfoRNN(d=1024)(2-layer) (PTB),Language,,2021-04-04,,Unreleased,,Unreleased,https://arxiv.org/abs/2104.01572,Lenovo Research,,,China,"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",,,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,,,,,Unverified,,,,,,,,,,,,,,,,"In this paper, we describe the use of recurrent neural networks to capture sequential information from the self-attention representations to improve the Transformers. Although self-attention mechanism provides a means to exploit long context, the sequential information, i.e. the arrangement of tokens, is not explicitly captured. We propose to cascade the recurrent neural networks to the Transformers, which referred to as the TransfoRNN model, to capture the sequential information. We found that the TransfoRNN models which consists of only shallow Transformers stack is suffice to give comparable, if not better, performance than a deeper Transformer model. Evaluated on the Penn Treebank and WikiText-2 corpora, the proposed TransfoRNN model has shown lower model perplexities with fewer number of model parameters. On the Penn Treebank corpus, the model perplexities were reduced up to 5.5% with the model size reduced up to 10.5%. On the WikiText-2 corpus, the model perplexity was reduced up to 2.2% with a 27.7% smaller model. Also, the TransfoRNN model was applied on the LibriSpeech speech recognition task and has shown comparable results with the Transformer models.",2024-05-09 13:30,Robi Rahman,TransfoRNN(d=1024)(2-layer) (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
SPN-4,Language,,2014-01-01,,,,,https://www.isca-archive.org/interspeech_2014/cheng14_interspeech.html,"Singapore University of Technology & Design,DSO National Laboratories",,,"Singapore,Singapore","W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",,,Language modeling with sum-product networks,,,,,Unverified,,,,,,,,,,,,,,,,"Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN's inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems, we are the first to use it for language modeling. Our empirical comparisons with six previous language models indicate that our SPN has superior performance.",2024-05-22 12:15,Robi Rahman,SPN-4,1,,,,,,,,,,"Academia,Government",,,,,"Academia,Government",,,
LLaMA-7B (LoRA finetuned),Language,,2023-05-23,,Unreleased,,Unreleased,https://arxiv.org/pdf/2305.14152.pdf,NAVER,,,Korea (Republic of),"Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee",,,"Memory-Efficient Fine-Tuning of Compressed Large
Language Models via sub-4-bit Integer Quantization",,,,,Unverified,,,,7000000000.00,,1.09,,,,,,,,,,"Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.",2024-05-09 14:15,Robi Rahman,LLaMA-7B (LoRA finetuned),1,LLaMA-7B,,,,,,,,,Industry,,,,,Industry,,,
DEQ-TrellisNet,Language,,2019-09-03,"repo but no code for TrellisNet
https://github.com/locuslab/deq/tree/master/DEQ-Sequence ",Unreleased,,Unreleased,https://arxiv.org/abs/1909.01377,"Carnegie Mellon University (CMU),Intel Labs",,,"United States of America,Multinational","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,Deep Equilibrium Models,,,,,Unverified,,,,,,12.00,,,,,,,,,,"We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective ""depth"" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at this https URL .",2024-05-22 11:58,Robi Rahman,DEQ-TrellisNet,1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
VD-LSTM+REAL Medium,Language,,2016-11-04,,,,,https://arxiv.org/abs/1611.01462,"Stanford University,Salesforce Research",,,"United States of America,United States of America","Hakan Inan, Khashayar Khosravi, Richard Socher",,,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,,,,,Likely,,,,22100000.00,"VD-LSTM+REAL Large has 51M parameters. The parameter count of the Medium model is not reported, but they say it has 650 hidden units per layer, compared to 1500 for the Large model. Neglecting the rest of the architecture,
51M * (650/1500) = 22.1M",,,,,,,,,,,,2024-04-17 06:03,Robi Rahman,VD-LSTM+REAL Medium,1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),Language,,2017-08-29,,,,,https://arxiv.org/abs/1708.08863,Ben-Gurion University,,,Israel,"Ziv Aharoni, Gal Rattner, Haim Permuter",,,Gradual Learning of Recurrent Neural Networks,,,,,Unverified,,,,,,1000.00,,,,,,,,,,"Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many sequence-to-sequence modeling tasks. However, RNNs are difficult to train and tend to suffer from overfitting. Motivated by the Data Processing Inequality (DPI), we formulate the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping. We found that applying our methods, combined with previously introduced regularization and optimization methods, resulted in improvements in state-of-the-art architectures operating in language modeling tasks.",2024-05-09 14:17,Robi Rahman,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),1,,,,,,,,,,Academia,,,,,Academia,,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),Language,,2018-08-30,,,,,https://arxiv.org/abs/1808.10143,"NTT Communication Science Laboratories,Tohoku University",,,"Japan,Japan","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Direct Output Connection for a High-Rank Language Model,,,,,Unverified,,,,,,300.00,,,,,,,,,,"This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.",2024-05-10 04:48,Robi Rahman,(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),1,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
MGK 8 heads (small),Language,,2021-10-16,"code: https://github.com/minhtannguyen/transformer-mgk/blob/main/language-modeling/lmtool-fwms/README.md

commercial license: https://github.com/minhtannguyen/transformer-mgk/blob/main/LICENSE ",Unreleased,,Open source,https://arxiv.org/abs/2110.08678,"FPT Software AI Center,University of California Los Angeles (UCLA),VinUniversity,Deezer Research,Rice University,University of Texas at Austin",,,"Viet Nam,United States of America,Viet Nam,France,United States of America,United States of America","Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",,,Improving Transformers with Probabilistic Attention Keys,,,,,Unverified,,,,,,120.00,,,,,,,,,,"Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.",2024-05-21 09:45,Robi Rahman,MGK 8 heads (small),1,,,,,,,,,,"Industry,Academia,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia,Academia",,,
AWD-LSTM-MoS+Noisin+dynamic evaluation,Language,,2018-05-03,,,,,https://arxiv.org/abs/1805.01500,"Columbia University,New York University (NYU),Princeton University",,,"United States of America,United States of America,United States of America","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",,,Noisin: Unbiased Regularization for Recurrent Neural Networks,,,,,Unverified,,,,,,400.00,,,,,,,,,,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",2024-05-10 00:34,Robi Rahman,AWD-LSTM-MoS+Noisin+dynamic evaluation ,1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Selfish-RNN (SNT-ASGD)RHNs,Language,,2021-01-22,"code, no clear license https://github.com/Shiweiliuiiiiiii/Selfish-RNN  ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2101.09048,Eindhoven University of Technology,,,Netherlands,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",,,Selfish Sparse RNN Training,Penn TreeBank,,,,Unverified,,,,,,500.00,,,,,,,,,,"Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at this https URL.",2024-05-10 04:57,Robi Rahman,Selfish-RNN (SNT-ASGD)RHNs,1,,,,,,,,,,Academia,,,,,Academia,,,
Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),Language,,2020-09-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2009.13774,"Johns Hopkins University,Xiaomi Corp",,,"United States of America,China","Ke Li, Daniel Povey, Sanjeev Khudanpur",,,Neural Language Modeling With Implicit Cache Pointers,,,,,Unverified,,,,,,,,,,,,,,,,"A cache-inspired approach is proposed for neural language models (LMs) to improve long-range dependency and better predict rare words from long contexts. This approach is a simpler alternative to attention-based pointer mechanism that enables neural LMs to reproduce words from recent history. Without using attention and mixture structure, the method only involves appending extra tokens that represent words in history to the output layer of a neural LM and modifying training supervisions accordingly. A memory-augmentation unit is introduced to learn words that are particularly likely to repeat. We experiment with both recurrent neural network- and Transformer-based LMs. Perplexity evaluation on Penn Treebank and WikiText-2 shows the proposed model outperforms both LSTM and LSTM with attention-based pointer mechanism and is more effective on rare words. N-best rescoring experiments on Switchboard indicate that it benefits both very rare and frequent words. However, it is challenging for the proposed model as well as two other models with attention-based pointer mechanism to obtain good overall WER reductions.",2024-05-10 15:22,Robi Rahman,Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
OPT-2.7B (finetuned on PTB),Language,Language modelling,2022-06-21,"MIT, weights+code https://github.com/facebookresearch/metaseq?tab=readme-ov-file",Open source,,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,2700000000.00,,1.67,,,,,,,,,,,2024-04-17 05:51,Robi Rahman,OPT-2.7B (finetuned on PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
SparseOPT-66B,Language,,2023-01-02,"code is Apache 2.0 (but OPT, which you'd need to recreate this model, is non-commercial)

https://github.com/IST-DASLab/sparsegpt",Unreleased,,Open source,https://arxiv.org/abs/2301.00774,Institute of Science and Technology Austria (ISTA),,,Austria,"Elias Frantar, Dan Alistarh",,,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,,,,,Unverified,,,,66000000000.00,,1.67,,,,,,,,,,"We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: this https URL.",2024-05-10 05:00,Robi Rahman,SparseOPT-66B,1,OPT-66B,,,,,,,,,Academia,,,,,Academia,,,
GPT-Neo-2.7B (finetuned),Language,,2021-03-21,MIT. don't think model weights for finetune are available: https://github.com/EleutherAI/gpt-neo,Unreleased,,Open source,https://github.com/EleutherAI/gpt-neo,EleutherAI,,,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,"The Pile,WikiText-2",,,,Unverified,,,,,,1.00,,,,,,,,,,,2024-05-09 13:26,Robi Rahman,GPT-Neo-2.7B (finetuned),1,GPT-Neo-2.7B,,,,,,,,,Research collective,,,,,Research collective,,,
Fraternal dropout + AWD-LSTM 3-layer (PTB),Language,,2017-10-31,,,,,https://arxiv.org/abs/1711.00066,"University of Montreal / Université de Montréal,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",,,"Canada,Canada","Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",,,Fraternal Dropout,,,,,Unverified,,,,,,520.00,,,,,,,,,,"Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.",2024-05-22 13:23,Robi Rahman,Fraternal dropout + AWD-LSTM 3-layer (PTB),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
LSTM (PTB),Language,,2016-12-13,,,,,https://arxiv.org/abs/1612.04426,Facebook AI Research,,,United States of America,"Edouard Grave, Armand Joulin, Nicolas Usunier",,,Improving Neural Language Models with a Continuous Cache,,,,,Unverified,,,,,,,,,,,,,,,,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",2024-05-10 00:37,Robi Rahman,LSTM (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
OPT-1.3B,Language,Language modelling,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,1300000000.00,,1.67,,,,,,,,,,,2024-04-30 15:04,Robi Rahman,OPT-1.3B,1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),Language,,2017-08-07,,,,,https://arxiv.org/abs/1708.02182,Salesforce Research,,,United States of America,"Stephen Merity, Nitish Shirish Keskar, Richard Socher",,,Regularizing and Optimizing LSTM Language Models,,,,,Unverified,,,,,,500.00,,,,,,,,,,"Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",2024-05-22 12:45,Robi Rahman,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
LLaMA-65B (LoRA finetuned),Language,,2023-05-23,,Unreleased,,Unreleased,https://arxiv.org/pdf/2305.14152.pdf,NAVER,,,Korea (Republic of),"Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee",,,"Memory-Efficient Fine-Tuning of Compressed Large
Language Models via sub-4-bit Integer Quantization",,,,,Unverified,,,,65200000000.00,,1.09,,,,,,,,,,"Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.",2024-05-09 14:15,Robi Rahman,LLaMA-65B (LoRA finetuned),1,LLaMA-65B,,,,,,,,,Industry,,,,,Industry,,,
Zoneout + Variational LSTM (PTB),Language,,2016-09-26,,,,,https://arxiv.org/abs/1609.07843,MetaMind Inc,,,United States of America,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",,,Pointer Sentinel Mixture Models,,,,,Unverified,,,,,,64.00,,,,,,,,,,Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.,2024-05-10 04:56,Robi Rahman,Zoneout + Variational LSTM (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-LSTM + dynamic eval (PTB),Language,,2017-09-21,,,,,https://arxiv.org/abs/1709.07432,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",,,Dynamic Evaluation of Neural Sequence Models,,,,,Unverified,,,,,,,,,,,,,,,,"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.",2024-05-21 05:02,Robi Rahman,AWD-LSTM + dynamic eval (PTB),1,,,,,,,,,,Academia,,,,,Academia,,,
OPT-1.3B (finetuned on PTB),Language,Language modelling,2022-06-21,"MIT, weights+code https://github.com/facebookresearch/metaseq?tab=readme-ov-file",Open source,,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,1300000000.00,,1.67,,,,,,,,,,,2024-04-17 05:51,Robi Rahman,OPT-1.3B (finetuned on PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
Transformer-XL DeFINE (107M),Language,,2019-11-27,,Unreleased,,Unreleased,https://arxiv.org/abs/1911.12385,University of Washington,,,United States of America,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",,,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,,,,,Unverified,,,,,,20.00,,,,,,,,,,"For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",2024-05-22 11:28,Robi Rahman,Transformer-XL DeFINE (107M),1,,,,,,,,,,Academia,,,,,Academia,,,
AdvSoft + 4 layer QRNN + dynamic evaluation,Language,,2019-06-10,"there's a repo, but the WT-103 experiments aren't there:

https://github.com/ChengyueGongR/advsoft ",Unreleased,,Unreleased,https://arxiv.org/abs/1906.03805,University of Texas at Austin,,,United States of America,"Dilin Wang, Chengyue Gong, Qiang Liu",,,Improving Neural Language Modeling via Adversarial Training,,,,,Unverified,,,,,,,,,,,,,,,,"Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",2024-05-22 16:43,Robi Rahman,AdvSoft + 4 layer QRNN + dynamic evaluation,1,,,,,,,,,,Academia,,,,,Academia,,,
Temporal Convolutional Attention-based Network(TCAN) (PTB),Language,,2020-02-28,MIT: https://github.com/haohy/TCAN ,Unreleased,,Open source,https://arxiv.org/pdf/2002.12530,Ant Group,,,China,"Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",,,Temporal Convolutional Attention-based Network For Sequence Modeling,Penn TreeBank,,,,Unverified,,,,,,,,,,,,,,,,,2024-05-16 16:27,Robi Rahman,Temporal Convolutional Attention-based Network(TCAN) (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
SparseOPT-13B,Language,,2023-01-02,"code is Apache 2.0 (but OPT, which you'd need to recreate this model, is non-commercial)

https://github.com/IST-DASLab/sparsegpt",Unreleased,,Open source,https://arxiv.org/abs/2301.00774,Institute of Science and Technology Austria (ISTA),,,Austria,"Elias Frantar, Dan Alistarh",,,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,,,,,Unverified,,,,13000000000.00,,1.67,,,,,,,,,,"We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: this https URL.",2024-05-10 05:00,Robi Rahman,SparseOPT-13B,1,OPT-13B,,,,,,,,,Academia,,,,,Academia,,,
CT-MoS (PTB),Language,,2020-12-25,,Unreleased,,Unreleased,https://arxiv.org/pdf/2012.13575,"National Tsing Hua University,Google",,,"Taiwan,United States of America","Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",,,Contextual Temperature for Language Modeling,,,,,Unverified,,,8.00,,,1000.00,,,,,,,,,,"Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",2024-05-10 15:15,Robi Rahman,CT-MoS (PTB),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Variational RHN + WT,Language,,2016-07-12,,,,,https://arxiv.org/abs/1607.03474,"ETH Zurich,IDSIA",,,"Switzerland,Switzerland","Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",,,Recurrent Highway Networks,,,,,Unverified,,,,,,20.00,,,,,,,,,,"Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.",2024-05-22 12:44,Robi Rahman,Variational RHN + WT,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Alleviated TOI 10 (PTB),Language,,2019-09-18,BSD 3-Clause License for code: https://github.com/nkcr/overlap-ml ,Unreleased,,Open source,https://arxiv.org/abs/1909.08700,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),Swisscom,University of Freiburg",,,"Switzerland,Switzerland,Germany","Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",,,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,Penn TreeBank,,,,Unverified,,,,,,1000.00,,,,,,,,,,"In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order information -Alleviated TOI- by iteratively overlapping the token composition of data points. For recurrent networks, we use prime numbers for the batch size to avoid redundancies when building batches from overlapped data points. The proposed method achieved state of the art performance in both text and speech related tasks.",2024-05-22 11:32,Robi Rahman,Alleviated TOI 10 (PTB),1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
LSTM-MemoryAug (WT2),Language,,2020-09-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2009.13774,"Johns Hopkins University,Xiaomi Corp",,,"United States of America,China","Ke Li, Daniel Povey, Sanjeev Khudanpur",,,Neural Language Modeling With Implicit Cache Pointers,,,,,Unverified,,,,,,,,,,,,,,,,"A cache-inspired approach is proposed for neural language models (LMs) to improve long-range dependency and better predict rare words from long contexts. This approach is a simpler alternative to attention-based pointer mechanism that enables neural LMs to reproduce words from recent history. Without using attention and mixture structure, the method only involves appending extra tokens that represent words in history to the output layer of a neural LM and modifying training supervisions accordingly. A memory-augmentation unit is introduced to learn words that are particularly likely to repeat. We experiment with both recurrent neural network- and Transformer-based LMs. Perplexity evaluation on Penn Treebank and WikiText-2 shows the proposed model outperforms both LSTM and LSTM with attention-based pointer mechanism and is more effective on rare words. N-best rescoring experiments on Switchboard indicate that it benefits both very rare and frequent words. However, it is challenging for the proposed model as well as two other models with attention-based pointer mechanism to obtain good overall WER reductions.",2024-05-10 15:22,Robi Rahman,LSTM-MemoryAug (WT2),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Tensor-Transformer(1core)+PN (PTB),Language,,2020-03-17,"repo, only code for translation: https://github.com/sIncerass/powernorm ",Unreleased,,Unreleased,https://arxiv.org/pdf/2003.07845,UC Berkeley,,,United States of America,"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",,,PowerNorm: Rethinking Batch Normalization in Transformers,,,,,Unverified,,,,,,30.00,,,,,,,,,,"The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \url{this https URL}.",2024-05-22 12:37,Robi Rahman,Tensor-Transformer(1core)+PN (PTB),1,,,,,,,,,,Academia,,,,,Academia,,,
BLOOM-1.7B,Language,,2022-07-05,"commercial, no harmful use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license",Open access (restricted use),,Unreleased,https://huggingface.co/bigscience/bloom-3b,"Hugging Face,BigScience",,,"Multinational,Multinational","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,BigScience ROOTS Corpus,,,,Unverified,,,,1700000000.00,,1.00,,,,,,,,,,,2024-04-30 14:44,Robi Rahman,BLOOM-1.7B,1,,,,,,,,,,"Industry,Research collective",,,,,"Industry,Research collective",,,
GLM-2B,Language,,2021-03-18,Apache 2.0 or MIT for code/weights: https://github.com/THUDM/GLM,Open source,,Open source,https://arxiv.org/abs/2103.10360,"Tsinghua University,Beijing Academy of Artificial Intelligence,Massachusetts Institute of Technology (MIT)",,,"China,China,United States of America","Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",,,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,The Pile,https://github.com/THUDM/GLM?tab=readme-ov-file ,,,Unverified,,,,,,1.00,,,,,,,,,,"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",2024-05-09 13:25,Robi Rahman,GLM-2B,1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
RNN Baseline,Language,,2019-07-14,,Unreleased,,Unreleased,https://ojs.aaai.org/index.php/AAAI/article/view/4437,"Massachusetts Institute of Technology (MIT),Rey Juan Carlos University",,,"United States of America,Spain","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Character n-Gram Embeddings to Improve RNN Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"A vine copula model is a flexible high-dimensional dependence model which uses only bivariate building blocks. However, the number of possible configurations of a vine copula grows exponentially as the number of variables increases, making model selection a major challenge in development. In this work, we formulate a vine structure learning problem with both vector and reinforcement learning representation. We use neural network to find the embeddings for the best possible vine model and generate a structure. Throughout experiments on synthetic and real-world datasets, we show that our proposed approach fits the data better in terms of loglikelihood. Moreover, we demonstrate that the model is able to generate high-quality samples in a variety of applications, making it a good candidate for synthetic data generation.",2024-05-22 14:40,Robi Rahman,RNN Baseline,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
GPT-2-Small+Pixelfly,Language,,2021-11-30,Apache for code: https://github.com/HazyResearch/fly,Unreleased,,Open source,https://arxiv.org/pdf/2112.00029,"Stanford University,""SambaNova Systems, Inc"",Peking University,Adobe,University at Buffalo",,,"United States of America,United States of America,China,United States of America,United States of America","Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",,,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,WikiText-103,"""We show training GPT-2-Small, Medium and its
Pixelfly model from scratch on a commonly used
NLP benchmarking dataset, wikiText-103.""",,,Unverified,,,,,,100.00,,,,,,,,,,"Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.",2024-05-22 12:36,Robi Rahman,GPT-2-Small+Pixelfly,1,,,,,,,,,,"Academia,Industry,Academia,Industry,Academia",,,,,"Academia,Industry,Academia,Industry,Academia",,,
LSTM(medium)+Sememe+cell,Language,,2019-10-20,code for Wikitext/PTB. no license. https://github.com/thunlp/SememeRNN/blob/master/LM/main.py ,Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1910.08910,"Tsinghua University,Beijing University of Posts and Telecommunications,Huawei Noah's Ark Lab",,,"China,China,China","Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",,,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,WikiText-2,,,,Unverified,,,,,,40.00,,,,,,,,,,"Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at this https URL.",2024-05-21 05:11,Robi Rahman,LSTM(medium)+Sememe+cell,1,,,,,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
NAS+ESS (23M),Language,,2020-05-06,,Unreleased,,Unreleased,https://arxiv.org/pdf/2005.02593,"Northeastern University (China),NiuTrans Research,Kingsoft",,,"China,China,China","Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",,,Learning Architectures from an Extended Search Space for Language Modeling,,,,,Unverified,,,,,,30.00,,,,,,,,,,,2024-05-16 14:39,Robi Rahman,NAS+ESS (23M),1,,,,,,,,,,Academia,,,,,Academia,,,
BLOOM-560M,Language,,2022-07-05,"commercial, no harmful use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license",Open access (restricted use),,Unreleased,https://huggingface.co/bigscience/bloom-3b,"Hugging Face,BigScience",,,"Multinational,Multinational","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,BigScience ROOTS Corpus,,,,Unverified,,,,560000000.00,,1.00,,,,,,,,,,,2024-04-30 14:44,Robi Rahman,BLOOM-560M,1,,,,,,,,,,"Industry,Research collective",,,,,"Industry,Research collective",,,
Hybrid H3-125M,Language,,2022-12-28,apache 2: https://github.com/HazyResearch/H3,Open source,,Open source,https://arxiv.org/abs/2212.14052,"Stanford University,University at Buffalo",,,"United States of America,United States of America","Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",,,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,The Pile,"""We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile [21] for 400B tokens""",,,Unverified,,,125.00,125000000.00,,509.02,,,,,,,,,,,2024-05-01 09:22,Robi Rahman,Hybrid H3-125M,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Dropout-LSTM+Noise(Laplace),Language,,2018-05-03,,,,,https://arxiv.org/abs/1805.01500,"Columbia University,New York University (NYU),Princeton University",,,"United States of America,United States of America,United States of America","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",,,Noisin: Unbiased Regularization for Recurrent Neural Networks,,,,,Unverified,,,,,,200.00,,,,,,,,,,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",2024-05-10 00:34,Robi Rahman,Dropout-LSTM+Noise(Laplace),1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
OPT-2.7B (finetuned on WT2),Language,Language modelling,2022-06-21,"MIT, weights+code https://github.com/facebookresearch/metaseq?tab=readme-ov-file",Open source,,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,2700000000.00,,1.67,,,,,,,,,,,2024-04-17 05:51,Robi Rahman,OPT-2.7B (finetuned on WT2),1,,,,,,,,,,Industry,,,,,Industry,,,
LLaMA-13B (LoRA finetuned),Language,,2023-05-23,,Unreleased,,Unreleased,https://arxiv.org/pdf/2305.14152.pdf,NAVER,,,Korea (Republic of),"Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee",,,"Memory-Efficient Fine-Tuning of Compressed Large
Language Models via sub-4-bit Integer Quantization",,,,,Unverified,,,,13000000000.00,,1.09,,,,,,,,,,"Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.",2024-05-09 14:15,Robi Rahman,LLaMA-13B (LoRA finetuned),1,LLaMA-13B,,,,,,,,,Industry,,,,,Industry,,,
CT-MoS + DynamicEval (PTB),Language,,2020-12-25,,Unreleased,,Unreleased,https://arxiv.org/pdf/2012.13575,"National Tsing Hua University,Google",,,"Taiwan,United States of America","Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",,,Contextual Temperature for Language Modeling,,,,,Unverified,,,8.00,,,1000.00,,,,,,,,,,"Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",2024-05-10 15:15,Robi Rahman,CT-MoS + DynamicEval (PTB),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
"Mogrifier (d2, MC) + dynamic eval",Language,,2019-09-04,might have been Mogrifier code but the link in the Readme is 404: https://github.com/google-deepmind/lamb?tab=readme-ov-file ,Unreleased,,Unreleased,https://arxiv.org/abs/1909.01792,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Gábor Melis, Tomáš Kočiský, Phil Blunsom",,,Mogrifier LSTM,,,,,Unverified,,,,,,145.00,,,,,,,,,,"Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.",2024-05-22 12:30,Robi Rahman,"""Mogrifier (d2, MC) + dynamic eval""",1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-LSTM-DOC (fin) (37M),Language,,2018-08-30,,,,,https://arxiv.org/abs/1808.10143,"NTT Communication Science Laboratories,Tohoku University",,,"Japan,Japan","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Direct Output Connection for a High-Rank Language Model,,,,,Unverified,,,,,,300.00,,,,,,,,,,"This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.",2024-05-10 04:48,Robi Rahman,AWD-LSTM-DOC (fin) (37M),1,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Deep RNN,Language,,2013-12-11,,,,,https://arxiv.org/abs/1609.07843,MetaMind Inc,,,United States of America,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",,,Pointer Sentinel Mixture Models,,,,,Unverified,,,,,,64.00,,,,,,,,,,Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.,2024-05-10 04:56,Robi Rahman,Deep RNN,1,,,,,,,,,,Industry,,,,,Industry,,,
WeNet (PTB),Language,,2019-04-08,,,,,https://arxiv.org/pdf/1904.03819,Amazon,,,United States of America,"Zhiheng Huang, Bing Xiang",,,WeNet: Weighted Networks for Recurrent Network Architecture Search,,,,,Unverified,,,,,,,,,,,,,,,,"In recent years, there has been increasing demand for automatic architecture search in deep learning. Numerous approaches have been proposed and led to state-of-the-art results in various applications, including image classification and language modeling. In this paper, we propose a novel way of architecture search by means of weighted networks (WeNet), which consist of a number of networks, with each assigned a weight. These weights are updated with back-propagation to reflect the importance of different networks. Such weighted networks bear similarity to mixture of experts. We conduct experiments on Penn Treebank and WikiText-2. We show that the proposed WeNet can find recurrent architectures which result in state-of-the-art performance.",2024-05-22 12:54,Robi Rahman,WeNet (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
CD-GraB (WT2),Language,,2023-02-02,"training code, Apache 2: https://github.com/GarlGuo/CD-GraB",Unreleased,,Open source,https://arxiv.org/abs/2302.00845,Cornell University,,,United States of America,"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",,,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,WikiText-2,,,,Unverified,,,,6846080.00,They provide the model initialization code in Appendix D. Running it  gives the parameter count.,50.00,,,,,,,,,,"Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings for SGD that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: while it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms distributed RR on a variety of benchmark tasks.",2024-05-21 06:32,Robi Rahman,CD-GraB (WT2),1,,,,,,,,,,Academia,,,,,Academia,,,
Hybrid H3-355M,Language,,2022-12-28,apache 2: https://github.com/HazyResearch/H3,Open source,,Open source,https://arxiv.org/abs/2212.14052,"Stanford University,University at Buffalo",,,"United States of America,United States of America","Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",,,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,The Pile,"""We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile [21] for 400B tokens""",,,Unverified,,,125.00,355000000.00,,509.02,,,,,,,,,,,2024-05-01 09:22,Robi Rahman,Hybrid H3-355M,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
AWD-LSTM+Behaviorial-Gating,Language,,2019-08-31,,Unreleased,,Unreleased,https://arxiv.org/abs/1909.00107,University of Southern California,,,United States of America,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",,,Behavior Gated Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"Most current language modeling techniques only exploit co-occurrence, semantic and syntactic information from the sequence of words. However, a range of information such as the state of the speaker and dynamics of the interaction might be useful. In this work we derive motivation from psycholinguistics and propose the addition of behavioral information into the context of language modeling. We propose the augmentation of language models with an additional module which analyzes the behavioral state of the current context. This behavioral information is used to gate the outputs of the language model before the final word prediction output. We show that the addition of behavioral context in language models achieves lower perplexities on behavior-rich datasets. We also confirm the validity of the proposed models on a variety of model architectures and improve on previous state-of-the-art models with generic domain Penn Treebank Corpus.",2024-05-22 11:59,Robi Rahman,AWD-LSTM+Behaviorial-Gating,1,,,,,,,,,,Academia,,,,,Academia,,,
Megatron-LM (2.5B),Language,,2019-09-17,"code (2.5B model is a GPT model): https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-overview 
open license: https://github.com/NVIDIA/Megatron-LM?tab=License-1-ov-file#readme ",Unreleased,,Open source,https://arxiv.org/abs/1909.08053,NVIDIA,,,United States of America,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",,,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,"OPENWEBTEXT,WikiText-103,Realnews,CC-Stories",,,,Unverified,,,,,,4.40,,,,,,,,,,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2024-05-22 12:29,Robi Rahman,Megatron-LM (2.5B),1,,,,,,,,,,Industry,,,,,Industry,,,
OPT-125M (finetuned),Language,Language modelling,2022-06-21,"MIT, weights+code https://github.com/facebookresearch/metaseq?tab=readme-ov-file",Open source,,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,125000000.00,,1.67,,,,,,,,,,,2024-04-17 05:51,Robi Rahman,OPT-125M (finetuned),1,,,,,,,,,,Industry,,,,,Industry,,,
TRIMELMext (7M),Language,,2022-05-25,"code and weights, no clear license: https://github.com/princeton-nlp/TRIME?tab=readme-ov-file ",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2205.12674,Princeton University,,,United States of America,"Zexuan Zhong, Tao Lei, Danqi Chen",,,Training Language Models with Memory Augmentation,,,,,Unverified,,,,7000000.00,,47.72,,,,,,,,,,"Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories--local, long-term, and external memory--at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",2024-05-21 05:43,Robi Rahman,TRIMELMext (7M),1,,,,,,,,,,Academia,,,,,Academia,,,
TCN (13M),Language,,2018-02-15,,,,,"https://openreview.net/forum?id=rk8wKk-R-
https://arxiv.org/abs/1803.01271","Carnegie Mellon University (CMU),Intel Labs",,,"United States of America,Multinational","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,Convolutional Sequence Modeling Revisited,Penn TreeBank,,,,Confident,,,,13000000.00,Table 1,,,,,,,,,,,"This paper revisits the problem of sequence modeling using convolutional architectures.  Although both convolutional and recurrent architectures have a long history in sequence prediction, the current ""default"" mindset in much of
the deep learning community is that generic sequence modeling is best handled using recurrent networks.  The goal of this paper is to question this assumption. Specifically, we consider a simple generic temporal convolution network (TCN), which adopts features from modern ConvNet architectures such as a dilations and residual connections.  We show that on a variety of sequence modeling tasks, including many frequently used as benchmarks for evaluating recurrent networks, the TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and sometimes even highly specialized approaches.  We further show that the potential ""infinite memory"" advantage that RNNs have over TCNs is largely absent in practice: TCNs indeed exhibit longer effective history sizes than their recurrent counterparts.   As a whole, we argue that it may be time to (re)consider ConvNets as the default ""go to"" architecture for sequence modeling. ",2024-05-21 09:15,Robi Rahman,TCN (13M),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
RNN + char3-MS-vec,Language,,2019-07-16,,Unreleased,,Unreleased,https://ojs.aaai.org/index.php/AAAI/article/view/4439,"Massachusetts Institute of Technology (MIT),Rey Juan Carlos University",,,"United States of America,Spain","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Character n-Gram Embeddings to Improve RNN Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"A vine copula model is a flexible high-dimensional dependence model which uses only bivariate building blocks. However, the number of possible configurations of a vine copula grows exponentially as the number of variables increases, making model selection a major challenge in development. In this work, we formulate a vine structure learning problem with both vector and reinforcement learning representation. We use neural network to find the embeddings for the best possible vine model and generate a structure. Throughout experiments on synthetic and real-world datasets, we show that our proposed approach fits the data better in terms of loglikelihood. Moreover, we demonstrate that the model is able to generate high-quality samples in a variety of applications, making it a good candidate for synthetic data generation.",2024-05-22 14:39,Robi Rahman,RNN + char3-MS-vec,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Selfish-RNN (AWD-LSTM-MoS),Language,,2021-01-22,"code, no clear license https://github.com/Shiweiliuiiiiiii/Selfish-RNN ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2101.09048,Eindhoven University of Technology,,,Netherlands,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",,,Selfish Sparse RNN Training,WikiText-2,,,,Unverified,,,,,,1000.00,,,,,,,,,,"Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at this https URL.",2024-05-10 04:57,Robi Rahman,Selfish-RNN (AWD-LSTM-MoS),1,,,,,,,,,,Academia,,,,,Academia,,,
Adversarial + AWD-LSTM-MoS + partial shuffled,Language,,2019-06-10,"code, no clear license: https://github.com/ChengyueGongR/advsoft ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1906.03805,University of Texas at Austin,,,United States of America,"Dilin Wang, Chengyue Gong, Qiang Liu",,,Improving Neural Language Modeling via Adversarial Training,,,,,Unverified,,,,,,450.00,,,,,,,,,,"Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",2024-05-22 16:43,Robi Rahman,Adversarial + AWD-LSTM-MoS + partial shuffled,1,,,,,,,,,,Academia,,,,,Academia,,,
Alleviated TOI 10 (WT2),Language,,2019-09-18,BSD 3-Clause License for code: https://github.com/nkcr/overlap-ml ,Unreleased,,Open source,https://arxiv.org/abs/1909.08700,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),Swisscom,University of Freiburg",,,"Switzerland,Switzerland,Germany","Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",,,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,WikiText-2,,,,Unverified,,,,,,1000.00,,,,,,,,,,"In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order information -Alleviated TOI- by iteratively overlapping the token composition of data points. For recurrent networks, we use prime numbers for the batch size to avoid redundancies when building batches from overlapped data points. The proposed method achieved state of the art performance in both text and speech related tasks.",2024-05-22 11:33,Robi Rahman,Alleviated TOI 10 (WT2),1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
GLM-10B-bidirectional,Language,,2021-03-18,Apache 2.0 or MIT for code/weights (not sure if weights are for unidirectional or bidirectional): https://github.com/THUDM/GLM ,Open source,,Open source,https://arxiv.org/abs/2103.10360,"Tsinghua University,Beijing Academy of Artificial Intelligence,Massachusetts Institute of Technology (MIT)",,,"China,China,United States of America","Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",,,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,The Pile,,,,Unverified,,,,,,1.00,,,,,,,,,,"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",2024-05-09 13:25,Robi Rahman,GLM-10B-bidirectional,1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
GPT-Neo-2.7B (finetuned on PTB),Language,,2021-03-21,MIT. don't think model weights for PTB finetune are available: https://github.com/EleutherAI/gpt-neo,Unreleased,,Open source,https://github.com/EleutherAI/gpt-neo,EleutherAI,,,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,"Penn TreeBank,The Pile",,,,Unverified,,,,,,1.00,,,,,,,,,,,2024-05-09 13:26,Robi Rahman,GPT-Neo-2.7B (finetuned on PTB),1,GPT-Neo-2.7B,,,,,,,,,Research collective,,,,,Research collective,,,
AWD-LSTM-DRILL + dynamic evaluation† (PTB),Language,,2019-05-14,"code and weights, GNU license: https://github.com/idiap/drill ",Open source,,Open source,https://arxiv.org/abs/1905.05513,IDIAP,,,Switzerland,"Nikolaos Pappas, James Henderson",,,Deep Residual Output Layers for Neural Language Generation,,,,,Unverified,,,,24000000.00,Table 1,1000.00,,,,,,,,,,"Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.",2024-05-22 17:44,Robi Rahman,AWD-LSTM-DRILL + dynamic evaluation† (PTB),1,,,,,,,,,,Academia,,,,,Academia,,,
Monarch-GPT-2-Small,Language,,2022-04-01,"apache for code: https://github.com/HazyResearch/fly/tree/master 

https://github.com/HazyResearch/fly/tree/master/configs/model/gpt2model ",Unreleased,,Open source,https://arxiv.org/abs/2204.00595,"Stanford University,University at Buffalo,University of Michigan",,,"United States of America,United States of America,United States of America","Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",,,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,,,,,Unverified,,,,124000000.00,"GPT-2 small has 124M parameters: https://huggingface.co/openai-community/gpt2
",110.00,,,,,,,,,,"Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called ""reverse sparsification,"" Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",2024-05-22 12:31,Robi Rahman,Monarch-GPT-2-Small,1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
2-layer skip-LSTM + dropout tuning (PTB),Language,,2018-05-23,,,,,https://arxiv.org/abs/1805.09208,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",,,Pushing the bounds of dropout,,,,,Unverified,,,,,,,,,,,,,,,,,2024-05-09 10:38,Robi Rahman,2-layer skip-LSTM + dropout tuning (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
"Segatron XL base, M=384",Language,,2020-04-30,"training code and weights, no clear license: https://github.com/rsvp-ai/segatron_aaai?tab=readme-ov-file ",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2004.14996,"University of Waterloo,RSVP.ai,Peking University",,,"Canada,China,China","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",,,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,WikiText-103,,,,Unverified,,,,,,18.64,,,,,,,,,,"Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning.",2024-05-22 12:46,Robi Rahman,"""Segatron XL base, M=384""",1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
RNN + char2-MS-vec,Language,,2019-07-15,,Unreleased,,Unreleased,https://ojs.aaai.org/index.php/AAAI/article/view/4438,"Massachusetts Institute of Technology (MIT),Rey Juan Carlos University",,,"United States of America,Spain","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Character n-Gram Embeddings to Improve RNN Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"A vine copula model is a flexible high-dimensional dependence model which uses only bivariate building blocks. However, the number of possible configurations of a vine copula grows exponentially as the number of variables increases, making model selection a major challenge in development. In this work, we formulate a vine structure learning problem with both vector and reinforcement learning representation. We use neural network to find the embeddings for the best possible vine model and generate a structure. Throughout experiments on synthetic and real-world datasets, we show that our proposed approach fits the data better in terms of loglikelihood. Moreover, we demonstrate that the model is able to generate high-quality samples in a variety of applications, making it a good candidate for synthetic data generation.",2024-05-22 14:40,Robi Rahman,RNN + char2-MS-vec,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RNN+LSA+KN5+cache (model combination w/ linear extrapolation),Language,,2012-12-01,,,,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,Microsoft Research,,,United States of America,"Tomas Mikolov, Geoffrey Zweig",,,Context dependent recurrent neural network language model,,,,,Unverified,,,,,,,,,,,,,,,,"Recurrent neural network language models (RNNLMs) have
recently demonstrated state-of-the-art performance across a
variety of tasks. In this paper, we improve their performance
by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual
information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding
text, we achieve a topic-conditioned RNNLM. This approach
has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data
subsets. We report perplexity results on the Penn Treebank
data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition
task, where we observe improvements in word-error-rate.",2024-05-10 04:34,Robi Rahman,RNN+LSA+KN5+cache (model combination w/ linear extrapolation),1,,,,,,,,,,Industry,,,,,Industry,,,
LSTM (WT2),Language,,2016-12-13,,,,,https://arxiv.org/abs/1612.04426,Facebook AI Research,,,United States of America,"Edouard Grave, Armand Joulin, Nicolas Usunier",,,Improving Neural Language Models with a Continuous Cache,,,,,Unverified,,,,,,,,,,,,,,,,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",2024-05-10 00:37,Robi Rahman,LSTM (WT2),1,,,,,,,,,,Industry,,,,,Industry,,,
GPT-Neo-125M,Language,,2021-03-21,"MIT license. weights here: https://huggingface.co/EleutherAI/gpt-neo-125m 
code: https://github.com/EleutherAI/gpt-neo ",Open source,,Open source,https://github.com/EleutherAI/gpt-neo,EleutherAI,,,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,The Pile,,,,Unverified,,,,,,1.00,,,,,,,,,,,2024-05-09 13:26,Robi Rahman,GPT-Neo-125M,1,,,,,,,,,,Research collective,,,,,Research collective,,,
Compress-LSTM (4.6M),Language,,2019-02-06,,,,,https://arxiv.org/abs/1902.02380,"Samsung R&D Institute Russia,National Research University Higher School of Economics",,,"Russia,Russia","Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",,,Compression of Recurrent Neural Networks for Efficient Language Modeling,,,,,Unverified,,,,,,90.00,,,,,,,,,,"Recurrent neural networks have proved to be an effective method for statistical language modeling. However, in practice their memory and run-time complexity are usually too large to be implemented in real-time offline mobile applications. In this paper we consider several compression techniques for recurrent neural networks including Long-Short Term Memory models. We make particular attention to the high-dimensional output problem caused by the very large vocabulary size. We focus on effective compression methods in the context of their exploitation on devices: pruning, quantization, and matrix decomposition approaches (low-rank factorization and tensor train decomposition, in particular). For each model we investigate the trade-off between its size, suitability for fast inference and perplexity. We propose a general pipeline for applying the most suitable methods to compress recurrent neural networks for language modeling. It has been shown in the experimental study with the Penn Treebank (PTB) dataset that the most efficient results in terms of speed and compression-perplexity balance are obtained by matrix decomposition techniques.",2024-05-21 06:40,Robi Rahman,Compress-LSTM (4.6M),1,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Tensorized Transformer (core-2),Language,,2019-06-24,,Unreleased,,Unreleased,https://arxiv.org/abs/1906.09777,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",,,"China,China,China","Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",,,A Tensorized Transformer for Language Modeling,,,,,Unverified,,,133.00,,,30.00,,,,,,,,,,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",2024-05-22 16:36,Robi Rahman,Tensorized Transformer (core-2),1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)",Language,,2018-09-18,,,,,https://arxiv.org/abs/1809.06858,"Peking University,Microsoft Research Asia",,,"China,China","Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",,,FRAGE: Frequency-Agnostic Word Representation,,,,,Unverified,,,,,,,,,,,,,,,,"Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn \emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.",2024-05-21 09:37,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)""",1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
AWD-LSTM + dynamic eval (WT2),Language,,2017-09-21,,,,,https://arxiv.org/abs/1709.07432,University of Edinburgh,,,United Kingdom of Great Britain and Northern Ireland,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",,,Dynamic Evaluation of Neural Sequence Models,,,,,Unverified,,,,,,,,,,,,,,,,"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.",2024-05-21 05:02,Robi Rahman,AWD-LSTM + dynamic eval (WT2),1,,,,,,,,,,Academia,,,,,Academia,,,
OPT-350M,Language,,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://arxiv.org/abs/2205.01068,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,"The Pile,""BookCorpus (BooksCorpus, Toronto Book Corpus)"",CC-Stories,Pushshift Reddit","""The pre-training corpus contains a concatenation
of datasets used in RoBERTa (Liu et al., 2019b),
the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021)""
...
""RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021. This CCNews v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).

The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. Other subsets of the Pile were eliminated
...

PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021). To convert the conversational trees
into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.",,,Unverified,,,,350000000.00,,1.67,,,,,,,,,,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",2024-05-22 12:35,Robi Rahman,OPT-350M,1,,,,,,,,,,Industry,,,,,Industry,,,
Transformer-XL-ptb,Language,,2019-01-09,,,,,https://arxiv.org/abs/1901.02860,"Carnegie Mellon University (CMU),Google Brain",,,"United States of America,United States of America","Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",,,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,,,,,Unverified,,,,,,,,,,,,,,,,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",2024-05-22 12:51,Robi Rahman,Transformer-XL-ptb,1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
GRU + p-tHSM (pretrain via Brown) (PTB),Language,,2017-08-19,,,,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,"Beihang University,University of Montreal / Université de Montréal,Chongqing University",,,"China,Canada,China","Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",,,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"Recently, variants of neural networks for computational linguistics have been proposed and successfully applied to neural language modeling and neural machine translation. These neural models can leverage knowledge from massive corpora but they are extremely slow as they predict candidate words from a large vocabulary during training and inference. As an alternative to gradient approximation and softmax with class decomposition, we explore the tree-based hierarchical softmax method and reform its architecture, making it compatible with modern GPUs and introducing a compact tree-based loss function. When combined with several word hierarchical clustering algorithms, improved performance is achieved in language modelling task with intrinsic evaluation criterions on PTB, WikiText-2 and WikiText-103 datasets.",2024-05-10 04:50,Robi Rahman,GRU + p-tHSM (pretrain via Brown) (PTB),1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
Engine-Base (NE),Language,,2021-12-11,"MIT, code and weights: https://github.com/Zhongping-Zhang/ENGINE",Open source,,Open source,https://arxiv.org/abs/2112.05917,Boston University,,,United States of America,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",,,Show and Write: Entity-aware Article Generation with Image Information,WikiText-2,,,,Unverified,,,,,,3.00,,,,,,,,,,"Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as understanding news stories, these articles are based on real-world events and may reference many named entities that are difficult to accurately recognize and predict by language models. To address this challenge, we propose an ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to explicitly incorporate named entities into language models. ENGINE has two main components: a named-entity extraction module to extract named entities from both metadata and embedded images associated with articles, and an entity-aware mechanism that enhances the model's ability to recognize and predict entity names. We conducted experiments on three public datasets: GoodNews, VisualNews, and WikiText, where our results demonstrate that our model can boost both article generation and article retrieval performance, with a 4-5 perplexity improvement in article generation and a 3-4% boost in recall@1 in article retrieval. We release our implementation at this https URL .",2024-05-21 05:42,Robi Rahman,Engine-Base (NE),1,,,,,,,,,,Academia,,,,,Academia,,,
LSTM-Medium+Behaviorial-Gating,Language,,2019-08-31,,Unreleased,,Unreleased,https://arxiv.org/abs/1909.00107,University of Southern California,,,United States of America,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",,,Behavior Gated Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"Most current language modeling techniques only exploit co-occurrence, semantic and syntactic information from the sequence of words. However, a range of information such as the state of the speaker and dynamics of the interaction might be useful. In this work we derive motivation from psycholinguistics and propose the addition of behavioral information into the context of language modeling. We propose the augmentation of language models with an additional module which analyzes the behavioral state of the current context. This behavioral information is used to gate the outputs of the language model before the final word prediction output. We show that the addition of behavioral context in language models achieves lower perplexities on behavior-rich datasets. We also confirm the validity of the proposed models on a variety of model architectures and improve on previous state-of-the-art models with generic domain Penn Treebank Corpus.",2024-05-22 11:59,Robi Rahman,LSTM-Medium+Behaviorial-Gating,1,,,,,,,,,,Academia,,,,,Academia,,,
SRU++ Base,Language,,2021-02-24,code with MIT license (no script for WT103) https://github.com/asappresearch/sru/blob/master/language_model/README.md ,Unreleased,,Unreleased,https://arxiv.org/abs/2102.12459,ASAPP,,,United States of America,Tao Lei,,,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,WikiText-103,,,,Unverified,,,,,,25.56,,,,,,,,,,"Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",2024-05-21 05:46,Robi Rahman,SRU++ Base,1,,,,,,,,,,Industry,,,,,Industry,,,
OPT-2.7B,Language,Language modelling,2022-06-21,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

code is MIT: https://github.com/facebookresearch/metaseq",Open access (non-commercial),,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,2700000000.00,,1.67,,,,,,,,,,,2024-04-30 15:04,Robi Rahman,OPT-2.7B,1,,,,,,,,,,Industry,,,,,Industry,,,
Verbatim Memory Transformer (117M),Language,,2022-10-24,CC BY 4.0 for code: https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,Unreleased,,Open source,https://arxiv.org/abs/2210.13569,"Johns Hopkins University,New York University (NYU)",,,"United States of America,United States of America","Kristijan Armeni, Christopher Honey, Tal Linzen",,,Characterizing Verbatim Short-Term Memory in Neural Language Models,WikiText-103,,,,Unverified,,,,117000000.00,,,,,,,,,,,,"When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM's retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.",2024-05-21 05:03,Robi Rahman,Characterizing Verbatim Short-Term Memory in Neural Language Models (117M),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
OPT-125M (finetuned on PTB),Language,Language modelling,2022-06-21,"MIT, weights+code https://github.com/facebookresearch/metaseq?tab=readme-ov-file",Open source,,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,125000000.00,,1.67,,,,,,,,,,,2024-04-17 05:51,Robi Rahman,OPT-125M (finetuned on PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
VD-LSTM+REAL Small,Language,,2016-11-04,,,,,https://arxiv.org/abs/1611.01462,"Stanford University,Salesforce Research",,,"United States of America,United States of America","Hakan Inan, Khashayar Khosravi, Richard Socher",,,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,,,,,Likely,,,,6800000.00,"VD-LSTM+REAL Large has 51M parameters. The parameter count of the Small model is not reported, but they say it has 200 hidden units per layer, compared to 1500 for the Large model. Neglecting the rest of the architecture,
51M * (200/1500) = 6.8M",60.00,,,,,,,,,,"Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",2024-05-22 12:53,Robi Rahman,VD-LSTM+REAL Small,1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Local Transformer,Language,,2020-03-12,code (Apache): https://github.com/google-research/google-research/blob/master/routing_transformer/problems/wikitext103.py ,Unreleased,,Open source,https://arxiv.org/abs/2003.05997,Google Research,,,Multinational,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",,,Efficient Content-Based Sparse Attention with Routing Transformers,WikiText-103,,,,Unverified,,,436.00,,,,,,,,,,,,,"Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.",2024-05-21 09:27,Robi Rahman,Local Transformer,1,,,,,,,,,,Industry,,,,,Industry,,,
Gopher (7.1B),Language,,2021-12-08,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2112.11446,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",,,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher""",,,,,Unverified,,,917.00,,,1.00,,,,,,,,,,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",2024-05-21 06:23,Robi Rahman,Gopher (7.1B),1,,,,,,,,,,Industry,,,,,Industry,,,
GPT-2 (117M),Language,,2019-02-14,,,,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,OpenAI,,,United States of America,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",,,Language Models are Unsupervised Multitask Learners,,,,,Unverified,,,,,,100.00,,,,,,,,,,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on taskspecific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding
the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples.
The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations.
",2024-05-10 04:25,Robi Rahman,GPT-2 (117M),1,,,,,,,,,,Industry,,,,,Industry,,,
N-gram+Cache,Language,,2014-12-24,,,,,https://arxiv.org/abs/1412.7753,Facebook AI Research,,,United States of America,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",,,Learning Longer Memory in Recurrent Neural Networks,,,,,Unverified,,,,,,,,,,,,,,,,"Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).",2024-05-21 05:18,Robi Rahman,N-gram+Cache,1,,,,,,,,,,Industry,,,,,Industry,,,
Multipop Adaptive Continuous Stack (PTB),Language,,2018-02-15,,,,,https://openreview.net/forum?id=SkFqf0lAZ,"DeepMind,University of Oxford",,,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland","Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",,,Memory Architectures in Recurrent Neural Network Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.",2024-05-21 05:25,Robi Rahman,Multipop Adaptive Continuous Stack (PTB),1,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
AFP+FPI (WT2),Language,,2021-06-04,,Unreleased,,Unreleased,https://arxiv.org/abs/2106.02417,University of Sheffield,,,United Kingdom of Great Britain and Northern Ireland,"Zhengxiong Wang, Anton Ragni",,,Approximate Fixed-Points in Recurrent Neural Networks,,,,,Unverified,,,,,,40.00,,,,,,,,,,"Recurrent neural networks are widely used in speech and language processing. Due to dependency on the past, standard algorithms for training these models, such as back-propagation through time (BPTT), cannot be efficiently parallelised. Furthermore, applying these models to more complex structures than sequences requires inference time approximations, which introduce inconsistency between inference and training. This paper shows that recurrent neural networks can be reformulated as fixed-points of non-linear equation systems. These fixed-points can be computed using an iterative algorithm exactly and in as many iterations as the length of any given sequence. Each iteration of this algorithm adds one additional Markovian-like order of dependencies such that upon termination all dependencies modelled by the recurrent neural networks have been incorporated. Although exact fixed-points inherit the same parallelization and inconsistency issues, this paper shows that approximate fixed-points can be computed in parallel and used consistently in training and inference including tasks such as lattice rescoring. Experimental validation is performed in two tasks, Penn Tree Bank and WikiText-2, and shows that approximate fixed-points yield competitive prediction performance to recurrent neural networks trained using the BPTT algorithm.",2024-05-09 13:34,Robi Rahman,AFP+FPI (WT2),1,,,,,,,,,,Academia,,,,,Academia,,,
TSLM+MoS (PTB),Language,,2019-01-31,,,,,https://arxiv.org/abs/1901.11167,"Tianjin University,Beijing Institute of Technology",,,"China,China","Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",,,A Generalized Language Model in Tensor Space,,,,,Unverified,,,,,,,,,,,,,,,,"In the literature, tensors have been effectively used for capturing the context information in language models. However, the existing methods usually adopt relatively-low order tensors, which have limited expressive power in modeling language. Developing a higher-order tensor representation is challenging, in terms of deriving an effective solution and showing its generality. In this paper, we propose a language model named Tensor Space Language Model (TSLM), by utilizing tensor networks and tensor decomposition. In TSLM, we build a high-dimensional semantic space constructed by the tensor product of word vectors. Theoretically, we prove that such tensor representation is a generalization of the n-gram language model. We further show that this high-order tensor representation can be decomposed to a recursive calculation of conditional probability for language modeling. The experimental results on Penn Tree Bank (PTB) dataset and WikiText benchmark demonstrate the effectiveness of TSLM.",2024-05-21 06:24,Robi Rahman,TSLM+MoS (PTB),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RGC+ASQ (PTB),Language,,2018-08-13,,,,,https://arxiv.org/abs/1808.04357,"Tsinghua University,University of California Los Angeles (UCLA)",,,"China,United States of America","Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",,,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,,,,,Unverified,,,,,,40.00,,,,,,,,,,"Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes. Since synchronizing a large number of gradients of the local model can be a bottleneck for large-scale distributed training, compressing communication data has gained widespread attention recently. Among several recent proposed compression algorithms, Residual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1\% of the gradient size) of each node and still achieve correct accuracy and the same convergence speed. However, the literature on compressing deep networks focuses almost exclusively on achieving good theoretical compression rate, while the efficiency of RGC in real distributed implementation has been less investigated. In this paper, we develop an RGC-based system that is able to reduce the end-to-end training time on real-world multi-GPU systems. Our proposed design called RedSync, which introduces a set of optimizations to reduce communication bandwidth requirement while introducing limited overhead. We evaluate the performance of RedSync on two different multiple GPU platforms, including 128 GPUs of a supercomputer and an 8-GPU server. Our test cases include image classification tasks on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which have long been considered with poor scalability, RedSync brings significant performance improvements.",2024-05-22 12:44,Robi Rahman,RGC+ASQ (PTB),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
GPT-2 (762M),Language,,2019-02-14,,,,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,OpenAI,,,United States of America,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",,,Language Models are Unsupervised Multitask Learners,,,,,Unverified,,,,,,100.00,,,,,,,,,,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on taskspecific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding
the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples.
The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations.
",2024-05-10 04:25,Robi Rahman,GPT-2 (762M),1,,,,,,,,,,Industry,,,,,Industry,,,
Mogrifier RLSTM (PTB),Language,,2022-11-03,,Unreleased,,Unreleased,https://arxiv.org/abs/2211.01848,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,Gábor Melis,,,Circling Back to Recurrent Models of Language,Penn TreeBank,,,,Unverified,,,,24000000.00,Table 1,400.00,,,,,,,,,,"Just because some purely recurrent models suffer from being hard to optimize and inefficient on today's hardware, they are not necessarily bad models of language. We demonstrate this by the extent to which these models can still be improved by a combination of a slightly better recurrent cell, architecture, objective, as well as optimization. In the process, we establish a new state of the art for language modelling on small datasets and on Enwik8 with dynamic evaluation.",2024-05-21 06:32,Robi Rahman,Mogrifier RLSTM (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
GPT-2 (345M),Language,,2019-02-14,"modified MIT
https://github.com/openai/gpt-2?tab=License-1-ov-file#readme",Open source,Unreleased,Unreleased,https://openai.com/blog/better-language-models/,OpenAI,,,United States of America,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",Highly cited,,Language Models are Unsupervised Multitask Learners,,,3000000000,"“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”
40GB is approximately 3e9 words.",Unverified,,,15225.00,,,100.00,,,,,,,,,,,2024-04-17 05:50,Robi Rahman,GPT-2 (345M),1,,,,,,,,,,Industry,,,,,Industry,,,
BERT-Large-CAS (WT103),Language,,2019-04-20,"code, no license specified: https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1904.09408,Amazon,,,United States of America,"Chenguang Wang, Mu Li, Alexander J. Smola",,,Language Models with Transformers,,,,,Unverified,,,,,,50.00,,,,,,,,,,"The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.
In this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",2024-05-22 17:45,Robi Rahman,BERT-Large-CAS (WT103),1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-FWM (PTB),Language,,2020-11-16,"code, repo license is MIT: https://github.com/ischlag/Fast-Weight-Memory-public/tree/main/language-modelling/fwm ",Unreleased,,Open source,https://arxiv.org/abs/2011.07831,"IDSIA,Microsoft Research",,,"Switzerland,United States of America","Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",,,Learning Associative Inference Using Fast Weight Memory,,,,,Unverified,,,,,,1000.00,,,,,,,,,,"Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.",2024-05-22 12:18,Robi Rahman,AWD-FWM (PTB),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
EGRU (PTB),Language,,2022-06-13,Apache 2.0: https://github.com/Efficient-Scalable-Machine-Learning/EvNN,Unreleased,,Open source,https://arxiv.org/abs/2206.06178v3,"Ruhr University Bochum,Technische Universität Dresden",,,"Germany,Germany","Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",,,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,Penn TreeBank,,,,Unverified,,,,55000000.00,"Table 3
",2500.00,,,,,,,,,,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.",2024-05-21 09:29,Robi Rahman,EGRU (PTB),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",checked,,
3-Layer-Tensor-Transformer+AdaHessian,Language,,2020-06-01,MIT for code. doesn't have training script for PTB but looks fairly adaptable: https://github.com/amirgholami/ADAHESSIAN ,Unreleased,,Open source,https://arxiv.org/abs/2006.00719,"UC Berkeley,""NERSC, Lawrence Berkeley National Laboratory""",,,"United States of America,United States of America","Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",,,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,Penn TreeBank,,,,Unverified,,,,,,30.00,,,,,,,,,,"We introduce ADAHESSIAN, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the HESSIAN. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and Adam. The main disadvantage of traditional second order methods is their heavier per iteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in ADAHESSIAN, including: (i) a fast Hutchinson based method to approximate the curvature matrix with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that ADAHESSIAN achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of Adam. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that ADAHESSIAN: (i) achieves 1.80%/1.45% higher accuracy on ResNets20/32 on Cifar10, and 5.55% higher accuracy on ImageNet as compared to Adam; (ii) outperforms AdamW for transformers by 0.13/0.33 BLEU score on IWSLT14/WMT14 and 2.7/1.0 PPL on PTB/Wikitext-103; (iii) outperforms AdamW for SqueezeBert by 0.41 points on GLUE; and (iv) achieves 0.032% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of ADAHESSIAN is comparable to first order methods, and that it exhibits robustness towards its hyperparameters.",2024-05-21 06:26,Robi Rahman,3-Layer-Tensor-Transformer+AdaHessian,1,,,,,,,,,,"Academia,Government",,,,,"Academia,Government",,,
OPT-1.3B (finetuned),Language,Language modelling,2022-06-21,"MIT, weights+code https://github.com/facebookresearch/metaseq?tab=readme-ov-file",Open source,,Open source,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,Meta AI,,,United States of America,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,,OPT: Open Pre-trained Transformer Language Models,,,,,Confident,,,,1300000000.00,,1.67,,,,,,,,,,,2024-04-17 05:51,Robi Rahman,OPT-1.3B (finetuned),1,,,,,,,,,,Industry,,,,,Industry,,,
LLaMA-33B (LoRA finetuned),Language,,2023-05-23,,Unreleased,,Unreleased,https://arxiv.org/pdf/2305.14152.pdf,NAVER,,,Korea (Republic of),"Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee",,,"Memory-Efficient Fine-Tuning of Compressed Large
Language Models via sub-4-bit Integer Quantization",,,,,Unverified,,,,33000000000.00,,1.09,,,,,,,,,,"Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.",2024-05-09 14:15,Robi Rahman,LLaMA-33B (LoRA finetuned),1,LLaMA-33B,,,,,,,,,Industry,,,,,Industry,,,
GRU + p-tHSM (pretrain via Brown) (WT2),Language,,2017-08-19,,,,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,"Beihang University,University of Montreal / Université de Montréal,Chongqing University",,,"China,Canada,China","Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",,,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"Recently, variants of neural networks for computational linguistics have been proposed and successfully applied to neural language modeling and neural machine translation. These neural models can leverage knowledge from massive corpora but they are extremely slow as they predict candidate words from a large vocabulary during training and inference. As an alternative to gradient approximation and softmax with class decomposition, we explore the tree-based hierarchical softmax method and reform its architecture, making it compatible with modern GPUs and introducing a compact tree-based loss function. When combined with several word hierarchical clustering algorithms, improved performance is achieved in language modelling task with intrinsic evaluation criterions on PTB, WikiText-2 and WikiText-103 datasets.",2024-05-10 04:51,Robi Rahman,GRU + p-tHSM (pretrain via Brown) (WT2),1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
AWD-LSTM + Phrase Induction + finetuning,Language,,2019-06-04,"code, BSD-3 Clause: https://github.com/luohongyin/PILM ",Unreleased,,Open source,https://arxiv.org/abs/1906.01702,"Massachusetts Institute of Technology (MIT),University of Illinois Urbana-Champaign (UIUC)",,,"United States of America,United States of America","Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",,,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",,,,,Unverified,,,,,,,,,,,,,,,,"Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",2024-05-22 16:50,Robi Rahman,AWD-LSTM + Phrase Induction + finetuning,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
base LM+GNN,Language,,2021-10-17,MIT: https://github.com/ShannonAI/GNN-LM,Unreleased,,Open source,https://arxiv.org/abs/2110.08743,"Shannon.AI,Nanjing University,Nanyang Technological University,Zhejiang University",,,"China,China,Singapore,China","Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",,,GNN-LM: Language Modeling based on Global Contexts via GNN,WikiText-103,,,,Unverified,,,,,,,,,,,,,,,,"Inspired by the notion that ``{\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \footnote{The code can be found at this https URL",2024-05-09 13:49,Robi Rahman,base LM+GNN,1,,,,,,,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
Tensorized Transformer (small),Language,,2019-06-24,,Unreleased,,Unreleased,https://arxiv.org/abs/1906.09777,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",,,"China,China,China","Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",,,A Tensorized Transformer for Language Modeling,,,,,Unverified,,,133.00,,,30.00,,,,,,,,,,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",2024-05-22 16:36,Robi Rahman,Tensorized Transformer (small),1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Engine-Medium(NE),Language,,2021-12-11,"MIT, code and weights: https://github.com/Zhongping-Zhang/ENGINE",Open source,,Open source,https://arxiv.org/abs/2112.05917,Boston University,,,United States of America,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",,,Show and Write: Entity-aware Article Generation with Image Information,WikiText-2,,,,Unverified,,,,,,3.00,,,,,,,,,,"Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as understanding news stories, these articles are based on real-world events and may reference many named entities that are difficult to accurately recognize and predict by language models. To address this challenge, we propose an ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to explicitly incorporate named entities into language models. ENGINE has two main components: a named-entity extraction module to extract named entities from both metadata and embedded images associated with articles, and an entity-aware mechanism that enhances the model's ability to recognize and predict entity names. We conducted experiments on three public datasets: GoodNews, VisualNews, and WikiText, where our results demonstrate that our model can boost both article generation and article retrieval performance, with a 4-5 perplexity improvement in article generation and a 3-4% boost in recall@1 in article retrieval. We release our implementation at this https URL .",2024-05-21 05:42,Robi Rahman,Engin-Medium(NE),1,,,,,,,,,,Academia,,,,,Academia,,,
D-LSRC(100)+KN5,Language,,2017-08-22,,,,,https://arxiv.org/abs/1708.06555,Saarland University,,,Germany,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",,,Long-Short Range Context Neural Networks for Language Modeling,,,,,Unverified,,,,,,,,,,,,,,,,"The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.",2024-05-21 05:19,Robi Rahman,D-LSRC(100)+KN5,1,,,,,,,,,,Academia,,,,,Academia,,,
EI-REHN-1200D,Language,,2017-08-14,,,,,https://arxiv.org/abs/1708.04116,Korea Advanced Institute of Science and Technology (KAIST),,,Korea (Republic of),"Hyunsin Park, Chang D. Yoo",,,Early Improving Recurrent Elastic Highway Network,,,,,Unverified,,,,,,100.00,,,,,,,,,,"To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.",2024-05-21 09:27,Robi Rahman,EI-REHN-1200D,1,,,,,,,,,,Academia,,,,,Academia,,,
Dropout-LSTM+Noise(Bernoulli) (PTB),Language,,2018-05-03,,,,,https://arxiv.org/abs/1805.01500,"Columbia University,New York University (NYU),Princeton University",,,"United States of America,United States of America,United States of America","Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",,,Noisin: Unbiased Regularization for Recurrent Neural Networks,,,,,Unverified,,,,,,200.00,,,,,,,,,,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",2024-05-10 00:34,Robi Rahman,Dropout-LSTM+Noise(Bernoulli) (PTB),1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
GPT-Neo-1.3B,Language,,2021-03-21,"MIT license, weights and code: https://github.com/EleutherAI/gpt-neo ",Open source,,Open source,https://github.com/EleutherAI/gpt-neo,EleutherAI,,,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,The Pile,,,,Unverified,,,,,,1.00,,,,,,,,,,,2024-05-09 13:26,Robi Rahman,GPT-Neo-1.3B,1,,,,,,,,,,Research collective,,,,,Research collective,,,
ERNIE-Doc (151M),Language,,2020-12-31,"weights (Ernie-Doc base, Apache license). don't see training code for WT103. https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-doc ",Open source,,Unreleased,https://arxiv.org/pdf/2012.15688,Baidu,,,China,"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",,,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,WikiText-103,"Section 4.1. 

I believe section 4.2 (pretraining on a larger corpus) is talking about a different model/experiment.",,,Unverified,,,37.00,,,190.88,,,,,,,,,,,2024-05-10 10:57,Robi Rahman,ERNIE-Doc (151M),1,,,,,,,,,,Industry,,,,,Industry,,,
Tensorized Transformer (large PTB),Language,,2019-06-24,,Unreleased,,Unreleased,https://arxiv.org/abs/1906.09777,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",,,"China,China,China","Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",,,A Tensorized Transformer for Language Modeling,,,,,Unverified,,,133.00,,,30.00,,,,,,,,,,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",2024-05-22 16:36,Robi Rahman,Tensorized Transformer (large PTB),1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
T2R 75% + Pretrain,Language,,2021-03-24,"There's a repo but it's kind of inscrutable with no docs about T2R, not clear if the training code for this paper is in it:

https://github.com/jungokasai/T2R/tree/master ",Unreleased,,Unreleased,https://arxiv.org/abs/2103.13076,"University of Washington,Microsoft,DeepMind",,,"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland","Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",,,Finetuning Pretrained Transformers into RNNs,,,,,Unverified,,,,,,34.47,,,,,,,,,,"Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.",2024-05-09 13:29,Robi Rahman,T2R 75% + Pretrain,1,,,,,,,,,,"Academia,Industry,Industry",,,,,"Academia,Industry,Industry",,,
Adaptive LSTM + DeFINE,Language,,2019-11-27,,Unreleased,,Unreleased,https://arxiv.org/abs/1911.12385,University of Washington,,,United States of America,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",,,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,,,,,Unverified,,,,,,20.00,,,,,,,,,,"For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",2024-05-22 11:29,Robi Rahman,Adaptive LSTM + DeFINE,1,,,,,,,,,,Academia,,,,,Academia,,,
TRIMELMlong (150M),Language,,2022-05-25,"code and weights, no clear license: https://github.com/princeton-nlp/TRIME?tab=readme-ov-file ",Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2205.12674,Princeton University,,,United States of America,"Zexuan Zhong, Tao Lei, Danqi Chen",,,Training Language Models with Memory Augmentation,,,,,Unverified,,,,150000000.00,,139.81,,,,,,,,,,"Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories--local, long-term, and external memory--at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",2024-05-21 05:43,Robi Rahman,TRIMELMlong (150M),1,,,,,,,,,,Academia,,,,,Academia,,,
DOC + Finetune∗ + Partial Shuffle (PTB),Language,,2019-03-11,,,,,https://arxiv.org/abs/1903.04167,University of Washington,,,United States of America,Ofir Press,,,Partially Shuffling the Training Data to Improve Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"Although SGD requires shuffling the training data between epochs, currently none of the word-level language modeling systems do this. Naively shuffling all sentences in the training data would not permit the model to learn inter-sentence dependencies. Here we present a method that partially shuffles the training data between epochs. This method makes each batch random, while keeping most sentence ordering intact. It achieves new state of the art results on word-level language modeling on both the Penn Treebank and WikiText-2 datasets.",2024-05-22 12:35,Robi Rahman,DOC + Finetune∗ + Partial Shuffle (PTB),1,,,,,,,,,,Academia,,,,,Academia,,,
aLSTM(depth-2)+RecurrentPolicy (PTB),Language,,2018-05-22,,,,,https://arxiv.org/abs/1805.08574,University of Manchester,,,United Kingdom of Great Britain and Northern Ireland,"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",,,Breaking the Activation Function Bottleneck through Adaptive Parameterization,,,,,Unverified,,,,,,180.00,,,,,,,,,,"Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half as many iterations.",2024-05-21 06:30,Robi Rahman,aLSTM(depth-2)+RecurrentPolicy (PTB),1,,,,,,,,,,Academia,,,,,Academia,,,
GLM-10B-unidirectional,Language,,2021-03-18,Apache 2.0 or MIT for code/weights (not sure if weights are for unidirectional or bidirectional): https://github.com/THUDM/GLM ,Open source,,Open source,https://arxiv.org/abs/2103.10360,"Tsinghua University,Beijing Academy of Artificial Intelligence,Massachusetts Institute of Technology (MIT)",,,"China,China,United States of America","Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",,,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,The Pile,,,,Unverified,,,,,,1.00,,,,,,,,,,"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",2024-05-09 13:25,Robi Rahman,GLM-10B-unidirectional,1,,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
TrellisNet-MoS (1.4x larger),Language,,2018-10-15,,,,Open source,https://arxiv.org/abs/1810.06682,"Carnegie Mellon University (CMU),Intel Labs,Bosch Center for Artificial Intelligence",,,"United States of America,Multinational,Germany","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",,,Trellis Networks for Sequence Modeling,,,,,Unverified,,,,,,25.00,,,,,,,,,,"We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at this https URL .",2024-05-22 12:53,Robi Rahman,TrellisNet-MoS (1.4x larger),1,,,,,,,,,,"Academia,Industry,Industry",,,,,"Academia,Industry,Industry",,,
4 layer QRNN + dynamic evaluation,Language,,2019-06-10,"there's a repo, but the WT-103 experiments aren't there:

https://github.com/ChengyueGongR/advsoft ",Unreleased,,Unreleased,https://arxiv.org/abs/1906.03805,University of Texas at Austin,,,United States of America,"Dilin Wang, Chengyue Gong, Qiang Liu",,,Improving Neural Language Modeling via Adversarial Training,,,,,Unverified,,,,,,,,,,,,,,,,"Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",2024-05-22 16:43,Robi Rahman,4 layer QRNN + dynamic evaluation,1,,,,,,,,,,Academia,,,,,Academia,,,
LSTM (WT103),Language,,2016-12-13,,,,,https://arxiv.org/abs/1612.04426,Facebook AI Research,,,United States of America,"Edouard Grave, Armand Joulin, Nicolas Usunier",,,Improving Neural Language Models with a Continuous Cache,,,,,Unverified,,,,,,,,,,,,,,,,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",2024-05-10 00:37,Robi Rahman,LSTM (WT103),1,,,,,,,,,,Industry,,,,,Industry,,,
GPT-Neo-125M (finetuned),Language,,2021-03-21,MIT for code. don't see model weights for finetune: https://github.com/EleutherAI/gpt-neo ,Unreleased,,Open source,"https://github.com/EleutherAI/gpt-neo, https://arxiv.org/pdf/2302.03773.pdf",EleutherAI,,,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, Michael Santacroce, Zixin Wen, Yelong Shen, Yuanzhi Li",,,"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, What Matters In The Structured Pruning of Generative Language Models?","The Pile,WikiText-103",,,,Unverified,,,,,,1.00,,,,,,,,,,,2024-05-09 13:38,Robi Rahman,"GPT-Neo-125M(finetuned),GPT-Neo-125M(finetuned)",1,,,,,,,,,,Research collective,,,,,Research collective,,,
DARTS (second order),Language,,2018-06-24,,,,,https://arxiv.org/abs/1806.09055,"Carnegie Mellon University (CMU),DeepMind",,,"United States of America,United Kingdom of Great Britain and Northern Ireland","Hanxiao Liu, Karen Simonyan, Yiming Yang",,,DARTS: Differentiable Architecture Search,,,,,Unverified,,,,,,300.00,,,,,,,,,,"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",2024-05-21 09:15,Robi Rahman,DARTS (second order),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
4 layer Densely Connected LSTM,Language,,2017-07-19,,,,,https://arxiv.org/pdf/1707.06130,Ghent University,,,Belgium,"Fréderic Godin, Joni Dambre, Wesley De Neve",,,Improving Language Modeling using Densely Connected Recurrent Neural Networks,,,,,Unverified,,,,,,100.00,,,,,,,,,,"In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.",2024-05-21 05:07,Robi Rahman,4 layer Densely Connected LSTM,1,,,,,,,,,,Academia,,,,,Academia,,,
Tensorized Transformer (151M),Language,,2019-06-24,,Unreleased,,Unreleased,https://arxiv.org/abs/1906.09777,"Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",,,"China,China,China","Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",,,A Tensorized Transformer for Language Modeling,,,,,Unverified,,,133.00,,,30.00,,,,,,,,,,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",2024-05-22 16:36,Robi Rahman,Tensorized Transformer (151M),1,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),Language,,2018-08-14,,,,,https://arxiv.org/abs/1808.05908,IBM,,,United States of America,Siddhartha Brahma,,,Improved Language Modeling by Decoding the Past,,,,,Unverified,,,,,,1200.00,,,,,,,,,,"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.",2024-05-21 09:41,Robi Rahman,AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-LSTM-DOC (fin) (23M),Language,,2018-08-30,,,,,https://arxiv.org/abs/1808.10143,"NTT Communication Science Laboratories,Tohoku University",,,"Japan,Japan","Sho Takase, Jun Suzuki, Masaaki Nagata",,,Direct Output Connection for a High-Rank Language Model,,,,,Unverified,,,,,,300.00,,,,,,,,,,"This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.",2024-05-10 04:48,Robi Rahman,AWD-LSTM-DOC (fin) (23M),1,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Subformer (83M),Language,,2021-01-01,"code, but not for language modeling: https://github.com/machelreid/subformer/blob/master/README.md",Unreleased,,Unreleased,https://arxiv.org/abs/2101.00234,"The University of Tokyo,National Institute of Advanced Industrial Science and Technology (AIST)",,,"Japan,Japan","Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",,,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,WikiText-103,,,,Unverified,,,,,,70.29,,,,,,,,,,"Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.",2024-05-21 05:45,Robi Rahman,Subformer (83M),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
T2R + Pretrain,Language,,2021-03-24,"There's a repo but it's kind of inscrutable with no docs about T2R, not clear if the training code for this paper is in it:

https://github.com/jungokasai/T2R/tree/master ",Unreleased,,Unreleased,https://arxiv.org/abs/2103.13076,"University of Washington,Microsoft,DeepMind",,,"United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland","Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",,,Finetuning Pretrained Transformers into RNNs,,,,,Unverified,,,,,,34.47,,,,,,,,,,"Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.",2024-05-09 13:29,Robi Rahman,T2R + Pretrain,1,,,,,,,,,,"Academia,Industry,Industry",,,,,"Academia,Industry,Industry",,,
Subformer (96M),Language,,2021-01-01,"code, but not for language modeling: https://github.com/machelreid/subformer/blob/master/README.md ",Unreleased,,Unreleased,https://arxiv.org/abs/2101.00234,"The University of Tokyo,National Institute of Advanced Industrial Science and Technology (AIST)",,,"Japan,Japan","Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",,,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,WikiText-103,,,,Unverified,,,,,,70.29,,,,,,,,,,"Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.",2024-05-21 05:45,Robi Rahman,Subformer (96M),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
SparseOPT-30B,Language,,2023-01-02,"code is Apache 2.0 (but OPT, which you'd need to recreate this model, is non-commercial)

https://github.com/IST-DASLab/sparsegpt",Unreleased,,Open source,https://arxiv.org/abs/2301.00774,Institute of Science and Technology Austria (ISTA),,,Austria,"Elias Frantar, Dan Alistarh",,,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,,,,,Unverified,,,,30000000000.00,,1.67,,,,,,,,,,"We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: this https URL.",2024-05-10 05:00,Robi Rahman,SparseOPT-30B,1,OPT-30B,,,,,,,,,Academia,,,,,Academia,,,
Transformer-XL + AutoDropout (PTB),Language,,2021-01-05,404: https://github.com/google-research/google-research/tree/master/auto_dropout ,Unreleased,,Unreleased,https://arxiv.org/abs/2101.01761,Google Research,,,Multinational,"Hieu Pham, Quoc V. Le",,,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,,,,,Unverified,,,,,,,,,,,,,,,,"Neural networks are often over-parameterized and hence benefit from aggressive regularization. Conventional regularization methods, such as Dropout or weight decay, do not leverage the structures of the network's inputs and hidden states. As a result, these conventional methods are less effective than methods that leverage the structures, such as SpatialDropout and DropBlock, which randomly drop the values at certain contiguous areas in the hidden states and setting them to zero. Although the locations of dropout areas random, the patterns of SpatialDropout and DropBlock are manually designed and fixed. Here we propose to learn the dropout patterns. In our method, a controller learns to generate a dropout pattern at every channel and layer of a target network, such as a ConvNet or a Transformer. The target network is then trained with the dropout pattern, and its resulting validation performance is used as a signal for the controller to learn from. We show that this method works well for both image recognition on CIFAR-10 and ImageNet, as well as language modeling on Penn Treebank and WikiText-2. The learned dropout patterns also transfers to different tasks and datasets, such as from language model on Penn Treebank to Engligh-French translation on WMT 2014. Our code will be available.",2024-05-21 06:29,Robi Rahman,Transformer-XL + AutoDropout (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
BLOOM-1B,Language,,2022-07-05,"commercial, no harmful use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license",Open access (restricted use),,Unreleased,https://huggingface.co/bigscience/bloom-3b,"Hugging Face,BigScience",,,"Multinational,Multinational","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,BigScience ROOTS Corpus,,,,Unverified,,,,1000000000.00,,1.00,,,,,,,,,,,2024-04-30 14:44,Robi Rahman,BLOOM-1B,1,,,,,,,,,,"Industry,Research collective",,,,,"Industry,Research collective",,,
BLOOM-3B,Language,,2022-07-05,"commercial, no harmful use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license",Open access (restricted use),,Unreleased,https://huggingface.co/bigscience/bloom-3b,"Hugging Face,BigScience",,,"Multinational,Multinational","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,BigScience ROOTS Corpus,,,,Unverified,,,,3000000000.00,,1.00,,,,,,,,,,,2024-04-30 14:44,Robi Rahman,BLOOM-3B,1,,,,,,,,,,"Industry,Research collective",,,,,"Industry,Research collective",,,
GPT-Neo-1.3B (finetuned),Language,,2021-03-21,MIT for code. don't see model weights for finetune: https://github.com/EleutherAI/gpt-neo ,Unreleased,,Open source,https://github.com/EleutherAI/gpt-neo,EleutherAI,,,Multinational,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",,,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,,,Unverified,,,,,,1.00,,,,,,,,,,,2024-05-09 13:26,Robi Rahman,GPT-Neo-1.3B (finetuned),1,,,,,,,,,,Research collective,,,,,Research collective,,,
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)",Language,,2017-11-10,,,,,https://arxiv.org/abs/1711.03953,Carnegie Mellon University (CMU),,,United States of America,"Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",,,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,,,,,Unverified,,,,,,1000.00,,,,,,,,,,"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.",2024-05-21 06:31,Robi Rahman,"""AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)""",1,,,,,,,,,,Academia,,,,,Academia,,,
Verbatim Memory Transformer (108M),Language,,2022-10-24,CC BY 4.0 for code: https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,Unreleased,,Open source,https://arxiv.org/abs/2210.13569,"Johns Hopkins University,New York University (NYU)",,,"United States of America,United States of America","Kristijan Armeni, Christopher Honey, Tal Linzen",,,Characterizing Verbatim Short-Term Memory in Neural Language Models,WikiText-103,,,,Unverified,,,,108000000.00,,,,,,,,,,,,"When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM's retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.",2024-05-21 05:03,Robi Rahman,Characterizing Verbatim Short-Term Memory in Neural Language Models (108M),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Linear Transformer (small),Language,,2021-02-22,"code, Apache license: https://github.com/IDSIA/lmtool-fwp/tree/master/example_scripts/2021_linear_transformers_are_secretly_fwps ",Unreleased,,Open source,https://arxiv.org/abs/2102.11174,"IDSIA,SUPSI",,,"Switzerland,Switzerland","Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",,,Linear Transformers Are Secretly Fast Weight Programmers,WikiText-103,,,,Unverified,,,,,,120.00,,,,,,,,,,"We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow"" neural net learns by gradient descent to program the ``fast weights"" of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.",2024-05-22 12:28,Robi Rahman,Linear Transformer (small),1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
"GPT-2 (117M, SLW 110K)",Language,,2021-08-13,there's a repo for the technique but I don't see training code for this model: https://github.com/microsoft/DeepSpeed ,Unreleased,,Unreleased,https://arxiv.org/abs/2108.06084,Microsoft,,,United States of America,"Conglong Li, Minjia Zhang, Yuxiong He",,,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,,,,,Unverified,,,,,,1.10,,,,,,,,,,"Recent works have demonstrated great success in pre-training large-scale autoregressive language models on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efficiency dilemma: increasing the batch sizes and learning rates leads to better training efficiency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model. We find that there is a strong correlation between training instability and extreme values of gradient variance, and that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a Sequence Length Warmup method that aims to solve the training stability-efficiency dilemma. Experiments replicating GPT-2 models show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate.",2024-05-09 13:44,Robi Rahman,"""GPT-2 (117M, SLW 110K)""",1,,,,,,,,,,Industry,,,,,Industry,,,
mini-GPT-2+Active-AdamW,Language,,2023-01-24,,Unreleased,,Unreleased,https://arxiv.org/abs/2301.10133,HEC Montreal,,"Must be below 1e23 FLOP, given it's mini GPT-2...",Canada,"Davood Wadi, Marc Fredette, Sylvain Senecal",,,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,WikiText-103,,,,Unverified,,,,,,200.00,,,,,,,,,,"We propose ActiveLR, an optimization meta algorithm that localizes the learning rate, α, and adapts them at each epoch according to whether the gradient at each epoch changes sign or not. This sign-conscious algorithm is aware of whether from the previous step to the current one the update of each parameter has been too large or too small and adjusts the α accordingly. We implement the Active version (ours) of widely used and recently published gradient descent optimizers, namely SGD with momentum, AdamW, RAdam, and AdaBelief. Our experiments on ImageNet, CIFAR-10, WikiText-103, WikiText-2, and PASCAL VOC using different model architectures, such as ResNet and Transformers, show an increase in generalizability and training set fit, and decrease in training time for the Active variants of the tested optimizers. The results also show robustness of the Active variant of these optimizers to different values of the initial learning rate. Furthermore, the detrimental effects of using large mini-batch sizes are mitigated. ActiveLR, thus, alleviates the need for hyper-parameter search for two of the most commonly tuned hyper-parameters that require heavy time and computational costs to pick. We encourage AI researchers and practitioners to use the Active variant of their optimizer of choice for faster training, better generalizability, and reducing carbon footprint of training deep neural networks.",2024-05-22 12:41,Robi Rahman,mini-GPT-2+Active-AdamW,1,,,,,,,,,,Academia,,,,,Academia,,,
RNNLM + Dynamic KL Regularization,Language,,2018-01-01,,,,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,Northwestern University,,,United States of America,"Thanapon Noraset, David Demeter, Doug Downey",,,Controlling Global Statistics in Recurrent Neural Network Text Generation,,,,,Unverified,,,,,,20.00,,,,,,,,,,"Recurrent neural network language models (RNNLMs) are an essential component for many language generation tasks such as machine translation, summarization, and automated conversation. Often, we would like to subject the text generated by the RNNLM to constraints, in order to overcome systemic errors (e.g. word repetition) or achieve application-specific goals (e.g. more positive sentiment). In this paper, we present a method for training RNNLMs to simultaneously optimize likelihood and follow a given set of statistical constraints on text generation.  The problem is challenging because the statistical constraints are defined over aggregate model behavior, rather than model parameters, meaning that a straightforward parameter regularization approach is insufficient.  We solve this problem using a dynamic regularizer that updates as training proceeds, based on the generative behavior of the RNNLMs.  Our experiments show that the dynamic regularizer outperforms both generic training and a static regularization baseline.  The approach is successful at improving word-level repetition statistics by a factor of four in RNNLMs on a definition modeling task.  It also improves model perplexity when the statistical constraints are $n$-gram statistics taken from a large corpus.",2024-05-21 05:05,Robi Rahman,RNNLM + Dynamic KL Regularization,1,,,,,,,,,,Academia,,,,,Academia,,,
LSTM-MemoryAug (PTB),Language,,2020-09-29,,Unreleased,,Unreleased,https://arxiv.org/abs/2009.13774,"Johns Hopkins University,Xiaomi Corp",,,"United States of America,China","Ke Li, Daniel Povey, Sanjeev Khudanpur",,,Neural Language Modeling With Implicit Cache Pointers,,,,,Unverified,,,,,,,,,,,,,,,,"A cache-inspired approach is proposed for neural language models (LMs) to improve long-range dependency and better predict rare words from long contexts. This approach is a simpler alternative to attention-based pointer mechanism that enables neural LMs to reproduce words from recent history. Without using attention and mixture structure, the method only involves appending extra tokens that represent words in history to the output layer of a neural LM and modifying training supervisions accordingly. A memory-augmentation unit is introduced to learn words that are particularly likely to repeat. We experiment with both recurrent neural network- and Transformer-based LMs. Perplexity evaluation on Penn Treebank and WikiText-2 shows the proposed model outperforms both LSTM and LSTM with attention-based pointer mechanism and is more effective on rare words. N-best rescoring experiments on Switchboard indicate that it benefits both very rare and frequent words. However, it is challenging for the proposed model as well as two other models with attention-based pointer mechanism to obtain good overall WER reductions.",2024-05-10 15:22,Robi Rahman,LSTM-MemoryAug (PTB),1,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Hybrid H3-1.3B,Language,,2022-12-28,apache 2: https://github.com/HazyResearch/H3,Open source,,Open source,https://arxiv.org/abs/2212.14052,"Stanford University,University at Buffalo",,,"United States of America,United States of America","Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",,,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,The Pile,"""We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile [21] for 400B tokens""",,,Unverified,,,125.00,1300000000.00,,509.02,,,,,,,,,,,2024-05-01 09:22,Robi Rahman,Hybrid H3-1.3B,1,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
SRU++ Large only 2 attention layers (k=5),Language,,2021-02-24,code with MIT license (no script for WT103) https://github.com/asappresearch/sru/blob/master/language_model/README.md ,Unreleased,,Unreleased,https://arxiv.org/abs/2102.12459,ASAPP,,,United States of America,Tao Lei,,,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,WikiText-103,,,,Unverified,,,,,,34.08,,,,,,,,,,"Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",2024-05-21 05:46,Robi Rahman,SRU++ Large only 2 attention layers (k=5),1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-LSTM + DeFINE,Language,,2019-11-27,,Unreleased,,Unreleased,https://arxiv.org/abs/1911.12385,University of Washington,,,United States of America,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",,,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,,,,,Unverified,,,,,,20.00,,,,,,,,,,"For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",2024-05-22 11:29,Robi Rahman,AWD-LSTM + DeFINE,1,,,,,,,,,,Academia,,,,,Academia,,,
N-gram,Language,,2014-12-24,,,,,https://arxiv.org/abs/1412.7753,Facebook AI Research,,,United States of America,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",,,Learning Longer Memory in Recurrent Neural Networks,,,,,Unverified,,,,,,,,,,,,,,,,"Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).",2024-05-21 05:18,Robi Rahman,N-gram,1,,,,,,,,,,Industry,,,,,Industry,,,
AWD-LSTM+WT+Cache+IOG (PTB),Language,,2017-09-26,,,,,https://arxiv.org/abs/1709.08907,NTT Communication Science Laboratories,,,Japan,"Sho Takase, Jun Suzuki, Masaaki Nagata",,,Input-to-Output Gate to Improve RNN Language Models,,,,,Unverified,,,,,,5.00,,,,,,,,,,"This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.
",2024-05-21 09:46,Robi Rahman,AWD-LSTM+WT+Cache+IOG (PTB),1,,,,,,,,,,Industry,,,,,Industry,,,
"Segatron -XL base, M=150 + HCP",Language,,2022-03-21,"code, no license: https://github.com/richardbaihe/robustLM ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/2203.10692,"Microsoft Research,University of Waterloo",,,"United States of America,Canada","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",,,Better Language Model with Hypernym Class Prediction,,,,,Unverified,,,8.00,59000000.00,Table 2,18.64,,,,,,,,,,"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and Arxiv. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",2024-05-21 06:30,Robi Rahman,"""Segatron -XL base, M=150 + HCP""",1,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
BERT-Large-CAS (WT2),Language,,2019-04-20,"code, no license specified: https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model ",Unreleased,,Open access (non-commercial),https://arxiv.org/abs/1904.09408,Amazon,,,United States of America,"Chenguang Wang, Mu Li, Alexander J. Smola",,,Language Models with Transformers,,,,,Unverified,,,,,,50.00,,,,,,,,,,"The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.
In this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",2024-05-22 17:45,Robi Rahman,BERT-Large-CAS (WT2),1,,,,,,,,,,Industry,,,,,Industry,,,
RNN,Language,,2012-12-01,,,,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,Microsoft Research,,,United States of America,"Tomas Mikolov, Geoffrey Zweig",,,Context dependent recurrent neural network language model,,,,,Unverified,,,,,,,,,,,,,,,,"Recurrent neural network language models (RNNLMs) have
recently demonstrated state-of-the-art performance across a
variety of tasks. In this paper, we improve their performance
by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual
information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding
text, we achieve a topic-conditioned RNNLM. This approach
has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data
subsets. We report perplexity results on the Penn Treebank
data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition
task, where we observe improvements in word-error-rate.",2024-05-10 04:34,Robi Rahman,RNN,1,,,,,,,,,,Industry,,,,,Industry,,,
Pointer Sentinel-LSTM,Language,,2016-09-26,,,,,https://arxiv.org/abs/1609.07843,MetaMind Inc,,,United States of America,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",,,Pointer Sentinel Mixture Models,,,,,Unverified,,,,,,64.00,,,,,,,,,,Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.,2024-05-10 04:56,Robi Rahman,Pointer Sentinel-LSTM,1,,,,,,,,,,Industry,,,,,Industry,,,
Cohere Command,Language,Language modelling/generation,2023-03-01,,API access,,,https://cohere.com/models/command,Cohere,,,Canada,,,,"World-class AI, at your command",,,,,Speculative,,,,52000000000.00,"52B for larger version

https://aws.amazon.com/bedrock/cohere-command-embed/

Cohere Command has had a few different sizes over time and is continuously updated, but there's been a 52B version since at least March 2023: https://twitter.com/percyliang/status/1638236921754443776",,,,,"https://docs.cohere.com/docs/environmental-impact

2696.16 kg carbon for base-light and 6689.76 kg carbon for embed-english. Nothing listed for the large model. 

It's possible to back out GPU-hours using this calculator, though it varies by region and Cohere doesn't specify the region.

https://mlco2.github.io/impact/",Google TPU v4,,,,Industry,,2024-04-03 10:01,Anonymous,,,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Midjourney V1,Image generation,Image generation,2022-02-15,,,,,,Midjourney,,,United States of America,,"Significant use,Historical significance","Significant historical usage and controversy as one of the first generative AI art models. For example:
https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html",,,,,,Unverified,,,,,,,,,,,,,,,,,2024-05-14 00:05,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Anthropic LM 52B,Language,,2023-04-12,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/2204.05862,Anthropic,,,United States of America,"Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan",,,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,,"The 52B preference model was trained on a mixture of helpfulness and harmlessness (red teaming) datasets collected by Anthropic using crowdworkers conversing with language models in a feedback interface.
The helpfulness dataset contains around 44k comparisons and the harmlessness dataset contains around 42k comparisons. The data consists of multi-turn dialogues where crowdworkers choose the more helpful or more harmful response at each turn.
The model was trained using a technique called preference model pretraining on additional datasets before finetuning on Anthropic's human feedback data.",,,Confident,,,837.00,52000000000.00,,,,,,,,Reinforcement learning,,,Industry,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",2024-04-03 11:28,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
6-Act Tether,Robotics,Object detection,2021-08-03,,,,,https://openaccess.thecvf.com/content/ICCV2021/html/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.html,"Facebook AI Research,Georgia Institute of Technology",,"""In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours)"". No GPU specified.","United States of America,United States of America","Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans",SOTA improvement,"""Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge""",Auxiliary Tasks and Exploration Enable ObjectGoal Navigation,Matterport,"""We experiment on the Matterport dataset (MP3D [4]), which has 90 scenes and 40 labeled semantic object categories.""",,,Confident,,,58.00,5000000.00,"""Agent parameter counts were all 5 − 6 million parameters, excluding parameters in auxiliary modules""",,,,,,,Reinforcement learning,,,,,2024-05-01 09:13,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
SemExp,Robotics,Object detection,2020-07-02,MIT code/weights: https://github.com/devendrachaplot/Object-Goal-Navigation,Open source,,Open source,https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf,"Carnegie Mellon University (CMU),Facebook AI Research",,,"United States of America,United States of America","Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov",SOTA improvement,"""Our method achieves state-of-the-art performance on the object goal navigation task and won the CVPR2020 Habitat ObjectNav challenge""",Object Goal Navigation using Goal-Oriented Semantic Exploration,"Gibson,Matterport3D (MP3D)","""We use the Gibson [46] and Matterport3D (MP3D) [6] datasets""",,,Unknown,,,349.00,,,,,,,,,Reinforcement learning,,,,"This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, ‘GoalOriented Semantic Exploration’ which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Diffusion-GAN,Image generation,Image generation,2022-06-05,,,,,https://arxiv.org/abs/2206.02262v4,"UT Austin,Microsoft",,"Must be <1e23 FLOP, all experiments were done with 4 or 8 V100s.","United States of America,United States of America","Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou",SOTA improvement,"""We demonstrate the advantages of Diffusion-GAN over strong GAN
baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.""",Diffusion-GAN: Training GANs with Diffusion,,"They experimented with the following datasets: ""CIFAR-10 (Krizhevsky, 2009), STL-10 (Coates et al., 2011), LSUN-Bedroom (Yu et al., 2015), LSUN-Church
(Yu et al., 2015), AFHQ(Cat/Dog/Wild) (Choi et al., 2020), and FFHQ (Karras et al., 2019)""",,,Unverified,,,115.00,,,,,,,,NVIDIA V100,,,,,"Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussianmixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator’s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator’s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.",2024-05-21 04:15,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Projected GAN,Image generation,Image generation,2021-11-01,,,,,https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html,Heidelberg University,,"""With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100,
for more details we refer to the appendix.""

""We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX
6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.""",Germany,"Axel Sauer, Kashyap Chitta, Jens Müller, Andreas Geiger",SOTA improvement,"""It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets""",Projected GANs Converge Faster,,They experiment with 22 image datasets.,,,Unknown,,,168.00,,,,,"""On images at resolution 2562, the wall-clock training times
measured in sec/kimg using 8 Quadro RTX 6000 are shown in
Table 10.""",,,"NVIDIA V100,NVIDIA Quadro RTX 6000",,,,,"Generative Adversarial Networks (GANs) produce high-quality images but are
challenging to train. They need careful regularization, vast amounts of compute,
and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space.
Motivated by the finding that the discriminator cannot fully exploit features from
deeper layers of the pretrained model, we propose a more effective strategy that
mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with
resolutions of up to one Megapixel and advances the state-of-the-art Fréchet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected
GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock
time from 5 days to less than 3 hours given the same computational resources.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
SimpleNet,Vision,Image classification,2016-08-22,,,,,https://arxiv.org/abs/1608.06037,"Sensifai,Islamic Azad University,Technicolor R&I,Institute for Research in Fundamental Sciences (IPM)",,,"Belgium,Iran (Islamic Republic of),France,Iran (Islamic Republic of)","Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou",SOTA improvement,"""We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures""","Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures","CIFAR-10,ImageNet","""We experimented on CIFAR-10/100 Krizhevsky & Hinton (2009), SVHN Netzer et al. (2011), MNIST
Lecun et al. (1998) and ILSVRC 2012 classification task Russakovsky et al. (2015) datasets in order
to evaluate and compare our architecture""",,,Confident,,,117.00,5480000.00,SOTA CIFAR-10 model was 5.48m params,,,,,,NVIDIA GeForce GTX 980,,,,,"Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and highly competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet",2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
PG-SWGAN,Image generation,Image generation,2019-06-15,"looks like code but no weights, no license specified: https://github.com/musikisomorphie/swd",Unreleased,Open access (non-commercial),Open access (non-commercial),https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html,ETH Zurich,,,Switzerland,"Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool",SOTA improvement,"""For fair comparison, we equip the same progressive growing architecture with our proposed SWGAN objective and its dual
SWD blocks (PG-SWGAN). As shown in Fig. 3 (Right)
and Fig. 5, our PG-SWGAN can outperform PG-WGAN in
terms of both qualitative and quantitative comparison on the
CelebA-HQ and LSUN datasets""",Sliced Wasserstein Generative Models,"CIFAR-10,LSUN,CelebA",,,,Unknown,,,115.00,,,,,,,,,,,,,"In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.",2024-05-21 13:03,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
LaNet-L (CIFAR-10),Vision,Image classification,2019-06-17,"code and weights here, non-commercial license: https://github.com/facebookresearch/LaMCTS/tree/main/LaNAS/LaNet/CIFAR10",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/1906.06832,"Brown University,Facebook",,"LaNet-L was trained on 150 GPU-days, however the GPU was not specified","United States of America,United States of America","Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian",SOTA improvement,"""In practice, LaNAS finds a network that achieves SOTA 99.0% accuracy on CIFAR-10""",Sample-Efficient Neural Architecture Search by Learning Action Space,CIFAR-10,,,,Likely,,,42.00,44100000.00,44.1M,600.00,,,,,,Supervised,,,,"Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at this https URL.",2024-04-18 13:41,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
OR-WideResNet,Vision,Image classification,2017-01-07,,,,,https://arxiv.org/abs/1701.01833v2,"Duke University,University of Chinese Academy of Sciences",,,"United States of America,China","Yanzhao Zhou, Qixiang Ye, Qiang Qiu and Jianbin Jiao",SOTA improvement,"""In Sec. 4.3, we upgrade the VGG [38], ResNet [18], and the
WideResNet [45] to ORNs, and train them on CIFAR10 and
CIFAR100 [22], showing the state-of-the-art performance
on the natural image classification task.""",Oriented Response Networks,CIFAR-10,,,,Likely,,,285.00,18200000.00,18.2M for largest OR-WideResNet model.,,,,,,NVIDIA Tesla K80,,,,,"Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
PyramidNet,Vision,Image classification,2017-09-06,,,,,https://arxiv.org/abs/1610.02915v4,Korea Advanced Institute of Science and Technology (KAIST),,,Korea (Republic of),"Dongyoon Han, Jiwhan Kim, Junmo Kim",SOTA improvement,"""In tests using CIFAR-10, CIFAR-100, and ImageNet1k datasets, our PyramidNets outperform all previous state-
of-the-art deep network architectures.""",Deep Pyramidal Residual Networks,CIFAR-10,,,,Likely,,,718.00,26000000.00,best model had 26M params,300.00,,,,,,,,,,"Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at this https URL}",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Spatially-Sparse CNN,Vision,Image classification,2014-09-23,,,,,https://arxiv.org/abs/1409.6070v1,University of Warwick,,,United Kingdom of Great Britain and Northern Ireland,Benjamin Graham,SOTA improvement,SOTA per https://paperswithcode.com/sota/image-classification-on-cifar-10,Spatially-sparse convolutional neural networks,CIFAR-10,,,,Unknown,,,260.00,,Parameter count not stated but is probably derivable from the paper.,,,,,,,,,,,"Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.
Motivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%.
Although pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
W2v-BERT,Speech,Speech recognition,2021-08-07,,,,,https://arxiv.org/abs/2108.06209v2,"Google Brain,Massachusetts Institute of Technology (MIT)",,,"United States of America,United States of America","Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu",SOTA improvement,"""Our experiments show that w2v-BERT achieves competitive results
compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the
unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model
shows 5% to 10% relative WER reduction on the test-clean and
test-other subsets""",W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,LibriLight,"""We use the Libri-Light unlab-60k subset [34], which contains
about 60,000 hours of unannotated speech audio, for pre-training
w2v-BERT models. For our main results, we use the LibriSpeech
960hr subset [35] as the supervised data, and use the 100hr subset
for ablation studies""",,,Confident,,,254.00,1000000000.00,1B for XXL model,,,,,,,,,,,"Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.",2024-05-19 12:05,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
DeBERTaV3-large + KEAR,Language,,2021-12-06,,,,,https://arxiv.org/abs/2112.03254v3,Microsoft,,this is a fine-tuned version of DeBERTaV3-large,United States of America,"Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang",SOTA improvement,"""The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.""

SOTA per https://paperswithcode.com/sota/common-sense-reasoning-on-commonsenseqa",Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,,"""We present details of the 17 datasets that we use for training
data retrieval in Table 1. All the datasets are multiple-choice
or classification datasets related to commonsense reasoning,
and we include dataset details in the appendix.""",,,Likely,,,46.00,418000000.00,"DeBERTaV3-large had 418M params, per Table 2",,,,,,,,,,,"Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.",2024-05-01 09:13,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
BASIC-L + Lion,Vision,Image classification,2023-02-13,,,,,https://arxiv.org/abs/2302.06675v4,"Google,University of California Los Angeles (UCLA)",,"This model is BASIC-L retrained with a different optimizer, Lion. Lion seems more compute-efficient, so we should expect compute to be less than BASIC-L.","United States of America,United States of America","Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le",SOTA improvement,"""On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively""",Symbolic Discovery of Optimization Algorithms,,,,,Likely,,,150.00,3070000000.00,parameter count of original BASIC-L,,,,,,,,,,,"We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, Lion (EvoLved Sign Momentum). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.",2024-05-01 09:22,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,4350,"Industry,Academia",,,
Conformer,Speech,Speech recognition,2020-05-16,,Unreleased,,Unreleased,https://arxiv.org/abs/2005.08100v1,Google,,,United States of America,"Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang","Highly cited,SOTA improvement","""Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother""",Conformer: Convolution-augmented Transformer for Speech Recognition,LibriSpeech,,,,Confident,,,2092.00,118800000.00,118.8M for Conformer(L),,,,,,,,,,,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",2024-04-24 13:18,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
ContextNet,Speech,Speech recognition,2020-05-07,,Unreleased,,Unreleased,https://arxiv.org/abs/2005.03191v3,Google,,,United States of America,"Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu",SOTA improvement,"""We demonstrate that on the widely used Librispeech
benchmark, ContextNet achieves a word error rate (WER) of
2.1%/4.6% without external language model (LM), 1.9%/4.1%
with LM and 2.9%/7.0% with only 10M parameters on the
clean/noisy LibriSpeech test sets. This compares to the
best previously published model of 2.0%/4.6% with LM and
3.9%/11.3% with 20M parameters""",ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,LibriSpeech,,,970 hours of speech,Likely,,,233.00,112700000.00,Table 5,,1000000000.00,"2.647 billion FLOP to process one second of audio. Our database guide says words are the standard datapoint for speech, and speech is usually a few words per second so I put down 1 billion, but unsure what exact ratio we should use.",,,,,,,,"Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.",2024-04-24 13:16,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Transformer - LibriVox + Decoding/Rescoring,Speech,Speech recognition,2019-11-19,BSD license: https://github.com/jakeju/wav2letter?tab=License-1-ov-file#readme,Open source,,,https://arxiv.org/abs/1911.08460v3,Facebook,,"""Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks""

GPU not specified",United States of America,"Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, Ronan Collobert",SOTA improvement,"""Results with decoding/rescoring are shown in Table 2, where we reach 2.09% and 4.11% on test-clean and test-other , respectively, and are further improvements on the state-of-the-art.""",End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures,"LibriSpeech,LibriVox","""LIBRIVOX2
is a large collection of freely-available audio books. Using tools provided with the LIBRILIGHT dataset [26], we select 72K hours of read speech from English book listings and run several preprocessing
steps. After filtering samples to remove readings of duplicate text and corrupted audio, we remove all audio for which
the speaker has overlap with a sample in LIBRISPEECH. """,,,Likely,,,233.00,296000000.00,,,,,,,,,,,,"We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.",2024-04-22 13:48,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
WeNet (Penn Treebank),Language,,2019-04-08,PTB dataset,Unreleased,Open access (non-commercial),Unreleased,https://arxiv.org/abs/1904.03819,Amazon,,,United States of America,"Zhiheng Huang, Bing Xiang",SOTA improvement,"""We show that an architecture found by WeNets arXiv:1904.03819v1 [cs.NE] 8 Apr 2019 WeNet: Weighted Networks for Recurrent Network Architecture Search achieves state-of-the-art results on the Penn Treebank language dataset""",WeNet: Weighted Networks for Recurrent Network Architecture Search,Penn TreeBank,,,,Confident,,,5.00,23000000.00,Table 1,6000.00,,,,,,,,,,,2024-04-16 12:30,Epoch AI,WeNet (WT2),,,,,,,,,,,Industry,,,,,Industry,,,
TCN (P-MNIST),Language,Image classification,2018-02-15,,,,,https://openreview.net/forum?id=rk8wKk-R-,"Carnegie Mellon University (CMU),Intel Labs",,,"United States of America,Multinational","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",SOTA improvement,"""For the permuted sequential MNIST, TCNs outperform state of the art results using recurrent nets (95.9%) with Zoneout+Recurrent BatchNorm (Cooijmans et al., 2016; Krueger et al., 2017), a highly optimized method for regularizing RNNs""",Convolutional Sequence Modeling Revisited,P-MNIST,,,,Confident,,,64.00,42000.00,,,,,,,,,,,,,2024-03-07 14:22,Epoch AI,,0,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
ViT-G/14 (LiT),Vision,Image classification,2021-11-15,,,,,https://arxiv.org/abs/2111.07991v3,Google,,"They start with the ViT-G/14 image model and train their own text model. ViT-G/14 is 3.4e21. 

They also say ""We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs"" which may be relevant.",United States of America,"Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer",SOTA improvement,"""For example, it achieves 82.5% accuracy on the challenging ObjectNet test set [1], outperforming the previous state-of-the-art
method [46] by 10.2%.""",Zero-Shot Transfer with Locked-image Text Tuning,,"CC12M, YFCC100m, and their novel dataset:
""Our dataset. We collect 4 billion image and alt-text
pairs following the same process as ALIGN [31], with the
same image-based filtering but simpler text-based filtering.
Appendix L shows that reducing text filtering does not harm
performance. To avoid misleading evaluation results, we
remove from our dataset near-duplicate images of all splits
from all datasets we evaluate on. We do not consider the
creation of our dataset a main contribution of this paper; we
just simplify the data collection process in ALIGN [31] to
demonstrate the efficacy of our methods at scale.""",4000000000,"Largest dataset is ""4 billion image and alt-text pairs"". This is rounded down slightly; the other datasets are much smaller.",Likely,,,377.00,3005000000.00,Table 7,4.50,,,,,Google TPU v3,,,,,"This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning ""Locked-image Tuning"" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.",2024-05-01 09:13,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
RBM Image Classifier,Vision,Image classification,2009-04-08,,,,,https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf,University of Toronto,,,Canada,Alex Krizhevsky,Highly cited,,Learning Multiple Layers of Features from Tiny Images,CIFAR-10,"This paper is the origin of CIFAR-10. However, CIFAR-10 is a labeled subset of all the training data used

""The tiny images dataset on which we based all of our experiments was collected by colleagues at MIT and NYU over the span of six months; it is described in detail in [14]. They assembled it by searching the web for images of every non-abstract English noun in the lexical database WordNet[15, 8]. They used several search engines, including Google, Flickr, and Altavista and kept roughly the rst 3000 results for each search term. After collecting all the images for a particular search term, they removed perfect duplicates and images in which an excessively large portion of the pixels were white, as they tended to be synthetic gures rather than natural images. The search term used to nd an image provides it with a rough label, although it is extremely unreliable due to the nature of online image search technology.
In total, the dataset contains 80 million colour images downscaled to 32 × 32 and spread out across 79000 search terms. Most of our experiments with unsupervised learning were performed on a subset of about 2 million images.""

""We paid students to label a subset of the tiny images dataset... We call this the CIFAR-10 dataset, after the Canadian Institute for Advanced Research, which funded the project""",2000000,,Likely,,,28141.00,80000000.00,"Best performing model (see Figure 3.1) had 10,000 hidden units in one hidden layer and 8000 visible units",,,,,,,,,,,"Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It
is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous
researchers who have tried this have found it dicult to learn a good set of lters from the images.
We show how to train a multi-layer generative model that learns to extract meaningful features which
resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute
the work among multiple machines connected on a network, we show how training such a model can be
done in reasonable time.
A second problematic aspect of the tiny images dataset is that there are no reliable class labels
which makes it hard to use for object recognition experiments. We created two sets of reliable labels.
The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of
each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly
improved by pre-training a layer of features on a large set of unlabeled tiny images.",2024-04-01 09:52,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
DiT-XL/2 + Discriminator Guidance,Image generation,Image generation,2022-11-28,,,,,https://arxiv.org/abs/2211.17091v4,"Korea Advanced Institute of Science and Technology (KAIST),NAVER",,"This is a finetune of DiT-XL/2, so its compute won't be much higher.","Korea (Republic of),Korea (Republic of)","Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",SOTA improvement,"""Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66)""",Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,,,,,Unknown,,,44.00,,,7.00,,,,,NVIDIA A100,,,,,"The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at this https URL.",2024-05-21 13:05,Anonymous,,,DiT-XL/2,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Platypus-70B,Language,Language generation,2023-08-14,"Non-Commercial Creative Commons license (CC BY-NC-4.0)

dataset has several licenses but includes non-comm: https://huggingface.co/datasets/garage-bAInd/Open-Platypus

code: https://github.com/arielnlee/Platypus",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),"https://arxiv.org/abs/2308.07317, https://platypus-llm.github.io/",Boston University,,,United States of America,"Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz",,"SOTA for open-source but not in general (per Table 3)

""As per the Hugging Face Open LLM Leaderboard data dated 8/10/23 (Table 3), our Platypus2-70Binstruct variant has outperformed its competitors, securing the top position with an average score of 73.13""","Platypus: Quick, Cheap, and Powerful Refinement of LLMs",,"Platypus is fine-tuned Llama 2, so starts with whatever dataset was used to train Llama 2. Then they fine-tuned using Open-Platypus:

""Open-Platypus, a small-scale dataset that consists of a curated sub-selection of public text datasets. The dataset is focused on improving LLMs’ STEM and logic knowledge, and is made up of 11 open-source datasets. It is comprised mainly of human-designed questions, with only 10% of questions generated by an LLM. """,,,Confident,,,59.00,70000000000.00,fine-tuned from LLaMa 2-70B,,,,,see finetune compute,NVIDIA A100,,,,,"We present Platypus, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset Open-Platypus, that is a subset of other open datasets and which we release to the public (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: this https URL",2024-05-23 07:23,Anonymous,,,Llama 2-70B,39540000000000000000,"22 hours on 4 A100 GPUs (""our 70B model using 4 A100s 80GB for 22 hours."")

If FP16, this was around 312 TFLOPS * 88 hours * 40% = 3.954*10^19 FLOP

It would be double this if they used INT8.",,,,,,,Academia,,,,,Academia,,,
Binarized Neural Network (MNIST),Vision,Image classification,2016-03-17,,,,,https://arxiv.org/abs/1602.02830,"Technion - Israel Institute of Technology,Columbia University,CIFAR AI Research,University of Montreal / Université de Montréal",,,"Israel,United States of America,Canada,Canada","Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio",Highly cited,,Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1,MNIST,,60000,"60k training images, 10k test in MNIST",Likely,,,3299.00,37000000.00,"Parameter count is not explicitly stated, but they give details:

""The MLP we train on MNIST consists of 3 hidden layers of 4096 binary units (see Section 1) and a L2-SVM output layer""

Approximately 37m, based on 784 pixels * 4096 + 2 * 4096^2",1000.00,,,,,,,,,,"We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.",2024-05-22 12:24,Anonymous,,,,,,,,,,,,"Academia,Academia,Research collective,Academia",,,,,"Academia,Academia,Research collective,Academia",,,
CodeGen2,Language,Code generation,2023-05-03,apache 2.0,Open source,,,https://arxiv.org/abs/2305.02309,Salesforce,,,United States of America,"Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou",,,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,Stack v1.1,"""We examine our recipe on four model sizes: 1B, 3.7B, 7B, and 16B, and
refer to them as CodeGen2.
1 For training a subset of the Stack v1.1 (Kocetkov et al., 2022), filtered
with a stronger permissive license guideline, is used""",,,Likely,,,60.00,16000000000.00,16B for largest CodeGen2 model,,,,,,,,,,,"Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.
In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.
We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: this https URL.",2024-04-03 10:01,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Claude 2.1,Language,"Language modelling,Chat",2023-11-21,,API access,,,https://www.anthropic.com/index/claude-2-1,Anthropic,,,United States of America,,Significant use,,Introducing Claude 2.1,,,,,Unknown,,,0.00,,,,,,,,,Reinforcement learning,,,Industry,"Our latest model, Claude 2.1, is now available over API in our Console and is powering our claude.ai chat experience. Claude 2.1 delivers advancements in key capabilities for enterprises—including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use.",2024-05-23 06:36,Anonymous,,0,Claude 2,,,,,,,,,Industry,checked,,,,Industry,checked,,
PaLI-X,"Multimodal,Language,Vision,Video","Image captioning,Video description",2023-05-29,,,,,https://arxiv.org/abs/2305.18565,Google,,,United States of America,"Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",SOTA improvement,"""PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them).""",PaLI-X: On Scaling up a Multilingual Vision and Language Model,WebLI,"""The main pretraining data for our model is based on WebLI [5], consisting of roughly one billion
images with alt-texts from the web and OCR annotations (using the GCP Vision API), covering over
100 languages. In addition to WebLI ⟨image, text⟩ pairs, we introduce here Episodic WebLI data,
where each episode corresponds to a set of such pairs. We aim to have each episode contain loosely
related images (i.e., they are clustered according to their URL field), so as to encourage attention
among examples in an “episode”. We find this new dataset (with 75M episodes and around 400M
images in total) important for developing the few-shot capabilities of the model.""",1400000000,"1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data",Likely,,,103.00,55000000000.00,55B (table 1),,,,,,,,,,,"We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",2024-05-13 08:48,Anonymous,,0,UL2,,,,,,,,,Industry,,,,,Industry,,,
PaLM-E,Robotics,,2023-03-06,,,,,https://arxiv.org/abs/2303.03378,"Google,TU Berlin",,"Based on Palm-540B and ViT-22B and then trained on robotics data.

","United States of America,Germany","Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence",SOTA improvement,"""Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains
generalist language capabilities with increasing scale.""",PaLM-E: An Embodied Multimodal Language Model,,"""Our three robot environments (Fig. 1) include a Task and Motion Planning (TAMP) domain where a robot has to manipulate (grasp and stack) objects, a table-top pushing environment, and a mobile manipulation domain. In each domain, PaLM-E is trained on expert data from that domain. In many cases, this is a sparse amount of data per task. The TAMP tasks involve large combinatorics over possible plans, and many decision sequences are infeasible. PaLM-E has to generate plans that consist of multiple steps, with complicated decision boundaries. The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset (Lynch et al., 2022) and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics""",,,Likely,,,746.00,562000000000.00,562B,,,,,,,,,,,"Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",2024-05-13 10:20,Anonymous,,0,PaLM (540B),,"Based on Palm-540B and ViT 22B. No compute details given.

""We scale PaLM-E up to 562B parameters, integrating the 540B PaLM (Chowdhery et al., 2022) LLM and the 22B Vision Transformer (ViT) (Dehghani et al., 2023) into, to our knowledge, the largest vision-language model currently reported.""",,,,,,,"Industry,Academia",checked,,,,"Industry,Academia",,,
UniPi,"Multimodal,Video",Video generation,2023-01-31,,,,,https://arxiv.org/abs/2302.00111,"Google DeepMind,Massachusetts Institute of Technology (MIT),UC Berkeley,Georgia Institute of Technology,University of Alberta",,"UniPi was trained on 256 TPUv4 for an unknown duration, which could just about be >10^23 if the training time was 3 months and utilization was 33%. On balance, probably training compute is below 1e23 FLOP.","Multinational,United States of America,United States of America,United States of America,Canada","Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, Pieter Abbeel",,,Learning Universal Policies via Text-Guided Video Generation,,"""Our training data consists of an internet-scale pretraining dataset and a smaller real-world robotic dataset. The pretraining dataset uses the same data as [19], which consists of 14 million video-text pairs, 60 million image-text pairs, and the publicly available LAION-400M image-text dataset. The robotic dataset is adopted from the Bridge dataset [29] with 7.2k video-text pairs, where we use the task IDs as texts. We partition the 7.2k video-text pairs into train (80%) and test (20%) splits. We pretrain UniPi on the pretraining dataset followed by finetuning on the train split of the Bridge data.""",,,Unknown,,,75.00,,"Appears to be a composite model, not sure about the total parameter count.",,,,,,Google TPU v4,,,,,"A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,"Industry,Academia,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia,Academia",,,
Persia,Recommendation,Recommender system,2021-11-23,"https://github.com/PersiaML/PERSIA/blob/main/LICENSE

MIT code",Unreleased,,Open source,https://arxiv.org/abs/2111.05897,"ETH Zurich,Kuaishou Technology",,,"Switzerland,China","Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying Lin, Chengchun Shu, Xuezhong Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai Yu, Sen Yang, Ce Zhang, Ji Liu",,,"Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters",,,,,Confident,,,18.00,100000000000000.00,100 trillion,,,,,,NVIDIA V100,,,,,"Deep learning based models have dominated the current landscape of production recommender systems. Furthermore, recent years have witnessed an exponential growth of the model scale--from Google's 2016 model with 1 billion parameters to the latest Facebook's model with 12 trillion parameters. Significant quality boost has come with each jump of the model capacity, which makes us believe the era of 100 trillion parameters is around the corner. However, the training of such models is challenging even within industrial scale data centers. This difficulty is inherited from the staggering heterogeneity of the training computation--the model's embedding layer could include more than 99.99% of the total model size, which is extremely memory-intensive; while the rest neural network is increasingly computation-intensive. To support the training of such huge models, an efficient distributed training system is in urgent need. In this paper, we resolve this challenge by careful co-design of both the optimization algorithm and the distributed system architecture. Specifically, in order to ensure both the training efficiency and the training accuracy, we design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by different synchronization mechanisms; then we build a system called Persia (short for parallel recommendation training system with hybrid acceleration) to support this hybrid training algorithm. Both theoretical demonstration and empirical study up to 100 trillion parameters have conducted to justified the system design and implementation of Persia. We make Persia publicly available (at this https URL) so that anyone would be able to easily train a recommender model at the scale of 100 trillion parameters.",2024-04-15 13:42,Anonymous,,0,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
XVERSE-13B-2,Language,Language generation,2023-11-06,must apply for commercial license,Open access (restricted use),,Open source,https://huggingface.co/xverse/XVERSE-13B,Shenzhen Yuanxiang Technology,,"Not enough info, eg number of epochs",China,,,,,,"""The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.""",2800000000000,"Multilingual, 3.2 trillion tokens. Likely majority Chinese and English, so I'll assume .87 words per token, or ~2.8 trillion words",Likely,,,,13000000000.00,13B,,,,,,,,,,,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
BLUUMI,Language,Language modelling,2023-11-03,,,,,https://arxiv.org/abs/2311.05640,"University of Turku,Hugging Face",,,"Finland,Multinational","Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo",SOTA improvement,"SOTA for Finnish: ""Our best monolingual model outperforms this result by over
10% points and the BLUUMI model by over 20% points, representing a substantial advance in the
state of the art in the capability of generative models trained for Finnish.""",FinGPT: Large Generative Models for a Small Language,,Finnish data from several sources,38000000000,38B tokens,Likely,,,9.00,176000000000.00,176 billion,,,,,,AMD Instinct MI250X,,,,,"Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at this https URL.",2024-05-01 09:22,Anonymous,,0,BLOOM-176B,,"They ""continued pretraining"" of BLOOM on Finnish data. Don't think they specify the number of tokens they trained BLOOM/BLUUMI on; for their smaller models it was 300b.",,,,,,,"Academia,Industry",checked,,,,"Academia,Industry",,,
Yuan 2.0,Language,,2023-11-27,"commercial ok, but nothing that ""may cause harm to the country and society, or for any services that have not undergone security assessment and filing""
https://huggingface.co/IEITYuan/Yuan2-102B-hf",Open access (restricted use),,,https://arxiv.org/abs/2311.15786v1,Inspur,,"Trained on 288B tokens

6*102.6b*288b = 1.78e23",China,"Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang",,,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention,,"""The pretraining corpus includes a mix of books, codes, and encyclopedia in both Chinese and English (Table
2)""

with synthetic code data:
""Code (CN). Considering the diversity of programming tasks, we also build a synthesized instruction dataset
with 4 million code samples in Chinese. To cover the concepts involved in programming tasks as many as
possible, we collect 15,000 words of programming, computer science, mathematics, and other relevant
topics from the Sogou input dictionary. Two topic words are randomly selected as the seeds for a wellcrafted prompt in each time. Then the prompt will be fed to GPT-3.5 to generate a programming task and
corresponding Python solution. ""

and translated open-source fine-tuning instruction data:
""We construct a fine-tuning dataset focused on code, math and chat tasks.
Code Instruction dataset. We collect some open-source code instruction datasets, including CodeAlpaca20k [28], Evol-Instruct-Code-80k[38], CodeFuse-CodeExercise-Python-27k and CodeFuse-Evolinstruction-66k [39]. We translate the English code instruction into Chinese with GPT-3.5""",,,Likely,,,,102600000000.00,102.6 billion,,,,,,,,,,,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
EXAONE 2.0,"Multimodal,Image generation,Language,Biology","Language modelling,Image generation",2023-07-19,,,,,https://aws.amazon.com/solutions/case-studies/lg-ai-research-case-study/,LG,,,Korea (Republic of),,,,LG AI Research Develops Foundation Model Using Amazon SageMaker,,"From KoreaTimes (https://www.koreatimes.co.kr/www/tech/2023/12/129_355258.html)

""EXAONE 2.0 studied about 45 million specialized documents and 350 million images, including patents and papers secured through partnerships.

Considering that much of the existing expertise data is in English, EXAONE 2.0 is developed as a bilingual model that can understand and answer both in Korean and English at the same time. It also learns over four times more data than the previous model.""",,,Speculative,,,,300000000000.00,300 billion,,,,,,,,,,,,2024-05-20 07:30,Anonymous,,0,,,,,,,,,,Industry,,1,,,Industry,checked,,
CPM-2,Language,Language generation,2021-06-24,,,,,https://arxiv.org/abs/2106.10715,"Tsinghua University,Beijing Academy of Artificial Intelligence",,,"China,China","Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun",,,CPM-2: Large-scale Cost-effective Pre-trained Language Models,WuDao Corpora,,444100000000,"""We pre-train our model on WuDaoCorpus (Yuan et al., 2021), which contains 2.3TB cleaned Chinese data as well as 300GB cleaned English data.""

2300*167 million + 300*200 million = 444,100,000,000 (444 billion)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",Likely,,,67.00,11000000000.00,11B,,,,,,,,,,,"In recent years, the size of pre-trained language models (PLMs) has grown by leaps and bounds. However, efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline, we pre-train two models: an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover, we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at this https URL.",2024-04-01 09:30,Anonymous,,0,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Jurassic-2 Jumbo,Language,"Language modelling,Translation",2023-03-01,,,,,https://www.ai21.com/blog/introducing-j2,AI21 Labs,,,Israel,,,,Announcing Jurassic-2 and Task-Specific APIs,,,,,,,,,178000000000.00,Source is https://crfm.stanford.edu/helm/latest/#/leaderboard as viewed on 2023-12-06,,,,,,,,,,Industry,"Announcing the launch of Jurassic-2, the latest generation of AI21 Studio’s foundation models, a game-changer in the field of AI, with top-tier quality and new capabilities. And that's not all - we're also releasing our task-specific APIs, with plug-and-play reading and writing capabilities that outperform competitors.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
ERNIE 4.0,"Multimodal,Language",Chat,2023-10-17,,,,,https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,Baidu,,,China,,Significant use,"Likely SOTA for Mandarin? But very little info available.

Lots of users (https://www.cnn.com/2023/12/15/tech/gpt4-china-baidu-ernie-ai-comparison-intl-hnk/index.html):

""Baidu says ERNIE has racked up 70 million users. That’s compared with 150 million users for ChatGPT, according to an estimate from Similarweb, a digital data and analytics company.""","Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",,,,,Unknown,,,,,,,,,,,,,,,,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Lyria,Audio,Audio generation,2023-11-16,,,,,https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/,Google DeepMind,,,Multinational,"Kazuya Kawakami, David Ding, Björn Winckler, Cătălina Cangea, Tobenna Peter Igwe, Will Grathwohl, Yan Wu, Yury Sulsky, Jacob Kelly, Charlie Nash, Conor Durkan, Yaroslav Ganin, Tom Eccles, Zach Eaton-Rosen, Jakob Bauer, Mikita Sazanovich, Morgane Rivière, Evgeny Gladchenko, Mikołaj Bińkowski, Ali Razavi, Jeff Donahue, Benigno Uria, Sander Dieleman, Sherjil Ozair, John Schultz, Ankush Gupta, Junlin Zhang, Drew Jaegle, Aäron van den Oord.",,,Transforming the future of music creation,,,,,Unknown,,,,,,,,,,,,,,,,"Music contains huge amounts of information — consider every beat, note, and vocal harmony in every second. When generating long sequences of sound, it’s difficult for AI models to maintain musical continuity across phrases, verses, or extended passages. Since music often includes multiple voices and instruments at the same time, it's much harder to create than speech.

Built by Google DeepMind, the Lyria model excels at generating high-quality music with instrumentals and vocals, performing transformation and continuation tasks, and giving users more nuanced control of the output’s style and performance.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,checked,,,,Industry,checked,,
Anthropic LM 175B,Language,,2023-02-15,,Unreleased,,,https://arxiv.org/abs/2302.07459,Anthropic,,,United States of America,"Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan",,,The Capacity for Moral Self-Correction in Large Language Models,,,,,Confident,,,96.00,175000000000.00,175B,,,,,,,Reinforcement learning,,,,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
CTM (CIFAR-10),Image generation,Image generation,2023-10-01,,,,,https://arxiv.org/abs/2310.02279v1,"Stanford University,Sony",,"Almost certainly <1e23 FLOP due to the small scale experiments.
","United States of America,Japan","Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon",SOTA improvement,"""CTM... achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73)""",Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion,CIFAR-10,,,,Unknown,,,24.00,,,,,,,,NVIDIA V100,,,,,"Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
Baichuan2-53B,Language,,2023-08-09,,,,,https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,Baichuan,,,China,,,,Chinese AI startup Baichuan rolls out third LLM in four months,,,,,Confident,,,,53000000000.00,,,,,,,,,,,,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese company’s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firm’s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",2024-05-15 16:56,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
YuYan 11B,Language,,2022-07-15,,,,,https://arxiv.org/abs/2104.12470,"Hong Kong Baptist University,NetEase",,,"Hong Kong,China","Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao",,,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,,,,,Confident,,,5.00,11000000000.00,https://huggingface.co/FUXI/yuyan-11b,,,,,,"NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti",Self-supervised learning,,,Industry,"Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",checked,,
Tongyi Qianwen (Qwen) 2.0,Language,Chat,2023-10-31,,API access,,,https://www.alibabacloud.com/blog/alibaba-cloud-launches-tongyi-qianwen-2-0-and-industry-specific-models-to-support-customers-reap-benefits-of-generative-ai_600526,Alibaba,,,China,,,,Alibaba Cloud Launches Tongyi Qianwen 2.0 and Industry-specific Models to Support Customers Reap Benefits of Generative AI,,,,,Unknown,,,,,"""Tongyi Qianwen 2.0, a generic LLM with a few hundreds of billions of parameters""",,,,,,,,,,,"Alibaba Cloud, the digital technology and intelligence backbone of Alibaba Group, today announced the launch of Tongyi Qianwen 2.0, its latest large language model (LLM), along with new industry-specific models at its annual flagship tech event Apsara Conference. This release signifies another significant progress in Alibaba Cloud's pursuit of cutting-edge AI innovation and its ongoing commitment to fuel digital transformation in businesses.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
ChatRhino,Language,Chat,2023-07-13,,,,,https://jdcorporateblog.com/jd-com-introduces-chatrhino-empowering-industry-innovations-with-an-advanced-large-language-model/,JD.com,,,China,,,,JD.com Introduces ChatRhino: Empowering Industry Innovations with an Advanced Large Language Model,,"Mix of general and supply chain data: ""By combining 70% generalized data with 30% native intelligent supply chain data, JD’s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city""",,,Likely,,,,100000000000.00,"""ChatRhino sets a new benchmark as a 100-billion-parameter model"", could be substantially rounded",,,,,,,,,,,"JD.com today unveiled its ChatRhino (Yanxi in Chinese) large language model (LLM) on its 2023 JDDiscovery tech summit, tailored to serve various industries. By combining 70% generalized data with 30% native intelligent supply chain data, JD’s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city. Building upon the success of the billion-parameter model K-PLUG launched in 2021 and the 10-billion-parameter model Vega introduced in 2022, JD’s ChatRhino sets a new benchmark as a 100-billion-parameter model.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
SenseNova,Language,Chat,2023-04-10,,,,,https://www.sensetime.com/en/news-detail/51166397?categoryId=1072,SenseTime,,,Hong Kong,,,,"SenseTime Launches “SenseNova” Foundation Model Sets and AI Computing Systems, Advancing AGI Development",,,,,Unknown,,,,,"hundreds of billions: ""Natural language serves as a crucial means of communication between humans and machines. “SenseNova” has introduced “SenseChat”, the latest large-scale language model (LLM) developed by SenseTime. As an LLM with hundreds of billions of parameters, SenseChat is trained using a vast amount of data, considering the Chinese context to better understand and process Chinese texts.""",,,,,,,,,,,"SenseTime hosted a Tech Day event, sharing their strategic plan for advancing AGI (Artificial General Intelligence) development through the combination of “foundation models + large-scale computing” systems. Under this strategy, SenseTime unveiled the “SenseNova” foundation model set, introducing a variety of foundation models and capabilities in natural language processing, content generation, automated data annotation, and custom model training. At the event, SenseTime not only showcased their large language model’s capabilities, but also demonstrated a series of generative AI models and applications, such as text-to-image creation, 2D/3D digital human generation, and complex scenario/detailed object generation. Additionally, they introduced their AGI research and development platform facilitated by the integration of “foundation models + large-scale computing” systems.",2024-05-21 13:05,Anonymous,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
360 Smart Brain,"Multimodal,Language,Image generation","Language generation,Chat,Image generation",2023-09-04,,,,,https://www.hayo.com/article/64f68f1d8578eea6c7ec663f,360 Security Technology,,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83

vague report from a Google-translated article, though",China,,,,The 360 ​​Brain Model is now open to the public,,,,,Unknown,,,,,"""hundreds of billions"" https://www.hayo.com/article/650babb37c769bcba319ed83",,,,,,,,,,,"According to news on September 5, the 360 ​​Smart Brain large model will be open to the public from now on and will be fully accessible to the 360 ​​“Family Bucket”.

360 Zhi Nao will be open to the public on five major platforms. Users can download the “360 Zhi Nao” App through the 360 ​​Zhi Nao official website and major application stores.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Grok-0,Language,,2023-11-04,,Hosted access (no API),,,https://x.ai/,xAI,,"Half of Llama 2-70B? (which we estimated at 8e23) ""This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources""",United States of America,,,,Announcing Grok,,,,,Likely,,,,33000000000.00,33 billion,,,,,,,,,,,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Dou Bao,Language,Chat,2023-08-18,,,,,https://pandaily.com/bytedance-launches-its-first-large-scale-ai-conversation-product-dou-bao/,ByteDance,,,China,,,,,,,,,Unknown,,,,,,,,,,,,,,,,"The first AI conversational app “Dou Bao” and its web version have recently been launched, with the download channel for Android already open. The “Dou Bao” is the internal codename “Grace” AI project by ByteDance, and currently has functions such as text-based conversation and image-based conversation.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Agile Soccer Robot,Robotics,,2023-04-26,,Unreleased,,,https://arxiv.org/abs/2304.13653,Google DeepMind,,,Multinational,"Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Markus Wulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess",SOTA improvement,"Likely the best bipedal soccer AI, since it's DeepMind, and related work section just discusses results involving specific soccer skills and quadruped robots:

""Whether bipedal or quadrupedal, navigation represents only a fraction of animal and human
capabilities. Motivated by this observation, there is a growing interest in whole body control, i.e.
tasks in which the whole body is used in flexible ways to interact with the environment. Examples
include climbing (Rudin et al., 2022a), getting-up from the ground (Ma et al., 2023), catching objects
(Ma et al., 2023), and mobile manipulation with legs (Cheng et al., 2023). Recently, reinforcement
learning has been applied to learn simple soccer skills, including goalkeeping (Huang et al., 2022),
ball manipulation on diverse terrains (Bohez et al., 2022; Ji et al., 2023), and shooting (Ji et al.,
2022). These works focus on a narrower set of skills than the 1v1 soccer game, and the quadrupedal
platform is inherently more stable and therefore presents an easier learning challenge.""",Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning,,self-play training in simulation,,""". The get-up teacher learns to get up relatively quickly and trained in total for approximately 2.4 · 10^8 environment steps,
equivalent to approximately 70 days of simulation time, or 14 hours of wall-clock time. The soccer
teacher was trained for 2 · 10^9 environment steps, which took 158 hours of training, equivalent to
approximately 580 days of simulated match""",Unknown,,,33.00,,,,,,240.0,"14+158+68 hours:
""Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-play
took 68 hours (see Appendix B for details)""",,Reinforcement learning,,,,"We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Sparrow,Language,Chat,2022-09-28,,,,,https://arxiv.org/abs/2209.14375,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving",,,Improving alignment of dialogue agents via targeted human judgements,,,,,Likely,,,311.00,70000000000.00,70B,,,,,,,,,,,"We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.",2024-04-01 09:35,Anonymous,,0,Chinchilla,,"Few clues from paper. Not clear how much fine-tuning data in total. Also they freeze some layers during fine-tuning:

""In all cases when fine-tuning, we freeze the bottom 64 transformer layers of Chinchilla, and
only fine-tune the final 16 layers; this allows sharing of the frozen layers between the rule model,
preference models, and the base LM/policy when reranking and during reinforcement learning
training, resulting in a reduced memory footprint (fig. 8).""",,,,,,,Industry,checked,,,,Industry,,,
Wu Dao Aquila-7B,Language,"Chat,Code generation",2023-06-10,"Apache 2.0 for code: https://huggingface.co/BAAI/Aquila-7B
BAAI license for weights, commercial but restrictions around rights/PRC laws: https://huggingface.co/BAAI/Aquila-7B/resolve/main/BAAI%20Aquila%20Model%20License%20Agreement.pdf",Open access (restricted use),,Open source,"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B

https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README_en.md",Beijing Academy of Artificial Intelligence,,,China,,,,,,,,,Likely,,,,33000000000.00,33B for largest model: https://huggingface.co/BAAI/Aquila-7B,,,,,,NVIDIA A100,,,,,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",2024-04-11 16:42,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,checked,,
Aquila2 34B,Language,Chat,2023-10-13,apache 2.0,Open source,Unreleased,Open source,https://github.com/FlagAI-Open/Aquila2,Beijing Academy of Artificial Intelligence,,,China,,,,,,,,,Likely,,,,34000000000.00,"34B

There's also a 70B ""experimental"" version: https://github.com/FlagAI-Open/Aquila2",,,,,,,,,,,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.",2024-04-11 16:22,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,checked,,
LingoWhale-8B,Language,,2023-11-01,"requires form for commercial:
https://github.com/DeepLangAI/LingoWhale-8B/blob/main/MODEL_LICENSE.md",Open access (non-commercial),Unreleased,Open access (non-commercial),https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,DeepLang AI,,,China,,,competitive with Qwen-7B on C-Eval,,,"""pre-trained on a large volume of high-quality bilingual data"" Chinese + English",,,Confident,,,,8000000000.00,,,,,,,,,,,,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",2024-05-15 16:59,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Jiutian,Language,,2023-10-12,,,,,https://www.globaltimes.cn/page/202310/1299716.shtml,China Mobile,,,China,,,,,,"Designed to enhance efficiency, the model has trained over 2 trillion tokens",,,Unknown,,,,,,,,,,,,,,,,"China Mobile, the largest telecom operator in the world by subscribers, unveiled its ""Jiutian"" artificial intelligence (AI) large-scale model on Thursday, which has reportedly won support from large enterprises including China Ocean Shipping (Group) Co and China Railway Construction Co.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Spark 3.0,Language,"Code generation,Language generation",2023-10-24,,,,,https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/,iFlytek,,"""The company said that Spark 3.0 has been trained on a dataset of 1.2 trillion words and code, and that it can perform a variety of tasks, including text generation, language understanding, knowledge answering, logical reasoning, mathematical computation, code generation, and multimodal interaction."" https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/",China,,,"""One such claim comes from Chinese company iFlytek, which asserts that its latest large language model, Spark 3.0, surpasses OpenAI’s GPT-3.5 in Chinese language tasks while demonstrating comparable performance in English contexts""",,,,,,Unknown,,,,,,,,,,,,,,,,,2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
RNN-WER,Speech,Speech recognition,2014-06-22,,,,,https://proceedings.mlr.press/v32/graves14.html,"DeepMind,University of Toronto",,,"United Kingdom of Great Britain and Northern Ireland,Canada","Alex Graves, Navdeep Jaitly","Highly cited,SOTA improvement","""Finally, by combining the new model with a baseline, we
have achieved state-of-the-art accuracy on the Wall Street
Journal corpus for speaker independent recognition.""",Towards End-To-End Speech Recognition with Recurrent Neural Networks,WSJ,"""The experiments were carried out on the Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B
and LDC94S13B). The RNN was trained on both the 14
hour subset ‘train-si84’ and the full 81 hour set""

",1100000,"dataset is 81 hours

At 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)
that's 81*228*60 = 1,108,080

another source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/",Likely,,,2805.00,26500000.00,"""The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of ∼ 26.5M weights.""",,,,,,,Supervised,,,,"This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Flan UL2,Language,Language generation,2023-03-03,Apache license. https://github.com/google-research/google-research/tree/master/ul2,Open source,Unreleased,Unreleased,"https://www.yitay.net/blog/flan-ul2-20b, https://arxiv.org/abs/2205.05131",Google Brain,,,United States of America,"Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler",,"""We compare Flan-UL2 20B with other models in the Flan series. We report relative improvements over Flan-T5-XXL. Generally, Flan-UL2 outperforms Flan-T5 XXL on all four setups with an overall decent performance lift of +3.2% relative improvement""",A New Open Source Flan 20B with UL2,"C4,Flan",UL2 was pre-trained on C4. Flan UL2 is then instruction-tuned using the Flan dataset,,,Likely,,,99.00,19500000000.00,19.5B per https://www.yitay.net/blog/flan-ul2-20b,,,,,,,,,,,"Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model released earlier last year. It was fine tuned using the ""Flan"" prompt tuning and dataset collection.",2024-05-21 07:27,Anonymous,,0,UL2,,"From paper (https://arxiv.org/pdf/2205.05131.pdf), they say that ""we opt to train UL2 for another 100K steps"" with Flan instruction tuning. Not sure what the batch size is here though. In other parts of the paper they use batch sizes of 128 or 1024.

Either way, small compared to pre-training compute.",,,,,,,Industry,,,,,Industry,,,
Stable Beluga 1,Language,Language generation,2023-07-21,non-comm license https://huggingface.co/stabilityai/StableBeluga1-Delta,Open access (non-commercial),Unreleased,Unreleased,https://huggingface.co/stabilityai/StableBeluga1-Delta,Stability AI,,,Multinational,,,"#4 on Open LLM leaderboard (in July, it's much lower now):

""These Stable Beluga results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.

As of July 27th, 2023, Stable Beluga 2 is the very best model (#1) on the leaderboard, and Stable Beluga 1 is #4""","Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models",,,,,Likely,,,,65200000000.00,65.2B,,,,,,,,,,,"Stability AI and its CarperAI lab proudly announce Stable Beluga 1 and its successor Stable Beluga 2 (formerly codenamed FreeWilly), two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. Stable Beluga 1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, Stable Beluga 2 leverages the LLaMA 2 70B foundation model to achieve industry-leading performance.",2024-04-09 11:44,Anonymous,,0,LLaMA-65B,,"Fine-tuned on a 600k dataset. Not sure how many epochs. https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models

""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” While our data generation process is similar, we differ in our data sources. Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used)...""",,,,,,,Industry,,,,,Industry,,,
Stable Beluga 2,Language,Language generation,2023-07-20,non-comm: https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt,Open access (non-commercial),Unreleased,Unreleased,"https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models, https://huggingface.co/stabilityai/StableBeluga2",Stability AI,,,Multinational,,,"#1 on Open LLM leaderboard (in July, it's much lower now):

""These Stable Beluga results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.

As of July 27th, 2023, Stable Beluga 2 is the very best model (#1) on the leaderboard, and Stable Beluga 1 is #4""","Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models",,,,,Likely,,,,70000000000.00,fine-tuned from Llama 2-70B,,,,,,,,,,,"Stability AI and its CarperAI lab proudly announce Stable Beluga 1 and its successor Stable Beluga 2 (formerly codenamed FreeWilly), two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. Stable Beluga 1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, Stable Beluga 2 leverages the LLaMA 2 70B foundation model to achieve industry-leading performance.",2024-04-09 12:03,Anonymous,,0,Llama 2-70B,,"Fine-tuned on a 600k dataset. Not sure how many epochs. https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models

""The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” While our data generation process is similar, we differ in our data sources. Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used)...""",,,,,,,Industry,,,,,Industry,,,
TeleChat,Language,,2023-07-07,,,,,https://m.thepaper.cn/baijiahao_23766944,China Telecom,,,China,,,"""In addition, TeleChat-E, the educational version of the large model based on TeleChat, ranks seventh on C-Eval, the global comprehensive examination evaluation list for large models. The top few include well-known large models such as GP4 and ChatGPT""",,,"(Google translated from https://m.thepaper.cn/baijiahao_23766944) ""TeleChat uses a large amount of high-quality Chinese and English corpus for pre-training, and uses tens of millions of question and answer data for fine-tuning""",,,Unknown,,,,,,,,,,,,,,,,"(Google translated) At China Telecom's ""Computing and Network Integration·Sunac Future"" sub-forum, China Telecom Digital Intelligence Technology Branch (hereinafter referred to as: Telecom Zhike) officially released China Telecom's large language model TeleChat and demonstrated the large model empowering data Products in three directions: middle platform, intelligent customer service and intelligent government affairs.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Honghu Graphic,"Multimodal,Vision,Image generation","Text-to-image,Image generation",2023-06-28,,,,,https://medium.com/@sdatokens/china-unicom-released-a-graphic-model-that-can-realize-with-text-pictures-video-clips-llm-ai-d50afd965a5b,China Unicom,,,China,,,,,,,,,Likely,,,,2000000000.00,"""The model now has 800 million training parameters and 2 billion training parameters in two versions, which can fulfill functions such as text-to-picture and picture-to-picture, said staff members of the company."" https://www.chinadaily.com.cn/a/202309/02/WS64f2715aa310d2dce4bb3865.html",,,,,,,,,,,"From https://www.chinadaily.com.cn/a/202309/02/WS64f2715aa310d2dce4bb3865.html:

""Among them, China Unicom is showcasing the Honghu Graphic Grand Model 1.0, its first large model for operators' value-added business.

The model now has 800 million training parameters and 2 billion training parameters in two versions, which can fulfill functions such as text-to-picture and picture-to-picture, said staff members of the company.""",2024-05-13 08:45,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
XuanYuan 2.0,Language,,2023-05-19,looks like commercial + responsible use: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,Open access (restricted use),,,"https://arxiv.org/abs/2305.12002, https://huggingface.co/Duxiaoman-DI/XuanYuan-176B",Du Xiaoman,,Fine-tuned from BLOOM-176B. More details in fine-tuning column,China,"Xuanyu Zhang, Qing Yang, Dongliang Xu",,,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,,"Fine-tuned on mix of financial data and general data, including instruction data (figure 1)",366000000000,Table 2,Confident,,,30.00,176200000000.00,176.2B,,,,,,NVIDIA A100 SXM4 80 GB,,,,,"In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.",2024-05-21 07:22,Anonymous,,0,BLOOM-176B,1,"13B tokens for fine-tuning, per table 2

176B * 13B * 6 = 1.37e22",,,,,,,Industry,,,,,Industry,,,
Table-GPT,Language,,2023-10-13,,Unreleased,,,https://arxiv.org/abs/2310.09263,Microsoft Research,,,United States of America,"Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri",,Likely the best model for the table-tasks the authors tested on. No standard benchmark though.,Table-GPT: Table-tuned GPT for Diverse Table Tasks,,"Fine-tuning data was synthesized examples of tables:

""In our default settings, we use a total of
14 types of table-tasks, listed as T-5 to T-18 in Table 2, as training
data for table-tuning.
In all but two task-types (T-6: Entity Matching and T-12: NL-toSQL), we use synthesized instances of table-tasks. For each task
type, we generate 1000 instances of table-tasks using a 50:50 mix
of zero-shot and few-shot templates, following a synthesis-thenaugment approach described in Section 4""",,,Likely,,,12.00,175000000000.00,,,,,,,,,,,,"Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \emph{one-dimensional} natural-language texts, whereas relational tables are \emph{two-dimensional} objects.
In this work, we propose a new ""\emph{table-tuning}"" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong \emph{generalizability}, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.",2024-04-01 09:30,Anonymous,,0,GPT-3.5 (text-davinci-003),,"They fine-tuned GPT-3.5 and ChatGPT-3.5 on roughly 14k examples of tables. Not sure how many tokens per instance.

""Training tasks and data. In our default settings, we use a total of
14 types of table-tasks, listed as T-5 to T-18 in Table 2, as training
data for table-tuning.
In all but two task-types (T-6: Entity Matching and T-12: NL-toSQL), we use synthesized instances of table-tasks. For each task
type, we generate 1000 instances of table-tasks using a 50:50 mix
of zero-shot and few-shot templates, following a synthesis-thenaugment approach described in Section 4""",,,,,,,Industry,,,,,Industry,,,
Dolly 2.0-12b,Language,Chat,2023-04-12,"""We are open-sourcing the entirety of Dolly 2.0, including the training code, the dataset, and the model weights, all suitable for commercial use.""

under a copyleft license:
https://creativecommons.org/licenses/by-sa/3.0/",Open source,Open source,Open source,https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm,Databricks,,,United States of America,"Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin",,,Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM,databricks-dolly-15k,Fine-tuned on instruction dataset with 15k examples.,,,Likely,,,,12000000000.00,,,,,,,,,,,,"Dolly 2.0 is a 12B parameter language model based on the EleutherAI pythia model family and fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced among Databricks employees.

We are open-sourcing the entirety of Dolly 2.0, including the training code, the dataset, and the model weights, all suitable for commercial use. This means that any organization can create, own, and customize powerful LLMs that can talk to people, without paying for API access or sharing data with third parties.",2024-04-16 15:37,Anonymous,,0,Pythia-12b,,Trained on 15k question-answer examples (so fine-tune compute is probably minor),,,,,,,Industry,,,,,Industry,,,
MegaSyn,Medicine,Drug discovery,2022-03-07,,Unreleased,,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/,Collaborations Pharmaceuticals,,,United States of America,"Fabio Urbina, Filippa Lentzos, Cédric Invernizzi, Sean Ekins",Historical significance,"Notable example of an AI model having a potential dual use for bio/chemical weapons:

""To narrow the universe of molecules we chose to drive the generative model towards compounds like the nerve agent VX, one of the most toxic chemical warfare agents developed during the 20th century—a few salt-sized grains of VX, (6–10 mg)5, is sufficient to kill a person. Nerve agents such as Novichoks have also been in the headlines recently6.

In less than 6 hours after starting on our in-house server, our model generated forty thousand molecules that scored within our desired threshold. In the process, the AI designed not only VX, but many other known chemical warfare agents that we identified through visual confirmation with structures in public chemistry databases. Many new molecules were also designed that looked equally plausible. These new molecules were predicted to be more toxic based on the predicted LD50 in comparison to publicly known chemical warfare agents (Figure 1). This was unexpected as the datasets we used for training the AI did not include these nerve agents. The virtual molecules even occupied a region of molecular property space that was entirely separate to the many thousands of molecules in the organism-specific LD50 model, which is mainly made up of pesticides, environmental toxins, and drugs (Figure 1). By inverting the use of our machine learning models, we had transformed our innocuous generative model from a helpful tool of medicine to a generator of likely deadly molecules.""",Dual Use of Artificial Intelligence-powered Drug Discovery,ChEMBL,"https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The initial model is trained on ChEMBL 28’s ~2 million compounds""",,,Unknown,,,148.00,,"model details here: https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The variational autoencoder utilizes an encoder-decoder architecture to map chemical space into a latent vector 34. The encoder is composed of 3 LSTM layers of 512 units each followed by a linear layer of 64 units (the latent space).
Our decoder is comprised of 3 LSTM layers of 512 units each with dropout of 0.2 between
all layers""",,,,,,,,,,,An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.,2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Consistency Decoder,Image generation,Image generation,2023-11-06,MIT,Open source,Unreleased,Unreleased,https://github.com/openai/consistencydecoder,OpenAI,,,United States of America,,,,,,,,,Unknown,,,,,"The architecture is described in the DALL-E 3 paper, which is linked to from the GitHub page.

""For DALL-E 3, we trained our own diffusion decoder on top of the latent space learned by the VAE trained
by Rombach et al. (2022). We found that using a diffusion decoder here provided marked improvements to
fine image details, for example text or human faces.
This diffusion decoder is a convolutional U-Net identical to the one described in Ho et al. (2020). Once
trained, we used the consistency distillation process described in Song et al. (2023) to bring it down to two
denoising steps.""

The models in Ho et al are 35.7 million to 256 million. Not sure what the parameter count for this one is.  ",,,,,,,,,,,"We are also open sourcing the Consistency Decoder, a drop in replacement for the Stable Diffusion VAE decoder. This decoder improves all images compatible with the by Stable Diffusion 1.0+ VAE, with significant improvements in text, faces and straight lines.

https://openai.com/blog/new-models-and-developer-products-announced-at-devday",2024-05-21 13:05,Anonymous,,0,Denoising Diffusion Probabilistic Models (LSUN Bedroom),,"This model is derived from the denoising model as described below:

""For DALL-E 3, we trained our own diffusion decoder on top of the latent space learned by the VAE trained
by Rombach et al. (2022). We found that using a diffusion decoder here provided marked improvements to
fine image details, for example text or human faces.

This diffusion decoder is a convolutional U-Net identical to the one described in Ho et al. (2020). Once
trained, we used the consistency distillation process described in Song et al. (2023) to bring it down to two
denoising steps.""

Song et al is here: https://arxiv.org/pdf/2303.01469.pdf That paper just describes the general process, not specifically for this model I believe.",,,,,,,Industry,,,,,Industry,,,
Adversarial Joint Adaptation Network (ResNet),Vision,Image classification,2017-08-17,,,,,https://arxiv.org/abs/1605.06636,"Tsinghua University,UC Berkeley",,,"China,United States of America","Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan",Highly cited,,Deep Transfer Learning with Joint Adaptation Networks,Office-31,"""Office-31 (Saenko et al., 2010) is a standard benchmark for
domain adaptation in computer vision, comprising 4,652
images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded
from amazon.com, Webcam (W) and DSLR (D)... We evaluate all
methods across three transfer tasks A → W, D → W and W
→ D, which are widely adopted by previous deep transfer
learning methods""",4652,,Speculative,,,2482.00,60000000.00,"Model is based on ResNet (60m params), might have more parameters though",,,,,,,,,,,"Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Mamba-24M (SC09),Speech,Audio generation,2023-12-01,,,,,https://arxiv.org/abs/2312.00752,"Carnegie Mellon University (CMU),Princeton University",,,"United States of America,United States of America","Albert Gu, Tri Dao",SOTA improvement,"""SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting
of 1-second clips sampled at 16000 Hz of the digits “zero” through “nine” with highly variable characteristics. We
largely follow the autoregressive training setup and generation protocol of Goel et al. (2022).
Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al.
(2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette
2019), DiffWave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art
(and much larger) GAN- and diffusion- based models. A larger model parameter-matched to the baselines further
improves on fidelity metrics dramatically.""",Mamba: Linear-Time Sequence Modeling with Selective State Spaces,SC09,"""SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting
of 1-second clips sampled at 16000 Hz of the digits “zero” through “nine” with highly variable characteristics""",,,Likely,,,277.00,23400000.00,Table 4,100.00,,,,,,,,,,"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",2024-05-01 09:22,Anonymous,,0,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Japanese-LM-3.6B,Language,Language modelling,2023-08-14,"Apache 2.0 for weights
https://huggingface.co/line-corporation/japanese-large-lm-3.6b",Open source,Unreleased,Unreleased,"https://engineering.linecorp.com/ja/blog/3.6-billion-parameter-japanese-language-model, https://huggingface.co/line-corporation/japanese-large-lm-3.6b",LINE Corporation,,"1.7B model was trained on 4000 A100-hours:

""Regarding the time required to build this model, for example, the 1.7B model is converted to A100 80GB and takes about 4000 GPU hours""

4000 * 3600 * 312 teraFLOPS * 0.3 =1.35e21

speculative, the 3.6B model may be about 2x that, but not clear it was trained on the same number of epochs

Using C=6ND, C = 6 * 3.6e9 * 650GB * 111e6 word/GB / 1 word/token. 1.6e21 FLOP.

If we assume 2 epochs, 3.2e21 FLOP. Certainly <1e23 FLOP.
",Japan,"Shun Kiyono, Sho Takase, Toshinori Sato",,,japanese-large-lm-3.6b,,"""Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar. We also incorporated the Web texts crawled by in-house system. The total size of our training corpus is about 650 GB.""

https://huggingface.co/line-corporation/japanese-large-lm-3.6b",72000000000,"650GB per huggingface

our guide says 111M Japanese words per GB, which would be ~72B words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",Likely,,,,3600000000.00,3.6B,,,,,,NVIDIA A100,,,,,"(google translated)

In this article, we will introduce the 3.6 Billion (3.6 Billion) and 1.7 Billion (1.7 Billion) parameters that we trained and published in Japanese. While introducing the language models (hereinafter referred to as the 3.6B model and 1.7B model, respectively), we will share the know-how on language model construction that we have gained along the way.",2024-04-09 11:37,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
RWKV-4 World (7B),Language,,2023-06-26,Apache 2.0: https://huggingface.co/BlinkDL/rwkv-4-world,Open source,Open source,Open source,https://huggingface.co/BlinkDL/rwkv-4-world,RWKV Foundation,,,,,,,RWKV-4 World,,"""RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).

World = Some_Pile + Some_RedPajama + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find""",,,Likely,,,,7000000000.00,7B,,,,,,,,,,,"RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).",2024-04-09 12:05,Anonymous,,0,,,,,,,,,,Research collective,,,,,Research collective,checked,,
YuLan-Chat-2 (13B),Language,Chat,2023-08-02,,,,,https://github.com/RUC-GSAI/YuLan-Chat,Renmin University of China,,,China,,,"""[Aug. 18, 2023] Our YuLan-Chat-2-13B achieves the 5th position of OpenCompass benchmark!""",YuLan-Chat: An Open-Source Bilingual Chatbot,,,,,Confident,,,,13000000000.00,,,,,,,,,,,,"YuLan-Chat models are chat-based large language models, which are developed by the researchers in GSAI, Renmin University of China (YuLan, which represents Yulan Magnolia, is the campus flower of Renmin University of China). The newest version is developed by continually-pretraining and instruction-tuning LLaMA-2 with high-quality English and Chinese data. The model has the following technical characteristics:
Due to continued pre-training on high-quality Chinese-English bilingual data, the language ability of the model has been improved.
To well support Chinese and longer inputs and outputs, we expand the original vocabulary with Chinese words and extend the maximum length of LLaMA-2. It can support 8k context now.
To well activate the bilingual instruction following capacity, we construct high-quality bilingual instructions, and perform multi-stage instruction-tuning.",2024-03-07 14:22,Anonymous,,0,LLaMA-13B,,"""Due to continued pre-training on high-quality Chinese-English bilingual data, the language ability of the model has been improved.""

Token count not stated",,,,,,,Academia,,,,,Academia,,,
Retrieval-Augmented Generator,Language,Question answering,2020-05-22,"It's in HF transformers library:
https://huggingface.co/docs/transformers/en/model_doc/rag

this library has an apache license: https://github.com/huggingface/transformers/blob/main/LICENSE",Open source,,Unreleased,https://arxiv.org/abs/2005.11401v4,"Facebook,New York University (NYU),University College London (UCL)",,"not enough info, e.g. no training time reported:

""We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though
training and inference can be run on one GPU""","United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland","Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela","Highly cited,SOTA improvement","""Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] """,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,Natural Questions,,,,Likely,,,1646.00,626000000.00,"""Our RAG models contain the trainable parameters for the BERT-base query and document encoder of
DPR, with 110M parameters each (although we do not train the document encoder ourselves) and
406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable parameters""",,,,,,NVIDIA Tesla V100 PCIe 32 GB,,,,,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",2024-04-24 14:49,Anonymous,,0,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
STT Conformer-Transducer XL,Speech,Speech recognition,2022-04-12,"CC-BY-4.0:

https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge",Open source,,Unreleased,https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_conformer_transducer_xlarge,NVIDIA,,,United States of America,,,"SOTA for open-source speech recognition, per OpenAI's Whisper paper: https://cdn.openai.com/papers/whisper.pdf",,NeMo ASRSET,"""All the models in this collection are trained on a composite dataset (NeMo ASRSET) comprising of several thousand hours of English speech:

Librispeech 960 hours of English speech
Fisher Corpus
...""",,"Tens of millions of words, at ~12k words/hour (~200 words per minute)",Likely,,,,600000000.00,600M,,,,,,,,,,,,2024-04-15 11:51,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
S-Norm,Language,Question answering,2017-10-29,,,,,https://arxiv.org/abs/1710.10723v2,"University of Washington,Allen Institute for AI",,,"United States of America,United States of America","Christopher Clark, Matt Gardner",SOTA improvement,"""Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.""",Simple and Effective Multi-Paragraph Reading Comprehension,TriviaQA,,2000000000,"""530k question-document training pairs""

average question length of 14 words and document length of 2895 words, per
 https://www.cs.utexas.edu/~eunsol/files/papers/acl17jcwz.pdf

530,000 * 2895 words on average * 1.33 tokens/word = ~2,000,000,000",Confident,,,453.00,,Not stated. Probably obtainable from github: https://github.com/allenai/document-qa/tree/master,1.00,,,,,,,,,,"We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.",2024-05-15 19:35,Anonymous,,0,,,,,,,,,,"Academia,Research collective",,,,,"Academia,Research collective",,,
Llama-2-Chinese 13B,Language,,2023-06-25,"license isn't clear, it's not on github (where the code is) but HF says apache:
https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat",Open source,,Open access (non-commercial),https://github.com/FlagAlpha/Llama2-Chinese,FlagAlpha,,,China,,,,,,"Several Chinese datasets:

https://github.com/FlagAlpha/Llama2-Chinese#-%E4%B8%AD%E6%96%87%E6%95%B0%E6%8D%AE",,,Likely,,,,13000000000.00,,,,,,,,,,,,"Welcome to the Llama Chinese community! We are an advanced technical community focusing on the optimization and upper-level construction of the Llama model in Chinese. *Based on large-scale Chinese data, the Chinese capabilities of the Llama2 model are continuously iteratively upgraded starting from pre-training* . We warmly welcome developers and researchers who are passionate about large model LLM to join us.",2024-04-02 10:50,Anonymous,,0,Llama 2-13B,,They did both additional pre-training in Chinese (token count not stated) and LoRA fine-tuning.,,,,,,,Research collective,,,,,Research collective,,,
Emu (Meta),Image generation,Text-to-image,2023-09-27,,,,,https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/,Meta AI,,,United States of America,"Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, Devi Parikh",,,Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack,,,1100000000,"""We curate a large internal pre-training dataset consisting of 1.1 billion images to train our model. The model is trained with progressively increasing resolutions""",Confident,,,,2800000000.00,"""We use a large U-Net with 2.8B trainable parameters.""",,,,,,,,,,,"Training text-to-image models with web scale image-text
pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often
face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment
post pre-training. In this paper, we propose quality-tuning
to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining
generality across visual concepts. Our key insight is that
supervised fine-tuning with a set of surprisingly small but
extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion
model on 1.1 billion image-text pairs and fine-tune it with
only a few thousand carefully selected high-quality images.
The resulting model, Emu, achieves a win rate of 82.9%
compared with its pre-trained only counterpart. Compared
to the state-of-the-art SDXLv1.0, Emu is preferred 68.4%
and 71.3% of the time on visual appeal on the standard
PartiPrompts and our Open User Input benchmark based
on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that
is also effective for other architectures, including pixel diffusion and masked generative transformer models.",2024-03-07 14:22,Robi Rahman,,,,,,,,,,,,Industry,checked,,,,Industry,,,
MemoReader,Language,Question answering,2018-10-31,,Unreleased,,,https://aclanthology.org/D18-1237/,"Samsung,Korea University",,"""Our model does require more memory than existing methods, but a single GPU (e.g., M40 with 12GB memory) was enough to train model within a reasonable amount of time""

""Reasonable"" could mean anything, maybe hours to a few days.","Korea (Republic of),Korea (Republic of)","Seohyun Back, Seunghak Yu, Sathish Indurthi, Jihie Kim, Jaegul Choo",SOTA improvement,"""TriviaQA. As shown in Table 2, our model,
even without DEBS, outperforms the existing
state-of-the-art method such as ‘BiDAF + SA +
SN’ by a large margin in all the cases""","MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller
",TriviaQA,,,,Unknown,,,17.00,,,,,,,"""reasonable amount of time"" with a single GPU",NVIDIA M40,,,,,"Machine reading comprehension helps machines learn to utilize most of the human
knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they
are still limited in understanding, up to a few paragraphs, failing to properly comprehend
lengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In
detail, our method has two novel aspects: (1) an advanced memory-augmented architecture
and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory.
Our proposed architecture is widely applicable
to other models. We have performed extensive experiments with well-known benchmark
datasets such as TriviaQA, QUASAR-T, and
SQuAD. The experimental results demonstrate
that the proposed method outperforms existing
methods, especially for lengthy documents.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
UnifiedQA,Language,Question answering,2020-05-02,,Unreleased,,,https://arxiv.org/abs/2005.00700v3,"Allen Institute for AI,University of Washington",,,"United States of America,United States of America","Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",SOTA improvement,"""We then introduce UNIFIEDQA (§3.2) that is a QA system
trained on datasets in multiple formats, indicating
new state-of-the-art results on 10 datasets and generalization to unseen datasets.""",UnifiedQA: Crossing Format Boundaries With a Single QA System,,"""We empirically chose the following 8 seed
datasets for training UNIFIEDQA,
3 based on their
effectiveness in our pilot study (details deferred
to Section 5) assessing which datasets are most
valuable for out-of-format training:
• EX: SQuAD 1.1, SQuAD 2.0
• AB: NarrativeQA
• MC: RACE, ARC, OBQA, MCTest
• YN: BoolQ""",,,Likely,,,599.00,11000000000.00,11B (based on T5-11B),,,,36.0,"""pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""",Google TPU v3,,,,,"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.",2024-05-01 09:06,Anonymous,,0,T5-11B,38000000000000000000,"""• Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for
BART models.
• Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55
hours, on T5(11B) and BART models, respectively.""

8 * 123 TFLOPS * 36 * 3600 * 0.3 (utilization assumption) = 3.8e19",,,,,,,"Research collective,Academia",,,,,"Research collective,Academia",,,
GNoME for crystal discovery,Materials science,,2023-11-29,,,,,https://www.nature.com/articles/s41586-023-06735-9,Google DeepMind,,"Pretraining involved at least 6 * 16.24M * 25.43M = 2.5e15 FLOP.
""The learning rate was decreased to a new value of 2 × 10−4 after approximately 23 million steps, to 5 × 10−5 after a further approximately 11 million steps and then trained for a final 2.43 million steps. Training was performed on four TPU v3 chips.""",Multinational,"Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, Ekin Dogus Cubuk",,,Scaling deep learning for materials discovery,,,,,Likely,,,,16240000.00,"""The pretrained potential has 16.24 million parameters.""
This refers to the GNoME network, which is a ""Gaussian Network Model of Energy"" for predicting crystal potential energy of new crystals.",,,,,,Google TPU v4,,,,,"Novel functional materials enable fundamental breakthroughs across technological applications from clean energy to information processing1,2,3,4,5,6,7,8,9,10,11. From microchips to batteries and photovoltaics, discovery of inorganic crystals has been bottlenecked by expensive trial-and-error approaches. Concurrently, deep-learning models for language, vision and biology have showcased emergent predictive capabilities with increasing data and computation12,13,14. Here we show that graph networks trained at scale can reach unprecedented levels of generalization, improving the efficiency of materials discovery by an order of magnitude. Building on 48,000 stable crystals identified in continuing studies15,16,17, improved efficiency enables the discovery of 2.2 million structures below the current convex hull, many of which escaped previous human chemical intuition. Our work represents an order-of-magnitude expansion in stable materials known to humanity. Stable discoveries that are on the final convex hull will be made available to screen for technological applications, as we demonstrate for layered materials and solid-electrolyte candidates. Of the stable structures, 736 have already been independently experimentally realized. The scale and diversity of hundreds of millions of first-principles calculations also unlock modelling capabilities for downstream applications, leading in particular to highly accurate and robust learned interatomic potentials that can be used in condensed-phase molecular-dynamics simulations and high-fidelity zero-shot prediction of ionic conductivity.",2024-03-07 14:22,Robi Rahman,,,,,,4,,,,,,Industry,,,,,Industry,,,
LDM-1.45B,Image generation,Image generation,2021-12-20,MIT: https://github.com/CompVis/latent-diffusion/blob/main/LICENSE,Open source,Open source,Open source,https://arxiv.org/abs/2112.10752,"Heidelberg University,Runway",,,"Germany,United States of America","Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer",Highly cited,,High-Resolution Image Synthesis with Latent Diffusion Models,LAION-400M,,400000000,400M image-text pairs,Likely,,,6121.00,1450000000.00,1.45B,0.66,,,,,NVIDIA A100,,,,,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL.",2024-04-04 11:20,Anonymous,,0,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
DiT-XL/2 + CADS,Image generation,Image generation,2023-10-26,,,,,https://arxiv.org/abs/2310.17347v2,ETH Zurich,,,Switzerland,"Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber",SOTA improvement,"""Further, using an existing pretrained
diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for
class-conditional ImageNet generation at 256×256 and 512×512 respectively""",CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling,ImageNet,,,,Likely,,,,675000000.00,original parameter count for DiT-XL/2,,,,,,,,,,,"While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256×256 and 512×512 respectively.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,,,
OmniVec,"Multimodal,Vision,Speech,Language","Image classification,Audio speech recognition",2023-11-07,,,,,https://arxiv.org/abs/2311.05709v1,TensorTour,,,United States of America,"Siddharth Srivastava, Gaurav Sharma",SOTA improvement,"Table 13.

E.g. SOTA on ImageNet at 92.4 top-1 accuracy",OmniVec: Learning robust representations with cross modal sharing,,Many datasets across several modalities,,,Unknown,,,12.00,,,,,,,,,,,,,"Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.\ visual, audio, text and 3D, and report results on 22 diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.",2024-05-21 13:05,Anonymous,,0,BERT-Large,,"Appears to build on several models, like BERT and ViT (Table 1)",,,,,,,Industry,,,,,Industry,,,
Contriever,Language,,2021-12-16,non-commercial for weights/code: https://github.com/facebookresearch/contriever/blob/main/LICENSE,Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2112.09118,"Meta AI,University College London (UCL),PSL University,Université Grenoble Alpes",,"""We optimize the model with the AdamW (Loshchilov & Hutter, 2019) optimizer, with learning rate of 5 · 10−5, batch size of 2,048 and 500,000 steps""","United States of America,United Kingdom of Great Britain and Northern Ireland,France,France","Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave",SOTA improvement,"""We observe that when
used as pre-training, contrastive learning leads to strong performance: contriever obtains the best results
among dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the
average recall@100 from 65.0 to 67.1). This strong recall@100 performance can be further exploited by using
a cross-encoder2
to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the
BEIR benchmark for the nDCG@10, as well as on average""",Unsupervised Dense Information Retrieval with Contrastive Learning,"Wikipedia,CCNet","""Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet
data """,,,Likely,,,336.00,110000000.00,"Based on BERT base, which had 110m params.

""We initialize the network with the publicly available BERT base uncased model.""",,,,,,,,,,,"Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",2024-05-01 09:13,Anonymous,,0,BERT-Large,,actually BERT base,,,,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
ImageBind,"Multimodal,Vision,Audio,Language,Image generation,Speech","Image classification,Speech recognition,Image generation,Language modelling/generation",2023-05-09,Creative commons non-commercial,Open access (non-commercial),,Open access (non-commercial),"https://arxiv.org/abs/2305.05665, https://github.com/facebookresearch/ImageBind",Meta AI,,,United States of America,"Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",SOTA improvement,"""we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models""",IMAGEBIND: One Embedding Space To Bind Them All,"SUN RGB-D,LLVIP,Ego4D,AudioSet",""" For the naturally available paired data, we use
the (video, audio) pairs from the Audioset dataset [19], (image, depth) pairs from the SUN RGB-D dataset [69], (image, thermal) pairs from the LLVIP dataset [32] and (video,
IMU) pairs from the Ego4D dataset [23].""",,,Likely,,,330.00,932000000.00,used ViT-Huge 630M as an image/video encoder and OpenCLIP-302m as text encoder,64.00,,,,,"NVIDIA V100,NVIDIA A100",,,,,"We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",2024-05-13 08:59,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Goat-7B,Language,Quantitative reasoning,2023-05-23,"no license noted. perhaps LLaMA 1 license by default (non-comm)

https://github.com/liutiedong/goat",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),https://arxiv.org/abs/2305.14201,National University of Singapore,,2.78e+22 for base LLaMA-7B,Singapore,"Tiedong Liu, Bryan Kian Hsiang Low",SOTA improvement,"""We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task.""",Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks,,"Model was fine-tuned from LLaMA-7B.

Fine-tuning dataset is a synthetic math dataset:

""We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains
the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence
ensuring a very low probability of instances being
duplicated, although small numbers may be sampled multiple times. We sample from log space to
ensure the numbers are equally likely to be sampled
from different orders of magnitude, which is similar to the sampling method used by Lee and Kim
(2023). The details of the dataset are presented in
Appendix F.""",,Fine-tune dataset had 1 million question-answer pairs. likely ~10 tokens per pair?,Speculative,,,40.00,7000000000.00,7B,1.00,,,,,NVIDIA A10 PCIe,,,,,"We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.",2024-05-01 09:24,Anonymous,,0,LLaMA-7B,2020000000000000000,"""Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU... In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy""

Info isn't very complete - no timeframe specified for the VRAM GPU, I'm not sure how many tokens are in the fine-tune dataset and they use LoRA. Maybe it's 15 A10-hours total (1M total instances)? But safe to assume it's a small fraction of Llama's pretrain compute.

125 trillion (A10 FLOPs) * 15 * 3600 * 0.3 = 2.02e18",,,,,,,Academia,,,,,Academia,,,
ABAB,Language,Language generation,2023-08-30,,,,,"https://kr-asia.com/baidus-ernie-bot-among-eight-chinese-llm-products-approved-for-public-launch, https://api.minimax.chat/",MiniMax,,,China,,,,,,,,,Unknown,,,,,"""hundreds of billions"" per https://api.minimax.chat/",,,,,,,,,,,"MiniMax has released three foundational model architectures: text-to-visual, text-to-audio, and text-to-text. The startup has also introduced a self-developed general LLM “ABAB”, named after the sound of baby babble.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
KwaiYii,Language,Chat,2023-08-16,,Unreleased,,,https://github.com/kwai/KwaiYii,Kuaishou Technology,,,China,,,"""The KwaiYii-13B-Base pre-training model has excellent general technical base capabilities and has achieved State-Of-The-Art effect under the same model size on most authoritative Chinese/English Benchmarks. For example, the KwaiYii-13B-Base pre-trained model is currently at the leading level of the same model size on MMLU, CMMLU, C-Eval, HumanEval and other benchmarks.""","""KwaiYii"" large-scale language model (KwaiYii)",,,,,Likely,,,,13000000000.00,13B,,,,,,,,,,,"""KwaiYii"" is a series of large-scale language models (LLM) independently developed by the Kuaishou AI team from scratch. It currently includes models with multiple parameter sizes and covers pre-training models. (KwaiYii-Base), dialogue model (KwaiYii-Chat). Here we introduce the 13B scale series model KwaiYii-13B.""",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Cohere Embed,Language,Semantic embedding,2023-11-02,,API access,,,https://txt.cohere.com/introducing-embed-v3/,Cohere,,"https://docs.cohere.com/docs/environmental-impact

Embed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the ""us-west1"" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",Canada,"Nils Reimers, Elliott Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, Abdullah Elkady",SOTA improvement,"""We are releasing new English and multilingual Embed versions with either 1024 or 384 dimensions. All models can be accessed via our APIs. As of October 2023, these models achieve state-of-the-art performance among 90+ models on the Massive Text Embedding Benchmark (MTEB) and state-of-the-art performance for zero-shot dense retrieval on BEIR.""",Cohere Command & Embed on Amazon Bedrock,,,,,Unknown,,,,,,,,,,,,,,,,"We're excited to introduce Embed v3, our latest and most advanced embeddings model. Embed v3 offers state-of-the-art performance per trusted MTEB and BEIR benchmarks.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
AbLang,Biology,Proteins,2022-01-22,,,,,https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac046/6609807,University of Oxford,,,United Kingdom of Great Britain and Northern Ireland,"Tobias H. Olsen, Iain H. Moal and Charlotte M. Deane",SOTA improvement,"""AbLang restores residues more accurately and faster than a current state-of-the-art protein language model ESM-1b, emphasizing the benefits and potential of an antibody specific language model"" - SOTA improvement for a very specific task","AbLang: an antibody language model for completing
antibody sequences",Observed antibody space (OAS) database,"""Here, we present AbLang, an antibody specific language model trained on either the heavy or light chain antibody sequences from OAS""",,,Confident,,,58.00,355000000.00,"""The hyperparameters were selected to be similar to those used
in the RoBERTa paper (Liu et al., 2019).""

Liu et al., 2019 link: https://arxiv.org/pdf/1907.11692.pdf
""We begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)""",,,,,,,,,,,"Motivation
General protein language models have been shown to summarize the semantics of protein sequences into representations that are useful for state-of-the-art predictive methods. However, for antibody specific problems, such as restoring residues lost due to sequencing errors, a model trained solely on antibodies may be more powerful. Antibodies are one of the few protein types where the volume of sequence data needed for such language models is available, e.g. in the Observed Antibody Space (OAS) database.

Results
Here, we introduce AbLang, a language model trained on the antibody sequences in the OAS database. We demonstrate the power of AbLang by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, e.g. over 40% of OAS sequences are missing the first 15 amino acids. AbLang restores the missing residues of antibody sequences better than using IMGT germlines or the general protein language model ESM-1b. Further, AbLang does not require knowledge of the germline of the antibody and is seven times faster than ESM-1b.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
AlphaMissense,Biology,"Protein variant pathogenicity prediction,Protein folding prediction,Proteins",2023-09-22,"Apache for code. weights not released

https://github.com/google-deepmind/alphamissense",Unreleased,,Open source,https://www.science.org/doi/10.1126/science.adg7492,Google DeepMind,,,Multinational,"Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvile ̇Žemgulyte ̇, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schneider,Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli,Žiga Avsec",SOTA improvement,"""By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data."" [Abstract]",Accurate proteome-wide missense variant effect prediction with AlphaMissense,,Multiple training datasets,,,Unverified,,,167.00,,,,,,,,,,,,,"The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic.",2024-05-01 09:22,Anonymous,,,AlphaFold 2,,,,,,,,,Industry,,,,,Industry,,,
DeepConPred2,Biology,"Proteins,Protein folding prediction",2018-10-11,,,,,https://www.sciencedirect.com/science/article/pii/S2001037018301466,Tsinghua University,,,China,"Wenze Ding, Wenzhi Mao, Di Shao, Wenxuan Zhang, Haipeng Gong",,,DeepConPred2: An Improved Method for the Prediction of Protein Residue Contacts,SCOPe 2.05,"""Specifically, all proteins in the training set of this work came from the SCOPe 2.05 version""",3443,"""Finally, our training set contained 3443 protein domains""",Unverified,,,31.00,,,,,,,,,,,,,"Information of residue-residue contacts is essential for understanding the mechanism of protein folding, and has been successfully applied as special topological restraints to simplify the conformational sampling in de novo protein structure prediction. Prediction of protein residue contacts has experienced amazingly rapid progresses recently, with prediction accuracy approaching impressively high levels in the past two years. In this work, we introduce a second version of our residue contact predictor, DeepConPred2, which exhibits substantially improved performance and sufficiently reduced running time after model re-optimization and feature updates. When testing on the CASP12 free modeling targets, our program reaches at least the same level of prediction accuracy as the best contact predictors so far and provides information complementary to other state-of-the-art methods in contact-assisted folding",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Eve,Biology,"Protein variant pathogenicity prediction,Proteins",2021-10-27,"The model code is available at https://github.com/OATML-Markslab/EVE
MIT license",,,Open source,https://www.nature.com/articles/s41586-021-04043-8#change-history,"Harvard Medical School,University of Oxford",,,"United States of America,United Kingdom of Great Britain and Northern Ireland","Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal and Debora S. Marks",SOTA improvement,"""Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification"" [Abstract] - SOTA improvement for very specific task",Disease variant prediction with deep generative models of evolutionary data,UniRef,"""a Bayesian VAE was trained on a multiple sequence alignment retrieved by searching approximately 250 million protein sequences in UniRef""",,,Unverified,,,312.00,,,,,,,,,,,,,"Quantifying the pathogenicity of protein variants in human disease-related genes
would have a marked effect on clinical decisions, yet the overwhelming majority
(over 98%) of these variants still have unknown consequences1–3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4–10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classifcation12–16. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.",2024-05-21 09:24,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Vespa,Biology,"Proteins,general-purpose protein or nucleotide language model (pLM/nLM)",2021-12-30,,,,,https://link.springer.com/article/10.1007/s00439-021-02411-y,Technical University of Munich,,,Germany,"Céline Marquet, Michael Heinzinger, Tobias Olenyi, Christian Dallago, Kyra Erckert, Michael Bernhofer, Dmitrii Nechaev & Burkhard Rost ",,,Embeddings from protein language models predict conservation and variant effects,"ConSurf10k,Eff10k,PMD4k,DMS4,DMS39",,,,Likely,,,,231000.00,"""(3) standard convolutional neural network (CNN; with two convolutional layers with a window size of 7, connected through ReLU activations; dropout rate of 0.25; 231 k free parameters)""",,,,,,,Supervised,,,,"The emergence of SARS-CoV-2 variants stressed the demand for tools allowing to interpret the effect of single amino acid variants (SAVs) on protein function. While Deep Mutational Scanning (DMS) sets continue to expand our understanding of the mutational landscape of single proteins, the results continue to challenge analyses. Protein Language Models (pLMs) use the latest deep learning (DL) algorithms to leverage growing databases of protein sequences. These methods learn to predict missing or masked amino acids from the context of entire sequence regions. Here, we used pLM representations (embeddings) to predict sequence conservation and SAV effects without multiple sequence alignments (MSAs). Embeddings alone predicted residue conservation almost as accurately from single sequences as ConSeq using MSAs (two-state Mat- thews Correlation Coefficient—MCC—for ProtT5 embeddings of 0.596 ± 0.006 vs. 0.608 ± 0.006 for ConSeq). Inputting the conservation prediction along with BLOSUM62 substitution scores and pLM mask reconstruction probabilities into a simplistic logistic regression (LR) ensemble for Variant Effect Score Prediction without Alignments (VESPA) predicted SAV effect magnitude without any optimization on DMS data. Comparing predictions for a standard set of 39 DMS experiments to other methods (incl. ESM-1v, DeepSequence, and GEMME) revealed our approach as competitive with the state-of-the-art (SOTA) methods using MSA input. No method outperformed all others, neither consistently nor statistically significantly, independently of the performance measure applied (Spearman and Pearson correlation). Finally, we investigated binary effect predictions on DMS experiments for four human proteins. Overall, embedding-based methods have become competi- tive with methods relying on MSAs for SAV effect prediction at a fraction of the costs in computing/energy. Our method predicted SAV effects for the entire human proteome (~ 20 k proteins) within 40 min on one Nvidia Quadro RTX 8000. All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/ Rostlab/VESPA, and PredictProtein.",2024-03-07 14:22,Anonymous,,,ProteinBERT,,,,,,,,,Academia,,,,,Academia,,,
TripletRes,Biology,"Proteins,Protein folding prediction",2019-08-13,,Unreleased,,,https://onlinelibrary.wiley.com/doi/10.1002/prot.25798,University of Michigan,,"4 GPUs, 50 epochs, batch size between 1-4 depending on sequence length",United States of America,"Yang Li, Chengxin Zhang, Eric W. Bell, Dong-Jun Yu, Yang Zhang",,,Ensembling multiple raw coevolutionary features with deep residual neural networks for contact-map prediction in CASP13,SCOPe 2.07,"""7,671 non-redundant domains, sequence length 30-400, with sequences for which PDB structure resolution is good""",,,Unknown,,,,,,50.00,,,,,,,,,,"We report the results of residue-residue contact prediction of a new pipeline built purely on the learning of coevolutionary features in the CASP13 experiment. For a query sequence, the pipeline starts with the collection of multiple sequence alignments (MSAs) from multiple genome and metagenome sequence databases using two complementary Hidden Markov Model (HMM)-based searching tools. Three profile matrices, built on covariance, precision, and pseudolikelihood maximization respectively, are then created from the MSAs, which are used as the input features of a deep residual convolutional neural network architecture for contact-map training and prediction. Two ensembling strategies have been proposed to integrate the matrix features through end-to-end training and stacking, resulting in two complementary programs called TripletRes and ResTriplet, respectively. For the 31 free-modeling domains that do not have homologous templates in the PDB, TripletRes and ResTriplet generated comparable results with an average accuracy of 0.640 and 0.646, respectively, for the top L/5 long-range predictions, where 71% and 74% of the cases have an accuracy above 0.5. Detailed data analyses showed that the strength of the pipeline is due to the sensitive MSA construction and the advanced strategies for coevolutionary feature ensembling. Domain splitting was also found to help enhance the contact prediction performance. Nevertheless, contact models for tail regions, which often involve a high number of alignment gaps, and for targets with few homologous sequences are still suboptimal. Development of new approaches where the model is specifically trained on these regions and targets might help address these problems.",2024-05-21 13:03,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
TranceptEve,Biology,"Proteins,Protein variant pathogenicity prediction",2022-12-10,,,,,https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1,"University of Oxford,Harvard University",,,"United Kingdom of Great Britain and Northern Ireland,United States of America","Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, Debora S. Marks",SOTA improvement,"""Besides its broader application scope, it achieves state-of- the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.""",TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction,,,,,Unknown,,,,,,,,,,,,,,,,"Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance – but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE – a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specifc models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.",2024-05-21 13:05,Anonymous,,,Tranception,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RaptorX-Contact,Biology,"Protein folding prediction,Proteins",2019-05-02,"this license for code:
https://github.com/j3xugit/RaptorX-Contact?tab=GPL-3.0-1-ov-file",Unreleased,,Open access (restricted use),https://www.biorxiv.org/content/biorxiv/early/2019/05/02/624460.full.pdf,Toyota Technological Institute at Chicago,,,United States of America,"Jinbo Xu, Sheng Wang",,,Analysis of distance-based protein structure prediction by deep learning in CASP13,PDB25 and UniProt,,,PDB25 is 11410 proteins,Unknown,,,,,,,,,,,,,,,,"This paper reports the CASP13 results of distance-based contact prediction, threading and folding methods implemented in three RaptorX servers, which are built upon the powerful deep convolutional residual neural network (ResNet) method initiated by us for contact prediction in CASP12. On the 32 CASP13 FM (free-modeling) targets with a median MSA (multiple sequence alignment) depth of 36, RaptorX yielded the best contact prediction among 46 groups and almost the best 3D structure modeling among all server groups without time-consuming conformation sampling. In particular, RaptorX achieved top L/5, L/2 and L long-range contact precision of 70%, 58% and 45%, respectively, and predicted correct folds (TMscore>0.5) for 18 of 32 targets. Although on average underperforming AlphaFold in 3D modeling, RaptorX predicted correct folds for all FM targets with >300 residues (T0950-D1, T0969-D1 and T1000-D2) and generated the best 3D models for T0950-D1 and T0969-D1 among all groups. This CASP13 test confirms our previous findings: (1) predicted distance is more useful than contacts for both template-based and free modeling; and (2) structure modeling may be improved by integrating alignment and co- evolutionary information via deep learning. This paper will discuss progress we have made since CASP12, the strength and weakness of our methods, and why deep learning performed much better in CASP13.",2024-05-21 13:03,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
ProteinDT,"Biology,Language",Proteins,2023-02-09,,Unreleased,,,https://arxiv.org/abs/2302.04611,"UC Berkeley,California Institute of Technology,University of Toronto,University of Wisconsin Madison,Texas A&M,NVIDIA,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",,,"United States of America,United States of America,Canada,United States of America,United States of America,United States of America,Canada","Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar",SOTA improvement,"""Compared to six state-of-the-art protein sequence representation methods, ProteinDT can obtain consistently superior performance on four of six benchmark tasks.""",A Text-guided Protein Design Framework,UniProtKB,They extract a subset of 441K protein-text pairs,,,Unknown,,,,,,,,,,,,,,,,"Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins’ high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.",2024-05-22 13:20,Anonymous,,,SciBERT,,,,,,,,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,
OntoProtein,"Biology,Language",Proteins,2022-01-23,,,,,https://openreview.net/pdf?id=yfe1VMYAXa4,Zhejiang University,,,China,"Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Shumin Deng, Qiang Zhang, Jiazhang Lian, Huajun Chen, Haosen Hong",SOTA improvement,Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.,ONTOPROTEIN: PROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING,ProteinKG25,"""the ProteinKG25 dataset used for pre-training contains about 612,483 entities and 4,990,097 triples, aligned with GO annotations and including protein sequences.""",,,,,,,420000000.00,"""For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020).""",,,,,,,,,,,"Self-supervised protein language models have proved their effectiveness in learn- ing the proteins representations. With the increasing computational power, cur- rent protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve re- markable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowl- edge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.",2024-03-07 14:22,Anonymous,,,ProtBERT-BFD,,,,,,,,,Academia,,,,,Academia,,,
Fold2Seq,Biology,"Proteins,Protein generation,Protein inverse folding",2021-06-24,,,,,https://arxiv.org/abs/2106.13058,"IBM,Texas A&M",,"""We train our model on 2 Tesla K80 GPUs, with batch size 128. In every training stage we train up to 200 epochs with an early stopping strategy based on the validation loss""","United States of America,United States of America","Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, Yang Shen",SOTA improvement,"""On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include data-driven deep generative models and physics-based RosettaDesign."" [Abstract]","Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model
for Protein Design",CATH,"""We used protein structure data from CATH 4.2""",45995,"""Training set includes 45995 proteins belonging to a total of 971 folds""",Unverified,,,44.00,,,200.00,,,,,NVIDIA Tesla K80,,,,,"Designing novel protein sequences for a desired 3D topological fold is a fundamental yet nontrivial task in protein engineering. Challenges exist due to the complex sequence–fold relationship, as well as the difficulties to capture the diversity of the sequences (therefore structures and functions) within a fold. To overcome these challenges, we propose Fold2Seq, a novel transformer-based
generative framework for designing protein sequences conditioned on a specific target fold. To model the complex sequence–structure relationship, Fold2Seq jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include datadriven deep generative models and physics-based RosettaDesign. The unique advantages of foldbased Fold2Seq, in comparison to a structurebased deep model and RosettaDesign, become more evident on three additional real-world challenges originating from low-quality, incomplete, or ambiguous input structures. Source code and data are available at https://github.com/IBM/fold2seq.",2024-05-01 09:13,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
GGNN,Biology,"Proteins,Protein interaction prediction",2023-08-05,,,,,https://www.nature.com/articles/s42003-023-05133-1,"Westlake University,Tsinghua University,Toyota Technological Institute at Chicago",,,"China,China,United States of America","Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu and Stan Z. Li",SOTA improvement,"""In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines.""",Integration of pre-trained protein language models into geometric deep learning networks,,,,,Unverified,,,11.00,,,,,,,,NVIDIA A100 SXM4 80 GB,,,,,,2024-05-01 09:22,Anonymous,,,ESM2-650M,,,,,,,,,"Academia,Academia,Academia",,,,,"Academia,Academia,Academia",,,
LEP-AD,Biology,"Proteins,Protein interaction prediction",2023-03-15,https://github.com/adaga06/LEP-AD,,,Open source,https://www.biorxiv.org/content/10.1101/2023.03.14.532563v1.full.pdf,"King Abdullah University of Science and Technology / KAUST,Karolinska Institute",,,"Saudi Arabia,Sweden","Anuj Daga, Sumeer Ahmad Khan, David Gomez Cabrero, Robert Hoehndorf, Narsis A. Kiani, Jesper Tegner",SOTA improvement,"""We report new best-in-class state-of-the-art results compared
to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA,
and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast,
and STITCH. Finally, we find that a pre-trained model with embedding of proteins
(the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold)""",LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts Drug Target Interactions,,,,,Unverified,,,1.00,,,,,,,,,,,,,"Predicting drug-target interactions is a tremendous challenge for drug development and lead optimization. Recent advances include training algorithms to learn drug-target interactions from data and molecular simulations. Here we utilize Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions. Our architecture, LEPAD, combines pre-trained ESM-2 and Transformer-GCN models predicting binding affinity values. We report new best-in-class state-of-the-art results compared to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold). The LEP-AD model
scales favorably in performance with the size of training data. Code available at https://github.com/adaga06/LEP-AD",2024-05-22 12:21,Anonymous,,,ESM2-3B,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
RT-1,Robotics,,2022-12-13,"weights and code here, apache 2.0:
https://github.com/google-research/robotics_transformer

data here, also apache: 
https://github.com/google-deepmind/open_x_embodiment",Open source,Open source,Open source,https://arxiv.org/abs/2212.06817,Google,,,United States of America,"Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich",SOTA improvement,"""Across each category, we find that RT-1 outperforms the prior
models significantly. On seen tasks, RT-1 is able to perform 97% of the more than 200 instructions successfully, which is 25% more than BC-Z and 32% more than Gato. On unseen tasks, RT-1
shows it is capable of generalizing to novel instructions, performing 76% of the never-before-seen
instructions, 24% more than the next best baseline""",RT-1: Robotics Transformer for Real-World Control at Scale,RT-1,"""We utilize a dataset that we gathered over the course of 17 months with a fleet of 13 robots, containing
∼130k episodes and over 700 tasks""

Episode is an example of robot following instructions",,,Confident,,,409.00,35000000.00,"""we also limit the size of the model compared to
the original publication, which was 1.2B parameters (resulting in on robot inference time of 1.9s),
to be of similar size to RT-1 (37M parameters for Gato vs. 35M for RT-1""

16M params for image tokenizer, 19M for the transformer",,,,,,,Reinforcement learning,,,,"By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at this http URL",2024-05-01 09:22,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
RT-2,Robotics,,2023-07-28,,,,,https://arxiv.org/abs/2307.15818,Google DeepMind,,"""""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""
Sequence length not stated",Multinational,"Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich",SOTA improvement,"""We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data... Here, on average, both instantiations of RT-2 perform similarly, resulting in ∼2x improvement over the next two baselines, RT-1 and MOO, and ∼6x better than the other baselines""",RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,RT-1,"""The vision-language datasets are based on the dataset mixtures from Chen et al. (2023b) and Driess et al. (2023). The bulk of this data consists of the WebLI dataset, which is around 10B image-text pairs across 109 languages, filtered to the top 10% scoring cross-modal similarity examples to give
1B training examples""
""The robotics dataset is based on the dataset from Brohan et al. (2022).""

Chen et al and Driess et al are the original Pali-X and Palm-E papers.  image-text web data

Brohan et al is the RT-1 paper",,,Confident,,,325.00,55000000000.00,"""We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).""

55B and 12B have similar overall performance",,,,,,,,,,,"We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).",2024-05-01 09:22,Anonymous,,0,PaLI-X,,"""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""

",,,,,,,Industry,checked,,,,Industry,,,
SARA-RT-2,Robotics,,2023-12-04,,,,,https://arxiv.org/abs/2312.01990,Google DeepMind,,,Multinational,"Isabel Leal, Krzysztof Choromanski, Deepali Jain, Avinava Dubey, Jake Varley, Michael Ryoo, Yao Lu, Frederick Liu, Vikas Sindhwani, Quan Vuong, Tamas Sarlos, Ken Oslund, Karol Hausman, Kanishka Rao",,"from blog: https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/

""The best SARA-RT-2 models were 10.6% more accurate and 14% faster than RT-2 models after being provided with a short history of images""

Not sure if this is SOTA since they used a smaller (5B) RT-2 variant",SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention,,,,,Unknown,,,,5000000000.00,"5B

""We focus on the 5B PaLI-X variant, as more practical for the on-robot deployment than the 55B variant""",,,,,,,,,,,"We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA.",2024-05-21 03:39,Anonymous,,0,RT-2,,,,,,,,,Industry,,,,,Industry,,,
RT-Trajectory,Robotics,,2023-11-03,,,,,https://arxiv.org/abs/2311.01977,"Google DeepMind,UC San Diego,Stanford University",,"Given the architecture seems to use 35M parameters, it seems unlikely this is above 1e23 FLOP.","Multinational,United States of America,United States of America","Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao",SOTA improvement,"from blog https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/

""When tested on 41 tasks unseen in the training data, an arm controlled by RT-Trajectory more than doubled the performance of existing state-of-the-art RT models: it achieved a task success rate of 63%, compared with 29% for RT-2.""",RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches,RT-1,"""We use the RT-1 (Brohan et al., 2023b) demonstration dataset for training""

also trained with retroactively-generated trajectories created by humans, by code written by GPT-4, and image generation models",,,Unknown,,,10.00,,seems to be based on the RT-1 architecture (35M parameters) with some modifications (section 3.3),,,,,,,Reinforcement learning,,,,"Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
RT-1 + AutoRT,Robotics,,2024-01-04,,Unreleased,,,https://auto-rt.github.io/,Google DeepMind,,,Multinational,"Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu",,"Some improvements on RT-1: ""The co-fine-tuned model is evaluated on two tasks we find RT-1 generalizes poorly to: picking from
different heights, and wiping. Exact evaluation instructions and details are in Appendix F. When co-fine-tuned, RT-1’s performance increases from 0% to 12.5% on picking from different height, and 10% to 30% on wiping.""",AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,RT-1,"fine-tuned on a mix of original RT-1 dataset, and dataset collected by AutoRT:

""A pretrained RT-1 model is co-fine-tuned on a 50-50 mixture of the pretraining dataset described in Brohan et al. (2022) and AutoRT’s dataset. RT-1 is used instead of RT-2 due to training more quickly and cheaply.""",,"""Data statistics: In total, 53 robots were used to collect 77,000 new episodes over the course of 7 months, with a peak load of over 20 simultaneous robots. Over 6,650 unique instructions appear
in the dataset""",Likely,,,,35000000.00,from RT-1,,,,,,,,,,,"Foundation models that incorporate language, vision, and more recently actions
have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation
models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the
deployment of operational robots in completely unseen scenarios with minimal
human supervision. AutoRT leverages vision-language models (VLMs) for scene
understanding and grounding, and further uses large language models (LLMs)
for proposing diverse and novel instructions to be performed by a fleet of robots.
Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT
proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies.
We experimentally show that such “in-the-wild” data collected by AutoRT is significantly more diverse, and that AutoRT’s use of LLMs allows for instruction
following data collection robots that can align to human preferences.",2024-03-13 17:35,Anonymous,,0,RT-1,,"fine-tuned on a mix of AutoRT data, which is 77k episodes (unclear how many network passes per episode) and original RT-1 data.

""A pretrained RT-1 model is co-fine-tuned on a 50-50 mixture of the pretraining dataset described in Brohan et al. (2022) and AutoRT’s dataset. RT-1 is used instead of RT-2 due to training more quickly and cheaply.""",,,,,,,Industry,,,,,Industry,,,
EnCodec,Audio,Audio generation,2022-10-24,"MIT for repo in general, non commercial weights. Dataset is in repo.",Open access (non-commercial),Open source,Open source,"https://arxiv.org/abs/2210.13438, https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md",Meta AI,,"""We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a
batch size of 64 examples of 1 second each, a learning rate of 3 · 10−4
, β1 = 0.5, and β2 = 0.9. All the models are traind using 8 A100 GPUs""",United States of America,"Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",SOTA improvement,""" Finally, our best model, EnCodec, reaches state-of-the-art scores for speech and for
music at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels.""",High Fidelity Neural Audio Compression,,"""We train EnCodec on 24 kHz monophonic across diverse domains, namely: speech, noisy speech, music and
general audio while we train the fullband stereo EnCodec on only 48 kHz music. For speech, we use the clean speech segments from DNS Challenge 4 (Dubey et al., 2022) and the Common Voice dataset (Ardila et al., 2019).
For general audio, we use on AudioSet (Gemmeke et al., 2017) together with FSD50K (Fonseca et al., 2021).
For music, we rely on the Jamendo dataset (Bogdanov et al., 2019) for training and evaluation and we further
evaluate our models on music using a proprietary music dataset.""",,"~17k hours total, per Table A.1",Unknown,,,198.00,,,300.00,,,,,NVIDIA A100,,,,,"We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at this http URL.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
MAGNeT,Audio,Audio generation,2024-01-09,MIT,Open source,,,"https://arxiv.org/abs/2401.04577, https://github.com/facebookresearch/audiocraft/blob/main/docs/MAGNET.md","Meta AI,Hebrew University of Jerusalem,Kyutai",,"""We train the models for
1M steps with the AdamW optimizer (Loshchilov & Hutter, 2017), a batch size of 192 examples... We train the models using respectively 32 GPUs for the small model and 64 GPUs for the large ones with float16 precision.""","United States of America,Israel,France","Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",,"Mainly efficiency benefits: ""The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline).""",Masked Audio Generation using a Single Non-Autoregressive Transformer,"ShutterStock,Pond5","""We follow the same setup as in Copet et al. (2023) and use 20K hours of licensed music to train
MAGNET.  Specifically, we rely on the same 10K high-quality music tracks, the ShutterStock,
and Pond5 music data collections as used in Copet et al. (2023)
8 with respectively 25K and 365K
instrument-only music tracks""

Copet et al is the MusicGen paper",,20k hours,Likely,,,,1500000000.00,"""We train non-autoregressive transformer models using 300M (MAGNET-small) and 1.5B
(MAGNET-large) parameters.""",,,,,,,,,,,"We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page this https URL.",2024-03-27 13:52,Anonymous,,0,,,,,,,,,,"Industry,Academia,Industry",,,,,"Industry,Academia,Industry",,,
VALL-E X,Speech,,2023-03-07,,Unreleased,,,https://arxiv.org/abs/2303.03926,Microsoft,,""" The batch sizes of speech and phonemes on each GPU are 1,400,000 (87.5 seconds) and 3,000, respectively. The maximum learning
rate is 5e-4 with warm-up steps of 32,000. The model is pre-trained on 32 V100 GPUs for 400K step""

not sure about passes per batch",United States of America,"Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",,,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,"LibriLight,WenetSpeech","""Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data
are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled
speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours
of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model
on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the
unlabeled LibriLight speech.
To train the speech recognition & translation model for S2ST, we also use additional MT and ST
data. The MT data are from AI Challenger5
, OpenSubtitles20186
and WMT20207, which contain
about 13M, 10M, and 50M sentence pairs in conversion, drama8
, and news domains, respectively.""",910000000,"70k hours, mix of Chinese and English but mostly English
70k * 13k words/hour = 910,000,000 words",Likely,,,55.00,700000000.00,"""For the cross-lingual codec language models, φMAR and φMNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096.""

These are two parts of the model.

According to Ben's script, that's 353M parameters, or ~700M for both
(https://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py)",,,,,,NVIDIA V100,,,,,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{this https URL}.",2024-04-01 09:28,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Mixtral 8x7B,Language,,2023-12-11,Apache 2.0,Open source,,,"https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",Mistral AI,,,France,"Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.",Significant use,"Frequently downloaded: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

Probably the best OS model by a big margin right now, e.g. #7 on Chatbot Arena, above Gemini Pro and Claude 2.1: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
",Mixtral of experts: A high quality Sparse Mixture-of-Experts.,,"""Mixtral is pretrained with multilingual data using a context size of 32k tokens""",,,Confident,,,,46700000000.00,"46.7B *sparse* params. 12.9B params used on average:

""Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.""",,,,,,,,,,,"Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.",2024-05-21 02:59,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
GPT-4 Turbo,"Multimodal,Vision,Language",Chat,2023-11-06,,API access,,,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,OpenAI,,,United States of America,,SOTA improvement,"""More capable"" than GPT-4 according to OpenAI, with larger context window",New models and developer products announced at DevDay,,,,,Unknown,,,,,Not known. Maybe smaller/sparser than GPT-4.,,,"Possibly ~1/2 GPT-4, since OpenAI charges 1/2 as much for GPT-4 Turbo as for GPT-4

https://openai.com/pricing",,,,,,,,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,checked,,,,Industry,checked,,
LMSI-Palm,Language,Language generation,2022-10-20,,Unreleased,,,https://arxiv.org/abs/2210.11610,"Google,University of Illinois Urbana-Champaign (UIUC)",,"(fine-tuned from Palm-540B, which was 2.52e24)","United States of America,United States of America","Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han",SOTA improvement,"""We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label.""",Large Language Models Can Self-Improve,,Trained on chain-of-thought PaLM output from several datasets of questions that require reasoning. See section 4,,,Confident,,,265.00,540000000000.00,540B,,,,,,,,,,,"Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate ""high-confidence"" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",2024-05-19 12:05,Anonymous,,0,PaLM (540B),,"""To reduce the training burden, we sample 5k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3. For each dataset, we fine-tune the model for 10k steps with a learning rate of 5e−5
and a batch size of 32."" Not sure about sequence length",,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Instruct-GPT + Mind's Eye,Language,Quantitative reasoning,2022-10-11,,,,,https://arxiv.org/abs/2210.05359,"Google,Dartmouth College",,"""Training of the JAX-based text-to-code LMs runs on TPU-v3 Pods. The learning
rates we use for training 0.3B and 1.5B LMs on C4 are {3.0e-4, 1.8e-4}, which are switched to
{1.8e-4, 0.5e-4} when fine-tuning on the text-code pairs. We use cosine annealing to control learning
rate over time with fixed warm-up steps (3k).""","United States of America,United States of America","Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M. Dai",,Best at their proposed Utopia benchmark (maybe a bit one-off?),Mind's Eye: Grounded Language Model Reasoning through Simulation,"Utopia,C4","""We propose a new multi-task physics alignment dataset, UTOPIA, whose aim is to benchmark
how well current LMs can understand and reason over some basic laws of physics (§2).
The dataset contains 39 sub-tasks covering six common scenes that involve understanding
basic principles of physics (e.g., conservation of momentum in elastic collisions), and all
the ground-truth answers are automatically generated by a physics engine""

Also C4

""Besides fine-tuning on the dataset with text-code pairs, we also pre-train the model on the C4 dataset """,,,Likely,,,53.00,176500000000.00,"Two models: a LM that converts the input to code for a physics simulator, and a foundation model (InstructGPT)

""the resulting models have 0.3B and 1.5B parameters (used as default)""

InstructGPT is 175B. 175B+1.5B = 176.5B",,,,,,Google TPU v3,,,,,"Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",2024-04-01 09:52,Anonymous,,0,InstructGPT,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
Zi Yue,Language,Chat,2023-07-28,,,,,https://www.chinadaily.com.cn/a/202307/28/WS64c3226ea31035260b8190a4.html,NetEase,,,China,,,"""Zhou Feng, CEO of NetEase Youdao, told China Daily that, in terms of translation ability, Zi Yue has ""already surpassed ChatGPT in an all-around way"" and is better than most major smart translators in the market.

""The biggest opportunity brought by the large model is individualized teaching and studying, which offers students personalized analysis and tutoring while guiding them in exploring answers on their own, just like human teachers"", Zhou said.""",NetEase Youdao launches first large model in education,,,,,Unknown,,,,,,,,,,,,,,,,"As Open AI's ChatGPT takes the tech world by storm, Chinese educational technology firm NetEase Youdao launched its large model, along with up to six applications, on Thursday, which marked the birth of one of China's first large models in the education sector.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Zi Yue 2.0,Language,Chat,2023-07-28,,,,,https://m.aastocks.com/en/usq/news/comment.aspx?source=AAFN&id=NOW.1317044&catg=1,NetEase,,,China,,,,"NetEase Youdao Upgrades 'Ziyue' Foundation Model to 2.0 Ver, Encompassing More Subjects & Teaching Areas",,,,,Unknown,,,,,,,,,,,,,,,,"Chinese media reported that Youdao (DAO.US)     has released the 2.0 upgrade for its Ziyue educational foundation model. Youdao also launched Youdao Speed Reading (literal translation of ""有道速讀""), new-generation virtual personality verbal language trainer, AI home tutors, and Youdao-branded new-generation intelligent hardware applications.

It is reported that Ziyue 2.0 has been upgraded in the knowledge question and answering ability within the education scene, with it expanding to more subjects and teaching areas. The amount of educational data has been largely expanded, the model's context window has been upgraded to 16,000 tokens, and new Agent and retrieval enhancement capabilities have been added.",2024-05-21 13:05,Epoch AI,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
AudioLM,Audio,Audio generation,2023-07-26,,,,,https://arxiv.org/abs/2209.03143,Google,,"""We train each stage on 16 TPUv4s with batch size of 256 for 1M steps.""

That's for the 900M-param transformers

If there's 256 passes in each batch, then using 6ND that's 900m * 256m * 6 = 1.3e18. sanity check: 16 tpu4s is 4.4e15 FLOP/s. 1.3e18 FLOP / 4.4e15 FLOP/s is 295 seconds. adjusting for utilization it would be ~1000 seconds or 15 minutes? probably too short, so 1.3e18 seems too low.
",United States of America,"Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour",SOTA improvement,"Compared to other systems without text supervision, AudioLM achieves the highest
sWUGGY scores across both splits. Similarly, it also attains the
highest score in the sBLIMP metric, improving by 8% relative
over the previous state-of-the-art (CPC-BERT [59]).",AudioLM: a Language Modeling Approach to Audio Generation,LibriLight,,820800000,"60k hours of English speech
13680*60000 = 820800000 words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce",Likely,,,260.00,1500000000.00,"""We use identical decoder-only Transformers in
all stages, with 12 layers, 16 attention heads, embedding
dimension of 1024, feed-forward layer dimension of 4096
and dropout of 0.1, together with T5-style relative positional
embeddings [38], resulting in a model parameter size of
0.3B per stage.""

Three stages (figure 2), and 300M per stage. Plus 600M parameters for w2v-BERT XL, so 1.5B total",,,,,,Google TPU v4,Self-supervised learning,,,,"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",2024-05-01 09:22,Anonymous,,0,,,,,,,,,,Industry,checked,,,,Industry,,,
DeepNash,Games,,2022-12-01,,,,,https://www.science.org/stoken/author-tokens/ST-887/full,DeepMind,,"""The final agent was trained using 768 MXU’s (matrix multiplication unit) for Learners and
256 MXU’s for Actors (using 256 TPU’s in total).""
Some more details in Table S1 (in supplementary materials)",United Kingdom of Great Britain and Northern Ireland,"Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, Karl Tuyls",SOTA improvement,"DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",Mastering the game of Stratego with model-free multiagent reinforcement learning,,,,"768 * 7.21M trajectories? (Table S1)

768 * 7.21M = 5,537,280,000

https://www.science.org/doi/suppl/10.1126/science.add4679/suppl_file/science.add4679_sm.pdf",Unknown,,,110.00,,,,,,,,,Reinforcement learning,,,,"We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level. Stratego is one of the few iconic board games that artificial intelligence (AI) has not yet mastered. It is a game characterized by a twin challenge: It requires long-term strategic thinking as in chess, but it also requires dealing with imperfect information as in poker. The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego through self-play from scratch. DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
MathGPT,Language,Quantitative reasoning,2023-08-24,,Hosted access (no API),,,https://www.gizmochina.com/2023/08/24/mathgpt-launch-public-beta-next-gen-math-assistant/,TAL Education Group (Xueersi),,,China,,,,TAL’s MathGPT launches public beta: your next-gen math assistant,,,,,Unknown,,,,,,,,,,,,,,,,"During its 20th-anniversary event, TAL Education Group launched the public beta testing of its innovative mathematical large model, MathGPT. This LLM model is designed primarily for global mathematics enthusiasts and research institutions, marking a significant milestone as China’s first large model tailored for mathematics.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
PaLM-SayCan,Robotics,,2022-08-16,,,,,https://arxiv.org/abs/2204.01691,Google,,"""The RL model is trained using 16 TPUv3 chips and for about 100 hours, as well as a pool of 3000 CPU workers to collect episodes and another 3000 CPU workers to compute target Q-values.""

1600 hours * 3600 * 123 teraflops * 0.4 (assumed utilization) = 2.83e20",United States of America,"Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng",,,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",,,,,Likely,,,923.00,540000000000.00,"540B from PaLM-540B.

",,,,100.0,The RL model is trained using 16 TPUv3 chips and for about 100 hours,Google TPU v3,Reinforcement learning,,,,"Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ""hands and eyes,"" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL.",2024-04-01 09:45,Anonymous,,0,PaLM (540B),,I think PaLM-540B wasn't actually involved in training and the authors trained a separate RL model. ,,,,,,,Industry,checked,,,,Industry,,,
Diffractive Deep Neural Network,Vision,Digit recognition,2018-04-14,,,,,https://arxiv.org/abs/1804.08711,University of California Los Angeles (UCLA),,,United States of America,"Xing Lin, Yair Rivenson, Nezih T Yardimci, Muhammed Veli, Yi Luo, Mona Jarrahi, and Aydogan Ozcan",Highly cited,,All-Optical Machine Learning Using Diffractive Deep Neural Networks,MNIST,"""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). """,55000,"size of MNIST
""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). """,Likely,,,1464.00,8000000000.00,"""For example, using five 3D-printed transmission layers, containing a total of 0.2 million neurons and ~8.0 billion connections that are trained using deep learning, we experimentally demonstrated the function of a handwritten digit classifier.""

My understanding is that every connection correspond to the parameter to learn.",,,,,,,Supervised,,,,"We introduce an all-optical Diffractive Deep Neural Network (D2NN) architecture that can learn to implement various functions after deep learning-based design of passive diffractive layers that work collectively. We experimentally demonstrated the success of this framework by creating 3D-printed D2NNs that learned to implement handwritten digit classification and the function of an imaging lens at terahertz spectrum. With the existing plethora of 3D-printing and other lithographic fabrication methods as well as spatial-light-modulators, this all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can implement, and will find applications in all-optical image analysis, feature detection and object classification, also enabling new camera designs and optical components that can learn to perform unique tasks using D2NNs.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
QMoE: compressed 1T model,Language,Language modelling,2023-10-25,"apache 2.0
github.com/IST-DASLab/qmoe",Open source,Unreleased,Open source,https://arxiv.org/abs/2310.16795,"Institute of Science and Technology Austria (ISTA),Neural Magic",,,"Austria,United States of America","Elias Frantar, Dan Alistarh",SOTA improvement,"low memory usage of compressed model from the abstract ""This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware""",QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models,,this works compress the model - there is no dataset,,this works compress the model - there is no dataset,Likely,,,7.00,1600000000000.00,"""Concretely, QMoE can compress the 1.6 trillion parameter witchTransformer-c2048 model to less than 160GB""
parametrs of base model (this works compress the model - there is no learning from the data) base model have 1.6T parameters",,,,,,NVIDIA RTX A6000,,,,,"Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.6 trillion parameters, requiring 3.2TB of accelerator memory to run efficiently, which makes practical deployment challenging and expensive. In this paper, we present a solution to this memory problem, in form of a new compression and execution framework called QMoE. Specifically, QMoE consists of a scalable algorithm which accurately compresses trillion-parameter MoEs to less than 1 bit per parameter, in a custom format co-designed with bespoke GPU decoding kernels to facilitate efficient end-to-end compressed inference, with minor runtime overheads relative to uncompressed execution. Concretely, QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss, in less than a day on a single GPU. This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference. The source code and compressed models are available at github.com/IST-DASLab/qmoe.",2024-05-23 07:12,Anonymous,,,Switch,1003363200000000000,"(1) * (38.71 * 10 ** 12) * (0.3) * (24 * 3600) = 1003363200000000000
(num gpu) * (peak flop) * (assumed utilization rate) * (time in seconds)
from the paper: ""This allows us to apply data-dependent compression to massive MoEs, while preserving the key feature of post-training
compression techniques: the ability to perform effective
compression using only modest computational resources,
e.g., a single NVIDIA A6000 GPU and less than one day of
compute.""
A6000 have 38.71 TFLOPs from https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
VLM-4,Language,Language modelling/generation,2022-04-12,,,,,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,LightOn,,,France,,,,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages",,,,,Unverified,,,,,,,,,,,,,,,,,2024-04-03 10:01,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Super-vector coding,Vision,Image classification,2010-01-01,,,,,http://tongzhang-ml.org/papers/eccv10_supervect.pdf,"University of Illinois Urbana-Champaign (UIUC),NEC Laboratories,Rutgers University",,,"United States of America,United States of America,United States of America","Xi Zhou, Kai Yu, Tong Zhang, and Thomas S. Huang",SOTA improvement,"""Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks.""",Image Classification using Super-Vector Coding of Local Image Descriptors,"PASCAL VOC 2007,PASCAL VOC 2009","""PASCAL VOC 2007 consists of 9,963 images which are divided into
three subsets: training data (2501 images), validation data (2510 images), and
test data (4952 images). PASCAL VOC 2009 consists of 14,743 images and corre-
spondingly are divided into three subsets: training data(3473 images), validation
data(3581 images), and testing data (7689 images).""",24706,"9,963 + 14,743
""PASCAL VOC 2007 consists of 9,963 images which are divided intothree subsets: training data (2501 images), validation data (2510 images), and
test data (4952 images). PASCAL VOC 2009 consists of 14,743 images and corre-
spondingly are divided into three subsets: training data(3473 images), validation
data(3581 images), and testing data (7689 images).""",Unverified,,,696.00,,it maybe possible to estimate number of parameters from the formulas,,,,,,,,,,,,2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Academia,Industry,Academia",,,,,"Academia,Industry,Academia",,,
Guanaco-65B,Language,Chat,2023-05-23,"LLaMA license, non-commercial for weights. code is MIT",Open access (non-commercial),,Open source,https://arxiv.org/abs/2305.14314; https://github.com/artidoro/qlora,University of Washington,,,United States of America,"Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer",,"""Our best model family, which we name Guanaco, outperforms
all previous openly released models on the Vicuna benchmark, reaching 99.3%
of the performance level of ChatGPT while only requiring 24 hours of finetuning
on a single GPU""",QLoRA: Efficient Finetuning of Quantized LLMs,,Fine-tuned on instruction datasets such as GLUE and Super-NaturalInstructions,,,Likely,,,600.00,65000000000.00,"from Llama-65B 
(also 33B, 13B, 7B variants)",,,,24.0,24 hours,,,,,,"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",2024-04-09 12:34,Anonymous,,0,LLaMA-65B,8000000000000000000,"""using a single professional GPU over 24 hours we achieve 99.3% with our largest model""

no model specified, but if it's an A100, 312 tflop/s * 24 * 3600 * 0.3 utilization = 8e18",,,,,,,Academia,,,,,Academia,,,
BLOOMZ-176B,Language,,2022-11-03,Apache 2.0,Open source,Open source,Unreleased,"https://arxiv.org/abs/2211.01786, https://huggingface.co/bigscience/bloomz",Hugging Face,,"fine-tuned from BLOOM-176B

1.37e22 fine-tune compute",Multinational,"Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel",SOTA improvement,"""Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.""

Table 1",Crosslingual Generalization through Multitask Finetuning,xP3,"""In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated
prompts. 

https://huggingface.co/datasets/bigscience/xP3",20000000000,"per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB 

if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",Likely,,,242.00,176000000000.00,176B,,,,,,,,,,,"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.",2024-04-12 14:11,Anonymous,,0,BLOOM-176B,1,"""We use publicly available pretrained BLOOM models ranging from 560 million to 176 billion parameters. BLOOM models are large decoder-only language models pretrained for around 350 billion tokens with an architecture similar to GPT-3
(Brown et al., 2020). We finetune the models for an additional 13 billion tokens with loss only being
computed on target tokens.""

13B * 176B * 6",,,,,,,Industry,,,,,Industry,,,
mT0-13B,Language,,2022-11-03,apache 2.0,Open source,Open source,Unreleased,"https://arxiv.org/abs/2211.01786, https://huggingface.co/bigscience/bloomz","Hugging Face,BigScience",,"fine-tuned from mT5

1.37e22 fine-tune compute","Multinational,Multinational","Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel",SOTA improvement,"""Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.""

Table 1",Crosslingual Generalization through Multitask Finetuning,xP3,"""In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. 

https://huggingface.co/datasets/bigscience/xP3",20000000000,"per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB 

if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",Confident,,,242.00,13000000000.00,13B,,,,,,,,,,,"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.",2024-05-23 07:50,Epoch AI,,0,mT5-XXL,1,"""We finetune the models for an additional 13 billion tokens with loss only being computed on target tokens...
For finetuning mT5, we follow the same procedure as described above for BLOOM, except that inputs are fed into the encoder and thus are not space-separated from targets.""

13B * 13B * 6 = 1.01e21",,,,,,,"Industry,Research collective",,,,,"Industry,Research collective",,,
DCN+,Language,Question answering,2017-10-31,,Unreleased,,,https://arxiv.org/abs/1711.00106v2,Salesforce Research,,"in Figure 4 we see that network was trained on 140k iterations
from https://github.com/lmn-extracts/dcn_plus/tree/master we see that batch size is 64
It should be possible to compute infrence FLOPs from repository and estimate training compute",United States of America,"Caiming Xiong, Victor Zhong, Richard Socher",SOTA improvement,"""On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. ""
https://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual
",DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,SQuAD,"From start of section 3: ""We train and evaluate our model on the Stanford Question Answering Dataset (SQuAD). ""

",4000000,"from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs
download-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download
wc -w on train-v.1.1 returns 4017471 words so around 4M words",Likely,,,121.00,,"Not directly repoted - It may be possible to extract number from:
https://github.com/lmn-extracts/dcn_plus/tree/master/question_answering",,,,,,,,,,,"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. ",2024-04-04 15:07,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
GSM,Language,Question answering,2017-07-30,,,,,https://aclanthology.org/P17-1018/,"Peking University,Microsoft Research",,,"China,United States of America","Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, Ming Zhou",SOTA improvement,"""At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.""",Gated Self-Matching Networks for Reading Comprehension and Question Answering,SQuAD,"""We specially focus on the SQuAD dataset to train and evaluate our model, """,4000000,"from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs
download-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download
wc -w on train-v.1.1 returns 4017471 words so around 4M words",Unknown,,,806.00,,It could be possible to estimate it from section 3.,,,,,,,,,,,"In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",2024-05-21 02:11,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
SEST,Language,Question answering,2017-03-02,,,,,https://arxiv.org/abs/1703.00572v3,Carnegie Mellon University (CMU),,,United States of America,"Rui Liu, Junjie Hu, Wei Wei, Zi Yang, Eric Nyberg",,,Structural Embedding of Syntactic Trees for Machine Comprehension,SQuAD,"start of section 4 ""We conducted systematic experiments on the SQuAD dataset""",4000000,size of SQuAD,Likely,,,53.00,,,,,,,,NVIDIA GeForce GTX 1080,,,,,"Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods. ",2024-03-07 14:22,Anonymous,,,,,,1,,,,,,Academia,,,,,Academia,,,
Mnemonic Reader,Language,Question answering,2017-05-08,,,,,https://arxiv.org/abs/1705.02798v6,"Fudan University,Microsoft Research",,may be possible to estimate from architecture description,"China,United States of America","Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, Ming Zhou",SOTA improvement,"from the abstract "" Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. """,Reinforced Mnemonic Reader for Machine Reading Comprehension,SQuAD,"at the start of section 5.1 Implementation Details (page 5) ""We mainly focus on the SQuAD dataset [Rajpurkar et al.,2016] to train and evaluate our model""",4000000,size of SQuAD,Likely,,,217.00,,may be possible to estimate architecture description,,,,,,,,,,,"In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. ",2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
KoGPT,Language,,2021-11-12,"Apache 2.0 code, CC-BY-NC-ND 4.0 (non-commercial) weights

https://github.com/kakaobrain/kogpt/blob/main/LICENSE",Open access (non-commercial),Unreleased,Open source,https://github.com/kakaobrain/kogpt,Kakao,,,Korea (Republic of),"Ildoo Kim, Gunsoo Han, Jiyeon Ham, Woonhyuk Baek",,"competitive with HyperCLOVA on benchmarks, per results table",KoGPT: KakaoBrain Korean(hangul) Generative Pre-trained Transformer,ryan,"""KakaoBrain KoGPT was trained on ryan dataset, a dataset known to contain profanity, lewd, political changed, and other harsh language.""

Not sure if that's the only dataset. Couldn't find any info on ""ryan"".

https://huggingface.co/kakaobrain/kogpt",,,Speculative,,,,6166502400.00,"from https://huggingface.co/kakaobrain/kogpt

possibly has 30B parameters based on this source: (would be a different version)

https://post.naver.com/viewer/postView.naver?volumeNo=34605062&memberNo=35753905&vType=VERTICAL",,,,,,,,,,,"""Kakao, the operator of South Korea’s No. 1 messenger app KakaoTalk, said Friday that it plans to expand its vertical Korean language-based artificial intelligence services based on an AI language model called KoGPT this year.""

https://www.koreaherald.com/view.php?ud=20230210000564",2024-04-15 13:55,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
HyperCLOVA X,Language,,2023-08-24,"API access for HyperCLOVA X is available at CLOVA Studio, a Hyperscale AI development tool optimized for businesses and provided via NAVER Cloud Platform. The chat service is available at https://clovax.naver.com/.",API access,,Unreleased,"https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",NAVER,,,Korea (Republic of),"Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang, Hyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim, Minkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan Kim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee, Jongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam Oh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyubin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo et al. (296 additional authors not shown)",,"Possible significant usage: ""HyperCLOVA X is available for creators and enterprise customers""",Korea’s internet giant Naver unveils generative AI services,,,,,Unknown,,,,,"Unknown

Previous version had 204B parameters: ""Naver says HyperCLOVA has more than 204 billion parameters, but it did not disclose how many parameters have been trained on the HyperCLOVA X""

Maybe ambiguous whether HyperCLOVA X is a new and separate model? But HyperClova is pretty old.

""HyperCLOVA X is built on HyperCLOVA and improves on the previous LLMs""

https://www.ncloud.com/solution/featured/hyperclovax",,,,,,,,,,,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,1,,,Industry,checked,,
RaSoR,Language,Question answering,2020-12-23,,,,,https://arxiv.org/abs/2012.12624,"Korea University,Princeton University",,"May be estimated from author's repository https://github.com/shimisalant/RaSoR
citation from the paper about training machine ""All models are implemented using TensorFlow3 and trained on the SQUAD training set using the ADAM (Kingma & Ba, 2015) optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine""","Korea (Republic of),United States of America","Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen",,,Learning Dense Representations of Phrases at Scale,SQuAD,"from section 5 ""We train on the 80k (question, passage, answer span) triples in the SQUAD training set and report results on the 10k examples in the SQUAD development and test sets.""",4000000,number of words in SQuAD,Likely,,,101.00,,May be estimated from author's repository https://github.com/shimisalant/RaSoR,,,,,,,,,,,"Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
TOME,Language,Question answering,2021-10-12,"Apache 2.0 (repo license)

https://github.com/google-research/language/tree/master/language/mentionmemory",Unreleased,,Open source,https://arxiv.org/abs/2110.06176v2,"University of Southern California,Google",,"We have information about hardware ""All models are pre-trained on 128 TPUs using AdamW optimizer (Loshchilov& Hutter, 2019) with learning rate 1e-4 and batch size of 4096."", but no information about training time and exact model of TPUs.

There is information about training 1.5M steps with batch size 4096 citation from appendix A:
""The Mention Encoder and BATCH-TOME are pre-trained
for 1 million steps with 50k warmup steps, and TOME is trained for 500k additional steps with 25k warmup steps after initialization from BATCH-TOME.""
after estimating inference compute we can estimate total compute.","United States of America,United States of America","Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William Cohen",,,Mention Memory: incorporating textual knowledge into Transformers through entity mention attention,English Wikipedia,"citation from the start of Appendix A ""We train on English Wikipedia, """,,,Likely,,,34.00,220000000.00,220M from Table 1 entry for TOME,,,,,,,,,,,"Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining. ",2024-04-04 12:01,Anonymous,,,,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
PhraseCond,Language,Question answering,2017-10-28,,,,,https://arxiv.org/abs/1710.10504v2,"Carnegie Mellon University (CMU),University of Pittsburgh",,,"United States of America,United States of America","Rui Liu, Wei Wei, Weiguang Mao, Maria Chikina",SOTA improvement,"""We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models.""",Phase Conductor on Multi-layered Attentions for Machine Comprehension,SQuAD 1.1,"from start of section 3: ""This paper focuses on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) to train and evaluate our model. """,4000000,size of SQuAD 1.1,Likely,,,22.00,,,,,,,,,,,,,"Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Diplodocus,Games,Diplomacy,2022-10-11,"creative commons (non comm) for model weights, MIT for code

https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights",Open access (non-commercial),,Open source,https://arxiv.org/abs/2210.05492,"Meta AI,Massachusetts Institute of Technology (MIT)",,,"United States of America,United States of America","Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown",SOTA improvement,"SOTA Improvement in no-press Diplomacy
""In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model. """,Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning,,"""we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.""
then self-play training",,"""we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.""
then self-play training",Unknown,,,22.00,,may be estimated from https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file,,,,,,,,,,,"No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.",2024-05-21 13:05,Anonymous,,,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
A Bayesian Approach to Unsupervised One-Shot Learning of Object Categories,Vision,Image classification,2003-10-13,,,,,http://vision.stanford.edu/documents/Fei-Fei_ICCV03.pdf,"California Institute of Technology,University of Oxford",,"not exactly provided, but rather low number","United States of America,United Kingdom of Great Britain and Northern Ireland","Li Fei-Fei, Rob Fergus, Pietro Perona",,,A Bayesian Approach to Unsupervised One-Shot Learning of Object Categories,,"description of Figure 1 ""This dataset is obtained by collecting images through the
Google image search engine (www.google.com). The keyword “things”
is used to obtain hundreds of random images. """,,"description of Figure 1 ""This dataset is obtained by collecting images through the
Google image search engine (www.google.com). The keyword “things”
is used to obtain hundreds of random images. """,Likely,,,806.00,100.00,"citation from section 2.1: In the constellation model, the dimensionality of \theta is large (~ 100) ",,,,,,,,,,,"Learning visual models of object categories notoriously requires thousands of training examples; this is due to the diversity and richness of object appearance which requires models containing hundreds of parameters. We present a method for learning object categories from just a few images (1 /spl sim/ 5). It is based on incorporating ""generic"" knowledge which may be obtained from previously learnt models of unrelated categories. We operate in a variational Bayesian framework: object categories are represented by probabilistic models, and ""prior"" knowledge is represented as a probability density function on the parameters of these models. The ""posterior"" model for an object category is obtained by updating the prior in the light of one or more observations. Our ideas are demonstrated on four diverse categories (human faces, airplanes, motorcycles, spotted cats). Initially three categories are learnt from hundreds of training examples, and a ""prior"" is estimated from these. Then the model of the fourth category is learnt from 1 to 5 training examples, and is used for detecting new exemplars a set of test images.",2024-03-07 14:22,Anonymous,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Mistral Medium,Language,Chat,2023-12-11,,API access,,,https://mistral.ai/news/la-plateforme/,Mistral AI,,,France,,,"One of the strongest models on Chatbot Arena, behind only GPT-4 and Bard/Gemini Pro: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard.

""Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks""
https://mistral.ai/news/la-plateforme/",La plateforme,,,,,Unknown,,,,,"May be 70B, based on this weird leak episode. 

https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/",,,,,,,,,,,"Mistral-medium. Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks. It masters English/French/Italian/German/Spanish and code and obtains a score of 8.6 on MT-Bench. The following table compare the performance of the base models of Mistral-medium, Mistral-small and the endpoint of a competitor.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Lyra-Fr 10B,Language,,2023-12-15,,API access,,,https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,LightOn,,,France,,,,LightOn Lyra-fr model is now available on Amazon SageMaker,,"""Lyra-fr was trained on a large corpus of French curated data""",,,Likely,,,,10000000000.00,10B,,,,,,,,,,,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Qwen-VL,"Multimodal,Language,Vision","Image captioning,Chat",2023-08-24,,Open access (restricted use),,,https://arxiv.org/abs/2308.12966,Alibaba,,"Qwen-7B and ViT as base models, trained on 1.5B image-text pairs",China,,SOTA improvement,"""As the results shown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous
generalist models in terms of both two tasks. Specifically, on zero-shot image caption task, Qwen-VL achieves
state-of-the-art performance (i.e., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms
previous generalist models with much more parameters (e.g., Flamingo-80B with 80B parameters).""","Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",,"""Our pre-training dataset is composed of several publicly accessible sources and some in-house data.
We made an effort to clean the dataset of certain patterns. As summarized in Table 2, the original dataset
contains a total of 5 billion image-text pairs, and after cleaning, 1.4 billion data remain, with 77.3% English
(text) data and 22.7% Chinese (text) data.""",1400000000,1.4B text-image pairs,Likely,,,95.00,9600000000.00,9.6B total - Table 1,1.00,,,,,,,,,,"We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",2024-05-13 08:35,Anonymous,,0,Qwen-7B,,"50k steps, 30k batch size (table 8)",,,,,,,Industry,,,,,Industry,,,
Qwen-VL-Max,"Multimodal,Language,Vision","Chat,Image captioning",2024-01-25,,Hosted access (no API),,,https://qwenlm.github.io/blog/qwen-vl/,Alibaba,,,China,,SOTA improvement,"""Notably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension""",Introducing Qwen-VL,,,,,Unknown,,,,,"Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B",,,,,,,,,,,"Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:

Substantially boost in image-related reasoning capabilities;
Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;
Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
MusicLM,Audio,Audio generation,2023-01-26,,,,,https://arxiv.org/abs/2301.11325,Google,,,United States of America,"Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, Christian Frank",SOTA improvement,"""We demonstrate that our method outperforms baselines on MusicCaps, a hand-curated, high-quality
dataset of 5.5k music-text pairs prepared by musicians.""",MusicLM: Generating Music From Text,Free Music Archive,"""We train SoundStream and w2v-BERT on the Free Music
Archive (FMA) dataset (Defferrard et al., 2017), whereas
the tokenizers and the autoregressive models for the semantic and acoustic modeling stages are trained on a dataset containing five million audio clips, amounting to 280k hours of
music at 24 kHz. Each of the stages is trained with multiple passes over the training data""",,>280k hours,Likely,,,220.00,860000000.00,"""We use decoder-only Transformers for modeling the semantic stage and the acoustic stages of AudioLM. The models
share the same architecture, composed of 24 layers, 16 attention heads, an embedding dimension of 1024, feed-forward
layers of dimensionality 4096, dropout of 0.1, and relative
positional embeddings (Raffel et al., 2020), resulting in
430M parameters per stage.""

""stage"" seems to mean semantic + acoustic, so 860M total",,,,,,,,,,,"We introduce MusicLM, a model generating high-fidelity music from text descriptions such as ""a calming violin melody backed by a distorted guitar riff"". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.",2024-03-07 14:22,Anonymous,,0,W2v-BERT,,also MuLan and SoundStream,,,,,,,Industry,checked,,,,Industry,,,
Neuro-Symbolic Concept Learner,"Vision,Language",Visual question answering,2019-04-26,MIT code: https://github.com/vacancy/NSCL-PyTorch-Release,Unreleased,Open access (non-commercial),Open source,https://arxiv.org/abs/1904.12584,"Massachusetts Institute of Technology (MIT),Tsinghua University,MIT-IBM Watson AI Lab,DeepMind",,,"United States of America,China,United States of America,United Kingdom of Great Britain and Northern Ireland","Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu",SOTA improvement,"""NS-CL’s modularized design enables interpretable, robust, and accurate visual reasoning: it achieves state-of-the-art performance on the CLEVR datase""","The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision","CLEVR,VQS,ImageNet","CLEVR, ImageNet, VQS
5000 in CLEVR
64509 in VQS
and whole ImageNet for pretraining
""We train NS-CL on 5K images (<10% of CLEVR’s 70K training images). We generate 20 questions for each image for the entire curriculum learning process""

section 4.3 ""All models use a pre-trained semantic parser on the full CLEVR dataset""

""The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training""

In appendix G.2 (VQS Dataset):""All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.",,"CLEVR, ImageNet, VQS
5000 in CLEVR
64509 in VQS
and whole ImageNet for pretraining
""We train NS-CL on 5K images (<10% of CLEVR’s 70K training images). We generate 20 questions for each image for the entire curriculum learning process""

section 4.3 ""All models use a pre-trained semantic parser on the full CLEVR dataset""

""The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training""

In appendix G.2 (VQS Dataset):
""All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.",Unknown,,,695.00,,,,,,,,,,,,,"We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",2024-05-21 13:03,Anonymous,,,,,,,,,,,,"Academia,Academia,Academia,Industry",,,,,"Academia,Academia,Academia,Industry",,,
Reading Twice for NLU,Language,Question answering,2017-06-08,,,,,https://arxiv.org/abs/1706.02596v3,DeepMind,,,United Kingdom of Great Britain and Northern Ireland,"Dirk Weissenborn, Tomáš Kočiský, Chris Dyer",SOTA improvement,"""Our results are competi-
tive with the best systems, achieving a new state
of the art on the recent TriviaQA benchmarks.""",Dynamic Integration of Background Knowledge in Neural NLU Systems,"TriviaQA,SQuAD","""We use 2 recent DQA
benchmark training and evaluation datasets,
SQuAD (Rajpurkar et al., 2016) and TriviaQA
(Joshi et al., 2017). """,,"both datasets have around 100k training examples.
SQuAD have around 4M words. TriviaQA is larger
""We use 2 recent DQAbenchmark training and evaluation datasets,
SQuAD (Rajpurkar et al., 2016) and TriviaQA
(Joshi et al., 2017). """,Unknown,,,62.00,,,,,,,,,,,,,"Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way. ",2024-05-21 13:05,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
Qwen1.5 72B,Language,"Chat,Language modelling/generation",2024-02-04,"restriction on >100m monthly users:

https://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE",Open access (restricted use),,,https://qwenlm.github.io/blog/qwen1.5/,Alibaba,,,China,,SOTA improvement,"#1 in C-Eval (84.1, better than Qwen-72B. https://qwenlm.github.io/blog/qwen1.5/, https://cevalbenchmark.com/static/leaderboard.html)",Introducing Qwen1.5,,,,,Likely,,,,72000000000.00,72B,,,,,,,,,,,"In recent months, our focus has been on developing a “good” model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year. With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, we’re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, we’ve merged Qwen1.5’s code into Hugging Face transformers, making it accessible with transformers>=4.37.0 without needing trust_remote_code.",2024-04-03 10:01,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
SeamlessM4T,"Speech,Language","Translation,Speech synthesis,Audio speech recognition",2023-12-08,"looks like code is MIT licensed, model is CC 4.0?

https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file
",Open source,Open source,Open source,"https://arxiv.org/abs/2312.05187, https://huggingface.co/facebook/seamless-m4t-v2-large","Facebook,INRIA,UC Berkeley",,,"United States of America,France,United States of America","Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-jussà, Maha Elbayad, Hongyu Gong, Francisco Guzmán, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson",SOTA improvement,"""As an improved version of SeamlessM4T,
SeamlessM4T v2 delivers state-of-the-art semantic accuracy across different speech and text translation tasks
while supporting nearly 100 languages as input speech or text""",Seamless: Multilingual Expressive and Streaming Speech Translation,,"Several datasets including unlabeled speech, ASR data, TTS data",,~5M hours of audio data (figure 2),Likely,,,10.00,2300000000.00,2.3B,,,,,,NVIDIA V100,Self-supervised learning,,,,"Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at this https URL",2024-05-01 09:22,Anonymous,,0,W2v-BERT,,expanded from 1M hours data to 4.5M hours,,,,,,,"Industry,Academia,Academia",,,,,"Industry,Academia,Academia",,,
Continuous speech recognition by statistical methods,Speech,Speech recognition,1976-04-30,,,,,https://ieeexplore.ieee.org/document/1454428,Massachusetts Institute of Technology (MIT),,,United States of America,Frederick Jelenick,Highly cited,,Continuous speech recognition by statistical methods,,"800 sentences, counting 15 words per sentence gives 12000
""All the results given are for a training set of 800 sentences and a test set of 100 sentences""",12000,"800 sentences, counting 15 words per sentence gives 12000 words
""All the results given are for a training set of 800 sentences and a test set of 100 sentences""",Likely,,,1591.00,,,,,,,,,,,,,"Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters, and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",2024-05-20 10:06,Anonymous,,,,,,,,,,,,Academia,,,,,Academia,,,
Pangu 3.0,"Multimodal,Language,Image generation,Vision","Language modelling/generation,Image generation",2023-07-07,,API access,,,https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,Huawei,,,China,,,,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
",,,,,Likely,,,,100000000000.00,"100B? I think the five foundation models are all included in the same system, instead of being five different variants of Pangu, but that's not very clear. I think that's implied by ""All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size"". 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
""Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).

The L1 layer consists of N industry-tailored models. Huawei Cloud can provide customers with industry models it has trained on open industry datasets, including Pangu models for government, finance, manufacturing, mining, and meteorology. Alternatively, customers can train their own models using their own datasets based on Huawei's L0 or L1 Pangu models.

The L2 layer provides pre-trained models for specific industry scenarios and tasks, such as intelligent government hotline, intelligent branch assistant, lead compound screening, conveyor belt foreign object detection, and typhoon trajectory prediction. These models can be quickly deployed off-the-shelf.""

",,,,,,,,,,,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",2024-05-13 08:43,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
NMT Transformer 437M,Language,Translation,2019-02-28,,Unreleased,Unreleased,Unreleased,https://arxiv.org/abs/1903.00089,"Google,Bar-Ilan University",,,"United States of America,Israel","Roee Aharoni, Melvin Johnson, Orhan Firat",SOTA improvement,"""We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages.""",Massively Multilingual Neural Machine Translation,,"""Since we are not aware of a publicly available resource for this purpose, we construct an in-house
dataset. This dataset includes 102 language pairs
which we “mirror” to-and-from English, with up
to one million examples per language pair. This
results in 103 languages in total, and 204 translation directions which we train simultaneously.""

96M total examples, per Table 4",,"96M total examples, per Table 4. One sentence per example?",Likely,,,520.00,437700000.00,"""Regarding the model, for these experiments we
use a larger Transformer model with 6 layers in
both the encoder and the decoder, model dimension set to 1024, hidden dimension size of 8192,
and 16 attention heads. This results in a model
with approximately 473.7M parameters.""",,,,,,,,,,,"Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",2024-04-16 09:51,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
M4-50B,Language,Translation,2019-10-11,,Unreleased,Unreleased,Unreleased,https://blog.research.google/2019/10/exploring-massively-multilingual.html,Google,,"Sparse architecture, so training compute is uncertain",United States of America,"Ankur Bapna, Orhan Firat",SOTA improvement,,"Exploring Massively Multilingual, Massive Neural Machine Translation",,"""we push the limits of research on multilingual NMT by training a single NMT model on 25+ billion sentence pairs, from 100+ languages to and from English, with 50+ billion parameters.""",,25+ billion sentence pairs,Likely,,,,50000000000.00,"(sparse architecture)

""By modifying the Transformer architecture through the substitution of the vanilla feed-forward layers with sparsely-gated mixture of experts, we drastically scale up the model capacity, allowing us to successfully train and pass 50 billion parameters, which further improved translation quality across the board.""",,,,,,,,,,,"Over the last few years there has been enormous progress in the quality of machine translation (MT) systems, breaking language barriers around the world thanks to the developments in neural machine translation (NMT). The success of NMT however, owes largely to the great amounts of supervised training data. But what about languages where data is scarce, or even absent? Multilingual NMT, with the inductive bias that “the learning signal from one language should benefit the quality of translation to other languages”, is a potential remedy.",2024-04-19 16:17,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
PPLX-70B-Online,Language,"Question answering,Chat",2023-11-29,,API access,,,https://blog.perplexity.ai/blog/introducing-pplx-online-llms,Perplexity,,,United States of America,"Lauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis Yarats",Significant use,"Probably significant use: ""Perplexity, which has just 41 employees and is based out of a shared working space in San Francisco, has 10 million monthly active users, an impressive number for a young start-up."" However, this includes everyone who uses Perplexity's app which also uses third party models like GPT-4.

https://www.nytimes.com/2024/02/01/technology/perplexity-search-ai-google.html

",Introducing PPLX Online LLMs ,,"Fine-tuned on website excerpts:

""Website excerpts, which we call “snippets”, are provided to our pplx-online models to enable responses with the most up-to-date information.

Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness.""",,,Likely,,,,70000000000.00,70B,,,,,,,,,,,"We’re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.",2024-03-20 12:18,Anonymous,,0,Llama 2-70B,,"""Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.""",,,,,,,Industry,,,,,Industry,,,
Fuyu-Heavy,"Multimodal,Language",Chat,2024-01-24,,,,,https://www.adept.ai/blog/adept-fuyu-heavy,Adept,,Nvidia hardware,United States of America,,,"According to Adept: ""Fuyu-Heavy is the world’s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger.""

""Fuyu-Heavy performs roughly on par with Gemini Pro on standard text-only evaluations, outperforming it on the commonly used MMLU benchmark.""",Adept Fuyu-Heavy: A new multimodal model,,"curated/generated image data:

""high-quality image pre-training data is scarce, we’ve devoted a lot of effort to collecting, curating, and even creating this data. There’s also a delicate balance between text and image tasks — we had to develop recipes for striking this balance at scale""",,,Speculative,,,,100000000000.00,"""Fuyu-Heavy is the world’s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger""

So possibly around ~100B params, though GPT-4/Gemini params aren't public",,,,,,,,,,,"We’re excited to introduce Adept Fuyu-Heavy, a new multimodal model designed specifically for digital agents. Fuyu-Heavy is the world’s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger. We’re excited about this model because:

It excels at multimodal reasoning. To us the killer feature is UI understanding, but it also performs well on more traditional multimodal benchmarks. In particular, Fuyu-Heavy scores higher on the MMMU benchmark than even Gemini Pro.
On standard text-based benchmarks, it matches or exceeds the performance of models in the same compute class despite having to devote some of its capacity to image modeling.
It demonstrates that (with some modifications) we can scale up the Fuyu architecture and reap all of the associated benefits, including handling arbitrary size/shape images and efficiently re-using existing transformer optimizations.",2024-05-13 07:38,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Fuyu-8B,"Multimodal,Language,Vision","Chat,Image classification",2023-10-17,non-commercial: https://huggingface.co/adept/fuyu-8b,Open access (non-commercial),Unreleased,Unreleased,"https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",Adept,,,United States of America,"Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar",,"""The Fuyu models perform well according to these metrics, even though they are heavily focused on natural images. Fuyu-8B improves over QWEN-VL and PALM-e-12B on 2 out of 3 metrics despite having 2B and 4B fewer parameters, respectively.""",Fuyu-8B: A Multimodal Architecture for AI Agents,,,,,Confident,,,,8000000000.00,"8B

Also a ""Fuyu-medium"" with unstated param count (<56B: ""Fuyu-Medium performs comparably to PALM-E-562B despite having fewer than a tenth as many parameters"")",,,,,,,,,,,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.",2024-05-13 08:31,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
CoEdiT-xxl,Language,Language generation,2023-05-17,"cc-by-nc (non commercial) for weights: https://huggingface.co/grammarly/coedit-large

code/data here with unclear licenses: https://github.com/vipulraheja/coedit",Open access (non-commercial),Open access (non-commercial),Open access (non-commercial),"https://arxiv.org/abs/2305.09857, https://huggingface.co/grammarly/coedit-large","University of Minnesota,Grammarly",,finetuned from Flan-T5,"United States of America,United States of America","Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang",SOTA improvement,"""We achieve state-of-the-art performance on multiple text editing tasks: grammatical error correction, text simplification, sentence fusion, iterative text editing, and three stylistic editing
tasks (formality style transfer, neutralization,
and paraphrasing).""",CoEdIT: Text Editing by Task-Specific Instruction Tuning,,"82k pairs of editing examples:

""we fine-tune a pre-trained
sequence-to-sequence model on a parallel corpus
of instruction-based 82K input-output pairs. The
inputs and outputs are sourced from publicly available corpora for different text editing tasks""

""Our dataset creation is based on the ITERATER+
dataset proposed by Kim et al. (2022) who combined datasets from various text editing tasks (See
Table 1). Their work, in turn, is based on Du et al (2022b), who categorized each edit into MEANINGCHANGED or NON-MEANING-CHANGED.""",3000000,"82k pairs of sentences. Roughly 20 words per sentence based on examples but mean length could be higher due to outliers.
40*82k = ~3,000,000",Likely,,,21.00,11000000000.00,11B,5.00,,,,,NVIDIA A100,,,,,"We introduce COEDIT, a state-of-the-art text editing system for writing assistance. COEDIT takes instructions from the user specifying the attributes of the desired text, such as ""Make the sentence simpler"" or ""Write it in a more neutral style,"" and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largestsized LLMs trained on instructions while being ∼60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by COEDIT, relative to other stateof-the-art text editing models1.",2024-05-01 09:24,Anonymous,,0,Flan-T5 11B,1000000000000000000,"""We fine-tune different versions of pre-trained FLANT5 (Chung et al., 2022a) models on the COEDIT dataset. Specifically, we use FLANT5-L (770M parameters), FLANT5-XL (3B parameters), FLANT5-XXL (11B parameters) models.""

""Each model is trained for 5 epochs with early stopping. All models were fine-tuned on A100 GPUs using Deepspeed""

6 * 5 epochs * 3 million words (rough estimate) * 11 billion = 9.9e17 ~= 1e18

",,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
LongT5,Language,"""Text summarization
""",2021-12-15,Apache 2.0: https://github.com/google-research/longt5,Open source,Open source,Open source,https://arxiv.org/abs/2112.07916,Google Research,,"architecture is sparse so we cannot use 6ND method,
from 3.1.1 ""we simply replace the encoder
self-attention operation in T5 with a sparse sliding-
window local attention operation following the im-
plementation in ETC ""
at the end of section 3.1.2 there is information about 
complexity O(l(r + l/k)) of local attention
from 4.1.1 ""We pre-train LongT5 models for 1M steps on
4096 input sequence length and 910 output se-
quence length.
batch size is 128 (from 4.1 configurations section)
so with l = 4096, k = 16, r = 127, 
so l(r+l/k) = 1568768, but we are not sure about constant.

if normal attention have complexity O(l^2), and l^2 = 16777216
16777216/1568768 = 10.7
We can try to estimate that LongT5 would have 10 times less compute that normal architecture.",Multinational,"Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang",SOTA improvement,"from abstract: ""We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.""",LongT5: Efficient Text-To-Text Transformer for Long Sequences,C4,"from 4.1.1 ""The same as
T5.1.1, we pre-train LongT5 only on the C4 dataset
(Raffel et al., 2019b), and we do not apply dropout
during pre-training.""",562500000000,"size of C4, from https://huggingface.co/datasets/c4 , C4 dataset is a collection of about 750GB of English-language text, so around 0.75 * 750e9 = 562500000000 words",Likely,,,194.00,3000000000.00,3B from section 4.1,,,,,,Google TPU v3,,,,,"Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",2024-05-01 09:13,Anonymous,,,,,,128,,,,,,Industry,,,,,Industry,,,
XLM-RoBERTa,Language,"Named entity recognition,Question answering,Text classification",2019-11-05,"non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme

data is wikipedia",Open access (non-commercial),Open source,Open access (non-commercial),https://arxiv.org/abs/1911.02116,Facebook AI,,"""We use the multilingual MLM loss and train our XLM-R model for
1.5 Million updates on five-hundred 32GB Nvidia
V100 GPUs with a batch size of 8192. ""

we may try to use 6ND aproximation 
It gives around : 6 * 550e6 * 1.5e6 * 8192 = 40550400000000000000
but number of tokens is speculative",United States of America,"Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
","Highly cited,SOTA improvement","citation ""which obtains state-of-the-art perfor-
mance on cross-lingual classification, sequence la-
beling and question answering""",Unsupervised Cross-lingual Representation Learning at Scale,CC100,"The training dataset and size are mentioned as ""using more than two terabytes of filtered CommonCrawl data"" and the model being trained on ""100 languages"".",125250000000,size of CC100 - copied from other rows,Likely,,,4775.00,550000000.00,"The number of parameters in the model is specified as ""550M params"" for XLM-R.",,,,,,NVIDIA Tesla V100 DGXS 32 GB,,,,,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",2024-04-04 13:02,Anonymous,,,,,,500,,,,,,Industry,,,,,Industry,,,
TrOCR,Vision,Character recognition,2021-09-21,MIT: https://github.com/microsoft/unilm/tree/master/trocr,Open source,Open source,Open source,https://arxiv.org/abs/2109.10282,"Beihang University,Microsoft",,may be computed from github and datasets details,"China,United States of America","Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei",SOTA improvement,"from conclusion ""Experiment results show that TrOCR achieves state-of-the-art results on printed, handwritten and scene text recognition with just a simple encoder-decoder model, without any post-processing steps""",TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models,,"""To build a large-scale high-quality dataset, we sample two million document pages from the publicly available PDF files on the Internet.""
From the Experiment section: ""In total, the first-stage pre-training dataset contains 684M textlines.""""In total, the printed dataset consists of 3.3M textlines.""
and from MJSynth, SynthText datasets there is ""about 16M text images.""",703300000,"The input data to the model are images.
684M + 3.3M + 16M
from Experiment section: ""In total, the first-stage pre-training dataset contains 684M textlines."" ""In total, the printed dataset consists of 3.3M textlines.""
and from MJSynth, SynthText datasets there is ""about 16M text images.""",Likely,,,173.00,558000000.00,558M table 5,,,,,,NVIDIA Tesla V100 DGXS 32 GB,,,,,"Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr",2024-04-04 12:03,Anonymous,,,,,,32,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
bge-reranker-large,Language,Semantic embedding,2023-09-14,MIT,Open source,,,"https://arxiv.org/abs/2309.07597v2
https://huggingface.co/BAAI/bge-reranker-large","Beijing Academy of Artificial Intelligence,Hugging Face",,"It is fine tuning of xlm-roberta-base
citation from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker
""This reranker is initialized from xlm-roberta-base,""","China,Multinational","Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff",,,C-Pack: Packaged Resources To Advance General Chinese Embedding,,"T2ranking, MMmarco, dulreader, Cmedqa-v2, and nli-zh
msmarco, nq, hotpotqa, and NLI
Mr.TyDi

citation from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker""This reranker is initialized from xlm-roberta-base, and we train it on a mixture of multilingual datasets:

    Chinese: 788,491 text pairs from T2ranking, MMmarco, dulreader, Cmedqa-v2, and nli-zh
    English: 933,090 text pairs from msmarco, nq, hotpotqa, and NLI
    Others: 97,458 text pairs from Mr.TyDi (including arabic, bengali, english, finnish, indonesian, japanese, korean, russian, swahili, telugu, thai)

In order to enhance the cross-language retrieval ability, we construct two cross-language retrieval datasets bases on MMarco. Specifically, we sample 100,000 english queries to retrieve the chinese passages, and also sample 100,000 chinese queries to retrieve english passages."" ",,"1819039 text pairs
citation from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker""This reranker is initialized from xlm-roberta-base, and we train it on a mixture of multilingual datasets:

    Chinese: 788,491 text pairs from T2ranking, MMmarco, dulreader, Cmedqa-v2, and nli-zh
    English: 933,090 text pairs from msmarco, nq, hotpotqa, and NLI
    Others: 97,458 text pairs from Mr.TyDi (including arabic, bengali, english, finnish, indonesian, japanese, korean, russian, swahili, telugu, thai)

In order to enhance the cross-language retrieval ability, we construct two cross-language retrieval datasets bases on MMarco. Specifically, we sample 100,000 english queries to retrieve the chinese passages, and also sample 100,000 chinese queries to retrieve english passages."" ",Likely,,,86.00,560000000.00,"560M from https://huggingface.co/BAAI/bge-reranker-large
""This reranker is initialized from xlm-roberta-base"" from https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker
",,,,,,,,,,,"We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at this https://github.com/FlagOpen/FlagEmbedding",2024-04-02 10:38,Anonymous,,,XLM-RoBERTa,,,,,,,,,"Academia,Industry",,,,,"Academia,Industry",,,
MMS-1B,Speech,Speech recognition,2023-05-22,CC-BY-NC 4.0 (non commercial),Open access (non-commercial),,Open access (non-commercial),"https://arxiv.org/abs/2305.13516
https://huggingface.co/facebook/mms-1b-all",Meta AI,,"from page 13 ""all models were
pre-trained for a total of one million updates on A100 GPUs with 80GB of memory. The MMS (0.3B)
model was trained with an effective batch size of 2.3 hours of data across 48 GPUs and the MMS
(1B) model was trained with an effective batch size of 3.5 hours on 64 GPUs""
",United States of America,"Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli",,,"Scaling Speech Technology to 1,000+ Languages","Common Voice,LibriSpeech","from page 13 ""Data. The pre-training data covers about 491K hours in 1,406 languages. This data is drawn from
six training corpora with different characteristics, including the corpora used in XLS-R [Babu et al.,
2022]:
• MMS-lab-U: 1,362 languages comprising 55K hours (§3.1).
• Multilingual Librispech (MLS): 8 European languages of read books totaling 50K
hours [Pratap et al., 2020c]
• CommonVoice (CV): 89 languages totaling 8.8 hours of read Wikipedia text; we use v9.0 of
the corpus [Ardila et al., 2020])
• VoxLingua-107 (VL): 107 languages totaling 5.3K hours of YouTube content [Valk and
Alumäe, 2020]
• BABEL (BBL): 17 African and Asian languages totaling about 1K hours of conversational
telephone data [Gales et al., 2014]
• VoxPopuli (VP): 371K hours of unlabeled speech data in 23 languages derived from Euro-
pean Parliament event recordings [Wang et al., 2021]""",,"491K hours
""Data. The pre-training data covers about 491K hours in 1,406 languages. """,Likely,,,97.00,1000000000.00,https://huggingface.co/facebook/mms-1b-all/tree/main,,,,,,NVIDIA A100 SXM4 80 GB,,,,,"Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data. ",2024-04-01 09:03,Anonymous,,,,,,64,,,,,,Industry,,,,,Industry,,,
NeMO Megatron GPT 20B,Language,Language modelling/generation,2022-09-15,"CC BY 4.0: https://creativecommons.org/licenses/by/4.0/

commercial, no restrictions other than to give credit",Open source,Open source,Unreleased,https://huggingface.co/nvidia/nemo-megatron-gpt-20B,NVIDIA,,,United States of America,,,, NeMo Megatron-GPT 20B ,The Pile,,,size of the pile,Likely,,,,20000000000.00,20B,,,,,,,Self-supervised learning,,,Industry,,2024-04-03 16:44,Anonymous,,,,,,,,,,,,Industry,,,,,Industry,,,
InternLM2-20b,Language,"Chat,Language modelling/generation",2024-01-12,need to apply for commercial license,Open access (restricted use),,,https://huggingface.co/internlm/internlm2-20b,InternLM,,,China,,,, InternLM ,,,,,Likely,,,,20000000000.00,20B,,40000000000.00,2N,,,,,,,,,2024-04-03 10:01,Bartosz Podkanowicz,,,,,,,,,,,,,,,,,,checked,,
Aya,Language,"Language modelling/generation,Chat,Translation",2024-02-12,Apache 2.0,Open source,,,https://arxiv.org/abs/2402.07827,"Cohere for AI,Brown University,Cohere,Carnegie Mellon University (CMU),Massachusetts Institute of Technology (MIT)",,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 × 10−4 and a batch size of 256. We find that using a smaller learning rate compared to 1 × 10−3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. ""","Multinational,United States of America,Canada,United States of America,United States of America","Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker",SOTA improvement,"from abstract ""We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99
language""",Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model,,"""Expansion of Language Coverage We significantly expand the size of available training data to directly address the linguistic inequality of recent NLP development. "" from the paper
""Datasets: xP3x, Aya Dataset, Aya Collection, DataProvenance collection, ShareGPT-Command."" from https://huggingface.co/CohereForAI/aya-101and https://huggingface.co/CohereForAI/aya-101#data-sources",,"at least 835 GB + size of ShareGPT-command + size of DataProvenance collection
https://huggingface.co/CohereForAI/aya-101#data-sourcesxP3x  - 680GB - from 
https://huggingface.co/datasets/CohereForAI/xP3x

aya_dataset - 138MB - https://huggingface.co/datasets/CohereForAI/aya_dataset

aya collection - 155GB - https://huggingface.co/datasets/CohereForAI/aya_collection",Speculative,,,18.00,13000000000.00,13B  - fine tune of mT5 - from last page - model card ,,26000000000.00,2N,,,Google TPU v4,,,,Industry,"Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at this https://huggingface.co/CohereForAI/aya-101",2024-05-01 09:13,Bartosz Podkanowicz,,,mT5-XXL,1,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 × 10−4 and a batch size of 256. We find that using a smaller learning rate compared to 1 × 10−3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. """,128,,Aya,,,,"Industry,Academia,Industry,Academia,Academia",,,,,"Industry,Academia,Industry,Academia,Academia",,,
PolySphere-1,Language,"Chat,Language modelling/generation",2023-06-08,,,,,"https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
https://inside.ai/en/news/2023/06/29/polysphere-token/",AI inside,,,Japan,AI Inside,,,"AI inside Establishes “XResearch” for R&D and Social Implementation of Generative AI and LLM, Providing the Alpha Version of a 14 Billion-Parameter Japanese LLM Service",,,,,Likely,,,,14000000000.00,"14B from https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
",,,,,,,,,,,,2024-05-21 06:28,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Qwen-Audio-Chat,"Language,Speech,Audio","audio question answering,Chat,Speech recognition,Translation,Transcription,Part of speech tagging,Text classification,Question answering,audio classification,singer indetification",2023-11-14,"Qwen license:

https://github.com/QwenLM/Qwen-Audio/blob/main/LICENSE",Open access (restricted use),,,https://arxiv.org/abs/2311.07919,Alibaba,,,China,"Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, Jingren Zhou",SOTA improvement,"""A notable achievement of Qwen-Audio is its state-of-the-art performance on the test set of Aishell1, cochlscene, ClothoAQA, and VocalSound""",Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models,,multiple audio and language sources,,not clear,Likely,,,26.00,8460000000.00,"the model has two components - audio and language.
670M + 7.7B = 8.46B
""The audio encoder is composed of 640M parameters""

""Qwen-Audio incorporates a large language model as its foundational component.
The model is initialized using pre-trained weights derived from Qwen-7B (Bai et al., 2023a). Qwen-7B is a 32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.""",,,,,,,,,,," Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios. ",2024-05-01 09:22,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
tsuzumi 7B,Language,"Chat,Language modelling/generation",2023-12-01,,,,,https://group.ntt/en/magazine/blog/tsuzumi/,NTT Communication Science Laboratories,,,Japan,,,,"NTT's Large Language Model ""tsuzumi"" is Here!",,,,,Likely,,,,7000000000.00,7B,,,,,,,,,,,,2024-04-03 10:01,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Qarasu-14B,Language,Chat,2023-12-29,Apache 2.0,Open source,Unreleased,,"https://note.com/peter_lightblue/n/ne08a7c8cc47a, https://huggingface.co/lightblue/qarasu-14B-chat-plus-unleashed",Lightblue,,,Japan,Peter Devine,,SOTA in class for Japanese,Karasu and Qarasu: The new top Japanese Chatbots,,"""We trained a Shisa model on roughly 7 billion tokens of mostly unstructured Japanese text. We chose the following data sources to use in our pre-training phase:

Aozora Bunko

Japanese Law Precedent Dataset

Japanese Wikipedia

.lg.jp, .go.jp, .ac.jp domain webscrapes from the Japanese subset of  CulturaX

English Ultrachat200K-gen""",7000000000,7B tokens,Confident,,,,14000000000.00,14B,,,,,,NVIDIA A100 SXM4 40 GB,,,,,"In this blog post, we introduce Karasu and Qarasu - state of the art open-source Japanese chatbots.

Try chatting to our best model, Qarasu, here ( https://lightblue-qarasu.serveo.net/ )!

These models are able to chat in Japanese to a higher level than all other available LLMs of a similar size, as rated by a popular conversation benchmark. In this article, we describe the design and engineering process behind building them.",2024-05-21 04:07,Anonymous,,0,Qwen-14B,590000000000000000000,"7B finetune tokens:

https://huggingface.co/lightblue/qarasu-14B-chat-plus-unleashed

7B * 14B * 6 = 5.9e20",,,,,,,Industry,,,,,Industry,,,
Luminous Sparse,Language,,2022-11-16,,,,,https://www.graphcore.ai/posts/graphcore-and-aleph-alpha-demonstrate-80-sparsified-ai-model,Aleph Alpha,,,Germany,,,,,,,,,Likely,,,,2600000000.00,,,,,,,,,,,,"Graphcore and our partner Aleph Alpha are unveiling a significant advance in AI compute efficiency, with the sparsification of a 13bn parameter model down to just 2.6bn parameters.

The advanced technique, which removes around 80% of the model’s weights while retaining most of its capabilities, utilizes the IPU’s support for point sparse matrix multiplications - a characteristic of its made-for-AI architecture.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Gemini Nano-2,"Multimodal,Language,Vision,Audio","Chat,Audio speech recognition,Image captioning",2023-12-19,,,,,https://arxiv.org/abs/2312.11805,Google DeepMind,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""

Chinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.

So compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. 

Touvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.",Multinational,Gemini Team,Significant use,"Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/

""Despite their size, they show exceptionally strong performance on factuality,
i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks""",Gemini: A Family of Highly Capable Multimodal Models,,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,Unknown,,,633.00,3250000000.00,3.25B,,,,,,Google TPU v5e,,,,,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",2024-05-20 11:38,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Gemini Nano-1,"Multimodal,Language,Vision,Audio","Chat,Audio speech recognition,Image captioning",2023-12-19,,,,,https://arxiv.org/abs/2312.11805,Google DeepMind,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach
in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve
performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""
",Multinational,Gemini Team,Significant use,"Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/

""Despite their size, they show exceptionally strong performance on factuality,
i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks""",Gemini: A Family of Highly Capable Multimodal Models,,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training
dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,Likely,,,633.00,1800000000.00,1.8B,,,,,,Google TPU v5e,,,,,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",2024-05-13 07:37,Epoch AI,,0,,,,,,,,,,Industry,,,,,Industry,,,
Solar-10.7B,Language,Chat,2023-12-23,"Apache 2.0, but instruct version is non-commercial",Open source,,Unreleased,https://arxiv.org/abs/2312.15166,Upstage,,,Korea (Republic of),"Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim",,"best in compute class: 

""We introduce SOLAR-10.7B, an advanced large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. It's compact, yet remarkably powerful, and demonstrates unparalleled state-of-the-art performance in models with parameters under 30B.""

https://huggingface.co/upstage/SOLAR-10.7B-v1.0",SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling,,"Instruction tuning: Alpaca-GPT4 (52k), OpenOrca (2.91M), and Synth.
Math-Instruct(126k)
Alignment tuning: Orca DPO Pairs (12.9k), Ultrafeedback
Cleaned (60.8k), and Synth. Math-Alignment (126k)

Note however that in actual training, only 100k OpenOrca samples, 52k Synth. Math-Instruct and 20.1k Synth. Math-Alignment examples were used.

Tokens per example unclear so dataset size not entered.",,,Likely,,,29.00,10700000000.00,10.7B,,,,,,,,,,,"We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",2024-05-17 18:29,Anonymous,,0,Mistral 7B,,They integrate Mistral 7B weights into a larger model. They don't state token count for continued pretraining/fine-tuning,,,,,,,Industry,,,,,Industry,,,
OpenChat 3.5-7B,Language,Chat,2023-11-01,"apache 2.0: https://github.com/imoneoi/openchat

data here (MIT): https://huggingface.co/datasets/openchat/openchat_sharegpt_v3 ",Open source,Open source,Open source,https://huggingface.co/openchat/openchat_3.5,Tsinghua University,,,China,,,Creators claim it's the best 7B model and better than GPT-3.5,,,"Fine-tuned on instruction data

""OpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:

OpenChat ShareGPT
Open-Orca with FLAN answers...""",,,Likely,,,,7000000000.00,7B,,,,,,,,,,,"OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strategy inspired by offline reinforcement learning. Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with ChatGPT, even with a 7B model. Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.",2024-04-05 12:47,Anonymous,,0,Llama 2-7B,,"Like a Llama 2 finetune though the HF page doesn't specify. Their paper describes a Llama 2-13B finetune: https://arxiv.org/abs/2309.11235

""OpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:

OpenChat ShareGPT
Open-Orca with FLAN answers
...
""",,,,,,,Academia,,,,,Academia,,,
Multitask Unified Model (MUM),Language,Language modelling/generation,2021-05-18,,,,,https://blog.google/products/search/introducing-mum/,Google,,,United States of America,,,May have been deployed in Google search and other products,MUM: A new AI milestone for understanding information,,,,,Unknown,,,,,,,,,,,,,,,,"When I tell people I work on Google Search, I’m sometimes asked, ""Is there any work left to be done?"" The short answer is an emphatic “Yes!” There are countless challenges we're trying to solve so Google Search works better for you. Today, we’re sharing how we're addressing one many of us can identify with: having to type out many queries and perform many searches to get the answer you need.

Take this scenario: You’ve hiked Mt. Adams. Now you want to hike Mt. Fuji next fall, and you want to know what to do differently to prepare. Today, Google could help you with this, but it would take many thoughtfully considered searches — you’d have to search for the elevation of each mountain, the average temperature in the fall, difficulty of the hiking trails, the right gear to use, and more. After a number of searches, you’d eventually be able to get the answer you need.

But if you were talking to a hiking expert; you could ask one question — “what should I do differently to prepare?” You’d get a thoughtful answer that takes into account the nuances of your task at hand and guides you through the many things to consider.  

This example is not unique — many of us tackle all sorts of tasks that require multiple steps with Google every day. In fact, we find that people issue eight queries on average for complex tasks like this one. 

Today's search engines aren't quite sophisticated enough to answer the way an expert would. But with a new technology called Multitask Unified Model, or MUM, we're getting closer to helping you with these types of complex needs. So in the future, you’ll need fewer searches to get things done. ",2024-05-21 13:05,Anonymous,,0,T5-11B,,"Based on T5 (specific model not specified). But also multimodal:

""MUM uses the T5 text-to-text framework and is 1,000 times more powerful than BERT.""",,,,,,,Industry,,,,,Industry,,,
Samsung Gauss Language,Language,Language modelling/generation,2023-11-08,,,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,Samsung,,,Korea (Republic of),,,"To be deployed in Samsung devices: ""Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets""",,,,,,Unknown,,,,,,,,,,,,,,,,"Just a few days after OpenAI’s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giant’s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the company’s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Samsung Gauss Code,Language,Code generation,2023-11-08,,,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,Samsung,,,Korea (Republic of),,,"To be deployed in Samsung devices: ""Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets""",,,,,,Unknown,,,,,,,,,,,,,,,,"Just a few days after OpenAI’s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giant’s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the company’s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",2024-05-21 13:05,Epoch AI,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
Samsung Gauss Image,Image generation,Image generation,2023-11-08,,,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,Samsung,,,Korea (Republic of),,,,,,,,,Unknown,,,,,,,,,,,,,,,,"Just a few days after OpenAI’s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giant’s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the company’s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",2024-05-21 13:05,Epoch AI,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
AlphaGeometry,Mathematics,Quantitative reasoning,2024-01-17,"Apache 2.0

Data is synthetic so can be reproduced using open code",Open source,Unreleased,Open source,https://www.nature.com/articles/s41586-023-06747-5,"Google DeepMind,New York University (NYU)",,"Training details. Don't think there's enough info for a FLOP estimate.

""Our customized tokenizer is trained with ‘word’ mode using
SentencePiece36 and has a vocabulary size of 757. We limit the maximum context length to 1,024 tokens and use T5-style relative position embedding37. Sequence packing38,39 is also used because more
than 90% of our sequences are under 200 in length. During training, a
dropout40 rate of 5% is applied pre-attention and post-dense. A 4 × 4 slice of TPUv3 (ref. 41) is used as its hardware accelerator. For pretraining, we train the transformer with a batch size of 16 per core
and a cosine learning-rate schedule that decays from 0.01 to 0.001
in 10,000,000 steps. For fine-tuning, we maintain the final learning rate of 0.001 for another 1,000,000 steps""","Multinational,United States of America","Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, Thang Luong",SOTA improvement,"""On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist.""",Solving olympiad geometry without human demonstrations,,Synthetic dataset of geometry proofs,,"100m examples of theorem-proofs

""By using existing symbolic engines on a diverse set of random theorem premises, we extracted 100 million synthetic theorems and their
proofs, many with more than 200 proof steps, four times longer than
the average proof length of olympiad theorems.""",Likely,,,53.00,151000000.00,"""Overall, the transformer has 151 million parameters, excluding embedding layers at its input and output heads.""",,,,,,Google TPU v3,,,,,"Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1,2,3,4, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.",2024-05-01 09:22,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
LTM-1,Language,Code generation,2023-06-06,,,,,https://magic.dev/blog/ltm-1,Magic,,"Must be below 1e23 FLOP, as it's trained with a single A100.",United States of America,,SOTA improvement,Very long context window - 5M tokens,"LTM-1: an LLM with a 5,000,000 token context window",,,,,Unknown,,,,,,,,,,,,,,,,"Magic’s LTM-1 enables 50x larger context windows than transformers
Magic's trained a Large Language Model (LLM) that’s able to take in the gigantic amounts of context when generating suggestions. For our coding assistant, this means Magic can now see your entire repository of code.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Tacotron 2,Speech,Speech synthesis,2017-12-19,,,,,https://arxiv.org/abs/1712.05884,"Google,UC Berkeley",,,"United States of America,United States of America","Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu",Highly cited,,Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Prediction,,"""We train all models on an internal US English dataset[12], which
contains 24.6 hours of speech from a single professional female
speaker.""",340000,"""We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker.""

13,680 words/hour * 24.6 = 336528 words",Likely,,,2886.00,,"some architecture details:

""Input characters are represented using a learned 512-dimensional
character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 × 1, i.e., where
each filter spans 5 characters, followed by batch normalization [18]
and ReLU activations. As in Tacotron, these convolutional layers
model longer-term context (e.g., N-grams) in the input character
sequence. The output of the final convolutional layer is passed into a
single bi-directional [19] LSTM [20] layer containing 512 units (256
in each direction) to generate the encoded features.""",,,,,,,,,,,"This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
WizardLM 70B,Language,"Language modelling/generation,Chat",2023-04-24,Llama 2 license,Open access (restricted use),,,"https://huggingface.co/WizardLM/WizardLM-70B-V1.0
https://arxiv.org/abs/2304.12244","Microsoft,Peking University",,,"United States of America,China","Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",,,WizardLM: Empowering Large Language Models to Follow Complex Instructions,,,,,Likely,,,421.00,70000000000.00,70B,,,,,,,,,,Industry,"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",2024-05-01 09:24,Bartosz Podkanowicz,,,Llama 2-70B,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
NEC LLM 13B,Language,"Language modelling/generation,Chat",2023-07-06,,,,,https://jpn.nec.com/press/202307/20230706_02.html,NEC Laboratories,,,United States of America,,,,NEC、130億パラメータで世界トップクラスの日本語性能を有する軽量なLLMを開発,,,,,Likely,,,,13000000000.00,13B,,,,,,,,,,,,2024-04-03 10:01,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Karakuri LM,Language,"Language modelling/generation,Chat",2024-01-26,borrows Llama 2 license,Open access (restricted use),,,https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1,KARAKURI Inc.,,,Japan,,,,KARAKURI LM,,,,,Likely,,,,70000000000.00,70B,,,,,,,,,,Industry,,2024-03-27 13:38,Bartosz Podkanowicz,,,Llama 2-70B,,,,,,,,,Industry,,,,,Industry,,,
GPT-NeoX-Japanese,Language,Language modelling/generation,2022-07-27,MIT license,Open source,,,"https://huggingface.co/abeja/gpt-neox-japanese-2.7b, https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207",Abeja,,,Japan,"Shinya Otani, Takayoshi Makabe, Anuj Arora, Kyo Hattori ",,,,,"""The model was trained on Japanese CC-100, Japanese Wikipedia, and Japanese OSCAR.""",,,Likely,,,,2700000000.00,2.7B,,,,,,,,,,,"https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese

We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of https://github.com/EleutherAI/gpt-neox. Japanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts. To address this distinct structure of the Japanese language, we use a special sub-word tokenizer. We are very grateful to tanreinama for open-sourcing this incredibly helpful tokenizer. Following the recommendations from Google’s research on PaLM, we have removed bias parameters from transformer blocks, achieving better model performance. Please refer this article in detail.

Development of the model was led by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori from ABEJA, Inc.. For more information on this model-building activity, please refer here (ja).",2024-04-15 11:35,Anonymous,,0,GPT-NeoX-20B,,actually 2.7B,,,,,,,Industry,,,,,Industry,,,
Japanese-GPT-1B,Language,Language modelling/generation,2022-01-19,MIT for weights,Open source,,,https://huggingface.co/rinna/japanese-gpt-1b,rinna,,,Japan,"Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui, Akio Kaga, Yukiya Hono, Toshiaki Wakatsuki, Koh Mitsuda",,,Release of Pre-Trained Models for the Japanese Language,"Japanese CC-100,Japanese C4,Wikipedia (ja)","""The model was trained on Japanese C4, Japanese CC-100 and Japanese Wikipedia to optimize a traditional language modelling objective. It reaches around 14 perplexity on a chosen validation set from the same data.""",,,Confident,,,,1300000000.00,,,,,,,,,,,,,2024-05-23 04:14,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Llama Guard,Language,Chat,2023-12-07,Llama 2 license,Open access (restricted use),Unreleased,Unreleased,https://arxiv.org/abs/2312.06674,Meta AI,,"1.7e17 finetune compute, plus Llama 2 pretrain compute",United States of America,"Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Davide Testuggine, Madian Khabsa",SOTA improvement,"""Llama Guard, a Llama2-7b model that is
instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on
existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its
performance matches or exceeds that of currently available content moderation tools. """,Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,,"Dataset of prompt-response pairs of human-AI conversations

""We leverage the human preference data about harmlessness from Anthropic (Ganguli et al., 2022). From
this dataset, we pick the first human prompt and discard the corresponding response from the assistant, as
well as all the other turns to create an initial single-turn prompt dataset. Next, we use one of our internal
Llama checkpoints to generate a mix of cooperating and refusing responses for these prompts. We employ
our expert, in-house red team to label the prompt and response pairs for the corresponding category based
on the taxonomy defined in Section 2. The red-teamers annotate the dataset for 4 labels: prompt-category,
response-category, prompt-label (safe or unsafe), and response-label (safe or unsafe). During the annotation
process, we also do data cleaning, and discard examples with badly formatted inputs or outputs. The final
dataset comprises of 13,997 prompts and responses, with their respective annotations.""",3000000,"14k prompt-response pairs. Based on training details it's 4M tokens, or 3M words",Likely,,,42.00,7000000000.00,7B,1.00,,,,,NVIDIA A100 SXM4 80 GB,,,,,"We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",2024-05-01 09:22,Anonymous,,0,Llama 2-7B,170000000000000000,"""We train on a single machine with 8xA100 80GB GPUs using a batch size of 2, with sequence length of 4096, using model parallelism of 1 and a learning rate of 2 × 10−6. We train for 500 steps, which corresponds to ∼1 epoch over our training set.""

6 * 2*4096*500 * 7 billion = 1.7e17",,,,,,,Industry,,,,,Industry,,,
YOLOX-X,Vision,Object detection,2021-08-06,"Apache 2.0 for code/weights

https://github.com/Megvii-BaseDetection/YOLOX/blob/main/LICENSE",Open source,Open access (non-commercial),Open source,https://arxiv.org/abs/2107.08430,Megvii Inc,,"""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017 [17]. We use stochastic gradient descent (SGD) for training. We use a learning rate of
lr×BatchSize/64 (linear scaling [8]), with a initial lr =
0.01 and the cosine lr schedule. The weight decay is 0.0005
and the SGD momentum is 0.9. The batch size is 128 by
default to typical 8-GPU devices""",China,"Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun","Highly cited,SOTA improvement",Table 6,YOLOX: Exceeding YOLO Series in 2021,COCO 2017,"""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017""",2500000,"2.5 million image-label pairs, per Coco paper https://arxiv.org/abs/1405.0312",Likely,,,3207.00,99100000.00,"99.1M, table 3",300.00,,,,,NVIDIA V100,,,,,"In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at this https URL.",2024-04-15 14:21,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
mPLUG-Owl2,"Vision,Language",,2023-11-07,,,,,https://arxiv.org/abs/2311.04257,Alibaba,,"ViT-L/14 and Llama 2-7b compute, plus 1.7e19 joint pretrain FLOP (6 * 400M * 7.1B) and 4e16 joint finetune FLOP. Everything is a negligible fraction except the Llama 2 compute.",China,"Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou",SOTA improvement,"""Extensive experiments illustrate the effectiveness and generalization abilities of mPLUG-Owl2, which achieves state-of-the-art performance on 8 classic vision-language benchmarks using a single generic model.""",mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration,,"""mPLUG-Owl2 is first pre-trained on image-text pairs and fine-tunes on mono-modal and multi-modal instruction data. For pre-training data, we randomly pick about 400 million image-text pairs from five public datasets: Conceptual Captions (CC3M/CC12M) [9], COCO [35], Laionen [49], COYO [7], DataComp [18]. For instruction data, we collect 5 types of datasets including 1) image captioning (i.e., TextCaps [53], COCO [35]); 2) image question answering (i.e., VQAv2 [21], OKVQA [43], OCR-VQA [44], GQA [24], and A-OKVQA [50]); 3) region-aware QA (i.e., Ref-COCO [69], VisualGenome [26]); 4) multi-modal instruct data (i.e., LLaVA-instruct-150K [38]); 5) text-only instruct data (i.e., ShareGPT-80K [1], SlimOrca [34]). Details can be found in the Appendix.""
According to the appendix, the instruction-tuning dataset was 1.23MB total across text, dialog, captions, and visual question-answering. This can't be much more than 1.5M updates per epoch, and the paper says ""For the instruction tuning stage, we train the whole model for 1 epoch with a learning rate of 2e-5 and batch size 256"".",400000000,,Speculative,,,,7120000000.00,"""As depicted in Figure 2, our model, referred to as mPLUGOwl2, is composed of three main components: a fundamental vision encoder, a visual abstractor, and a language decoder. Specifically, we utilize ViT-L/14 as the
vision encoder and LLaMA-2-7B [58] as the language decoder""
ViT-L/14 has 123M parameters and Llama 2 7B has 7B parameters.",1.00,,,,,,,,,,"Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.
",2024-03-07 14:22,Robi Rahman,,,Llama 2-7B,17000000001000000000,https://www.wolframalpha.com/input?i=6+*+400+million+*+7.12+billion,,,,,,,Industry,,,,,Industry,,,
Firefly,Image generation,Image generation,2023-03-21,,,,,https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx,Adobe,,,United States of America,,Significant use,"Integrated into Photoshop. Users generate >200m images within a few months of release:

https://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/","Adobe Unveils Firefly, a Family of new Creative Generative AI",,"""The current Firefly generative AI model is trained on a dataset of licensed content, such as Adobe Stock, and public domain content where copyright has expired.""

https://www.adobe.com/products/firefly.html",,,Unknown,,,,,,,,,,,,,,,,"Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobe’s clouds.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
eDiff-I,Image generation,Image generation,2022-11-02,,API access,,,https://arxiv.org/abs/2211.01324,NVIDIA,,"""The base model was trained using 256 NVIDIA A100 GPUs, while the two super-resolution models were trained with 128 NVIDIA A100 GPUs each"" 
no info on duration",United States of America,"Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu",SOTA improvement,"SOTA zero-shot FID on COCO 2014, Table 1

May be significantly used, via Nvidia Picasso: https://www.nvidia.com/en-us/gpu-cloud/picasso/",eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers,,"""We use a collection of public and proprietary
datasets to train our model. To ensure high-quality training
data, we apply heavy filtering using a pretrained CLIP model
to measure the image-text alignment score as well as an
aesthetic scorer to rank the image quality""",1000000000,"""The final dataset to train our model contains about one billion text-image pairs""",Likely,,,474.00,9100000000.00,"9.1B for config D, Table 1",,,,,,NVIDIA A100,,,,,"Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's ""paint-with-words"" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at this https URL",2024-05-01 09:22,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
StripedHyena-Hessian-7B,Language,Chat,2023-11-27,Apache 2.0,Open source,,,"https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B
https://github.com/togethercomputer/stripedhyena",Together,,,United States of America,"Poli, Michael and Wang, Jue and Massaroli, Stefano and Quesnelle, Jeffrey and  Carlow, Ryan and Nguyen, Eric and Thomas, Armin",,, StripedHyena-Hessian-7B (SH 7B) ,,,,,Likely,,,,7000000000.00,7B,,,,,,,,,,,,2024-04-03 10:01,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Claude Instant,Language,"Language modelling,Chat",2023-08-09,,API access,,,https://www.anthropic.com/news/releasing-claude-instant-1-2,Anthropic,,,United States of America,,,"Possibly widely used, seems to only be available via API and not Anthropic's chat interface. 
Used by one company for customer service: https://www.anthropic.com/news/prompt-engineering-for-business-performance",Releasing Claude Instant 1.2,,,,,Unknown,,,,,"speculatively, Anthropic charges 1/10 as much for Claude Instant as Claude 2, so it may have around 1/10 the parameters (Claude 2 parameters are not public info)

https://cdn.sanity.io/files/4zrzovbb/website/90df03aed08b794ab03c5a7bf28b2ad9cf26cf3c.pdf",,,,,,,,,,,"Businesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API. Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.

Claude Instant 1.2 incorporates the strengths of our latest model Claude 2 in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.",2024-05-23 06:35,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Starling-LM-7B-alpha,Language,Chat,2023-11-25,Apache 2.0,Open source,,,"https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha
https://starling.cs.berkeley.edu/",UC Berkeley,,,United States of America,"Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao",,, Starling-LM-7B-alpha,,"'We present Nectar, the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking.' 'Our dataset’s prompts are an amalgamation of diverse sources, including lmsys-chat-1M, ShareGPT, Antropic/hh-rlhf, UltraFeedback, Evol-Instruct, and Flan. Responses are primarily derived from a variety of models, namely GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, and Mistral-7B-Instruct, alongside other existing datasets and models.'",,"'We release Nectar, a GPT-4 labeled ranking dataset composed of 183K chat prompts' size on https://huggingface.co/datasets/berkeley-nest/Nectar is 518MB",Confident,,,,7000000000.00,7B,,,,,,NVIDIA A100,,,,,,2024-05-15 16:34,Bartosz Podkanowicz,,,OpenChat 3.5-7B,,The model is trained on 8 A100 GPUs with batch size 28 and 10k steps in total.,8,,,,,,Academia,,,,,Academia,,,
CodeWhisperer,Language,Code generation,2022-06-24,,Hosted access (no API),,,https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/,Amazon,,,United States of America,,,Possibly widely used; deployed on AWS,"Introducing Amazon CodeWhisperer, the ML-powered coding companion",,,,,Unknown,,,,,,,,,,,,,,,,"We are excited to announce Amazon CodeWhisperer, a machine learning (ML)-powered service that helps improve developer productivity by providing code recommendations based on developers’ natural comments and prior code. With CodeWhisperer, developers can simply write a comment that outlines a specific task in plain English, such as “upload a file to S3.” Based on this, CodeWhisperer automatically determines which cloud services and public libraries are best suited for the specified task, builds the specific code on the fly, and recommends the generated code snippets directly in the IDE.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Xinghan Foundation Model,"Multimodal,Video,Language,Vision","Video description,Visual question answering,Language modelling/generation",2023-10-25,,,,,https://www.prnewswire.com/news-releases/dahua-announces-think-2-0-strategy-to-accelerate-innovation-for-a-digital-intelligent-future-301967122.html,Dahua Technology,,,China,,,,Dahua Announces Think# 2.0 Strategy to Accelerate Innovation for a Digital Intelligent Future,,,,,Unknown,,,,,,,,,,,,,,,,"At the summit, Dahua's ""Xinghan"" Foundation Model was launched. With video as the core, this multimodal fusion industry foundation model catapults the accuracy and generalization of AI algorithms, with breakthroughs in visual cognition capabilities, independent analysis of various scenarios, and efficient fulfillment of massive fragmented needs, especially urban governance and power industry applications.",2024-05-21 13:05,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
SPHINX (Llama 2 13B),"Vision,Language",,2023-11-13,,,,,https://arxiv.org/abs/2311.07575,"Shanghai AI Lab,Chinese University of Hong Kong (CUHK),ShanghaiTech University",,,"China,Hong Kong","Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao",SOTA improvement,"""as shown in Figure 2, SPHINX can achieve impressive fine-grained visual perception for high-resolution images, which exhibits state-of-the-art performance on extensive evaluation benchmarks, e.g., MMBench (Liu et al., 2023f), MME (Fu et al., 2023a), and POPE (Li et al., 2023e).""","SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",LAION-400M,,,,Likely,,,,13000000000.00,SPHINX + Llama 2 13B,,,,290.0,"""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model.""
"" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""",NVIDIA A100 SXM4 40 GB,,,,,"We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research.",2024-02-29 15:59,Robi Rahman,,0,Llama 2-13B,4,"32 A100 * 312 TFLOPS/A100 * 290 hours * 40% utilization ~= 4e21 FLOP
https://www.wolframalpha.com/input?i=250+hours+*+312+TFLOPS+*+32+*+0.4",32,,,,,,"Academia,Academia",,,,9280,"Academia,Academia",,,
InternImage,Vision,Image classification,2022-11-10,,,,,https://arxiv.org/abs/2211.05778,"Shanghai AI Lab,Tsinghua University,Nanjing University,SenseTime,Chinese University of Hong Kong (CUHK)",,,"China,China,China,Hong Kong,Hong Kong","Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao",SOTA improvement,"""InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs""",InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,LAION-400M,"""To further explore the capability of our model and match the large-scale private
data used in previous methods [16, 20, 59], we adopt M3I
Pre-training [60], a unified pre-training approach available
for both unlabeled and weakly-labeled data, to pre-train
InternImage-H on a 427 million joint dataset of public
Laion-400M [61], YFCC-15M [62], and CC12M [63] for
30 epochs, and then we fine-tune the model on ImageNet1K for 20 epochs.""",427000000,427M images,Likely,,,286.00,1080000000.00,"1.08B, table 1",30.00,,,,,,,,,,"Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs. The code will be released at this https URL.",2024-03-07 14:22,Anonymous,,0,,,,,,,,,,"Academia,Academia,Industry,Academia",,,,,"Academia,Academia,Industry,Academia",,,
OpenFlamingo,Image generation,Image generation,2023-08-02,"MIT. code and weights:
https://github.com/mlfoundations/open_flamingo?tab=readme-ov-file",Open source,Unreleased,Open source,https://arxiv.org/abs/2308.01390,"University of Washington,Stanford University,Allen Institute for AI,Hebrew University of Jerusalem,Columbia University,Google DeepMind,University of California Santa Barbara (UCSB),Research Center Juelich",,,"United States of America,United States of America,United States of America,Israel,United States of America,Multinational,United States of America","Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt",,,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"LAION-2B,MMC4",,180000000,"""OpenFlamingo models were trained for 60M interleaved (MMC4) examples1 and 120M LAION-2B examples.""",Likely,,,,9000000000.00,"""We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters.""",,,,,,NVIDIA A100 SXM4 80 GB,,,,,"We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at this https URL.",2024-04-02 10:40,Robi Rahman,,,,,,64,,,,,,"Academia,Academia,Research collective,Academia,Academia,Industry,Academia",,,,,"Academia,Academia,Research collective,Academia,Academia,Industry,Academia",,,
Gemini 1.0 Pro,"Multimodal,Language,Vision","Language modelling,Visual question answering,Chat,Translation",2023-12-06,,API access,,,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Google DeepMind,,"Not known.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",Multinational,Gemini Team,Significant use,"Default/free model on gemini.google.com

From paper:
""Broadly, we find that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with several of the most capable models available, and Gemini Ultra outperforms all current models. In this section, we examine some of these findings.""",Gemini: A Family of Highly Capable Multimodal Models,,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",,,Unknown,,,633.00,,,,,,,,Google TPU v4,,,,Industry,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",2024-05-21 13:05,Epoch AI,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Otter,"Multimodal,Language,Vision","Chat,Visual question answering,Image captioning,Image generation,Vision-language generation",2023-05-05,MIT: https://github.com/Luodian/Otter,Open source,,Open source,https://arxiv.org/abs/2305.03726,Nanyang Technological University,,rather low as only 4 RTX 3090 were used.,Singapore,"Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu",,,Otter: A Multi-Modal Model with In-Context Instruction Tuning,,custom,,,Likely,,,265.00,1300000000.00,1.3B from section 3.2:  'This results in approximately 1.3 billion trainable parameters for the Otter model.',,,,,,NVIDIA GeForce RTX 3090,,,,,"Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1× A100 GPU to 4× RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines. ",2024-05-13 09:00,Bartosz Podkanowicz,,,,,,4,,,,,,Academia,,,,,Academia,,,
Sparse Non-negative Matrix (SNM) estimation,Language,Language modelling,2014-12-03,,,,,https://arxiv.org/abs/1412.1454,Google,,probably could be estimated from the description in the paper,United States of America,"Noam Shazeer, Joris Pelemans, Ciprian Chelba",SOTA improvement,'When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. ' - from abstract,Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation,One Billion Word benchmark,'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',1000000000,1B from 'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',Likely,,,14.00,62000000000.00,62B from Table 2,,,,,,,,,,,"We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do. ",2024-03-07 14:22,Bartosz Podkanowicz,,,,,,,,,,,,Industry,,,,,Industry,,,
RNN for 1B words,Language,Language modelling,2013-12-11,,,,,https://arxiv.org/abs/1312.3005,Google,,"240 hours on 24 CPUs from Tabale 1. CPU model is not given, but there is mention of using SIMD instructions. 1 SIMD operation is around 4 FLOP. CPU can have around 3e9 operations per second. so around 12e9*24 * 240*3600 = 2.4e17 operations. This estimation doesn;t include use of multiple threads. Including use of threads we would probably have around 10 times more operations  so around 2.4e18 FLOPs. This estimation is speculative.",United States of America,"Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson","Highly cited,SOTA improvement","from abstract: 'We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. '",One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,One Billion Word benchmark,"from abstract: ""With almost one billion words of training data, """,1000000000,"from abstract: 'With almost one billion words of training data, '",Speculative,,,1205.00,20000000000.00,20B from Table 1,,,,240.0,"from Table 1,240 hours on 24 CPUs",,,,,,"We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.
The benchmark is available as a this http URL project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models. ",2024-03-03 15:45,Bartosz Podkanowicz,,,,,,24,,,,,,Industry,,,,5760,Industry,,,
OtterHD-8B,"Multimodal,Vision,Language","Chat,Visual question answering",2023-11-07,,,,,https://arxiv.org/abs/2311.04219,Nanyang Technological University,,,Singapore,"Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu",,"from page 2: ""We present OtterHD-8B, a novel model based on the Fuyu-8B architecture, optimized for varying input resolutions. Our empirical evaluations suggest that the model exhibits state-of-the-art performance across multiple tasks when instruction-tuned with higher resolutions.""",OtterHD: A High-Resolution Multi-modality Model,,"""We compiled a total of 370K instruction/response pairs sourced from the follow-
ing public datasets: LLaVA-Instruct [ 30], VQAv2 [2], GQA [ 23 ], OKVQA [ 36 ], OCRVQA [38 ],
A-OKVQA [ 45], COCO-GOI [33 ], COCO-Caption [ 10], TextQA [ 48], RefCOCO [58], COCO-ITM [ 28 ], ImageNet [17 ], and LLaVA-RLHF [ 51 ].""",,"""We compiled a total of 370K instruction/response pairs sourced from the follow-
ing public datasets: LLaVA-Instruct [ 30], VQAv2 [2], GQA [ 23 ], OKVQA [ 36 ], OCRVQA [38 ],
A-OKVQA [ 45], COCO-GOI [33 ], COCO-Caption [ 10], TextQA [ 48], RefCOCO [58], COCO-ITM [ 28 ], ImageNet [17 ], and LLaVA-RLHF [ 51 ].""",Likely,,,13.00,8000000000.00,8B,,,,3.0,3 hours,NVIDIA A100,,,,,"In this paper, we present OtterHD-8B, an innovative multimodal model evolved from Fuyu-8B, specifically engineered to interpret high-resolution visual inputs with granular precision. Unlike conventional models that are constrained by fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexible input dimensions, ensuring its versatility across various inference requirements. Alongside this model, we introduce MagnifierBench, an evaluation framework designed to scrutinize models' ability to discern minute details and spatial relationships of small objects. Our comparative analysis reveals that while current leading models falter on this benchmark, OtterHD-8B, particularly when directly processing high-resolution inputs, outperforms its counterparts by a substantial margin. The findings illuminate the structural variances in visual information processing among different models and the influence that the vision encoders' pre-training resolution disparities have on model effectiveness within such benchmarks. Our study highlights the critical role of flexibility and high-resolution input capabilities in large multimodal models and also exemplifies the potential inherent in the Fuyu architecture's simplicity for handling complex visual data. ",2024-05-13 08:21,Bartosz Podkanowicz,,,Fuyu-8B,8087040000000000000,"flops = (8) * (312 * 10**12) * (3 * 3600) * (0.3) = 8e18
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'Our implementation permits the completion of full-parameter training within 3 hours per epoch on 8×A100 GPUs. '",8,,,,,,Academia,,,,24,Academia,,,
OneLLM,"Multimodal,Language,Vision","Chat,Visual question answering,Question answering,Image captioning",2023-12-06,,,,,https://arxiv.org/abs/2312.03700,"Chinese University of Hong Kong (CUHK),Shanghai AI Lab",,,"Hong Kong,China","Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue",,It is using many modalities - possible SOTA improvement on chating about fMRI scans.,OneLLM: One Framework to Align All Modalities with Language,,"""The source dataset is a subset of
CC3M [73], around 0.5M image-text pairs. For IMU-text
pairs, we use the IMU sensor data of Ego4D [27] and the
corresponding video narrations (i.e., text annotations). For
fMRI-text pairs, we use the subj01 imaging session of
NSD [5] and follow the same data split with [72]. Note that
the visual stimulus, i.e., images shown to participants, are
from MS COCO [14]. Therefore, we use the image cap-
tions in COCO Captions as text annotations of fMRI-text
pairs""",,"""we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. "" - from abstract",Likely,,,7.00,7000000000.00,7B from Table 1,,,,,,NVIDIA A100,,,,,"Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at this https URL",2024-05-13 07:36,Bartosz Podkanowicz,,,Llama 2-7B,42000000000000000000,"assuming that sizes in Table 9 are tokens, we can use 6ND approximation:
6*1B*7B = 4.2e19",16,,,,,,Academia,,,,,Academia,,,
KwaiYiiMath,Language,"Quantitative reasoning,Chat",2023-10-19,,,,,https://arxiv.org/abs/2310.07488,Kuaishou Technology,,,China,"Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai",,,KwaiYiiMath: Technical Report,,,,,Likely,,,,13000000000.00,13B - from Table 2,,,,,,,,,,,"Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively. ",2024-04-03 10:46,Bartosz Podkanowicz,,,KwaiYii,,,,,,,,,Industry,,,,,Industry,,,
CoRe,Mathematics,,2023-12-29,,,,,https://arxiv.org/abs/2210.16257,Tsinghua University,,,China,"Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang",SOTA improvement,"We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",Solving Math Word Problems via Cooperative Reasoning induced Language Models,"GSM8K,ASDiv","We consider several widely-used math word problem datasets: GSM8K (Cobbe et al., 2021), ASDivA (Miao et al., 2020), SingleOp (Roy et al., 2015), SinlgeEq (Koncel-Kedziorski et al., 2015) and MultiArith (Roy and Roth, 2015). (Details in Appendix A). Following the general setting as in (Kojima et al., 2022; Wei et al., 2022c), we employ accuracy as the evaluation metric for all datasets.",,,Speculative,,,29.00,12400000000.00,"""Since the default setting consists of two GPT-J (6B) and a DeBERTa-large (0.4B), we note our backbone as “GPT-J 12B”, which implies around 12.4 billion parameters in total. """,,,,,,NVIDIA A100 SXM4 40 GB,,,,,"Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",2024-03-07 14:22,Anonymous,,0,GPT-J-6B,,,,,,,,,Academia,,,,,Academia,,,
Volcano 13B,Language,Language modelling/generation,2023-11-13,"dataset and weights are open, no license",Open access (non-commercial),Open access (non-commercial),,https://arxiv.org/abs/2311.07362,"Korea University,Korea Advanced Institute of Science and Technology (KAIST),LG",,"1.3e19 finetune compute
There are several layers of fine-tuning here, but the base model is LLaMA-13B, which was 4.55e22 FLOP","Korea (Republic of),Korea (Republic of),Korea (Republic of)","Seongyun Lee, Sue Hyun Park, Yongrae Jo, Minjoon Seo",SOTA improvement,"""Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE"" (hallucination benchmarks)",Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision,,"trained on synthetic data: ""To train VOLCANO, we collect initial responses for
visual questions from an open-source LMM and
generate feedback and revisions using a proprietary
LLM as shown in Figure 3 (Akyürek et al., 2023;
Madaan et al., 2023; Ye et al., 2023b; Wang et al.,
2023d; Kim et al., 2023).""

https://huggingface.co/datasets/kaist-ai/volcano-train",,"https://huggingface.co/datasets/kaist-ai/volcano-train

558k image-text pairs, rest of dataset is ~1M examples of text data; length per sequence is not clear",Likely,,,9.00,13000000000.00,13B,1.00,,,30.0,,NVIDIA A100 SXM4 80 GB,,,,,"Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that Volcano's feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release Volcano models of 7B and 13B sizes along with the data and code at this https URL.",2024-04-01 09:02,Anonymous,,0,LLaVA 1.5,13000000000000000000,"""For this research, we used an NVIDIA A100- SXM4-80GB GPU and an AMD EPYC 7513 32-
Core Processor running at 2.0778 GHz. Training VOLCANO 7B required 8 GPUs and took a total
of 15 hours, while training VOLCANO 13B took 30 hours""

30 A100-hours
= 30 * 312 teraflops * 3600 * 0.4 utilization (assumed)
= 1.3e19
",,,,,,,"Academia,Academia,Industry",,,,,"Academia,Academia,Industry",,,
LLaVA + LVIS-INSTRUCT4V,"Multimodal,Language,Vision","Language modelling/generation,Visual question answering",2023-11-13,MIT for weights and data,Open source,Open source,,https://arxiv.org/abs/2311.07574,"Fudan University,University of Maryland",,,"China,United States of America","Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang",,"""Notably, by simply replacing the LLaVA-Instruct with our LVISINSTRUCT4V, we achieve better results than LLaVA on most challenging LMM
benchmarks, e.g., LLaVAw (76.7 vs. 70.7) and MM-Vet (40.2 vs. 35.4).""",To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning,LVIS-Instruct4V,"""To address this challenge, we introduce a fine-grained visual instruction dataset,
LVIS-INSTRUCT4V, which contains 220K visually aligned and context-aware
instructions produced by prompting the powerful GPT-4V with images from LVIS.""",,"""To the end, we use 110K images from LVIS and generate 220K high-quality visual instructions"" provided examples are roughly 30-60 words each",Likely,,,19.00,13000000000.00,,1.00,,,,,,,,,,"Existing visual instruction tuning methods typically prompt large language models with textual descriptions to generate instruction-following data. Despite the promising performance achieved, these descriptions are derived from image annotations, which are oftentimes coarse-grained. Furthermore, the instructions might even contradict the visual content without observing the entire visual context. To address this challenge, we introduce a fine-grained visual instruction dataset, LVIS-Instruct4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS. Through experimental validation and case studies, we demonstrate that high-quality visual instructional data could improve the performance of LLaVA-1.5, a state-of-the-art large multimodal model, across a wide spectrum of benchmarks by clear margins. Notably, by simply replacing the LLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA on most challenging LMM benchmarks, e.g., LLaVAw (76.7 vs. 70.7) and MM-Vet (40.2 vs. 35.4). We release our data and model at this https URL.",2024-05-13 07:53,Anonymous,,0,LLaVA 1.5,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Claude 3 Haiku,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modelling/generation",2024-03-04,,API access,,,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,Anthropic,,,United States of America,,,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,,Unknown,,,,,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,,,,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2024-05-23 06:35,Anonymous,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
Claude 3 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modelling/generation",2024-03-04,,API access,,Unreleased,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,Anthropic,,,United States of America,,,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,,Unknown,,,,,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,,,,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2024-05-23 06:34,Robi Rahman,,1,,,,,,,,,,Industry,,,,,Industry,checked,,
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modelling/generation",2024-03-04,,API access,,,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,Anthropic,,,United States of America,,SOTA improvement,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,,Unknown,,,,,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,,,,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2024-05-23 06:35,Robi Rahman,,0,,,,,,,,,,Industry,,,,,Industry,checked,,
Mistral 7B + OVM,Language,Quantitative reasoning,2023-11-16,no license,Open access (non-commercial),,Open access (non-commercial),https://arxiv.org/abs/2311.09724,Chinese University of Hong Kong (CUHK),,,Hong Kong,"Fei Yu, Anningzhe Gao, Benyou Wang",,"""Notably, in GSM8K, our OVM-7B model achieves state-of-the-art
results among LLMs up to 13B parameters;
especially it does not utilize GPT-4 or code
execution""",Outcome-supervised Verifiers for Planning in Mathematical Reasoning,,"They generated a synthetic dataset using fine-tuned Mistral 7B, which they then use to further train the model:

""OVMs were initialized from the corresponding generator checkpoints. We sampled 100 solution paths for each
question from the respective generator (resulting
in 747,300 for GSM8K and 90,000 for Game of
24), with a temperature setting of 0.7. In GSM8K,
OVM was trained for 1 epoch with a batch size of
512. In Game of 24, OVM is trained for 10 epochs
with a batch size of 128, due to its smaller training
set. ",,,Likely,,,16.00,7000000000.00,"7B, based on Mistral 7B",,,,,,,,,,,"Large language models (LLMs) often struggle with maintaining accuracy across a sequence of intermediate reasoning steps in mathematical reasoning, leading to error propagation that undermines the final result. The current methodology to mitigate this issue primarily involves using a verifier model to assess the correctness of generated solution candidates, focusing either on the overall reasoning path or on an incomplete reasoning path. By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a \textit{planning} problem. Our proposed verifier, the Outcome-supervision Value Model (OVM), employs outcome supervision for training, offering an efficient and intuitive method for \textit{planning} by prioritizing steps that lead to accurate conclusions over mere per-step correctness. Furthermore, the OVM eschews the need for labor-intensive annotations on step-level correctness, enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training verifiers for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for planning.",2024-04-01 09:02,Anonymous,,0,Mistral 7B,,,,,,,,,Academia,,,,,Academia,,,
CogView2,Image generation,Image generation,2022-04-28,,,,,https://arxiv.org/pdf/2204.14217.pdf,"Tsinghua University,Beijing Academy of Artificial Intelligence",,Presumably used more than original CogView,"China,China","Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang",,,CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,,,,,Unverified,,,,6000000000.00,"The backbone of our pretrained CogLM is a Transformer with Sandwich LayerNorm [3]. The model has 6 billion parameters (48 layers, hidden size 3072, 48 attention heads)",,,,,,,,,,,,2024-04-03 11:46,Epoch AI,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",checked,,
Sora,Video,Video generation,2024-02-15,,,,,https://openai.com/research/video-generation-models-as-world-simulators,OpenAI,,,United States of America,,SOTA improvement,,Video generation models as world simulators,,,,,Unverified,,,,,,,,,,,,,,,,,2024-04-03 12:50,Epoch AI,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Gemini 1.5 Pro,"Language,Multimodal","Language modelling,Visual question answering",2024-02-15,,,,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Google DeepMind,,,Multinational,Gemini Team,Significant use,"Google DeepMind's current best public model, being used for their products.",Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,,,,,Unverified,,,,,,,,,,,,,,,,,2024-05-20 04:36,Epoch AI,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Deep Blue,Games,Chess,1997-05-01,,Unreleased,,,https://www.sciencedirect.com/science/article/pii/S0004370201001291,IBM,,"The 8000 features were tuned using a mix of human judgment and automated tools using data on chess matches. Unclear how much total ""compute"" went into this.",United States of America,"Murray Campbell, A. Joseph Hoane Jr., Feng-hsiung Hsu","Historical significance,Highly cited","Defeated Kasparov in 1997, which was a famous AI milestone.",Deep Blue,,,,,Speculative,,,1992.00,8000.00,"""The new chess chip had a completely redesigned evaluation function, going from around 6400 features to over 8000""",,1000000000000.00,"IBM says Deep Blue used 11 gigaFLOP/s to evaluate 200 million positions per second. https://www.ibm.com/history/deep-blue.

If one datapoint is a single chess move, and computing a move takes ~100 seconds (Deep Blue's matches lasted ~40 moves: https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov#1997_rematch), then that's ~1 teraFLOP per move. 

However this may be a substantial underestimate because it doesn't the compute on the chess chip, see https://x.com/michael_nielsen/status/1775639928656810097?s=20",,,,,,,,"Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including:
•a single-chip chess search engine,

•a massively parallel system with multiple levels of parallelism,

•a strong emphasis on search extensions,

•a complex evaluation function, and

•effective use of a Grandmaster game database.


This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.",2024-04-04 10:33,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
ReALM,Language,"Named entity recognition,Part of speech tagging,Language modelling",2024-03-29,,Unreleased,,,https://arxiv.org/abs/2403.20329,Apple,,Fine-tuned from FLAN-T5,United States of America,"Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree",SOTA improvement,"""We show that ReaLM outperforms previous approaches, and performs roughly as well as the state-of-the-art LLM today, GPT-4, despite consisting of far fewer parameters.""

""We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.""",ReALM: Reference Resolution As Language Modeling,,"Mix of synthetic and manually annotated data. 

""Each data point contains the user query and a list of entities, along with the ground-truth entity (or set of entities) that are relevant to the corresponding user query. Each entity, in turn, contains information about its type and other properties such as the name and other textual details associated with the entity (the label and time of an alarm, for example)""",16300,2300 training examples from conversation; 3900 synthetically generated training examples; 10100 training examples using context from a phone screen.,Confident,,,,3000000000.00,Fine-tuned FLAN-T5 models ranging from 80M to 3B,,,,,,,Supervised,,,Industry,"Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.",2024-04-09 19:29,Anonymous,,0,,,No training details given,,,,,,,Industry,,,,,Industry,,,
Mobile V-MoEs,Vision,Image classification,2023-09-08,,,,,https://arxiv.org/abs/2309.04354,Apple,,,United States of America,"Erik Daxberger, Floris Weers, Bowen Zhang, Tom Gunter, Ruoming Pang, Marcin Eichner, Michael Emmersberger, Yinfei Yang, Alexander Toshev, Xianzhi Du",,,Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts,ImageNet-1k,"We train all models from scratch on the ImageNet-1k training set of
1.28M images, and then evaluate their top-1 accuracy on
the held-out validation set of 50K images.",1280000,,Unverified,,,,,"total number of layers: 12
embedding size: 384
number of multi-head self-attention heads: 6
number of experts: 10 ",,2297000000.00,Figure 3 for the most accurate model,,,,,,,Industry,"Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only 54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.",2024-04-10 12:30,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
Command R+,Language,"Language modelling/generation,Language generation,Translation,Code autocompletion",2024-04-04,,API access,,,https://txt.cohere.com/command-r-plus-microsoft-azure/,"Cohere,Cohere for AI",,,"Canada,Multinational",,,,,,,,,,,,,104000000000.00,,,,,,,,,,,Industry,"C4AI Command R+ is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks. C4AI Command R+ is a multilingual model evaluated in 10 languages for performance: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Arabic, and Simplified Chinese. Command R+ is optimized for a variety of use cases including reasoning, summarization, and question answering.",2024-04-10 14:01,Natalia Martemianova,,,,,,,,,,,,"Industry,Industry",checked,,,,"Industry,Industry",checked,,
Lumiere,"Vision,Multimodal",Video generation,2024-01-23,,Unreleased,,,https://lumiere-video.github.io/,"Google Research,Weizmann Institute of Science,Tel Aviv University,Technion - Israel Institute of Technology",,Actual # of newly trained parameters is unclear. The 3B Imagen weights are frozen.,"Multinational,Israel,Israel,Israel","Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, Inbar Mosseri",,"Authors claim to attain state-of-the-art text-to-video generation results, though quantitative results in Table 1 indicate that Lumiere is did not set records for either metric (though it is at the Pareto frontier of these metrics). Figure 10 indicates qualitative improvements over baseline systems, but the baseline appears to be an average of all other systems.",Lumiere: A Space-Time Diffusion Model for Video Generation,,,6400000000,"We train our T2V model on a dataset containing 30M videos along with their text caption. The videos are 80 frames long at 16 fps (5 seconds). The base model is trained at 128×128 and the SSR outputs 1024 × 1024 frames.

80M videos * 80 frame/video = 6.4B",Confident,,,22.00,,"The authors ""inflate"" a pre-trained T2I Imagen model (3B parameters). These pre-trained weights are frozen. An overview of the inflated architecture is given in Figure 4, but is not sufficient to calculate total trainable parameters.",200.00,,,,,,Self-supervised learning,,,Industry,"We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.",2024-04-10 17:45,Anonymous,,0,Imagen,,,,,,,,,"Industry,Academia,Academia,Academia",,,,,"Industry,Academia,Academia,Academia",,,
Voice Engine,"Audio,Speech",Audio generation,2024-03-29,,,,,https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices,OpenAI,,,United States of America,,,,Navigating the Challenges and Opportunities of Synthetic Voices,,,,,Unverified,,,,,,,,,,,,,,,,,2024-04-11 15:41,Robi Rahman,,,,,,,,,,,,Industry,,,,,Industry,,,
Wu Dao Aquila-33B,Language,"Chat,Code generation",2023-06-10,"""coming soon"" per https://huggingface.co/BAAI/Aquila-7B. This page was published a year ago so seems unlikely it will ever be released.",Unreleased,,,"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B",Beijing Academy of Artificial Intelligence,,,China,,,,,,,,,Likely,,,,33000000000.00,33B for largest model: https://huggingface.co/BAAI/Aquila-7B,,,,,,NVIDIA A100,,,,,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",2024-04-11 16:24,Epoch AI,,0,,,,,,,,,,Academia,,,,,Academia,checked,,
Vicuna-7B,Language,,2023-03-30,,,,,https://lmsys.org/blog/2023-03-30-vicuna/,"Large Model Systems Organization,UC Berkeley",,Might be possible to estimate training compute from the training cost. Fine-tuning cost $140,"United States of America,United States of America","Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing",,,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,,,,,Unverified,,,,7000000000.00,,,,,,,,,$120.00,"$140 in 2023, adjusted to 2020 dollars",,,2024-05-21 05:57,Luke Frymire,,,,,,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
Mixtral 8x22B,Language,"Language modelling/generation,Code generation",2024-04-17,Apache 2.0 license,Open source,,,https://mistral.ai/news/mixtral-8x22b/,Mistral AI,,,France,"Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",,Previous release (Mixtral 8x7B) seems to have significant usage. Appears to be SOTA in several benchmarks (MMLU) for open source models,Mixtral 8x22B,,,,,Unverified,,,,141000000000.00,"141B params, 39B active: https://mistral.ai/news/mixtral-8x22b/ ",,,,,,,Self-supervised learning,,,,"Mixtral 8x22B is our latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.

Mixtral 8x22B comes with the following strengths:

- It is fluent in English, French, Italian, German, and Spanish
- It has strong mathematics and coding capabilities
- It is natively capable of function calling; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale
- Its 64K tokens context window allows precise information recall from large documents
",2024-05-10 11:11,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Grok-1.5,Language,"Language modelling,Chat",2024-03-28,"Musk noted that Grok-1.5 will power xAI’s ChatGPT-challenging chatbot on the X platform, while Grok-2, the successor of the new model, is still in the training phase",Hosted access (no API),,,https://x.ai/blog/grok-1.5,xAI,,,United States of America,,,,"Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the 𝕏 platform in the coming days.",,,,,Unverified,,,,,,,,,,,,,,"The X and xAI CEO said that training the Grok 3 model and beyond will require 100,000 Nvidia H100s",,,2024-04-18 06:42,Natalia Martemianova,,,,,,,,,Oracle,,,Industry,,2,,,Industry,checked,,
Grok-1.5V,"Multimodal,Language,Vision","Language modelling,Chat,Image captioning,Code autocompletion,Code generation",2024-03-28,,Hosted access (no API),,,https://x.ai/blog/grok-1.5v,xAI,,,United States of America,,,,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users.",,,,,Unverified,,,,,,,,,,,,,,"The X and xAI CEO said that training the Grok 3 model and beyond will require 100,000 Nvidia H100s",,,2024-05-13 08:13,Natalia Martemianova,,,,,,,,,Oracle,,,Industry,,2,,,Industry,checked,,
Reka Core,"Multimodal,Language,Vision","Chat,Language modelling/generation,Image captioning,Code generation,Code autocompletion",2024-04-15,,API access,,Unreleased,https://publications.reka.ai/reka-core-tech-report.pdf,Reka AI,,"No information about Reka Core model (""Reka Core has not finished training
and is still improving."")
The smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens ",United States of America,"Aitor Ormazabal Che Zheng Cyprien de Masson d’Autume Dani Yogatama
Deyu Fu Donovan Ong Eric Chen Eugenie Lamprecht Hai Pham Isaac Ong
Kaloyan Aleksiev Lei Li Matthew Henderson Max Bain Mikel Artetxe
Nishant Relan Piotr Padlewski Qi Liu Ren Chen Samuel Phua
Yazheng Yang Yi Tay Yuqi Wang Zhongkai Zhu Zhihui Xie",,,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",Wikipedia,"The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset
knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and
audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively
deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly
defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are
STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to
math. O",,,Unverified,,,,,,,,,,,"NVIDIA A100,NVIDIA H100 SXM5",,,,,,2024-05-13 07:52,Natalia Martemianova,,,,,,,,,,,,Industry,,6,,,Industry,checked,,
VideoPoet,"Video,Language,Audio","Video generation,Audio generation",2023-12-21,,Unreleased,,,"https://arxiv.org/abs/2312.14125
https://sites.research.google/videopoet/","Google Research,Carnegie Mellon University (CMU),Google DeepMind",,"Not directly calculable, since we don't know how many epochs the main model was trained for. 

From section 5.2: ""We investigate the learning capabilities of different combinations of pretraining tasks using a model with 300 million parameters. All task combinations are trained using a learning rate of 10^−3 for the same number of steps (300k) with a batch size of 1024... The last row, “ALL (8B)”, is the model with 8 billion parameters, trained on the pretraining tasks as discussed in Section 3 and utilized significantly more compute.""

Compute per epoch (2T tokens) can be estimated as 9.6e22:
2 * 8B connections * 3 * 2T tokens = 9.6e22","Multinational,United States of America,Multinational","Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, Lu Jiang",,"Beats out all competing video generation systems in subjective evaluations across several categories. 

Notably, Lumiere wins for ""Video quality"" (presumably the main metric), however this comparison is only made in revised versions of the paper – VideoPoet was released a couple of months before Lumiere.",VideoPoet: A Large Language Model for Zero-Shot Video Generation,,"Section 5.1: ""We train on a total of 1B image-text pairs and ∼270M videos (∼100M with paired text, of which ∼50M are used for high-quality finetuning, and ∼170M with paired audio) from the public internet and other sources, i.e. around 2 trillion tokens across all modalities.""",2000000000000,"Section 5.1: ""We train on a total of 1B image-text pairs and ∼270M videos (∼100M with paired text, of which ∼50M are used for high-quality finetuning, and ∼170M with paired audio) from the public internet and other sources, i.e. around 2 trillion tokens across all modalities.""",Confident,,,24.00,8000000000.00,Biggest model has 8B parameters; there are also experiments with 300M and 1B models.,,,,,,,Self-supervised learning,,,Industry,"We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions.",2024-04-18 20:36,Anonymous,,0,,,,,,,,,,"Industry,Academia,Industry",,,,,"Industry,Academia,Industry",,,
SIMA,"Games,Video",,2024-04-17,,Unreleased,,Unreleased,https://arxiv.org/abs/2404.10179,Google DeepMind,,,Multinational,"SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York, Nick Young",,,Scaling Instructable Agents Across Many Simulated Worlds,,,,,Unverified,,,,,,,,,,,,,,,,,2024-04-19 12:41,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
RT-2-X,Robotics,,2023-10-13,,Unreleased,,,https://arxiv.org/abs/2310.08864,Google DeepMind,,,Multinational,Open X-Embodiment Collaboration,SOTA improvement,"""Emergent skills evaluation. To investigate the transfer
of knowledge across robots, we conduct experiments with
the Google Robot, assessing the performance on tasks like
the ones shown in Fig. 5. These tasks involve objects and
skills that are not present in the RT-2 dataset but occur in the
Bridge dataset [95] for a different robot (the WidowX robot).
Results are shown in Table II, Emergent Skills Evaluation
column. Comparing rows (1) and (2), we find that RT-2-X
outperforms RT-2 by ∼ 3×, suggesting that incorporating
data from other robots into the training improves the range
of tasks that can be performed even by a robot that already
has large amounts of data available. Our results suggest that
co-training with data from other platforms imbues the RT-2-
X controller with additional skills for the platform that are
not present in that platform’s original dataset.""",Open X-Embodiment: Robotic Learning Datasets and RT-X Models,Open X-Embodiment,"""The Open X-Embodiment Dataset contains 1M+ real robot
trajectories spanning 22 robot embodiments, from single
robot arms to bi-manual robots and quadrupeds. The dataset
was constructed by pooling 60 existing robot datasets from
34 robotic research labs around the world and converting
them into a consistent data format for easy download and
usage. We use the RLDS data format [119], which saves data
in serialized tfrecord files and accommodates the various
action spaces and input modalities of different robot setups,
such as differing numbers of RGB cameras, depth cameras
and point clouds. It also supports efficient, parallelized data
loading in all major deep learning frameworks. For more
details about the data storage format and a breakdown of all
60 datasets, see robotics-transformer-x.github.io.""",,,Likely,,,85.00,55000000000.00,55B,,,,,,,,,,,"Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website this https URL.",2024-05-01 09:05,Anonymous,,0,RT-2,,"""RT-2-X is trained via
co-fine-tuning (similarly to the original RT-2 [9]), with an approximately one to one split of the original VLM data
and the robotics data mixture. N""

RT-2 is in turn a fine-tune of Pali-X 55B",,,,,,,Industry,,,,,Industry,,,
Qwen 1.5 110B,Language,"Chat,Language modelling/generation",2024-04-25,https://huggingface.co/Qwen/Qwen1.5-110B,Open source,,Unreleased,https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,Alibaba,,,China,,,,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series,Unspecified unreleased,"We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.",,,Unverified,,,,110000000000.00,,,,,,,,,,,,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",2024-04-30 05:58,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
ruDalle: Kandinsky 3.1,Image generation,"Text-to-image,Image generation",2023-12-11,"Apache 2 license

https://github.com/ai-forever/Kandinsky-3",Open source,,Open source,https://ai-forever.github.io/Kandinsky-3/K31/,Sber,,lower bound is taken from Kandinsky 3.0 estimate,Russia,,,,,"Unspecified unreleased,LAION,COYO-700M",,,,Unknown,,,,,,,,,,,NVIDIA A100,,,,,"We present Kandinsky 3.1, the follow-up to the Kandinsky 3.0 model, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation, which we have enhanced and enriched with a variety of useful features and modes to give users more opportunities to fully utilise the power of our new model.",2024-05-21 13:05,Natalia Martemianova,,,,,,,,,,,,Industry,,201490800000000000000,,,Industry,,,
LIMA,Language,"Chat,Language modelling/generation",2023-05-18,,,,,https://arxiv.org/abs/2305.11206,Meta AI,,,United States of America,"Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy",,,LIMA: Less Is More for Alignment,"Stack Exchange,wikiHow,Pushshift Reddit","
",750000,"The total amount of training data is roughly 750,000 tokens, split over exactly 1,000 sequences.",Unverified,,,,65000000000.00,same as base model LLaMa 65B,15.00,,,,,,,,,,"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",2024-05-03 10:09,Natalia Martemianova,,,LLaMA-65B,292500000000000000,6*750000*65000000000 = 292500000000000000,,,,,,,Industry,,,,,Industry,,,
SenseNova 5.0,Language,"Chat,Language modelling/generation",2024-04-23,,Hosted access (no API),,Unreleased,https://zhidx.com/p/421866.html,SenseTime,,,Hong Kong,,,,,,,1670000000000,"Words per gygabyte for Mandarin Chinese: 167M

10000*167000000 = 1670000000000

"" is trained based on more than 10TB of tokens, covers a large amount of synthetic data""",Unverified,,,,,,,,,,,,,,,,"SenseTime has newly upgraded its ""SenseNova 5.0"" large model system , and its comprehensive capabilities are fully comparable to GPT-4 Turbo .",2024-05-06 02:20,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,checked,,
Tk-Instruct,Language,Instruction interpretation,2022-10-24,"https://instructions.apps.allenai.org/

https://github.com/yizhongw/Tk-Instruct
https://huggingface.co/models?search=tk-instruct-",Open source,,Open source,https://arxiv.org/abs/2204.07705,"University of Washington,Arizona State University,Allen Institute for AI",,,"United States of America,United States of America,United States of America","Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi",,,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,Super-natural Instructions,,5120091466,"56.6 words*1616 tasks + (1,048,576 tokens per batch/1,024 examples per batch)*5M instances = 5120091465.6

In total, the dataset includes 1616 tasks
and 5M instances. On average, each instruction is
paired with 2.8 positive and 2.4 negative examples.
The average definition length is 56.6 in words.",Speculative,1048576,"batch size of 1,048,576 tokens (1,024 examples)",,11000000000.00,11B,,,,20.0,"1000 steps * 1024 examples = 10^6 examples per 4 hours
5M instances -> 20 hours of total training time
""These experiments are run on Google V3-256 TPUs using a batch size of 1,048,576 tokens (1,024 examples), a constant learning rate of 1e-5 and a total of 1000 steps. Each training run takes 4 hours to complete.""",Google TPU v3,,,,,"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",2024-05-06 08:39,Natalia Martemianova,,,T5-11B,337926036729600000000,=5120091466*11B*6=3.379 × 10^20,,,,,,,"Academia,Academia,Research collective",,,,,"Academia,Academia,Research collective",,,
HuatuoGPT,Language,,2023-05-24,"https://github.com/FreedomIntelligence/HuatuoGPT

Apache 2.0 license",Open source,,Open source,https://arxiv.org/abs/2305.15075,"Shenzhen Research Institue of Big Data,Chinese University of Hong Kong (CUHK)",,,"China,Hong Kong","Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li",,,"HuatuoGPT, towards Taming Language Model to Be a Doctor",HuatuoGPT-sft-data-v1,"To leverage the best of both distilled data (from ChatGPT) and real-world data (from Doctors), we firstly fine-tune HuatuoGPT using four types of data:
Distilled Instructions from ChatGPT	61,400
Real-world Instructions from Doctors	69,768
Distilled Conversations from ChatGPT	68,888
Real-world Conversations with Doctors	25,986 
(scale is undefined)",55611000,"167000000 (mln of  Mandarin Chinese words per GB) *0.333 GB = 55611000

The dataset weoght at HuggingFace: https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1/blob/main/HuatuoGPT_sft_data_v1.jsonl",Speculative,,,,13000000000.00,HuatuoGPT-13B is trained on Ziya-LLaMA-13B-Pretrain-v1,,,,,,,,,,,"In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \textit{distilled data from ChatGPT} and \textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \url{this https URL}. The online demo is available at \url{this https URL}.",2024-05-06 08:39,Natalia Martemianova,,,LLaMA-13B,4337658000000000000,55611000*13000000000*6,,,,,,,"Academia,Academia",,,,,"Academia,Academia",,,
NASA SMD,Language,,2023-12-01,"License:
apache-2.0
https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1",Open source,,,https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1,"NASA,IBM",,,"United States of America,United States of America","Masayasu Maraoka, Bishwaranjan Bhattacharjee, Muthukumaran Ramasubramanian, Ikhsa Gurung, Rahul Ramachandran, Manil Maskey, Kaylin Bugbee, Rong Zhang, Yousef El Kurdi, Bharath Dandala, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdell, Alberto Accomazzi, Sanaz Vahidinia, Ryan McGranaghan, Armin Mehrabian, Tsendgar Lee",,,nasa-smd-ibm-v0.1 (Revision f01d42f),"Wikipedia,PubMed Abstracts,NASA ADS","Total: 66.24B tokens
",66240000000,"Training Data
Wikipedia English (Feb 1, 2020)
AGU Publications
AMS Publications
Scientific papers from Astrophysics Data Systems (ADS)
PubMed abstracts
PubMedCentral (PMC) (commercial license subset)",Speculative,,,,125000000.00,,,,,,,,,,,,"nasa-smd-ibm-v0.1 is a RoBERTa-based, Encoder-only transformer model, domain-adapted for NASA Science Mission Directorate (SMD) applications. It's fine-tuned on scientific journals and articles relevant to NASA SMD, aiming to enhance natural language technologies like information retrieval and intelligent search.

IBM and NASA will build two foundation models. The first will be trained on reams of earth science journals to thematically organize the literature and make it easier to search and discover new knowledge. ",2024-05-06 08:39,Natalia Martemianova,,,,49680000000000000000,66240000000*125000000.00*6,,,,,,,"Government,Industry",,,,,"Government,Industry",,,
Humanoid Locomotion,Robotics,,2024-02-29,,Unreleased,,,https://arxiv.org/abs/2402.19469,UC Berkeley,,,United States of America,"Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik",,"No SOTA tag because there is no recognized benchmark, but does appear to improve on SOTA for metrics like trajectory adherence, positional tracking error (see figures 5-8).",Humanoid Locomotion as Next Token Prediction,,"4 sources. First, they use a neural network policy pretrained with RL, and collect 10k trajectories of 10s each using the Agility Robotics’ simulator on flat ground, without domain randomization. Second, 10k trajectories of 10s each from the model-based controller developed by Agility Robotics. Third, a 1k trajectory subset of motion capture (MoCap) recordings of humans from the KIT dataset. Fourth, a collection of YouTube videos which are converted to trajectories with inverse kinematics (no details about how many).",,,Likely,,,4.00,8000000.00,"Largest model trained has 8M params. Note actual model used in real-world experiments appears to be 2M, to improve latency. See sections 5.1 and 5.9.",,,,,,,,,,Academia,"We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.",2024-05-06 14:54,Anonymous,,0,,,,,,,,,,Academia,,,,,Academia,,,
Imagen 2,Image generation,"Text-to-image,Image generation,Video generation",2023-12-13,Accessible through Google's Vertex AI platform. Some features currently limited to whitelisted testers.,API access,,,https://deepmind.google/technologies/imagen-2/,Google DeepMind,,,Multinational,"Aäron van den Oord, Ali Razavi, Benigno Uria, Çağlar Ünlü, Charlie Nash, Chris Wolff, Conor Durkan, David Ding, Dawid Górny, Evgeny Gladchenko, Felix Riedel, Hang Qi, Jacob Kelly, Jakob Bauer, Jeff Donahue, Junlin Zhang, Mateusz Malinowski, Mikołaj Bińkowski, Pauline Luc, Robert Riachi, Robin Strudel, Sander Dieleman, Tobenna Peter Igwe, Yaroslav Ganin, Zach Eaton-Rosen",,Does not appear to meet any notability criteria; could plausibly reach significant usage given integrations into other Google products.,Imagen 2,,,,,Unverified,,,,,,,,,,,,,,,Industry,"Imagen 2 is our most advanced text-to-image diffusion technology, delivering high-quality, photorealistic outputs that are closely aligned and consistent with the user’s prompt. It can generate more lifelike images by using the natural distribution of its training data, instead of adopting a pre-programmed style.",2024-05-06 17:38,Anonymous,,0,,,,,,,,,,Industry,,,,,Industry,,,
Delphi,Language,Chat,2022-07-12,https://delphi.allenai.org/,Hosted access (no API),,,https://arxiv.org/abs/2110.07574,"Allen Institute for AI,University of Washington",,,"United States of America,United States of America","Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, Yejin Choi",,,Can Machines Learn Morality? The Delphi Experiment, Commonsense Norm Bank,"To teach Delphi, we compile a new dataset, COMMONSENSE NORM BANK (or NORM BANK in
short), which contains 1.7 million examples of descriptive judgments on everyday situations.2 All of
these examples are drawn from existing datasets to cover diverse aspects of social norms and ethics.
The relevant data sources for this paper include SOCIAL CHEMISTRY (Forbes et al., 2020) for social norms and commonsense moral judgments, the commonsense morality subsection of ETHICS
(Hendrycks et al., 2021a) for additional moral judgments, MORAL STORIES (Emelin et al., 2021)
for contextualized moral judgments in simple commonsense stories, and SOCIAL BIAS INFERENCE
CORPUS (Sap et al., 2020) for unjust social biases such as racism and sexism",,Training on the proposed COMMONSENSE NORM BANK is carried out for 400k gradient updates,Speculative,16,,,11000000000.00,,4.00,,,72.0,"We train Delphi using TPU v3-32 and evaluate it using TPU v3-8, with model parallelisms of 32 and 8 respectively, on Google Cloud Virtual Machines. Training Delphi on COMMONSENSE NORM BANK for 4 epochs takes approximately 72 hours.",Google TPU v3,,,,,"As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.
To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., ""helping a friend"" is generally good, while ""helping a friend spread fake news"" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.
Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.",2024-05-08 05:05,Natalia Martemianova,,,Unicorn,1689600000000000000,'=6*11B*400k gradient updates*16 (batch size)*4 epochs=1.6896 × 10^18,,,,,,,"Research collective,Academia",,,,,"Research collective,Academia",,,
Unicorn,Language,,2021-03-24,"https://github.com/allenai/rainbow#downloading-the-weights

Apache 2.0",Open source,,Open source,https://arxiv.org/abs/2103.13009,Allen Institute for AI,,"All experiments were run on Google Cloud using two
Google Compute Engine virtual machine (VM) instances
communicating with various TPUs. Experimental results
were saved into Google Cloud Storage. Each VM had 20
vCPUs with 75GB of memory and ran Debian 9 (Stretch).
One VM used Intel Skylake vCPUs while the other used Intel Haswell. Specific versions of libraries and other dependencies used are available and tracked in the code repository.
For hardware acceleration, we ran all the experiments using v3-8 TPUs when building off of T5-LARGE or smaller.
For T5-SMALL and T5-LARGE we used a model parallelism of 8, while for T5-BASE we used 4. The T5-11B
models were trained using TPU v2-256 and v3-256s with a
model parallelism of 16. Training times usually took several
hours per run, so we ran many experiments in parallel on the
VMs using GNU Parallel (Tange 2011)",United States of America,"Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi",,,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,,,,,Unverified,,,,11000000000.00,,,,,,,,,,,,"Commonsense AI has long been seen as a near impossible goal -- until recently. Now, research interest has sharply increased with an influx of new benchmarks and models.
We propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency.
We perform extensive experiments -- over 200 experiments encompassing 4800 models -- and report multiple valuable and sometimes surprising findings, e.g., that transfer almost always leads to better or equivalent performance if following a particular recipe, that QA-based commonsense datasets transfer well with each other, while commonsense knowledge graphs do not, and that perhaps counter-intuitively, larger models benefit more from transfer than smaller ones.
Last but not least, we introduce a new universal commonsense reasoning model, UNICORN, that establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA (90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA (79.3%).",2024-05-08 04:47,Natalia Martemianova,,,T5-11B,,,,,,,,,Research collective,,,,,Research collective,,,
GroundingGPT,"Multimodal,Language,Video,Vision,Speech","Chat,Language modelling/generation,Image embedding,Image captioning,Video description,Audio speech recognition",2024-03-05,"https: //github.com/lzw-lzw/GroundingGPT

 Apache-2.0 license",Open source,,Open source,https://arxiv.org/abs/2401.06071v5,"ByteDance,Fudan University",,,"China,China","Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang",,,GroundingGPT:Language Enhanced Multi-modal Grounding Model,"LLaVA-Pretrain-595k,Valley-Pretrain-703k,Wavcaps,COCO,OCR-VQA,TextVQA,VisualGenome,Flickr30K Entities,DiDeMo,VGGSS,VCR ,Activitynet Captions,Clotho","Stage1
Image LLaVA-Pretrain-595k
Video Valley-Pretrain-703k
Audio Wavcaps
Stage2
Image RefCOCO, RefCOCOg, RefCOCO+, Visual Genome
Video DiDeMo, HiREST, Charades-STA, Didemo
Audio VGGSS
Stage3
Image LLava-1.5-mix665k, Flickr30k Entities, VCR
Video Valley-Instruct-73k, Videochat-Instruct-11k, Activitynet Captions
Audio Clotho",,can probably be extracted from their github dataset preparation guide,Unverified,,,,7000000000.00,,,,,,,,,,,,"Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose GroundingGPT, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/GroundingGPT.",2024-05-13 08:15,Natalia Martemianova,,,Vicuna-7B,,,,,,,,,"Industry,Academia",,,,,"Industry,Academia",,,
POKE´LLMON,"Games,Language",,2024-04-02,"https://play.pokemonshowdown.com/
https://github.com/git-disl/PokeLLMon",Hosted access (no API),,Open source,https://arxiv.org/abs/2402.01118,Georgia Institute of Technology,,,United States of America,"Sihao Hu, Tiansheng Huang, Ling Liu",,,PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models,,,,,Unverified,,,,,,,,,,,,,,,,"We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: this https URL.",2024-05-08 10:19,Natalia Martemianova,,,GPT-4,,,,,,,,,Academia,,,,,Academia,,,
30B-Lazarus,Language,,2023-05-27,"""same as Llama model license. Research purposes only, not for commercial use"".",Open access (non-commercial),,,https://huggingface.co/CalderaAI/30B-Lazarus,Caldera AI,,,,,,,,,,,,Unverified,,,,30000000000.00,,,,,,,,,,,,,2024-05-09 10:00,Natalia Martemianova,,,,,,,,,,,,Research collective,,,,,Research collective,,,
Layer Normalization: Order embeddings of images and language,Vision,Image captioning,2016-07-21,,,,,https://arxiv.org/abs/1607.06450,University of Toronto,,,Canada,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",,,Layer Normalization,COCO,,,,Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",2024-05-09 11:34,Natalia Martemianova,,,Order-Embeddings of Images and Language,,,,,,,,,Academia,,,,,Academia,,,
Order-Embeddings of Images and Language,Vision,Image captioning,2016-03-01,"https://github.com/ivendrov/order-embedding
Apache License 2.0",Open source,,Open source,https://arxiv.org/abs/1511.06361,University of Toronto,,,Canada,"Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun",,,Order-Embeddings of Images and Language,COCO,,120000,"We use the data splits of Karpathy & Li (2015) for training
(113,287 images), validation (5000 images), and test (5000 images).",Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",2024-05-09 11:34,Natalia Martemianova,,,,,,,,,,,,Academia,,,,,Academia,,,
The Attentive Reader,Language,Question answering,2015-11-19,"https://github.com/cooijmanstim/Attentive_reader/tree/bn

BSD 3-Clause ""New"" or ""Revised"" License",Open source,,Open source,https://arxiv.org/abs/1506.03340,Google DeepMind,,,Multinational,"Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom",,,Teaching Machines to Read and Comprehend,Question Answering Corpus (CNN + Daily Mail),,1000000,This results in a combined corpus of roughly 1M data points (Table 1).,Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",2024-05-09 12:03,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
Layer Normalization: The Attentive Reader,Language,Question answering,2016-07-21,,,,,https://arxiv.org/abs/1607.06450,University of Toronto,,,Canada,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",,,Layer Normalization,Question Answering Corpus (CNN + Daily Mail),,,,Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",2024-05-09 11:34,Natalia Martemianova,,,The Attentive Reader,,,,,,,,,Academia,,,,,Academia,,,
Skip-Thoughts,Language,,2015-06-22,"https://github.com/ryankiros/skip-thoughts
Apache 2.0",Open source,,Open source,https://arxiv.org/abs/1506.06726,"University of Toronto,Massachusetts Institute of Technology (MIT),CIFAR AI Research",,,"Canada,United States of America,Canada","Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler",,,Skip-Thought Vectors,"""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,984846357,Table 1,Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",2024-05-22 13:16,Natalia Martemianova,,,,,,,,,,,,"Academia,Academia,Research collective",,,,,"Academia,Academia,Research collective",,,
Layer Normalization: Skip Thoughts,Language,,2016-07-21,,,,,https://arxiv.org/abs/1607.06450,University of Toronto,,,Canada,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",,,Layer Normalization,"""BookCorpus (BooksCorpus, Toronto Book Corpus)""",,,,Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",2024-05-09 12:03,Natalia Martemianova,,,Skip-Thoughts,,,,,,,,,Academia,,,,,Academia,,,
Draw,Image generation,,2015-05-20,,,,,https://arxiv.org/abs/1502.04623,Google DeepMind,,,Multinational,"Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra",,,DRAW: A Recurrent Neural Network For Image Generation,"SVHN (Street View House Numbers),MNIST,CIFAR-10",,,,Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",2024-05-09 12:03,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
Layer Normalization: Draw,Image generation,,2016-07-21,,,,,https://arxiv.org/abs/1607.06450,University of Toronto,,,Canada,"Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton",,,Layer Normalization,MNIST,,70000,"The dataset has been split into 50,000
training, 10,000 validation and 10,000 test",Unverified,,,,,possibly can be calculated based on architecture description,,,,,,,,,,,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",2024-05-09 12:03,Natalia Martemianova,,,Draw,,,,,,,,,Academia,,,,,Academia,,,
Dropout: SVHN,Vision,,2014-06-01,http://www.cs.toronto.edu/~nitish/dropout,,,Open source,https://jmlr.org/papers/v15/srivastava14a.html,University of Toronto,,,Canada,"Nitish Shrivasta, Geoffrey Hinton, Alex Krizhevsky",Highly cited,,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,SVHN (Street View House Numbers),,600000,,Unverified,,,35853.00,,can possibly be calculated using architecture description and code,,,,,,,,,,Academia,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2024-05-09 12:39,Natalia Martemianova,,,,,,,,,,,,Academia,,,,,Academia,,,
DeepSA,Biology,Synthetic accessibility model,2023-11-02,https://github.com/Shihang-Wang-58/DeepSA,Hosted access (no API),,Open access (non-commercial),https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00771-3,ShanghaiTech University,,,,"Shihang Wang, Lin Wang, Fenglei Li, Fang Bai",,,DeepSA: a deep-learning driven predictor of compound synthesis accessibility,ChEMBL,"DeepSA is a chemical language model that was developed by training on a dataset of 3,593,053 molecules

The training dataset contains 800,000 molecules, of which 150,000 are from the ChEMBL [27] or GDBChEMBL[28]",3593053,,Unverified,,,,,can possibly be calculated from architecture description,,,,,,NVIDIA GeForce RTX 3090,,,,,"With the continuous development of artificial intelligence technology, more and more computational models for generating new molecules are being developed. However, we are often confronted with the question of whether these compounds are easy or difficult to synthesize, which refers to synthetic accessibility of compounds. In this study, a deep learning based computational model called DeepSA, was proposed to predict the synthesis accessibility of compounds, which provides a useful tool to choose molecules. DeepSA is a chemical language model that was developed by training on a dataset of 3,593,053 molecules using various natural language processing (NLP) algorithms, offering advantages over state-of-the-art methods and having a much higher area under the receiver operating characteristic curve (AUROC), i.e., 89.6%, in discriminating those molecules that are difficult to synthesize. This helps users select less expensive molecules for synthesis, reducing the time and cost required for drug discovery and development. Interestingly, a comparison of DeepSA with a Graph Attention-based method shows that using SMILES alone can also efficiently visualize and extract compound’s informative features. DeepSA is available online on the below web server (https://bailab.siais.shanghaitech.edu.cn/services/deepsa/) of our group, and the code is available at https://github.com/Shihang-Wang-58/DeepSA.",2024-05-10 05:43,Natalia Martemianova,,,,,,,,,,,,Academia,,,,,Academia,,,
GPT-2 (AMPS),"Mathematics,Language",,2021-11-08,,,,,https://arxiv.org/abs/2103.03874,UC Berkeley,,,United States of America,"Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt",,,Measuring Mathematical Problem Solving With the MATH Dataset,Auxiliary Mathematics Problems and Solutions (AMPS),,,,Speculative,,,,1500000.00,,,,,24.0,"Models are trained with 8 A100 GPUs, each requiring less than a day. Unless otherwise specified, for GPT-2 we use the default HuggingFace (Wolf et al., 2020) generation parameters, except that we use beam search.",NVIDIA A100,,,,,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",2024-05-13 07:56,Natalia Martemianova,,,GPT-2 (1.5B),64663142400000000000,8*24*3600*311.84*10^12*0.3=64663142400000000000,8,,,,,,Academia,,,,,Academia,,,
Mi:dm 7B,Language,,2023-11-15,"https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1
Mi:dm (Midm-bitext-S) is released under the CC-BY-NC 4.0 license. Users can retrain part or all of this model or use only part of it. However, the author must be indicated and cannot be used for commercial purposes. Additionally, when sharing secondary works using this model, they must be distributed under the same CC-BY-NC 4.0 license.",Open access (non-commercial),,,https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1,KT,,,Korea (Republic of),,,,"Mi:dm: KT Bilingual (Korean,English) Generative Pre-trained Transformer","AI-HUB,National Institute of Korean Language Corpus","The Mi:dm-bitext-S model was pre-trained using Korean/English publicly available data. For fine-tuning, we used also publicly available data and went through some processing or refining. KT collected public data directly or obtained it under legal permission conditions. The korean corpus data from AI-HUB (https://www.aihub.or.kr/) and the National Institute of Korean Language (https://corpus.korean.go.kr/) were used in the pre-training stage.

We did not use any customer data held by KT.",,,Unverified,,,,7000000000.00,7B,,,,,,,,,,,,2024-05-16 06:55,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
A.X (Adot) 39B,"Language,Speech",Chat,2023-05-15,,Hosted access (no API),,Unreleased,"https://a.sktelecom.com/
https://m.ddaily.co.kr/page/view/2023092614585090151",SK Telecom,,,Korea (Republic of),,,,,,,,,Unverified,,,,39000000000.00,,,,,,,,,,,,,2024-05-16 14:19,Natalia Martemianova,,,,,,,,,,,,Industry,,1350000000000000000,,,Industry,,,
A.X (Adot) 7B,"Language,Speech",Chat,2023-08-15,,,,Unreleased,"https://a.sktelecom.com/
https://m.ddaily.co.kr/page/view/2023092614585090151",SK Telecom,,,Korea (Republic of),,,,,,,,,Unverified,,,,7000000000.00,,,,,,,,,,,,,2024-05-16 13:54,Natalia Martemianova,,1,,,,,,,,,,Industry,,,,,Industry,,,
Piecewise linear model,Vision,Image classification,1973-11-01,,,,,https://ieeexplore.ieee.org/document/4309314,University of Kansas,,,United States of America,"R. Haralick, K. Shanmugam, I. Dinstein","Historical significance,Highly cited",,Textural Features for Image Classification,,,314,"""Number of training samples = 314;""",Unverified,,,,357.00,"16 input features + bias = 17 input features
7*6/2 = 21 Hyperplanes
17*21 = 357 parameters
""For the multicategory problem involving NR categories, a total of NR(NR - 1)/2 hyperplanes are used to partition the pattern space.""
""The input variables to the classifier consisted of the mean variance of the four textural features (f1,f2,f3, andfg obtained from the distance 1 gray-tone spatial-dependence matrices) and eight spectral features (comprised of the mean variance of the image gray-tone values) in each of the four spectral bands""
",,,,,,,,,,,"Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.",2024-05-22 12:56,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
Decision tree adaline,Medicine,Medical diagnosis,1969-05-01,,,,,https://pubmed.ncbi.nlm.nih.gov/5820353/,Tokyo Medical and Dental University,,,Japan,"T Sano, S Tsuchiya, F Suzuki",Historical significance,,A use of Adaline as an automatic method for interpretation of the electrocardiogram and the vectorcardiogram,,,80,40 positive and negative training examples (p. 2),Unverified,,,,2450.00,"5 adaline were trained on binary decisions (p. 1)
Each adaline had up to 490 input weights (“meshes”)
Total parameters = 5*490=2450
",,,,,,,,,,,"A learning machine"" adaline neuron"" was employed for automatic diagnosis of the vectorcardiogram and the electrocardiogram. The frontal circle and the horizontal circle were divided into 480 meshes. The features were expressed by a binary digit, whether the vector loops passed through each mesh or not. In a part of the trials, 5 sets of binary digits were applied in addition to QRS duration and direction of inscription of QRS loops and T loops. In this trial a total of 490 meshes were used. Vectorcardiograms were taken by FRANK'S method in 235 cases. Several methods of adaline usage were tried. The best result was obtained so far by successive dichotomies based on the principle of the logical decision tree. First the normal patterns and the abnormal patterns were divided. The correct ratio was 96.5% when the 490 meshes were employed, cases of an output value within±10 units being regarded as undecided. Next the abnormal cases were divided into two groups depending on whether the QRS duration was more than 0.12 seconds or less. The group of cases with a QRS duration of less than 0.12 seconds was divided into right ventricular hypertrophy and others. The correct ratio was 98.6%. The remaining cases were divided into left ventricular hypertrophy and myocardial infarction, the correct ratio being 88.8%. The group of cases with a QRS duration of more than 0.12 seconds was easily divided into complete left and right bundle branch block in all cases. Here the number of meshes could be decreased to 59 meshes without changing the accuracy appreciably. This attempt showed that the application of the adaline for automatic diagnosis of the vectorcardiogram and the electrocardiogram is promising.",2024-05-22 12:57,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
Gemini 1.5 Flash,"Multimodal,Language,Vision,Audio","Chat,Audio speech recognition,Image captioning,Visual question answering",2024-05-10,,API access,,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,Google DeepMind,,,Multinational,Gemini Team,,,"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of
context",,"""Our pre-training dataset includes data sourced across many different domains, including web documents and code, and incorporates image, audio, and video content. For the instructiontuning phase we finetuned Gemini 1.5 models on a collection of multimodal data (containing paired instructions and appropriate responses), with further tuning based on human preference data. We refer readers to the Gemini 1.0 Technical Report (Gemini-Team et al., 2023) for further information.""
",,,Unverified,,,,,,,,,,"""Gemini 1.5 models are trained on multiple 4096-chip pods of Google’s TPUv4 accelerators, distributed across multiple datacenters""",Google TPU v4,,,,,"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra’s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5’s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professions on their completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.
",2024-05-20 04:36,Natalia Martemianova,,,,,,,,,,,,Industry,,,,,Industry,,,
HyperCLOVA 204B,Language,,2021-09-10,,,,Unreleased,,NAVER,,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Korea (Republic of),,SOTA improvement,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""",,Unspecified unreleased,,,,Speculative,,,92.00,204000000000.00,https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,,,,,,NVIDIA A100,Self-supervised learning,$103802.31,,Industry,,2024-05-20 07:52,Natalia Martemianova,,0,,,,,,HyperCLOVA,,,,Industry,checked,1,,,Industry,checked,,
Hierarchical Cognitron,Other,Pattern recognition,1984-04-01,,,,,https://link.springer.com/article/10.1007/BF00337157,NHK Broadcasting Science Research Laboratories,,,Japan,K. Fukushima,Historical significance,,A hierarchical neural network model for associative memory,,,5,"""Five training patterns used for the self-organization are shown in Fig. 4""",Unverified,,,,9315.00,"Parameters 5*5*9*3*3 + 3*3*9*3*3*9 + 9*3*3*9 = 9315
""The numbers of excitatory cells in these four layers were: 7x7 in U0, 5x5 in  U1, 3x3 in U2, and 9 in U3""
""Each feature-extracting cell in layer U1 receives excitatory modifiable afferent connections from 3x3 cells in layer U0""
""On the other hand, each feature, each extracting cell in layers U2 and U3 receives excitatory modifiable connections from all 9 cells in each of the 3 x 3 hypercolumns in the layer preceding it. Therefore, it receives 3 x 3 x 9 afferent excitatory modifiable connections altogether""
",,,,,,,,,,,"A hierarchical neural network model with feedback interconnections, which has the function of associative memory and the ability to recognize patterns, is proposed. The model consists of a hierarchical multi-layered network to which efferent connections are added, so as to make positive feedback loops in pairs with afferent connections. The cell-layer at the initial stage of the network is the input layer which receives the stimulus input and at the same time works as an output layer for associative recall. The deepest layer is the output layer for pattern-recognition. Pattern-recognition is performed hierarchically by integrating information by converging afferent paths in the network. For the purpose of associative recall, the integrated information is again distributed to lower-order cells by diverging efferent paths. These two operations progress simultaneously in the network. If a fragment of a training pattern is presented to the network which has completed its self-organization, the entire pattern will gradually be recalled in the initial layer. If a stimulus consisting of a number of training patterns superposed is presented, one pattern gradually becomes predominant in the recalled output after competition between the patterns, and the others disappear. At about the same time when the recalled pattern reaches a steady state in he initial layer, in the deepest layer of the network, a response is elicited from the cell corresponding to the category of the finally-recalled pattern. Once a steady state has been reached, the response of the network is automatically extinguished by inhibitory signals from a steadiness-detecting cell. If the same stimulus is still presented after inhibition, a response for another pattern, formerly suppressed, will now appear, because the cells of the network have adaptation characteristics which makes the same response unlikely to recur. Since inhibition occurs repeatedly, the superposed input patterns are recalled one by one in turn.",2024-05-20 09:42,Lovis Heindrich,,,,,,,,,,,,Industry,,,,,Industry,,,
MLP baggage detector,Vision,Image classification,1989-01-01,,,,,https://www.semanticscholar.org/paper/Detection-of-explosives-in-checked-airline-baggage-Shea-Lin/71da4057401f459bc079696a029aee45a0a89728,Science Applications International Corporation / SAIC,,,United States of America,"Patrick M Shea, Vincent Lin",Historical significance,One of the first real-world use cases of neural networks,Detection of explosives in checked airline baggage using an artificial neural system,,,40000,"""The database contains about 20,000 bags without simulants and a like number with; although, because of changes made in the systems as they were developed, not all measurements are on the same basis.""",Unverified,,,,,"3 layer network, input layer (<20), hidden layer, output layer (3)",2000.00,,,,,,,,,,"An artificial neural system (ANS) has been applied to the problem of discriminating between suitcases with and without explosives. The input to the ANS was data gathered during the field tests of a prototype explosive detection system. The performance of the ANS is contrasted with the standard statistical technique (discriminant analysis) used, and is shown to exceed the performance of the standard technique over a substantial range. The system that generated the data, the nature of the data, the basics of discriminant analysis, and the technique used in developing the network are described.",2024-05-22 13:01,Lovis Heindrich,,,,,,,,,,,,Industry,,,,,Industry,,,
Truck backer-upper,Driving,Self-driving car,1989-06-18,,,,,https://ieeexplore.ieee.org/document/118723,Stanford University,,,United States of America,"Derrick Nguyen, Bernard Widrow",Historical significance,,"The truck backer-upper: an example of self-learning in neural networks
",,,,,Unverified,,,,805.00,6*25+25+8*45+45*6=805 (see Figure 6),,,,,,,,,,,"Neural networks can be used to solve highly nonlinear control problems. A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed. It is not yet known how to design a controller to perform this steering task. Nevertheless, the neural net was able to learn of its own accord to do this, regardless of initial conditions. Experience gained with the truck backer-upper should be applicable to a wide variety of nonlinear control problems.",2024-05-22 01:56,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
SC-NLM,Multimodal,Image captioning,2014-11-10,,,,,https://www.semanticscholar.org/paper/Unifying-Visual-Semantic-Embeddings-with-Multimodal-Kiros-Salakhutdinov/2e36ea91a3c8fbff92be2989325531b4002e2afc,University of Toronto,,,Canada,"Ryan Kiros, R. Salakhutdinov, R. Zemel",Highly cited,,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,"COCO,Flickr30K Entities",,600000,"Our LSTM encoder and SC-NLM decoder were trained by concatenating the Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined give us over 100,000 images and over 500,000 descriptions for training",Unverified,,,,,,,,,,,,,,,,"Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",2024-05-22 03:27,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
AdaRNN,Language,Sentiment classification,2014-06-01,,,,,https://www.semanticscholar.org/paper/Adaptive-Recursive-Neural-Network-for-Twitter-Dong-Wei/06e122f475a21d92dba137609c40f35690217475,Beihang University,,,China,"Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, M. Zhou, Ke Xu",Highly cited,,Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,,,6248,"""Training data consists of 6,248 tweets,""",Unverified,,,,13040.00,"D=25 ""For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN.""
Composition matrices: ""W ∈ R D×2D is the composition matrix, and b is the bias vector.""
C=10 ""We employ 10 composition matrices in AdaRNN.""
Combination matrix: ""S ∈ R C×(2D+|e|) is the matrix used to determine which composition function we use, vl , vr are the left and right child vectors, and e are external feature vector. In this work, e is a one-hot binary feature vector which indicates what the dependency type is.""
|e| > 4: (see Figure 2)
Weights: 10 * 25 * 50 + 10 * (50+4) =13040 (ignoring embedding to the 25 dimension embedding space)",,,,,,,,,,,"We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.",2024-05-22 04:30,Lovis Heindrich,,,,,,,,,,,,Academia,,,,,Academia,,,
,,,,,,,,,,,,,,,,,,,,,Unverified,,,,,,,,,,,,,,,,,2024-05-23 04:46,Natalia Martemianova,,,,,,,,,,,,,,,,,,,,