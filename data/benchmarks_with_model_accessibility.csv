System,Model size (parameters),Active Parameters,Dataset size,Date,Open/Closed,Training compute (FLOP),Training compute notes,BBH,GPQA,MMLU,HELM MMLU,SEAL Coding,SEAL Instruction Following,SEAL Math,LMSys Elo,LMSys Elo Notes,LMSys Elo 95% CI,BBH Notes,GPQA Notes,MMLU Notes,HELM MMLU Notes,Trust in benchmark results,Trust notes
BLOOM-176B,176000000000,"176,000,000,000",390000000000,2022-11-09,Open,4.12E+23,,0.4491,,0.3913,,,,,,,,,,,,0,
BloombergGPT,50000000000,"50,000,000,000",708000000000,2023-03-30,Closed,2.12E+23,,0.4197,,0.3918,,,,,,,,,,,,0,
Camelidae-8x34B,,,,2024-01-05,Open,,,,,0.756,,,,,,,,,,,,0,
ChatGLM-6B,6000000000,"6,000,000,000",,2023-03-01,Open,,,0.1873,,,,,,,880,,,,,,,0,
ChatGLM2-12B-base,12000000000,"12,000,000,000",,2023-06-25,Open,,,0.3602,,,,,,,,,,,,,,0,
ChatGLM2-6B-base,6000000000,"6,000,000,000",,2023-06-25,Open,,,0.3368,,,,,,,924,,,,,,,0,
ChatGLM3-6B,6000000000,"6,000,000,000",,2023-10-27,Open,5.04E+22,,0.661,,,,,,,955,,,,,,,0,
Chinchilla 70B,,"70,000,000,000",,2022-03-29,Closed,5.76E+23,,,,0.675,,,,,,,,,,,,0,
Claude 2,,,,2023-07-11,Closed,,,,0.353,0.785,,,,,1132,,,,Epoch evaluation,"Actually CoT, so probably an overestimate. HELM gives 73.5%.",,0,
Claude 2.1,,,,2023-11-21,Closed,,,,0.361,,,,,,,,,,Epoch evaluation,,,0,Doesn't perform worse on GSM1k relative to GSM8k
Claude 3 Haiku,,,,2024-03-07,Closed,,,,0.344,,,,,,,,,,Epoch evaluation,,,0,
Claude 3 Opus,,,,2024-03-04,Closed,,,0.868,0.479,0.868,0.846,1060,0.847,0.9519,1248,20240229,,3-shot,Epoch evaluation,,20240620,1,Doesn't perform worse on GSM1k relative to GSM8k
Claude 3 Sonnet,,,,2024-03-04,Closed,,,,0.391,0.79,,980,0.833,0.9328,1201,20240229,,,Epoch evaluation,,20240229,1,Doesn't perform worse on GSM1k relative to GSM8k
Claude 3.5 Sonnet,,,,2024-06-20,Closed,,,0.931,0.562,0.887,0.865,1176,0.908,,1271,,"""+3/-4""",3-shot,Epoch evaluation,5-shot,,0,
code-cushman-001,12000000000,"12,000,000,000",400000000000,2021-07-07,Closed,2.88E+22,,0.3312205128,,,,,,,,,,,,,,0,
code-davinci-001,175000000000,"175,000,000,000",300000000000,2021-07-07,Closed,3.15E+23,,0.3181948718,,,,,,,,,,,,,,0,
code-davinci-002,,,,2022-03-01,Closed,2.58E+24,,0.528,,0.682,,,,,,,,,,,,0,
CodeLlama 34B Instruct,,"34,000,000,000",,2023-08-24,Open,5.30E+23,,,,,,699,0.668,0.3751,1042,,,,,,,0,
Cohere Command R+,,"104,000,000,000",,2024-04-04,Open,,,,,0.757,,,,,1190,,,,,https://artificialanalysis.ai/models/command-r-plus,,0,
DBRX-Instruct,,"36,000,000,000",,2024-03-27,Open,2.60E+24,,0.485,0.304,0.737,,,,,1103,,,https://x.com/_lewtun/status/1774194211891396983,Epoch evaluation,,,-1,Performs worse on GSM1k relative to GSM8k
DeepSeek Chat,,,,2023-11-29,Open,,,,0.415,,,,,,,,,,Epoch evaluation,,,0,
DeepSeek-67B,67000000000,"67,000,000,000",2000000000000,2024-01-05,Open,8.04E+23,,,0.211,0.713,,,,,1076,chat,,,Epoch evaluation,,,-1,"Performs worse on GSM1k relative to GSM8k, MMLU-GPQA performance difference is relatively large"
DeepSeek-7B,7000000000,"7,000,000,000",2000000000000,2023-11-29,Open,8.40E+22,,,,0.482,,,,,,,,,,,,0,
DeepSeek-Coder,,,,2023-11-01,Open,,,,0.430,,,,,,,,,,Epoch evaluation,,,0,
DeepSeek-Coder-V2,236000000000,"236,000,000,000",10200000000000,2024-06-17,Open,1.28E+24,,0.839,,0.792,,,,,,,,,,,,0,
DeepSeek-V2,236000000000,"236,000,000,000",,2024-05-07,Open,1.00E+24,,0.789,,0.785,,,,,,,,,,,,0,
Falcon 180B,180000000000,"180,000,000,000",3500000000000,2023-09-06,Open,3.78E+24,,0.54,,0.705,,,,,1033,chat,,,,,,-1,Chat model performs worse on GSM1k relative to GSM8k
Falcon 40B,40000000000,"40,000,000,000",1500000000000,2023-03-15,Open,3.60E+23,,0.371,,0.554,,,,,,,,,,,,0,
Falcon 7B,7000000000,"7,000,000,000",1500000000000,2023-03-15,Open,6.30E+22,,0.28,,0.262,,,,,,,,,,,,0,
Flan-cont-PaLM,62500000000,"62,500,000,000",795000000000,2022-04-04,Closed,2.98E+23,,0.51,,0.661,,,,,,,,,,,,0,
Flan-PaLM 540B,540000000000,"540,000,000,000",780000000000,2022-04-04,Closed,2.53E+24,,0.579,,0.735,,,,,,,,,,,,0,
Flan-PaLM 63B,62500000000,"62,500,000,000",780000000000,2022-04-04,Closed,2.93E+23,,0.475,,0.596,,,,,,,,,,,,0,
Flan-PaLM 9B,8630000000,"8,630,000,000",780000000000,2022-04-04,Closed,4.04E+22,,0.364,,0.493,,,,,,,,,,,,0,
Flan-T5-Base,250000000,"250,000,000",150000000000,2019-10-23,Open,2.25E+20,,0.313,,0.359,,,,,,,,,,,,0,
Flan-T5-Large,780000000,"780,000,000",150000000000,2019-10-23,Open,7.02E+20,,0.375,,0.451,,,,,,,,,,,,0,
Flan-T5-Small,80000000,"80,000,000",150000000000,2019-10-23,Open,7.20E+19,,0.291,,0.287,,,,,,,,,,,,0,
Flan-T5-XL,3000000000,"3,000,000,000",150000000000,2019-10-23,Open,2.70E+21,,0.41,,0.524,,,,,,,,,,,,0,
Flan-T5-XXL,11000000000,"11,000,000,000",150000000000,2019-10-23,Open,9.90E+21,,0.453,,0.551,,,,,,,,,,,,0,
Flan-U-PaLM,540000000000,"540,000,000,000",780000000000,2022-04-04,Closed,2.53E+24,,0.593,,0.741,,,,,,,,,,,,0,
Gemini 1.0 Pro,,,,2023-12-06,Closed,,,,0.344,0.718,,790,0.736,0.7983,1111,,,,Epoch evaluation,,,1,Doesn't perform worse on GSM1k relative to GSM8k
Gemini 1.5 Flash Preview,,,,2024-05-14,Closed,,,0.855,0.405,0.789,,1026,0.826,0.9012,1228,gemini-1.5-flash-api-0514,,,Epoch evaluation,,,0,
Gemini 1.5 Pro (April 2024),,,,2024-04-09,Closed,,,,0.447,0.819,0.81,996,0.829,0.9054,1257,Gemini-1.5-Pro-API-0409-Preview,"""+4/-3""",,Epoch evaluation,,0409 preview,1,Doesn't perform worse on GSM1k relative to GSM8k
Gemini 1.5 Pro (Exp-0801),,,,2024-08-01,Closed,,,,,,,,,,1300,,"""+6/-5""",,,,,0,
Gemini 1.5 Pro (May 2024),,,,2024-05-14,Closed,,,0.892,,0.859,0.827,1095,0.852,0.9228,1261,Gemini-1.5-Pro-API-0514,"""+3/-3""",3-shot,,1,,0,
Gemini Ultra,,,,2023-12-06,Closed,5.00E+25,,,,0.8396,,,,,,,,,,,,0,
Gemma 1 2B,2000000000,"2,000,000,000",,2024-04-09,Open,,,,,0.423,,,,,,,,,,,,0,
Gemma 2 27B,,"27,000,000,000",,2024-06-24,Open,2.10E+24,,0.749,0.355,0.752,,,,,1218,gemma-2-27b-it,,,Epoch evaluation,,,-1,
Gemma 2 9B,,"9,000,000,000",,2024-06-24,Open,4.32E+23,,0.682,0.324,0.713,,,,,1188,,,,Epoch evaluation,,,0,
GLM 130B,130000000000,"130,000,000,000",,2022-10-05,Open,3.55E+23,,,,0.448,,,,,,,,,,,,0,
GLM-4 (0116),,,,2024-01-17,Closed,1.20E+25,,,0.360,,,,,,,,,,Epoch evaluation,,,0,
Gopher 280B,,"280,000,000,000",,2021-12-08,Closed,6.31E+23,,,,0.6,,,,,,,,,,,,0,
GPT-3.5-turbo-16k,,,,2023-06-13,Closed,2.60E+24,Assuming the same as our estimate for GPT-3.5,,0.288,0.7,,,,,1117,,,,Epoch evaluation,,,1,Doesn't perform worse on GSM1k relative to GSM8k
GPT-3.5-turbo-16k-0125,,,,2024-01-25,Closed,2.60E+24,Assuming the same as our estimate for GPT-3.5,,0.307,,,,,,,,,,Epoch evaluation,,,0,
GPT-4 (original),,"280,000,000,000",,2023-03-15,Closed,2.10E+25,,,,0.864,,,,,1186,,,,,,,1,Doesn't perform worse on GSM1k relative to GSM8k
gpt-4-0125-preview,,,,2024-01-25,Closed,2.10E+25,Assuming the same as original,,,0.854,,1146,0.876,0.951,1245,,,,,,,-1,MMLU-GPQA performance difference is relatively large (based on external GPQA evaluation)
gpt-4-0613,,,,2023-06-13,Closed,2.10E+25,Assuming the same as original,,0.328,0.813,0.824,,,,1162,,,,Epoch evaluation,https://github.com/GPT-Fathom/GPT-Fathom?tab=readme-ov-file,,-1,MMLU-GPQA performance difference is relatively large
gpt-4-1106-preview,,,,2023-11-06,Closed,2.10E+25,Assuming the same as original,,0.425,,,,,,,,,,Epoch evaluation,,,0,
gpt-4-turbo-2024-04-09,,,,2024-04-09,Closed,,,,,,,,,,1257,,"""+3/-2""",,,,,1,Doesn't perform worse on GSM1k relative to GSM8k
GPT-4o,,,,2024-05-13,Closed,,,,0.490,0.872,0.842,1138,0.886,0.9485,1286,2024-05-13,"""+3/-2""",,Epoch evaluation,MMLU Pro paper,2024-05-13,0,
GPT-4o mini,,,,2024-07-18,Closed,,,,0.403,0.82,,,,,1280,2024-07-18,"""+6/-4""",,Epoch evaluation,Suspect this is actually 0-shot CoT.,,0,
GPT-4o mini-0718,,,,2024-07-18,Closed,,,,0.400,,,,,,,,,,Epoch evaluation,,,0,
GPT-4o-0806,,,,2024-08-06,Closed,,,,0.486,,,,,,,,,,Epoch evaluation,,,0,
GPT-NeoX 20B,20000000000,"20,000,000,000",473000000000,2022-02-09,Open,5.67E+22,,0.4025,,0.336,,,,,,,,,,,,0,
Grok-1,,"78,500,000,000",,2023-11-03,Open,3.00E+24,slightly speculative but not too much - xAI says it's the the compute class of GPT-3.5,,,0.73,,,,,,,,,,,,0,
Hermes-2 Theta Llama 3 70B,,,,2024-06-14,Open,,,,0.366,,,,,,,,,,Epoch evaluation,,,0,
Inflection-2,,,,2023-11-22,Closed,1.00E+25,,,,0.796,,,,,,,,,,,,0,
Inflection-2.5,,,,2024-03-07,Closed,1.00E+25,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs."" This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. But Inflection 2, the previous model, was trained on ~1e25 FLOP per Inflection. Inflection-2.5 also does better on benchmarks than 2. Intuitively Inflection-2.5 would be trained on appreciably more compute. 1e25 seems like a rough, perhaps conservative guess given all this.",,,0.796,,,,,,,,,,Inflection-2 result for few-shot no-CoT.,,0,
Jamba 1.5 Large,,"94,000,000,000",,2024-08-22,Open,,probably 24 scale. 2T tokens would be 1.1e24 FLOP,,,0.8,,,,,,,,,,,,0,
Llama 3 70B,,"70,000,000,000",,2024-04-18,Open,6.30E+24,,0.655,0.385,0.82,,981,0.855,0.9012,1206,instruct,,,Epoch evaluation,,,1,Doesn't perform worse on GSM1k relative to GSM8k
Llama 3 8B,,"8,000,000,000",,2024-04-18,Open,7.20E+23,,,0.297,0.684,,,,,1152,instruct,,,Epoch evaluation,,,-1,Performs worse on GSM1k relative to GSM8k
Llama 3.1 405B,,"405,000,000,000",,2024-07-23,Open,3.80E+25,,,0.509,0.873,0.845,1123,0.9035,0.956,1262,instruct,"""+6/-7""",,Epoch evaluation,0-shot,Instruct Turbo,0,
Llama 3.1 70B,,"70,000,000,000",,2024-07-23,Open,6.30E+24,"Sounds like all models were trained on 15T tokens: ""All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity. [...] We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2."" 6 * 70e9 * 15e12 = 6.3e24 FLOP.",,0.455,0.836,0.801,,,,1243,,,,Epoch evaluation,,Instruct Turbo,0,
Llama 3.1 8B,,,,2024-07-23,Open,7.20E+23,"Sounds like all models were trained on 15T tokens: ""All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity. [...] We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2."" 6 * 70e9 * 15e12 = 6.3e24 FLOP.",,0.251,,,,,,,,,,Epoch evaluation,,,0,
LLaMa-1 13B,13000000000,"13,000,000,000",1000000000000,2023-02-24,Open,7.80E+22,,0.37,,0.469,,,,,799,,,,,,,0,
LLaMa-1 33B,33000000000,"33,000,000,000",1400000000000,2023-02-24,Open,2.77E+23,,0.398,,0.578,,,,,,,,,,,,0,
LLaMa-1 65B,65000000000,"65,000,000,000",1400000000000,2023-02-24,Open,5.46E+23,,0.435,,0.634,,,,,,,,,,,,0,
LLaMa-1 7B,7000000000,"7,000,000,000",1000000000000,2023-02-24,Open,4.20E+22,,0.303,,0.351,,,,,,,,,,,,0,
LLaMa-2 13B,13000000000,"13,000,000,000",2000000000000,2023-07-18,Open,1.56E+23,,0.394,,0.548,,,,,1063,chat,,,,,,0,
LLaMa-2 34B,34000000000,"34,000,000,000",2000000000000,2023-07-18,Open,4.08E+23,,0.441,,0.626,,,,,,,,,,,,0,
LLaMa-2 70B,70000000000,"70,000,000,000",2000000000000,2023-07-18,Open,8.40E+23,,0.512,0.251,0.689,,,,,1093,chat,,,Epoch evaluation,,,0,
LLaMa-2 7B,7000000000,"7,000,000,000",2000000000000,2023-07-18,Open,8.40E+22,,0.326,,0.453,,,,,1036,chat,,,,,,0,
Mistral 7B,7000000000,"7,000,000,000",,2023-10-10,Open,,,0.375,0.145,0.601,,,,,1008,instruct,,,Epoch evaluation,,,-1,"Performs worse on GSM1k relative to GSM8k, MMLU-GPQA performance difference is relatively large"
Mistral Large,,,,2024-02-26,Closed,1.00E+25,,,,0.812,,915,0.854,0.8747,1157,mistral-large-2402,,,,,,1,Doesn't perform worse on GSM1k relative to GSM8k
Mistral Large 2,,"123,000,000,000",,2024-07-24,Open,2.13E+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",,0.490,0.84,,,,,,,,,Epoch evaluation,,,0,
Mistral Nemo,,,,2024-07-18,Open,,,,0.301,,,,,,,,,,Epoch evaluation,,,0,
Mixtral 8x22B,,"39,000,000,000",,2024-04-17,Open,1.00E+24,,0.789,0.335,0.778,,,,,1146,instruct-v0.1,,,Epoch evaluation,,,-1,Performs worse on GSM1k relative to GSM8k
Mixtral 8x7B,,,,2023-12-11,Open,,,,0.294,,,,,,,,,,Epoch evaluation,,,0,
MPT 30B,30000000000,"30,000,000,000",1050000000000,2023-06-22,Open,1.89E+23,,0.38,,0.48,,,,,1045,chat,,,,,,0,
MPT 7B,7000000000,"7,000,000,000",1000000000000,2023-05-05,Open,4.20E+22,,0.31,,0.308,,,,,928,chat,,,,,,0,
Nemotron-4-340B Base,,"340,000,000,000",,2024-06-14,Open,1.80E+25,,0.8544,,0.811,,,,,,,,,,,,0,
Nemotron-4-340B Instruct,,"340,000,000,000",,2024-06-14,Open,1.80E+25,,,0.422,0.787,,,,,1209,,,,Epoch evaluation,0-shot,,0,
o1 mini,,,,2024-09-12,Closed,,,,0.607,,,,,,,,,,Epoch evaluation,,,0,
o1 preview,,,,2024-09-12,Closed,,,,0.697,,,,,,,,,,Epoch evaluation,https://openai.com/index/learning-to-reason-with-llms/,,0,
OPT-175B,175000000000,"175,000,000,000",180000000000,2022-05-02,Open,1.89E+23,,0.302,,,,,,,,,,,,,,0,
OPT-1B,1300000000,"1,300,000,000",180000000000,2022-05-02,Open,1.40E+21,,0.279,,,,,,,,,,,,,,0,
OPT-30B,30000000000,"30,000,000,000",180000000000,2022-05-02,Open,3.24E+22,,0.283,,,,,,,,,,,,,,0,
OPT-66B,66000000000,"66,000,000,000",180000000000,2022-05-02,Open,7.13E+22,,0.3958,,0.3599,,,,,,,,,,,,0,
PaLM 540B,540000000000,"540,000,000,000",780000000000,2022-04-04,Closed,2.53E+24,,0.491,,0.713,,,,,,,,,,,,0,
PaLM 63B,62500000000,"62,500,000,000",780000000000,2022-04-04,Closed,2.93E+23,,0.374,,0.551,,,,,,,,,,,,0,
PaLM 9B,8630000000,"8,630,000,000",780000000000,2022-04-04,Closed,4.04E+22,,0.308,,0.243,,,,,,,,,,,,0,
PaLM-2,340000000000,"340,000,000,000",3600000000000,2023-05-10,Closed,7.34E+24,,0.624,,0.783,,,,,,,,,,,,0,
Qwen 7B,,"7,000,000,000",,2023-09-28,Open,1.01E+23,,,,0.567,,,,,,,,,,,,0,
Qwen1.5 110B,,,,2024-04-25,Open,,,,0.301,,,,,,,,,,Epoch evaluation,,,0,
Qwen1.5 32B,,,,2024-04-01,Open,,,,0.185,,,,,,,,,,Epoch evaluation,,,0,
Qwen1.5 72B,,,,2024-01-30,Open,1.30E+24,,,0.293,,,,,,,,,,Epoch evaluation,,,0,
Qwen2-72B,,"72,000,000,000",,2024-06-06,Open,3.00E+24,,0.824,0.378,0.842,0.824,,,,1187,instruct,,,Epoch evaluation,,Instruct,-1,MMLU-GPQA performance difference is relatively large
Qwen2idae-16x14B,,"15,000,000,000",,2024-01-05,Open,,,,,0.667,,,,,,,,,,,,0,
Random chance,,,,,,1.00E+20,,0.25,0.250,0.25,0.25,0,0,0,0,,,,Epoch evaluation,,,0,
Reka Core,,,,2024-04-15,Closed,8.40E+24,Rough guess from hardware.,,,0.832,,,,,1199,20240501,,,,,,-1,MMLU-GPQA performance difference is relatively large (based on external GPQA evaluation)
StableLM-3B-4E1T,3000000000,"3,000,000,000",4000000000000,2023-09-29,Open,7.20E+22,,,,0.4523,,,,,,,,,,,,0,
StableLM-alpha-7b,7000000000,"7,000,000,000",1500000000000,2023-04-03,Open,6.30E+22,,0.289,,0.2621,,,,,840,tuned,,InstructEval,,,,0,
StableLM-alpha-7b-v2,7000000000,"7,000,000,000",1500000000000,2023-08-05,Open,6.30E+22,,,,0.451,,,,,,,,,,,,0,
T5-Base,250000000,"250,000,000",150000000000,2019-10-23,Open,2.25E+20,,0.278,,0.257,,,,,,,,,,,,0,
T5-Large,780000000,"780,000,000",150000000000,2019-10-23,Open,7.02E+20,,0.277,,0.251,,,,,,,,,,,,0,
T5-Small,80000000,"80,000,000",150000000000,2019-10-23,Open,7.20E+19,,0.27,,0.267,,,,,,,,,,,,0,
T5-XL,3000000000,"3,000,000,000",150000000000,2019-10-23,Open,2.70E+21,,0.274,,0.257,,,,,,,,,,,,0,
T5-XXL,11000000000,"11,000,000,000",150000000000,2019-10-23,Open,9.90E+21,,0.295,,0.259,,,,,,,,,,,,0,
text-ada-001,350000000,"350,000,000",300000000000,2020-05-28,Closed,6.30E+20,,0.2928,,0.238,,,,,,,,,,,,0,
text-babbage-001,1300000000,"1,300,000,000",300000000000,2020-05-28,Closed,2.34E+21,,0.2888,,0.229,,,,,,,,,,,,0,
text-curie-001,6700000000,"6,700,000,000",300000000000,2020-05-28,Closed,1.21E+22,,0.2954871795,,0.237,,,,,,,,,,,,0,
text-davinci-001,175000000000,"175,000,000,000",300000000000,2020-05-28,Closed,3.15E+23,,0.336,,0.397,,,,,,,,,,davinci,,0,
text-davinci-002,,"175,000,000,000",,2022-03-01,Closed,2.58E+24,,0.486,,0.631,,,,,,,,,,,,0,
text-davinci-003,,"175,000,000,000",,2022-11-28,Closed,2.58E+24,,0.509,,0.648,,,,,,,,,,,,0,
U-PaLM,540000000000,"540,000,000,000",780000000000,2022-04-04,Closed,2.53E+24,,0.492,,0.715,,,,,,,,,,,,0,
XVerse-13B,13000000000,"13,000,000,000",1400000000000,2023-09-26,Open,1.09E+23,,0.3806,,0.551,,,,,,,,,,,,0,
XVerse-13B-2,13000000000,"13,000,000,000",3200000000000,2023-11-06,Open,2.50E+23,,,,0.612,,,,,,,,,,,,0,
XVerse-65B,,"65,000,000,000",,2023-11-05,Open,,,,,,,,,,,,,,,,,0,
XVerse-7B,,"7,000,000,000",,2023-09-26,Open,,,,,,,,,,,,,,,,,0,
Yi-1.5-34B,,,,2024-05-10,Open,,,,0.060,,,,,,,,,,Epoch evaluation,,,0,
Yi-34B,34000000000,"34,000,000,000",3000000000000,2023-11-02,Open,6.12E+23,,0.543,0.165,0.7635,,,,,1111,chat,,,Epoch evaluation,,,-1,MMLU-GPQA performance difference is relatively large
Yi-6B,6000000000,"6,000,000,000",3000000000000,2023-11-02,Open,1.08E+23,,0.428,,0.6385,,,,,,,,,,,,0,
Yi-Large,,,,2024-05-13,Closed,1.80E+24,6ND = 6*100000000000*3000000000000=1.8e+24 (speculative confidence because training dataset size is very uncertain),,,0.838,,,,,1212,,,,,https://x.com/01AI_Yi/status/1789929378467426794,,0,