System,Author(s),Publication date,Reference,Citations,Peer reviewed?,Link,Parameters,Training Compute,Epoch,Epoch (pretrain),Epoch (finetune),uncertain,Pretrain Dataset Size,Inferred_compute (6ND),Finetune Dataset Size,Dataset Size,Dataset(s),Perplexity (WT103),Perplexity (WT2),Perplexity (PTB),Zero-shot?,Uses Cache,Architecture,Base Model,GitHub,Complete row,All ML Systems,System (from All ML Systems)
(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018-08-30,Direct Output Connection for a High-Rank Language Model,36.00,,https://arxiv.org/abs/1808.10143,114000000.00,,300.00,,,0.00,929000.00,191000000000000000.00,,929000.00,Penn TreeBank,,,47.17,0.0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,1,(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018-08-30,Direct Output Connection for a High-Rank Language Model,36.00,,https://arxiv.org/abs/1808.10143,185000000.00,,300.00,,,0.00,2080000.00,693000000000000000.00,,2080000.00,WikiText-2,,53.09,,0.0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,1,(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)
$\infty$-former (SM),"Pedro Henrique Martins, Zita Marinho, André F. T. Martins",2021-09-01,$\infty$-former: Infinite Memory Transformer,31.00,,https://arxiv.org/abs/2109.00301,117000000.00,1.2e+22,1.00,,,0.00,4000000000.00,2810000000000000000.00,255000000.00,4260000000.00,WikiText-103,16.61,,,1.0,0,Transformer,GPT,https://github.com/deep-spin/infinite-former,1,$\infty$-former (SM),$\infty$-former (SM)
1-layer-LSTM,"H. T. Kung, Bradley McDanel, Sai Qian Zhang",2020-07-13,Term Revealing: Furthering Quantization at Run Time on Quantized DNNs,9.00,,https://arxiv.org/pdf/2007.06389,86500000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,86.85,,0.0,0,Recurrent,LSTM,,1,1-layer-LSTM,1-layer-LSTM
2-layer skip-LSTM + dropout tuning (PTB),"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",2018-05-23,Pushing the bounds of dropout,14.00,,https://arxiv.org/abs/1805.09208,5400000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,55.30,0.0,0,Recurrent,LSTM,,1,2-layer skip-LSTM + dropout tuning (PTB),2-layer skip-LSTM + dropout tuning (PTB)
2-layer skip-LSTM + dropout tuning (WT2),"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",2018-05-23,Pushing the bounds of dropout,14.00,,https://arxiv.org/abs/1805.09208,5400000.00,,,,,0.00,2080000.00,0.00,,2080000.00,WikiText-2,,63.70,,0.0,0,Recurrent,LSTM,,1,2-layer skip-LSTM + dropout tuning (WT2),2-layer skip-LSTM + dropout tuning (WT2)
2-layer-LSTM+Deep-Gradient-Compression,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally",2017-12-05,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,1270.00,,https://arxiv.org/pdf/1712.01887,6020000.00,,40.00,,,0.00,929000.00,1340000000000000.00,,929000.00,,,,72.24,0.0,0,Recurrent,LSTM,https://github.com/synxlin/deep-gradient-compression,1,2-layer-LSTM+Deep-Gradient-Compression,2-layer-LSTM+Deep-Gradient-Compression
2nd order FOFE-FNNLM,"Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai",2015-05-06,A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models,18.00,,https://arxiv.org/abs/1505.01504,6000000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,108.00,0.0,0,Feed-forward,FNN-LM,,1,2nd order FOFE-FNNLM,2nd order FOFE-FNNLM
3-Layer-Tensor-Transformer+AdaHessian,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",2020-06-01,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,151.00,,https://arxiv.org/pdf/2006.00719,12000000.00,,30.00,,,1.00,929000.00,2010000000000000.00,0.00,929000.00,,,,51.50,0.0,0,Transformer,Tensorized Transformer,https://github.com/amirgholami/ADAHESSIAN.git,1,3-Layer-Tensor-Transformer+AdaHessian,3-Layer-Tensor-Transformer+AdaHessian
4 layer Densely Connected LSTM,"Fréderic Godin, Joni Dambre, Wesley De Neve",2017-07-19,Improving Language Modeling using Densely Connected Recurrent Neural Networks,7.00,,https://arxiv.org/pdf/1707.06130,14000000.00,,100.00,,,0.00,929000.00,7800000000000000.00,,929000.00,,,,76.80,0.0,0,Recurrent,LSTM,,1,4 layer Densely Connected LSTM,4 layer Densely Connected LSTM
4 layer QRNN (h=2500),"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2018-03-22,An Analysis of Neural Language Modeling at Multiple Scales,183.00,,https://arxiv.org/abs/1803.08240,26000000.00,240000000000000000.00,14.00,,,0.00,103000000.00,225000000000000000.00,0.00,103000000.00,WikiText-103,33.00,,,0.0,0,Recurrent/Convolutional,QRNN,https://github.com/salesforce/awd-lstm-lm,1,4 layer QRNN (h=2500),4 layer QRNN (h=2500)
4 layer QRNN + dynamic evaluation,"Dilin Wang, Chengyue Gong, Qiang Liu",2019-06-10,Improving Neural Language Modeling via Adversarial Training,95.00,,https://arxiv.org/abs/1906.03805,26000000.00,360000000000000000.00,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,31.60,,,0.0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,1,4 layer QRNN + dynamic evaluation,4 layer QRNN + dynamic evaluation
4-gram + 8 DENN,"Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran",2014-12-22,Diverse Embedding Neural Network Language Models,1.00,0.0,https://arxiv.org/pdf/1412.7063,16100000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,109.32,0.0,0,N-gram,,,1,4-gram + 8 DENN,4-gram + 8 DENN
6-Layer-Tensor-Transformer+AdaHessian,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",2020-06-01,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,151.00,,https://arxiv.org/pdf/2006.00719,85300000.00,,30.00,,,1.00,103000000.00,1580000000000000000.00,0.00,103000000.00,,19.90,,,0.0,0,Transformer,Tensorized Transformer,https://github.com/amirgholami/ADAHESSIAN.git,1,6-Layer-Tensor-Transformer+AdaHessian,6-Layer-Tensor-Transformer+AdaHessian
Adaptive Input Transformer + RD,"Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu",2021-06-28,R-Drop: Regularized Dropout for Neural Networks,245.00,,https://web.archive.org/web/20220518153557/https://arxiv.org/pdf/2106.14448.pdf,247000000.00,82000000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,,18.07,,,0.0,0,Transformer,Transformer-XL,https://github.com/dropreg/R-Drop,1,Adaptive Input Transformer + RD,Adaptive Input Transformer + RD
Adaptive Inputs + LayerDrop,"Angela Fan, Edouard Grave, Armand Joulin",2019-09-25,Reducing Transformer Depth on Demand with Structured Dropout,435.00,,https://arxiv.org/abs/1909.11556,423000000.00,,,,,0.00,103000000.00,0.00,103000000.00,206000000.00,WikiText-103,17.70,,,0.0,0,Transformer,Transformer-XL,,1,Adaptive Inputs + LayerDrop,Adaptive Inputs + LayerDrop
Adaptive LSTM + DeFINE,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019-11-27,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.00,,https://arxiv.org/abs/1911.12385,48700000.00,6200000000000000000.00,20.00,,,0.00,103000000.00,602000000000000000.00,,103000000.00,WikiText-103; Penn Treebank,35.94,,,0.0,0,Recurrent,LSTM,,1,Adaptive LSTM + DeFINE,Adaptive LSTM + DeFINE
ADP-FAIRSEQ,"Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",2022-10-26,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,0.00,1.0,https://web.archive.org/web/20221027013457/https://arxiv.org/pdf/2210.14431.pdf,247000000.00,,,,,0.00,103000000.00,,,103000000.00,,18.90,,,0.0,0,Transformer,Transformer-XL,https://github.com/ghrua/NgramRes,1,ADP-FAIRSEQ + NGRAMRES,ADP-FAIRSEQ + NGRAMRES
ADP-FAIRSEQ+NGRAMRES,"Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",2022-10-26,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,0.00,1.0,https://web.archive.org/web/20221027013457/https://arxiv.org/pdf/2210.14431.pdf,247000000.00,,,,,0.00,103000000.00,,101000000.00,204000000.00,,18.20,,,0.0,0,Transformer,Transformer-XL,https://github.com/ghrua/NgramRes,1,ADP-FAIRSEQ+NGRAMRES,ADP-FAIRSEQ+NGRAMRES
Adversarial + AWD-LSTM-MoS + partial shuffled,"Dilin Wang, Chengyue Gong, Qiang Liu",2019-06-10,Improving Neural Language Modeling via Adversarial Training,95.00,,https://arxiv.org/abs/1906.03805,22000000.00,,450.00,,,0.00,923000.00,54800000000000000.00,,923000.00,Penn TreeBank,,,46.01,0.0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,1,Adversarial + AWD-LSTM-MoS + partial shuffled,Adversarial + AWD-LSTM-MoS + partial shuffled
AdvSoft + 4 layer QRNN + dynamic evaluation,"Dilin Wang, Chengyue Gong, Qiang Liu",2019-06-10,Improving Neural Language Modeling via Adversarial Training,95.00,,https://arxiv.org/abs/1906.03805,26000000.00,360000000000000000.00,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,28.00,,,0.0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,1,AdvSoft + 4 layer QRNN + dynamic evaluation,AdvSoft + 4 layer QRNN + dynamic evaluation
AFP+FPI (PTB),"Zhengxiong Wang, Anton Ragni",2021-06-04,Approximate Fixed-Points in Recurrent Neural Networks,1.00,0.0,https://arxiv.org/pdf/2106.02417,2040000.00,,20.00,,,0.00,929000.00,227000000000000.00,,929000.00,Penn TreeBank,,,129.40,0.0,0,Recurrent,AFP,,1,AFP+FPI (PTB),AFP+FPI (PTB)
AFP+FPI (WT2),"Zhengxiong Wang, Anton Ragni",2021-06-04,Approximate Fixed-Points in Recurrent Neural Networks,1.00,0.0,https://arxiv.org/pdf/2106.02417,80200.00,,40.00,,,0.00,2080000.00,40000000000000.00,,2080000.00,WikiText-2,,149.35,,0.0,0,Recurrent,AFP,,1,AFP+FPI (WT2),AFP+FPI (WT2)
"ALiBi (L=3072, Lvalid = 3072)","Ofir Press, Noah A. Smith, Mike Lewis",2021-08-27,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",124.00,,https://arxiv.org/abs/2108.12409,1300000000.00,180000000000000000000.00,205.00,,,0.00,103000000.00,165000000000000000000.00,0.00,103000000.00,WikiText-103,18.30,,,0.0,0,Transformer,Transformer-XL,https://github.com/ofirpress/attention_with_linear_biases,1,"""ALiBi (L=3072, Lvalid = 3072)""","ALiBi (L=3072, Lvalid = 3072)"
All-attention network + adaptive span,"Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin",2019-07-02,Augmenting Self-attention with Persistent Memory,94.00,,https://arxiv.org/abs/1907.01470,133000000.00,46000000000000000000.00,,,,0.00,103000000.00,0.00,0.00,103000000.00,WikiText-103,20.60,,,0.0,0,Transformer,All-attention network,,1,All-attention network + adaptive span,All-attention network + adaptive span
Alleviated TOI 10 (PTB),"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019-09-18,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,0.00,1.0,https://arxiv.org/abs/1909.08700,,,1000.00,,,1.00,103000000.00,,0.00,103000000.00,Penn TreeBank,,,56.46,0.0,0,Recurrent,LSTM,https://github.com/nkcr/overlap-ml,0,Alleviated TOI 10 (PTB),Alleviated TOI 10 (PTB)
Alleviated TOI 10 (WT2),"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019-09-18,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,0.00,1.0,https://arxiv.org/abs/1909.08700,,,1000.00,,,1.00,103000000.00,,0.00,103000000.00,WikiText-2,,64.73,,0.0,0,Recurrent,LSTM,https://github.com/nkcr/overlap-ml,0,Alleviated TOI 10 (WT2),Alleviated TOI 10 (WT2)
Alleviated TOI 10 (WT103),"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019-09-18,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,0.00,1.0,https://arxiv.org/abs/1909.08700,,,1000.00,,,1.00,103000000.00,,0.00,103000000.00,WikiText-103,32.85,,,0.0,0,Recurrent,LSTM,https://github.com/nkcr/overlap-ml,0,Alleviated TOI 10 (WT103),Alleviated TOI 10 (WT103)
aLSTM(depth-2)+RecurrentPolicy (PTB),"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",2018-05-22,Breaking the Activation Function Bottleneck through Adaptive Parameterization,12.00,,https://arxiv.org/pdf/1805.08574,24000000.00,,180.00,,,0.00,929000.00,24100000000000000.00,,929000.00,,,,55.30,0.0,0,Recurrent,LSTM,https://github.com/flennerhag/alstm,1,aLSTM(depth-2)+RecurrentPolicy (PTB),aLSTM(depth-2)+RecurrentPolicy (PTB)
aLSTM(depth-2)+RecurrentPolicy (WT2),"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",2018-05-22,Breaking the Activation Function Bottleneck through Adaptive Parameterization,12.00,,https://arxiv.org/pdf/1805.08574,32000000.00,,190.00,,,0.00,2080000.00,75900000000000000.00,,2080000.00,,,64.50,,0.0,0,Recurrent,LSTM,https://github.com/flennerhag/alstm,1,aLSTM(depth-2)+RecurrentPolicy (WT2),aLSTM(depth-2)+RecurrentPolicy (WT2)
Amended-DARTS,"Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian",2019-10-25,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,48.00,,https://arxiv.org/pdf/1910.11831,23000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,54.80,0.0,0,NAS,DARTS,,1,Amended-DARTS,Amended-DARTS
AWD-FWM (PTB),"Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",2020-11-16,Learning Associative Inference Using Fast Weight Memory,29.00,,https://arxiv.org/abs/2011.07831,24000000.00,,1000.00,,,0.00,929000.00,134000000000000000.00,,929000.00,Penn TreeBank,,,54.48,0.0,0,Recurrent,LSTM,,1,AWD-FWM (PTB),AWD-FWM (PTB)
AWD-FWM (WT2),"Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",2020-11-16,Learning Associative Inference Using Fast Weight Memory,29.00,,https://arxiv.org/abs/2011.07831,37000000.00,,1600.00,,,0.00,2080000.00,739000000000000000.00,,2080000.00,WikiText-2,,61.65,,0.0,0,Recurrent,LSTM,,1,AWD-FWM (WT2),AWD-FWM (WT2)
AWD-LSTM,"Gábor Melis, Chris Dyer, Phil Blunsom",2017-07-18,On the State of the Art of Evaluation in Neural Language Models,555.00,,https://arxiv.org/abs/1707.05589,33000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,WikiText-2,,65.80,,0.0,0,Recurrent,LSTM,,1,AWD-LSTM,AWD-LSTM
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2017-08-07,Regularizing and Optimizing LSTM Language Models,1176.00,,https://arxiv.org/abs/1708.02182,24000000.00,,500.00,,,0.00,929000.00,66900000000000000.00,,929000.00,Penn TreeBank,,,52.80,0.0,1,Recurrent,LSTM,https://github.com/salesforce/awd-lstm-lm,1,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB)
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2017-08-07,Regularizing and Optimizing LSTM Language Models,1176.00,,https://arxiv.org/abs/1708.02182,33000000.00,,750.00,,,0.00,2080000.00,309000000000000000.00,,2080000.00,WikiText-2,,52.00,,0.0,1,Recurrent,LSTM,https://github.com/salesforce/awd-lstm-lm,1,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)
AWD-LSTM + DeFINE,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019-11-27,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.00,,https://arxiv.org/abs/1911.12385,20000000.00,,20.00,,,0.00,929000.00,2230000000000000.00,,929000.00,WikiText-103; Penn Treebank,,,54.20,0.0,0,Recurrent,LSTM,,1,AWD-LSTM + DeFINE,AWD-LSTM + DeFINE
AWD-LSTM + dynamic eval (PTB),"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2017-09-21,Dynamic Evaluation of Neural Sequence Models,130.00,,https://arxiv.org/abs/1709.07432,24000000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,51.10,0.0,0,Recurrent,LSTM,https://github.com/benkrause/dynamic-evaluation,1,AWD-LSTM + dynamic eval (PTB),AWD-LSTM + dynamic eval (PTB)
AWD-LSTM + dynamic eval (WT2),"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2017-09-21,Dynamic Evaluation of Neural Sequence Models,130.00,,https://arxiv.org/abs/1709.07432,33000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,WikiText-2,,44.30,,0.0,0,Recurrent,LSTM,https://github.com/benkrause/dynamic-evaluation,1,AWD-LSTM + dynamic eval (WT2),AWD-LSTM + dynamic eval (WT2)
AWD-LSTM + MoS + Partial Shuffled,"Dilin Wang, Chengyue Gong, Qiang Liu",2019-06-10,Improving Neural Language Modeling via Adversarial Training,95.00,,https://arxiv.org/abs/1906.03805,35000000.00,,750.00,,,0.00,2080000.00,328000000000000000.00,,2080000.00,WikiText-2,,38.07,,0.0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,1,AWD-LSTM + MoS + Partial Shuffled,AWD-LSTM + MoS + Partial Shuffled
AWD-LSTM + Phrase Induction + finetuning,"Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",2019-06-04,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",12.00,,https://arxiv.org/abs/1906.01702,24000000.00,,,,,0.00,,0.00,929000.00,929000.00,,,,55.70,0.0,0,Recurrent,LSTM,https://github.com/luohongyin/PILM,1,AWD-LSTM + Phrase Induction + finetuning,AWD-LSTM + Phrase Induction + finetuning
AWD-LSTM-DOC (fin) (23M),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018-08-30,Direct Output Connection for a High-Rank Language Model,36.00,,https://arxiv.org/abs/1808.10143,23000000.00,,300.00,,,0.00,2080000.00,86100000000000000.00,,2080000.00,WikiText-2,,,52.38,0.0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,1,AWD-LSTM-DOC (fin) (23M),AWD-LSTM-DOC (fin) (23M)
AWD-LSTM-DOC (fin) (37M),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018-08-30,Direct Output Connection for a High-Rank Language Model,36.00,,https://arxiv.org/abs/1808.10143,37000000.00,,300.00,,,0.00,2080000.00,139000000000000000.00,,2080000.00,WikiText-2,,58.03,,0.0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,1,AWD-LSTM-DOC (fin) (37M),AWD-LSTM-DOC (fin) (37M)
AWD-LSTM-DRILL + dynamic evaluation† (PTB),"Nikolaos Pappas, James Henderson",2019-05-14,Deep Residual Output Layers for Neural Language Generation,7.00,,https://arxiv.org/abs/1905.05513,24000000.00,,1000.00,,,0.00,929000.00,134000000000000000.00,,929000.00,Penn TreeBank,,,49.40,0.0,0,Recurrent,LSTM,https://github.com/idiap/drill,1,AWD-LSTM-DRILL + dynamic evaluation† (PTB),AWD-LSTM-DRILL + dynamic evaluation† (PTB)
AWD-LSTM-DRILL + dynamic evaluation† (WT2),"Nikolaos Pappas, James Henderson",2019-05-14,Deep Residual Output Layers for Neural Language Generation,7.00,,https://arxiv.org/abs/1905.05513,34000000.00,,1000.00,,,0.00,2080000.00,424000000000000000.00,,2080000.00,WikiText-2,,42.00,,0.0,0,Recurrent,LSTM,https://github.com/idiap/drill,1,AWD-LSTM-DRILL + dynamic evaluation† (WT2),AWD-LSTM-DRILL + dynamic evaluation† (WT2)
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)","Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",2018-09-18,FRAGE: Frequency-Agnostic Word Representation,152.00,,https://arxiv.org/abs/1809.06858,24000000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,46.54,0.0,0,Recurrent,LSTM,https://github.com/ChengyueGongR/Frequency-Agnostic,1,"""AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)""","AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)"
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)","Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",2017-11-10,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,358.00,,https://arxiv.org/abs/1711.03953,22000000.00,,1000.00,,,0.00,929000.00,123000000000000000.00,,929000.00,Penn TreeBank,,,47.69,0.0,0,Recurrent,LSTM,https://github.com/zihangdai/mos,1,"""AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)""","AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)"
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)","Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",2017-11-10,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,358.00,,https://arxiv.org/abs/1711.03953,35000000.00,,1000.00,,,0.00,2080000.00,437000000000000000.00,,2080000.00,WikiText-2,,40.68,,0.0,0,Recurrent,LSTM,https://github.com/zihangdai/mos,1,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)""","AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)"
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)","Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",2018-09-18,FRAGE: Frequency-Agnostic Word Representation,152.00,,https://arxiv.org/abs/1809.06858,35000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,WikiText-2,,39.14,,0.0,0,Recurrent,LSTM,https://github.com/ChengyueGongR/Frequency-Agnostic,1,"""AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)""","AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)"
AWD-LSTM-MoS+Noisin+dynamic evaluation ,"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.00,,https://arxiv.org/pdf/1805.01500,22000000.00,,400.00,,,0.00,929000.00,49100000000000000.00,,929000.00,,,,47.60,0.0,0,Recurrent,LSTM,,1,AWD-LSTM-MoS+Noisin+dynamic evaluation,AWD-LSTM-MoS+Noisin+dynamic evaluation
AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),Siddhartha Brahma,2018-08-14,Improved Language Modeling by Decoding the Past,5.00,,https://arxiv.org/abs/1808.05908,22000000.00,,1200.00,,,0.00,888000.00,141000000000000000.00,,888000.00,Penn TreeBank,,,47.30,0.0,0,Recurrent,LSTM,,1,AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),AWD-LSTM-MoS+PDR + dynamic evaluation (PTB)
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),Siddhartha Brahma,2018-08-14,Improved Language Modeling by Decoding the Past,5.00,,https://arxiv.org/abs/1808.05908,35000000.00,,,,,0.00,2050000.00,0.00,,2050000.00,WikiText-2,,40.30,,0.0,0,Recurrent,LSTM,,1,AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)
AWD-LSTM+Behaviorial-Gating,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",2019-08-31,Behavior Gated Language Models,3.00,0.0,https://arxiv.org/pdf/1909.00107,27000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,56.92,0.0,0,Recurrent,LSTM,,1,AWD-LSTM+Behaviorial-Gating,AWD-LSTM+Behaviorial-Gating
AWD-LSTM+WT+Cache+IOG (PTB),"Sho Takase, Jun Suzuki, Masaaki Nagata",2017-09-26,Input-to-Output Gate to Improve RNN Language Models,7.00,,https://arxiv.org/pdf/1709.08907,30000000.00,,5.00,,,0.00,929000.00,836000000000000.00,,929000.00,,,,53.00,0.0,1,Recurrent,LSTM,https://github.com/nttcslab-nlp/iog,1,AWD-LSTM+WT+Cache+IOG (PTB),AWD-LSTM+WT+Cache+IOG (PTB)
AWD-LSTM+WT+Cache+IOG (WT2),"Sho Takase, Jun Suzuki, Masaaki Nagata",2017-09-26,Input-to-Output Gate to Improve RNN Language Models,7.00,,https://arxiv.org/pdf/1709.08907,53000000.00,,5.00,,,0.00,2080000.00,3310000000000000.00,,2080000.00,,,51.70,,0.0,1,Recurrent,LSTM,https://github.com/nttcslab-nlp/iog,1,AWD-LSTM+WT+Cache+IOG (WT2),AWD-LSTM+WT+Cache+IOG (WT2)
B2T connection (16L),"Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki",2022-06-01,On Layer Normalizations and Residual Connections in Transformers,4.00,1.0,https://web.archive.org/web/20220602013934/https://arxiv.org/pdf/2206.00330.pdf,247000000.00,28000000000000000000.00,150.00,,,0.00,103000000.00,22900000000000000000.00,0.00,103000000.00,,19.20,,,0.0,0,Transformer,Transformer-XL,,1,B2T connection (16L),B2T connection (16L)
Base LM + kNN LM + Continuous Cache,"Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",2019-11-01,Generalization through Memorization: Nearest Neighbor Language Models,410.00,,https://arxiv.org/abs/1911.00172,247000000.00,7300000000000000000.00,200.00,,,0.00,103000000.00,30500000000000000000.00,,103000000.00,WikiText-103,15.79,,,0.0,1,Transformer,Transformer-XL,https://github.com/urvashik/knnlm,1,Base LM + kNN LM + Continuous Cache,Base LM + kNN LM + Continuous Cache
base LM+GNN,"Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",2021-10-17,GNN-LM: Language Modeling based on Global Contexts via GNN,22.00,,https://arxiv.org/abs/2110.08743,274000000.00,7300000000000000000.00,,,,0.00,103000000.00,0.00,103000000.00,206000000.00,WikiText-103,16.80,,,0.0,0,Transformer,Transformer-XL,https://github.com/ShannonAI/GNN-LM,1,base LM+GNN,base LM+GNN
base LM+GNN+kNN,"Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",2021-10-17,GNN-LM: Language Modeling based on Global Contexts via GNN,22.00,,https://arxiv.org/abs/2110.08743,274000000.00,7300000000000000000.00,,,,0.00,103000000.00,0.00,103000000.00,206000000.00,WikiText-103,14.80,,,0.0,0,Transformer,Transformer-XL,https://github.com/ShannonAI/GNN-LM,1,base LM+GNN+kNN,base LM+GNN+kNN
BERT-Large-CAS (PTB+WT2+WT103),"Chenguang Wang, Mu Li, Alexander J. Smola",2019-04-20,Language Models with Transformers,110.00,,https://arxiv.org/abs/1904.09408,395000000.00,,50.00,,,0.00,4400000000.00,521000000000000000000.00,929000.00,4400000000.00,Penn TreeBank; WikiText-2; WikiText-103,,,31.34,0.0,0,Transformer,BERT,https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model,1,BERT-Large-CAS (PTB+WT2+WT103),BERT-Large-CAS (PTB+WT2+WT103)
BERT-Large-CAS (WT2),"Chenguang Wang, Mu Li, Alexander J. Smola",2019-04-20,Language Models with Transformers,110.00,,https://arxiv.org/abs/1904.09408,395000000.00,,50.00,,,0.00,4400000000.00,521000000000000000000.00,2080000.00,4400000000.00,WikiText-2,,34.11,,0.0,0,Transformer,BERT,https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model,1,BERT-Large-CAS (WT2),BERT-Large-CAS (WT2)
BERT-Large-CAS (WT103),"Chenguang Wang, Mu Li, Alexander J. Smola",2019-04-20,Language Models with Transformers,110.00,,https://arxiv.org/abs/1904.09408,395000000.00,,50.00,,,0.00,4400000000.00,521000000000000000000.00,103000000.00,4500000000.00,WikiText-103,20.42,,,0.0,0,Transformer,BERT,https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model,1,BERT-Large-CAS (WT103),BERT-Large-CAS (WT103)
BLOOM-1.7B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-05,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.00,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",1720000000.00,,1.00,,,0.00,350000000000.00,3.6200000000000005e+21,,350000000000.00,,,20.17,,0.5,0,Transformer,Megatron-LM GPT2,,1,BLOOM-1.7B,BLOOM-1.7B
BLOOM-1B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-05,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.00,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",1070000000.00,,1.00,,,0.00,350000000000.00,2.24e+21,,350000000000.00,,,23.70,,0.5,0,Transformer,Megatron-LM GPT2,,1,BLOOM-1B,BLOOM-1B
BLOOM-3B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-05,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.00,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",3000000000.00,,1.00,,,0.00,350000000000.00,6.3e+21,,350000000000.00,,,17.57,,0.5,0,Transformer,Megatron-LM GPT2,,1,BLOOM-3B,BLOOM-3B
BLOOM-560M,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-05,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.00,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",560000000.00,,1.00,,,0.00,350000000000.00,1.18e+21,,350000000000.00,,,30.05,,0.5,0,Transformer,Megatron-LM GPT2,,1,BLOOM-560M,BLOOM-560M
BLOOM-7.1B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-07-05,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.00,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",7070000000.00,,1.00,,,0.00,350000000000.00,1.48e+22,,350000000000.00,,,14.72,,0.5,0,Transformer,Megatron-LM GPT2,,1,BLOOM-7.1B,BLOOM-7.1B
bRSM + cache,"Jeremy Gordon, David Rawlinson, Subutai Ahmad",2019-12-02,Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling,4.00,0.0,https://arxiv.org/abs/1912.01116,2550000.00,,15.00,,,0.00,929000.00,213000000000000.00,,929000.00,,,,103.50,0.0,1,Recurrent,bRSM,,1,bRSM + cache,bRSM + cache
Byte-mLSTM+emb+WN+VD,"Ben Krause, Liang Lu, Iain Murray, Steve Renals",2016-09-26,Multiplicative LSTM for sequence modelling,216.00,,https://arxiv.org/pdf/1609.07959,46000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,88.80,,1.0,0,Recurrent,LSTM,https://github.com/benkrause/mLSTM,1,Byte-mLSTM+emb+WN+VD,Byte-mLSTM+emb+WN+VD
CD-GraB (WT2),"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",2023-02-02,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,0.00,0.0,https://arxiv.org/pdf/2302.00845.pdf,,,50.00,,,0.00,2080000.00,,,2080000.00,WikiText-2,,223.44,,0.0,0,Recurrent,LSTM,,0,CD-GraB (WT2),CD-GraB (WT2)
CD-GraB (WT103),"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",2023-02-02,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,0.00,0.0,https://arxiv.org/pdf/2302.00845.pdf,,,30.00,,,0.00,103000000.00,,,103000000.00,WikiText-103,66.11,,,0.0,0,Transformer,GPT,,0,CD-GraB (WT103),CD-GraB (WT103)
Char-CNN-BiLSTM,"Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller",2019-06-13,Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise,1.00,0.0,https://arxiv.org/pdf/1906.05678,,,,,,0.00,929000.00,,,929000.00,,,,37.49,0.0,0,Recurrent,LSTM,,0,Char-CNN-BiLSTM,Char-CNN-BiLSTM
Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),"Kristijan Armeni, Christopher Honey, Tal Linzen",2022-10-24,Characterizing Verbatim Short-Term Memory in Neural Language Models,3.00,1.0,https://arxiv.org/pdf/2210.13569.pdf,182000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,41.90,,,0.0,0,,,https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,1,Verbatim Memory Transformer (182M),Verbatim Memory Transformer (182M)
Characterizing Verbatim Short-Term Memory in Neural Language Models (108M),"Kristijan Armeni, Christopher Honey, Tal Linzen",2022-10-24,Characterizing Verbatim Short-Term Memory in Neural Language Models,3.00,1.0,https://arxiv.org/pdf/2210.13569.pdf,108000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,40.30,,,0.0,0,,,https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,1,Verbatim Memory Transformer (108M),Verbatim Memory Transformer (108M)
Characterizing Verbatim Short-Term Memory in Neural Language Models (117M),"Kristijan Armeni, Christopher Honey, Tal Linzen",2022-10-24,Characterizing Verbatim Short-Term Memory in Neural Language Models,3.00,1.0,https://arxiv.org/pdf/2210.13569.pdf,117000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,37.50,,,0.0,0,,,https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,1,Verbatim Memory Transformer (117M),Verbatim Memory Transformer (117M)
Chinchilla,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",2022-03-29,Training Compute-Optimal Large Language Models,136.00,,https://arxiv.org/abs/2203.15556,70000000000.00,5.76e+23,1.00,,,0.00,1400000000000.00,5.88e+23,0.00,1400000000000.00,WikiText-103,7.16,,,1.0,0,Transformer,GPT,,1,Chinchilla,Chinchilla
CODA,"Lin Zheng, Zhiyong Wu, Lingpeng Kong",2021-05-31,Cascaded Head-colliding Attention,2.00,1.0,https://arxiv.org/pdf/2105.14850,247000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,18.48,,,0.0,0,Transformer,CODA,"https://github.com/LZhengisme/CODA, ",1,CODA,CODA
Compress-LSTM (66M),"Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",2019-02-06,Compression of Recurrent Neural Networks for Efficient Language Modeling,37.00,,"https://arxiv.org/abs/1902.02380#:~:text=Compression%20of%20Recurrent%20Neural%20Networks%20for%20Efficient%20Language%20Modeling,-Artem%20M.&text=Recurrent%20neural%20networks%20have%20proved,real%2Dtime%20offline%20mobile%20applications.",66000000.00,,90.00,,,0.00,929000.00,33100000000000000.00,,929000.00,,,,78.29,0.0,0,Recurrent,LSTM,,1,Compress-LSTM (66M),Compress-LSTM (66M)
Compress-LSTM (4.6M),"Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",2019-02-06,Compression of Recurrent Neural Networks for Efficient Language Modeling,37.00,,"https://arxiv.org/abs/1902.02380#:~:text=Compression%20of%20Recurrent%20Neural%20Networks%20for%20Efficient%20Language%20Modeling,-Artem%20M.&text=Recurrent%20neural%20networks%20have%20proved,real%2Dtime%20offline%20mobile%20applications.",4640000.00,,90.00,,,0.00,929000.00,2330000000000000.00,,929000.00,,,,117.66,0.0,0,Recurrent,LSTM,,1,Compress-LSTM (4.6M),Compress-LSTM (4.6M)
Compressive Transformers for Long-Range Sequence Modelling,"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap",2019-11-13,Compressive Transformers for Long-Range Sequence Modelling,330.00,,https://arxiv.org/abs/1911.05507,,160000000000000000000.00,328.32,,,0.00,103000000.00,,,103000000.00,WikiText-103,17.10,,,0.0,0,Transformer,Transformer-XL,,0,Compressive Transformers for Long-Range Sequence Modelling,Compressive Transformers for Long-Range Sequence Modelling
CryptoGRU,"Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox",2020-10-22,CryptoGRU: Low Latency Privacy-Preserving Text Analysis With GRU,12.00,,https://arxiv.org/pdf/2010.11796,,,,,,0.00,929000.00,0.00,,929000.00,,,,,0.0,0,Recurrent,GRU,,0,CryptoGRU,CryptoGRU
CT-MoS (PTB),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020-12-25,Contextual Temperature for Language Modeling,6.00,,https://arxiv.org/pdf/2012.13575,24000000.00,,1000.00,,,0.00,929000.00,134000000000000000.00,,929000.00,,,,54.69,0.0,0,Recurrent,LSTM,,1,CT-MoS (PTB),CT-MoS (PTB)
CT-MoS (WT2),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020-12-25,Contextual Temperature for Language Modeling,6.00,,https://arxiv.org/pdf/2012.13575,45000000.00,,1000.00,,,0.00,2080000.00,562000000000000000.00,,2080000.00,,,62.21,,0.0,0,Recurrent,LSTM,,1,CT-MoS (WT2),CT-MoS (WT2)
CT-MoS + DynamicEval (PTB),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020-12-25,Contextual Temperature for Language Modeling,6.00,,https://arxiv.org/pdf/2012.13575,24000000.00,,1000.00,,,0.00,929000.00,134000000000000000.00,,929000.00,,,,47.42,0.0,0,Recurrent,LSTM,,1,CT-MoS + DynamicEval (PTB),CT-MoS + DynamicEval (PTB)
CT-MoS + DynamicEval (WT2),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020-12-25,Contextual Temperature for Language Modeling,6.00,,https://arxiv.org/pdf/2012.13575,45000000.00,,1000.00,,,0.00,2080000.00,562000000000000000.00,,2080000.00,,,40.96,,0.0,0,Recurrent,LSTM,,1,CT-MoS + DynamicEval (WT2),CT-MoS + DynamicEval (WT2)
D-LSRC(100)+KN5,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",2017-08-22,Long-Short Range Context Neural Networks for Language Modeling,19.00,,https://arxiv.org/pdf/1708.06555,5970000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,93.00,0.0,0,Recurrent,LSRC,,1,D-LSRC(100)+KN5,D-LSRC(100)+KN5
D-LSRC(200)+KN5,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",2017-08-22,Long-Short Range Context Neural Networks for Language Modeling,19.00,,https://arxiv.org/pdf/1708.06555,7160000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,92.00,0.0,0,Recurrent,LSRC,,1,D-LSRC(200)+KN5,D-LSRC(200)+KN5
DARTS,"Hanxiao Liu, Karen Simonyan, Yiming Yang",2018-06-24,DARTS: Differentiable Architecture Search,3990.00,,https://arxiv.org/abs/1806.09055,33000000.00,11000000000000000.00,300.00,,,0.00,2080000.00,124000000000000000.00,,2080000.00,WikiText-2,,69.60,,0.0,0,NAS,DARTS,https://github.com/quark0/darts,1,DARTS,DARTS
DARTS (second order),"Hanxiao Liu, Karen Simonyan, Yiming Yang",2018-06-24,DARTS: Differentiable Architecture Search,3990.00,,https://arxiv.org/abs/1806.09055,23000000.00,11000000000000000.00,300.00,,,0.00,929000.00,38500000000000000.00,,929000.00,Penn TreeBank,,,55.70,0.0,0,NAS,DARTS,https://github.com/quark0/darts,1,DARTS (second order),DARTS (second order)
Decay RNN,"Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal",2020-05-17,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,7.00,,https://arxiv.org/abs/2005.08199,1400000.00,,,,,0.00,,0.00,,0.00,Lizen et al 2016,76.67,,,1.0,0,Recurrent,,https://github.com/bhattg/Decay-RNN-ACL-SRW2020,0,Decay RNN,Decay RNN
Decaying Fast Weights Transformer,Huanru Henry Mao,2022-10-09,Fine-Tuning Pre-trained Transformers into Decaying Fast Weights,0.00,1.0,https://arxiv.org/pdf/2210.04243.pdf,242000000.00,13000000000000000000.00,192.12,,,0.00,103000000.00,28700000000000000000.00,103000000.00,206000000.00,,20.50,,,0.0,0,Transformer,Transformer-XL,https://github.com/jenni-ai/T2FW,1,Decaying Fast Weights Transformer (WT-103),Decaying Fast Weights Transformer (WT-103)
Deep RNN,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2013-12-11,Pointer Sentinel Mixture Models,1558.00,,https://arxiv.org/abs/1609.07843,6000000.00,,64.00,,,0.00,929000.00,2140000000000000.00,,929000.00,Penn TreeBank,,,107.50,0.0,0,Recurrent,,,1,Deep RNN,Deep RNN
DeLight,"Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",2020-08-03,DeLighT: Deep and Light-weight Transformer,98.00,,https://arxiv.org/abs/2008.00623,99000000.00,24000000000000000000.00,62.14,,,0.00,103000000.00,3800000000000000000.00,0.00,103000000.00,WikiText-103,24.14,,,0.0,0,Transformer,Transformer-XL,https://github.com/sacmehta/delight,1,DeLight,DeLight
Delta RNN (+ full context),"Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber",2021-06-11,Going Beyond Linear Transformers with Recurrent Fast Weight Programmers,42.00,,https://proceedings.neurips.cc/paper/2021/file/3f9e3767ef3b10a0de4c256d7ef9805d-Paper.pdf,44600000.00,,40.00,,,0.00,103000000.00,1100000000000000000.00,0.00,103000000.00,WikiText-103,32.80,,,0.0,0,Recurrent,RNN,https://github.com/IDSIA/recurrent-fwp,1,Delta RNN (+ full context),Delta RNN (+ full context)
dense-IndRNN+dynamic eval,"Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao",2019-10-11,Deep Independently Recurrent Neural Network (IndRNN),45.00,,https://arxiv.org/abs/1910.06251,44100000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,49.95,0.0,0,Recurrent,,,1,dense-IndRNN+dynamic eval,dense-IndRNN+dynamic eval
Densely Connected LSTM + Var. Dropout,"Fréderic Godin, Joni Dambre, Wesley De Neve",2017-07-19,Improving Language Modeling using Densely Connected Recurrent Neural Networks,7.00,,https://arxiv.org/pdf/1707.06130,23000000.00,,100.00,,,0.00,929000.00,12800000000000000.00,,929000.00,,,,78.30,0.0,0,Recurrent,,,1,Densely Connected LSTM + Var. Dropout,Densely Connected LSTM + Var. Dropout
"DEQ-Transformer (Medium, Adaptive Embedding)","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2019-09-03,Deep Equilibrium Models,496.00,,https://arxiv.org/abs/1909.01377,110000000.00,,12.00,,,0.00,103000000.00,816000000000000000.00,0.00,103000000.00,WikiText-103,23.20,,,0.0,0,Transformer,DEQ,https://github.com/locuslab/deq,1,"""DEQ-Transformer (Medium, Adaptive Embedding)""","DEQ-Transformer (Medium, Adaptive Embedding)"
DEQ-Transformer (Post-LN) + Jacobian Regularisation,"Shaojie Bai, Vladlen Koltun, J. Zico Kolter",2021-06-28,Stabilizing Equilibrium Models by Jacobian Regularization,45.00,,https://arxiv.org/abs/2106.14342,98000000.00,29000000000000000000.00,23.00,,,0.00,103000000.00,1390000000000000000.00,0.00,103000000.00,WikiText-103,24.90,,,0.0,0,Transformer,DEQ,,1,DEQ-Transformer (Post-LN) + Jacobian Regularisation,DEQ-Transformer (Post-LN) + Jacobian Regularisation
DEQ-TrellisNet,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2019-09-03,Deep Equilibrium Models,496.00,,https://arxiv.org/abs/1909.01377,110000000.00,,12.00,,,0.00,103000000.00,816000000000000000.00,0.00,103000000.00,Penn TreeBank,,,57.10,0.0,0,Transformer,DEQ,https://github.com/locuslab/deq,1,DEQ-TrellisNet,DEQ-TrellisNet
DiffQ Transformer (16L),"Alexandre Défossez, Yossi Adi, Gabriel Synnaeve",2021-04-20,Differentiable Model Compression via Pseudo Quantization Noise,20.00,,https://arxiv.org/abs/2104.09987,257000000.00,3360000000000000000.00,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,18.10,,,0.0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/diffq,1,DiffQ Transformer (16L),DiffQ Transformer (16L)
DiffStk-MRNN,"Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles",2020-04-04,Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack,8.00,,https://arxiv.org/pdf/2004.07623,1010000.00,,50.00,,,1.00,929000.00,282000000000000.00,,929000.00,,,,115.00,0.0,0,Recurrent,,,1,DiffStk-MRNN,DiffStk-MRNN
DITTO,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",2022-06-06,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,14.00,,https://web.archive.org/web/20221011104229/https://arxiv.org/pdf/2206.02369.pdf,750000000.00,11000000000000000000.00,7.16,,,0.00,103000000.00,3320000000000000000.00,0.00,103000000.00,,24.33,,,0.0,0,Transformer,Transformer-XL,https://github.com/Jxu-Thu/DITTO,1,DITTO,DITTO
DOC + Finetune∗ + Partial Shuffle (PTB),Ofir Press,2019-03-11,Partially Shuffling the Training Data to Improve Language Models,4.00,0.0,https://arxiv.org/abs/1903.04167,37000000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,52.00,0.0,0,Recurrent,LSTM,https://github.com/ofirpress/PartialShuffle,1,DOC + Finetune∗ + Partial Shuffle (PTB),DOC + Finetune∗ + Partial Shuffle (PTB)
DOC + Finetune∗ + Partial Shuffle (WT2),Ofir Press,2019-03-11,Partially Shuffling the Training Data to Improve Language Models,4.00,0.0,https://arxiv.org/abs/1903.04167,67300000.00,,,,,0.00,2080000.00,0.00,,2080000.00,WikiText-2,,57.85,,0.0,0,Recurrent,LSTM,https://github.com/ofirpress/PartialShuffle,1,DOC + Finetune∗ + Partial Shuffle (WT2),DOC + Finetune∗ + Partial Shuffle (WT2)
DOT(S)-RNN,"Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio",2013-12-20,How to Construct Deep Recurrent Neural Networks,1255.00,,https://arxiv.org/pdf/1312.6026.pdf,6160000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,107.50,0.0,0,Recurrent,RNN,,1,DOT(S)-RNN,DOT(S)-RNN
Dropout-LSTM+Noise(Bernoulli) (PTB),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.00,,https://arxiv.org/pdf/1805.01500,51000000.00,,200.00,,,0.00,929000.00,56900000000000000.00,,929000.00,,,,66.10,0.0,0,Recurrent,,,1,Dropout-LSTM+Noise(Bernoulli) (PTB),Dropout-LSTM+Noise(Bernoulli) (PTB)
Dropout-LSTM+Noise(Bernoulli) (WT2),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.00,,https://arxiv.org/pdf/1805.01500,51000000.00,,200.00,,,0.00,2080000.00,127000000000000000.00,,2080000.00,,,76.80,,0.0,0,Recurrent,,,1,Dropout-LSTM+Noise(Bernoulli) (WT2),Dropout-LSTM+Noise(Bernoulli) (WT2)
Dropout-LSTM+Noise(Laplace),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.00,,https://arxiv.org/pdf/1805.01500,13000000.00,,200.00,,,0.00,2080000.00,32400000000000000.00,,2080000.00,,,82.10,,0.0,0,Recurrent,,,1,Dropout-LSTM+Noise(Laplace),Dropout-LSTM+Noise(Laplace)
E-SPA,"Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L Smith, Yee Whye Teh",2023-02-20,Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation,3.00,1.0,https://arxiv.org/pdf/2302.10322.pdf,243000000.00,,,,,0.00,103000000.00,,,103000000.00,,19.70,,,0.0,0,Transformer,Transformer,,1,E-SPA,E-SPA
EGRU (PTB),"Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",2022-06-13,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,1.00,1.0,https://arxiv.org/pdf/2206.06178v3.pdf,55000000.00,,2500.00,,,0.00,929000.00,766000000000000000.00,,929000.00,,,,57.00,0.0,0,Recurrent,,https://github.com/Efficient-Scalable-Machine-Learning/EvNN,1,EGRU (PTB),EGRU (PTB)
EGRU (WT2),"Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",2022-06-13,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,1.00,1.0,https://arxiv.org/pdf/2206.06178v3.pdf,74000000.00,,2500.00,,,0.00,2080000.00,2310000000000000000.00,,2080000.00,,,68.90,,0.0,0,Recurrent,,https://github.com/Efficient-Scalable-Machine-Learning/EvNN,1,EGRU (WT2),EGRU (WT2)
EI-REHN-1000D,"Hyunsin Park, Chang D. Yoo",2017-08-14,Early Improving Recurrent Elastic Highway Network,6.00,,https://arxiv.org/pdf/1708.04116,19000000.00,,100.00,,,0.00,929000.00,10600000000000000.00,,929000.00,,,,68.70,0.0,0,Recurrent,RHN,,1,EI-REHN-1000D,EI-REHN-1000D
EI-REHN-1200D,"Hyunsin Park, Chang D. Yoo",2017-08-14,Early Improving Recurrent Elastic Highway Network,6.00,,https://arxiv.org/pdf/1708.04116,12000000.00,,100.00,,,0.00,929000.00,6690000000000000.00,,929000.00,,,,66.20,0.0,0,Recurrent,RHN,,1,EI-REHN-1200D,EI-REHN-1200D
EN^2AS with performance reward,"Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su",2019-07-22,Efficient Novelty-Driven Neural Architecture Search,1.00,1.0,https://arxiv.org/pdf/1907.09109,23000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,57.36,0.0,0,NAS,NAS,,1,EN^2AS with performance reward,EN^2AS with performance reward
ENAS,"Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean",2018-02-09,Efficient Neural Architecture Search via Parameter Sharing,2760.00,,https://arxiv.org/abs/1802.03268,24000000.00,,150.00,,,0.00,929000.00,20100000000000000.00,,929000.00,Penn TreeBank,,,55.80,0.0,0,NAS,NAS,,1,ENAS,ENAS
Engine-Base (NE),"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",2021-12-11,Show and Write: Entity-aware Article Generation with Image Information,0.00,0.5,https://arxiv.org/pdf/2112.05917,124000000.00,,3.00,,,0.00,,0.00,,0.00,,,20.70,,0.0,0,,,,0,Engine-Base (NE),Engine-Base (NE)
Engin-Medium(NE),"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",2021-12-11,Show and Write: Entity-aware Article Generation with Image Information,0.00,0.5,https://arxiv.org/pdf/2112.05917,355000000.00,,3.00,,,0.00,,0.00,,0.00,,,15.40,,0.0,0,,,,0,Engine-Medium(NE),Engine-Medium(NE)
Engine-XL(NE),"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",2021-12-11,Show and Write: Entity-aware Article Generation with Image Information,0.00,0.5,https://arxiv.org/pdf/2112.05917,1500000000.00,,3.00,,,0.00,,0.00,,0.00,,,16.30,,0.0,0,,,,0,Engine-XL(NE),Engine-XL(NE)
ERNIE-Doc (151M),"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020-12-31,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,35.00,,https://arxiv.org/pdf/2012.15688,151000000.00,,190.88,,,0.00,103000000.00,17800000000000000000.00,,103000000.00,,21.00,,,1.0,0,Transformer,Transformer-XL,https://github.com/PaddlePaddle/ERNIE/,1,ERNIE-Doc (151M),ERNIE-Doc (151M)
ERNIE-Doc (247M),"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020-12-31,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,35.00,,https://arxiv.org/pdf/2012.15688,247000000.00,,190.88,,,0.00,103000000.00,29100000000000000000.00,,103000000.00,,16.80,,,1.0,0,Transformer,Transformer-XL,https://github.com/PaddlePaddle/ERNIE/,1,ERNIE-Doc (247M),ERNIE-Doc (247M)
Fairseq + UID: variance,"Jason Wei, Clara Meister, Ryan Cotterell",2021-05-15,A Cognitive Regularizer for Language Modeling,11.00,,https://web.archive.org/web/20221010230611/https://arxiv.org/pdf/2105.07144.pdf,,,,,,0.00,103000000.00,,0.00,103000000.00,,29.58,,,0.0,0,Transformer,Transformer,,0,Fairseq + UID: variance,Fairseq + UID: variance
FAIRSEQ Adaptive Inputs,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli",2019-04-01,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",2428.00,,https://arxiv.org/abs/1904.01038,247000000.00,7300000000000000000.00,,,,0.00,103000000.00,0.00,0.00,103000000.00,WikiText-103,18.70,,,0.0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/fairseq,1,FAIRSEQ Adaptive Inputs,FAIRSEQ Adaptive Inputs
Feedback Transformer,"Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar",2020-02-21,Addressing Some Limitations of Transformers with Feedback Memory,41.00,,https://arxiv.org/abs/2002.09402,126000000.00,44100000000000000000.00,267.23,,,0.00,103000000.00,20800000000000000000.00,,103000000.00,WikiText-103,18.30,,,0.0,0,Transformer,Feedback transformer,,1,Feedback Transformer,Feedback Transformer
Fine-tuned-AWD-LSTM-DOC(fin),"Vadim Popov, Mikhail Kudinov",2018-11-12,Fine-tuning of Language Models with Discriminator,2.00,0.0,https://arxiv.org/pdf/1811.04623,23000000.00,,15.00,,,0.00,929000.00,1920000000000000.00,,929000.00,,,,52.12,0.0,0,Recurrent,LSTM,,1,Fine-tuned-AWD-LSTM-DOC(fin),Fine-tuned-AWD-LSTM-DOC(fin)
FMMformer (2-kernel fast weight + Band20),"Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang",2021-08-05,FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention,15.00,,https://web.archive.org/web/20220803154831/https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf,40000000.00,430000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,,34.71,,,0.0,0,Transformer,FMMformer,,1,FMMformer (2-kernel fast weight + Band20),FMMformer (2-kernel fast weight + Band20)
FNetAR Medium,"Tim Lou, Michael Park, Mohammad Ramezanali, Vincent Tang",2021-07-22,FNetAR: Mixing Tokens with Autoregressive Fourier Transforms,2.00,0.0,https://arxiv.org/abs/2107.10932,34300000.00,,,,,0.00,103000000.00,0.00,0.00,103000000.00,WikiText-103,25.81,,,0.0,0,Transformer,Transformer-XL,,1,FNetAR Medium,FNetAR Medium
Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020-09-29,Neural Language Modeling With Implicit Cache Pointers,4.00,1.0,https://arxiv.org/pdf/2009.13774,24000000.00,,,,,1.00,929000.00,0.00,,929000.00,,,,52.50,0.0,1,Recurrent,,,1,Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB)
Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020-09-29,Neural Language Modeling With Implicit Cache Pointers,4.00,1.0,https://arxiv.org/pdf/2009.13774,33000000.00,,,,,1.00,2080000.00,0.00,,2080000.00,,,55.60,,0.0,1,Recurrent,,,1,Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2)
Fraternal dropout + AWD-LSTM 3-layer (PTB),"Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",2017-10-31,Fraternal Dropout,55.00,,https://arxiv.org/abs/1711.00066,24000000.00,,520.00,,,0.00,929000.00,69600000000000000.00,,929000.00,Penn TreeBank,,,56.80,0.0,0,Recurrent,,,1,Fraternal dropout + AWD-LSTM 3-layer (PTB),Fraternal dropout + AWD-LSTM 3-layer (PTB)
Fraternal dropout + AWD-LSTM 3-layer (WT2),"Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",2017-10-31,Fraternal Dropout,55.00,,https://arxiv.org/abs/1711.00066,34000000.00,,520.00,,,0.00,929000.00,98500000000000000.00,,929000.00,WikiText-2,,64.10,,0.0,0,Recurrent,,,1,Fraternal dropout + AWD-LSTM 3-layer (WT2),Fraternal dropout + AWD-LSTM 3-layer (WT2)
Gated HORNN (3rd order),"Rohollah Soltani, Hui Jiang",2016-04-30,Higher Order Recurrent Neural Networks,77.00,,https://arxiv.org/pdf/1605.00064,8970000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,100.00,0.0,0,Recurrent,RNN,,1,Gated HORNN (3rd order),Gated HORNN (3rd order)
GCNN-14,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",2016-12-23,Language Modeling with Gated Convolutional Networks,2176.00,,https://arxiv.org/abs/1612.08083,,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,,,108.70,0.0,0,Recurrent,GCNN,,0,GCNN-14,GCNN-14
GCNN-14,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",2016-12-23,Language Modeling with Gated Convolutional Networks,2176.00,,https://arxiv.org/abs/1612.08083,,,35.00,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,37.20,,,0.0,0,Recurrent,GCNN,,0,GCNN-14,GCNN-14
"GCRN-M1, dropout","Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, Xavier Bresson",2016-12-22,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,674.00,,https://arxiv.org/pdf/1612.07659,42000000.00,,13.00,,,0.00,929000.00,3040000000000000.00,,929000.00,,,,98.67,0.0,0,Recurrent,GCRN,,1,"""GCRN-M1, dropout""","GCRN-M1, dropout"
genCNN + dyn eval,"Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu",2015-03-17,genCNN: A Convolutional Architecture for Word Sequence Prediction,33.00,,https://aclanthology.org/P15-1151/,8000000.00,73000000000000000.00,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,106.30,0.0,0,,,,1,genCNN + dyn eval,genCNN + dyn eval
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),"Ziv Aharoni, Gal Rattner, Haim Permuter",2017-08-29,Gradual Learning of Recurrent Neural Networks,4.00,1.0,https://arxiv.org/abs/1708.08863,26000000.00,,1000.00,,,0.00,929000.00,145000000000000000.00,,929000.00,Penn TreeBank,,,46.34,0.0,0,Recurrent,LSTM,,1,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB)
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),"Ziv Aharoni, Gal Rattner, Haim Permuter",2017-08-29,Gradual Learning of Recurrent Neural Networks,4.00,1.0,https://arxiv.org/abs/1708.08863,38000000.00,,1000.00,,,0.00,2080000.00,474000000000000000.00,,2080000.00,WikiText-2,,40.46,,0.0,0,Recurrent,LSTM,,1,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)
GLM-10B,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021-03-18,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.00,,https://arxiv.org/abs/2103.10360,10000000000.00,,1.00,,,0.00,632000000000.00,3.79e+22,,632000000000.00,,,12.00,22.52,1.0,0,Transformer,GLM,https://github.com/THUDM/GLM,1,GLM-10B,GLM-10B
GLM-10B-bidirectional,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021-03-18,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.00,,https://arxiv.org/abs/2103.10360,10000000000.00,,1.00,,,0.00,632000000000.00,3.79e+22,,632000000000.00,,11.33,,,1.0,0,Transformer,GLM,https://github.com/THUDM/GLM,1,GLM-10B-bidirectional,GLM-10B-bidirectional
GLM-10B-unidirectional,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021-03-18,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.00,,https://arxiv.org/abs/2103.10360,10000000000.00,,1.00,,,0.00,632000000000.00,3.79e+22,,632000000000.00,,12.22,,,1.0,0,Transformer,GLM,https://github.com/THUDM/GLM,1,GLM-10B-unidirectional,GLM-10B-unidirectional
GLM-130B,"Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",2022-10-05,GLM-130B: An Open Bilingual Pre-trained Model,131.00,,https://openreview.net/forum?id=-Aw0rrrPUF,130000000000.00,,1.00,,,1.00,632000000000.00,4.93e+23,,632000000000.00,,10.76,10.55,18.90,1.0,0,Transformer,GLM,https://github.com/THUDM/GLM-130B,1,GLM-130B,GLM-130B
GLM-2B,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021-03-18,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.00,,https://arxiv.org/abs/2103.10360,2000000000.00,,1.00,,,0.00,632000000000.00,7.58e+21,,632000000000.00,,11.65,14.90,33.31,1.0,0,Transformer,GLM,https://github.com/THUDM/GLM,1,GLM-2B,GLM-2B
Gopher (280B),"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",2021-12-08,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",441.00,,https://arxiv.org/abs/2112.11446,280000000000.00,6.31e+23,1.00,,,1.00,300000000000.00,5.04e+23,,300000000000.00,WikiText-103,10.81,,,1.0,0,Transformer,GPT,,1,Gopher (280B),Gopher (280B)
Gopher (7.1B),"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",2021-12-08,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",441.00,,https://arxiv.org/abs/2112.11446,7100000000.00,1.28e+22,1.00,,,0.00,300000000000.00,1.28e+22,,300000000000.00,WikiText-103,8.12,,,1.0,0,Transformer,GPT,,1,Gopher (7.1B),Gopher (7.1B)
"GPT-2 (1.5B, Curriculum Learning 45K)","Conglong Li, Minjia Zhang, Yuxiong He",2021-08-13,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,20.00,,https://arxiv.org/abs/2108.06084,1500000000.00,600000000000000000000.00,2.20,,,0.00,157000000000.00,3.11e+21,,157000000000.00,Wikipedia; CC-Stories; RealNews; OpenWebtext,13.72,,,1.0,0,Transformer,GPT,,1,"""GPT-2 (1.5B, Curriculum Learning 45K)""","GPT-2 (1.5B, Curriculum Learning 45K)"
"GPT-2 (117M, SLW 110K)","Conglong Li, Minjia Zhang, Yuxiong He",2021-08-13,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,20.00,,https://arxiv.org/abs/2108.06084,117000000.00,130000000000000000000.00,1.10,,,0.00,157000000000.00,120999999999999980000.00,,157000000000.00,Wikipedia; CC-Stories; RealNews; OpenWebtext,26.03,,,1.0,0,Transformer,GPT,,1,"""GPT-2 (117M, SLW 110K)""","GPT-2 (117M, SLW 110K)"
GPT-2 (117M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019-02-14,Language Models are Unsupervised Multitask Learners,6654.00,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,117000000.00,116999999999999980000.00,100.00,,,0.00,4000000000.00,281000000000000000000.00,,4000000000.00,WebText,37.50,,,1.0,0,Transformer,GPT,https://github.com/openai/gpt-5,1,GPT-2 (117M),GPT-2 (117M)
GPT-2 (1542M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019-02-14,Language Models are Unsupervised Multitask Learners,6654.00,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,1500000000.00,1.5e+21,20.00,,,0.00,4000000000.00,720000000000000000000.00,,4000000000.00,WebText,17.48,18.34,35.76,1.0,0,Transformer,GPT,https://github.com/openai/gpt-2,1,GPT-2 (1.5B),GPT-2 (1.5B)
GPT-2 (345M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019-02-14,Language Models are Unsupervised Multitask Learners,6654.00,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,345000000.00,345000000000000000000.00,100.00,,,0.00,4000000000.00,828000000000000000000.00,,4000000000.00,WebText,26.37,,,1.0,0,Transformer,GPT,https://github.com/openai/gpt-4,1,GPT-2 (345M),GPT-2 (345M)
GPT-2 (762M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019-02-14,Language Models are Unsupervised Multitask Learners,6654.00,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,762000000.00,762000000000000000000.00,100.00,,,0.00,4000000000.00,1.83e+21,,4000000000.00,WebText,22.05,,,1.0,0,Transformer,GPT,https://github.com/openai/gpt-3,1,GPT-2 (762M),GPT-2 (762M)
GPT-2 (fine-tuned with HYDRA),"Kabir Nagrecha, Arun Kumar",2021-10-16,Hydra: A System for Large Multi-Model Deep Learning,4.00,0.0,https://arxiv.org/abs/2110.08633,1540000000.00,,1.00,,,0.00,2080000.00,19200000000000000.00,,2080000.00,WikiText-2,,15.17,,0.0,0,Transformer,GPT,,1,GPT-2 (fine-tuned with HYDRA),GPT-2 (fine-tuned with HYDRA)
GPT-2-Medium+Pixelfly,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",2021-11-30,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,29.00,,https://arxiv.org/pdf/2112.00029,203000000.00,834000000000000000000.00,100.00,,,0.00,103000000.00,12500000000000000000.00,0.00,103000000.00,,21.00,,,0.0,0,Transformer,GPT,https://github.com/HazyResearch/pixelfly,1,GPT-2-Medium+Pixelfly,GPT-2-Medium+Pixelfly
GPT-2-Small+Pixelfly,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",2021-11-30,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,29.00,,https://arxiv.org/pdf/2112.00029,68000000.00,834000000000000000000.00,100.00,,,0.00,103000000.00,4200000000000000000.00,0.00,103000000.00,,22.50,,,0.0,0,Transformer,GPT,https://github.com/HazyResearch/pixelfly,1,GPT-2-Small+Pixelfly,GPT-2-Small+Pixelfly
GPT-2+Active-SGD,"Davood Wadi, Marc Fredette, Sylvain Senecal",2023-01-24,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,0.00,0.0,https://arxiv.org/pdf/2301.10133.pdf,124000000.00,,200.00,,,0.00,2080000.00,310000000000000000.00,,2080000.00,,,20.59,,0.0,0,Transformer,GPT,,1,GPT-2+Active-SGD (WT 103),GPT-2+Active-SGD (WT 103)
GPT-3 175B (davinci),"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-05-28,Language Models are Few-Shot Learners,13822.00,,https://arxiv.org/abs/2005.14165,175000000000.00,3.14e+23,0.60,,,0.00,499000000000.00,3.15e+23,,499000000000.00,CommonCrawl; WebText2; Books1; Books2; Wikipedia,,,20.50,1.0,0,Transformer,GPT,https://github.com/openai/gpt-3/,1,GPT-3 175B (davinci),GPT-3 175B (davinci)
GPT-J-6B,"Ben Wang, Aran Komatsuzaki",2021-06-09,GPT-J-6B: 6B JAX-Based Transformer,,,https://huggingface.co/EleutherAI/gpt-j-6b,6050000000.00,,1.00,,,1.00,402000000000.00,1.4600000000000002e+22,,402000000000.00,,,10.88,,1.0,0,Transformer,GPT,https://github.com/kingoflolz/mesh-transformer-jax/,1,GPT-J-6B,GPT-J-6B
GPT-Neo-1.3B,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,1300000000.00,,1.00,,,0.00,400000000000.00,3.12e+21,,400000000000.00,,,13.10,,1.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-1.3B,GPT-Neo-1.3B
GPT-Neo-1.3B (finetuned),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2700000000.00,,1.00,,,0.00,400000000000.00,6.48e+21,2080000.00,400000000000.00,,,12.09,,0.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-1.3B (finetuned),GPT-Neo-1.3B (finetuned)
GPT-Neo-125M,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,125000000.00,,1.00,,,0.00,400000000000.00,300000000000000000000.00,,400000000000.00,,,32.29,,1.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-125M,GPT-Neo-125M
GPT-Neo-125M(finetuned),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2700000000.00,,1.00,,,0.00,400000000000.00,6.48e+21,2080000.00,400000000000.00,,,21.96,,0.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-125M (finetuned),GPT-Neo-125M (finetuned)
GPT-Neo-125M(finetuned),"Michael Santacroce, Zixin Wen, Yelong Shen, Yuanzhi Li",2021-03-21,What Matters In The Structured Pruning of Generative Language Models?,1.00,1.0,https://arxiv.org/pdf/2302.03773.pdf,125000000.00,,40.00,,,1.00,300000000000.00,9.undefined,,300000000000.00,,16.14,,,0.0,0,Transformer,GPT-Neo,https://github.com/santacml/nn_pruning_uniqueness,1,GPT-Neo-125M (finetuned),GPT-Neo-125M (finetuned)
GPT-Neo-2.7B,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2700000000.00,,1.00,,,1.00,400000000000.00,6.48e+21,,400000000000.00,,,11.39,,1.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-2.7B,GPT-Neo-2.7B
GPT-Neo-2.7B (finetuned on PTB),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2700000000.00,,1.00,,,0.00,400000000000.00,6.48e+21,929000.00,400000000000.00,,,,14.70,0.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-2.7B (finetuned on PTB),GPT-Neo-2.7B (finetuned on PTB)
GPT-Neo-2.7B (finetuned),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021-03-21,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2700000000.00,,1.00,,,0.00,400000000000.00,6.48e+21,2080000.00,400000000000.00,,,10.78,,0.0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,1,GPT-Neo-2.7B (finetuned),GPT-Neo-2.7B (finetuned)
GPT-NeoX-20B,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",2022-04-14,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,235.00,,https://arxiv.org/abs/2204.06745,20000000000.00,,1.00,,,1.00,473000000000.00,5.67e+22,,473000000000.00,,,9.20,,1.0,0,Transformer,GPT,https://github.com/EleutherAI/gpt-neox,1,GPT-NeoX-20B,GPT-NeoX-20B
GPT2-Large+LHOPT,"Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba",2021-06-02,A Generalizable Approach to Learning Optimizers,13.00,,https://web.archive.org/web/20221027150413/https://arxiv.org/pdf/2106.00958.pdf,760000000.00,1.6e+21,1.00,,,0.00,103000000.00,470000000000000000.00,0.00,103000000.00,,32.50,,,0.0,0,Transformer,GPT,https://github.com/openai/LHOPT,1,GPT2-Large+LHOPT,GPT2-Large+LHOPT
GPT2-LayerFusion-WS,"James O' Neill, Greg Ver Steeg, Aram Galstyan",2020-07-29,Compressing Deep Neural Networks via Layer Fusion,5.00,,https://arxiv.org/pdf/2007.14917,,,,,,0.00,103000000.00,0.00,2080000.00,105000000.00,,,13.71,,0.0,0,Transformer,GPT,,0,GPT2-LayerFusion-WS,GPT2-LayerFusion-WS
GPT2+CoreLM+Fine-Tuning,"Nikolaos Stylianou, Ioannis Vlahavas",2021-11-04,CoreLM: Coreference-aware Language Model Fine-Tuning,2.00,1.0,https://arxiv.org/pdf/2111.02687,132000000.00,,10.00,,,0.00,4000000.00,31700000000000000.00,,4000000.00,,29.51,31.80,,1.0,0,Transformer,GPT,,1,GPT2+CoreLM+Fine-Tuning,GPT2+CoreLM+Fine-Tuning
GPT3-6.7B (rerun of original),"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",2020-05-28,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,37.00,,https://web.archive.org/web/20221014063419/https://arxiv.org/pdf/2203.03466.pdf,6700000000.00,1.2e+22,1.00,,,0.00,499000000000.00,2.01e+22,,499000000000.00,,9.13,,,1.0,0,Transformer,GPT,https://github.com/microsoft/mup,1,GPT3-6.7B (rerun of original),GPT3-6.7B (rerun of original)
GPT3-6.7B + muP,"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",2022-03-07,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,37.00,,https://web.archive.org/web/20221014063419/https://arxiv.org/pdf/2203.03466.pdf,6700000000.00,1.28e+22,1.00,,,0.00,499000000000.00,2.01e+22,103000000.00,499000000000.00,,8.56,,,1.0,0,Transformer,GPT,https://github.com/microsoft/mup,1,GPT3-6.7B + muP,GPT3-6.7B + muP
Grown to Prune Two-layer stacked LSTM,"Xin Yuan, Pedro Savarese, Michael Maire",2020-07-30,Growing Efficient Deep Networks by Structured Continuous Sparsification,37.00,,https://arxiv.org/pdf/2007.15353,,,,,,0.00,929000.00,0.00,,929000.00,,,,78.68,0.0,0,Recurrent,LSTM,,0,Grown to Prune Two-layer stacked LSTM,Grown to Prune Two-layer stacked LSTM
GRU + p-tHSM (pretrain via Brown) (PTB),"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",2017-08-19,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,5.00,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,5200000.00,,,,,0.00,929000.00,,,929000.00,6.97E+05,,,128.78,0.0,0,Recurrent,GRU,,1,GRU + p-tHSM (pretrain via Brown) (PTB),GRU + p-tHSM (pretrain via Brown) (PTB)
GRU + p-tHSM (pretrain via Brown) (WT2),"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",2017-08-19,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,5.00,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,206000000.00,,,,,0.00,2000000.00,0.00,,2000000.00,1.50E+06,,189.58,,0.0,0,Recurrent,GRU,,1,GRU + p-tHSM (pretrain via Brown) (WT2),GRU + p-tHSM (pretrain via Brown) (WT2)
GRU + p-tHSM (pretrain via Brown) (WT103),"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",2017-08-19,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,5.00,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,206000000.00,,,,,0.00,103000000.00,,,103000000.00,WikiText-103,161.55,,,0.0,0,Recurrent,GRU,,1,GRU + p-tHSM (pretrain via Brown) (WT103),GRU + p-tHSM (pretrain via Brown) (WT103)
H-LSTM+wg+rcp+rcg+wp,"Hongxu Yin, Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, Niraj K. Jha",2019-01-30,"Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM",10.00,,https://arxiv.org/pdf/1901.10997,800000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,72.10,0.0,0,Recurrent,LSTM,,1,H-LSTM+wg+rcp+rcg+wp,H-LSTM+wg+rcp+rcg+wp
HSO,"Davis Yoshida, Kevin Gimpel",2021-12-16,Reconsidering the Past: Optimizing Hidden States in Language Models,1.00,1.0,https://web.archive.org/web/20230220145200/https://arxiv.org/pdf/2112.08653.pdf,345000000.00,345000000000000000000.00,,,,0.00,4000000000.00,,103000000.00,4100000000.00,,20.30,,,0.0,0,Transformer,GPT,,1,HSO,HSO
Hybrid H3-1.3B,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022-12-28,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.00,,https://arxiv.org/pdf/2212.14052,1300000000.00,,509.02,,,0.00,103000000.00,409000000000000000000.00,,103000000.00,,12.50,,,0.0,0,State Space Model,H3,https://github.com/HazyResearch/H3,1,Hybrid H3-1.3B,Hybrid H3-1.3B
Hybrid H3-125M,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022-12-28,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.00,,https://arxiv.org/pdf/2212.14052,125000000.00,,509.02,,,0.00,103000000.00,39300000000000000000.00,,103000000.00,,23.70,,,0.0,0,State Space Model,H3,https://github.com/HazyResearch/H3,1,Hybrid H3-125M,Hybrid H3-125M
Hybrid H3-2.7B,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022-12-28,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.00,,https://arxiv.org/pdf/2212.14052,2700000000.00,,509.02,,,0.00,103000000.00,849000000000000000000.00,,103000000.00,,10.60,,,0.0,0,State Space Model,H3,https://github.com/HazyResearch/H3,1,Hybrid H3-2.7B,Hybrid H3-2.7B
Hybrid H3-355M,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022-12-28,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.00,,https://arxiv.org/pdf/2212.14052,355000000.00,,509.02,,,0.00,103000000.00,112000000000000000000.00,,103000000.00,,16.90,,,0.0,0,State Space Model,H3,https://github.com/HazyResearch/H3,1,Hybrid H3-355M,Hybrid H3-355M
Hyena-3-slim,"Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré",2023-02-21,Hyena Hierarchy: Towards Larger Convolutional Language Models,21.00,,https://arxiv.org/pdf/2302.10866,125000000.00,,,,,0.00,103000000.00,,,103000000.00,,18.50,,,0.0,0,Transformer,Hyena,,1,Hyena-3-slim,Hyena-3-slim
Integer Transformer,"Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu",2020-09-17,Towards Fully 8-bit Integer Inference for the Transformer Model,26.00,,https://arxiv.org/pdf/2009.08034,247000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,30.79,,,0.0,0,Transformer,,,1,Integer Transformer,Integer Transformer
Integer Transformer,"Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu",2020-09-17,Towards Fully 8-bit Integer Inference for the Transformer Model,26.00,,https://arxiv.org/pdf/2009.08034,59700000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,18.16,,,0.0,0,Transformer,,,1,Integer Transformer,Integer Transformer
ISS,"Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li",2017-09-15,Learning Intrinsic Sparse Structures within Long Short-Term Memory,146.00,,https://arxiv.org/pdf/1709.05027,11100000.00,,55.00,,,0.00,929000.00,3400000000000000.00,,929000.00,,,,65.40,0.0,0,Recurrent,LSTM,https://github.com/wenwei202/iss-rnns,1,ISS,ISS
KnGPT2,"Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh",2021-10-15,Kronecker Decomposition for GPT Compression,14.00,,https://web.archive.org/web/20221111092612/https://arxiv.org/pdf/2110.08152.pdf,83000000.00,124000000000000000000.00,1.00,,,0.00,4000000000.00,1990000000000000000.00,400000000.00,4400000000.00,,20.50,,,1.0,0,Transformer,GPT,,1,KnGPT2,KnGPT2
L_UL-seq,"Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston",2019-08-12,Neural Text Generation with Unlikelihood Training,365.00,,https://arxiv.org/abs/1908.04319,247000000.00,,,,,0.00,103000000.00,0.00,0.00,103000000.00,WikiText-103,25.42,,,0.0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/unlikelihood_training,1,L_UL-seq,L_UL-seq
LaMemo,"Haozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng Hu, Minlie Huang",2022-04-15,LaMemo: Language Modeling with Look-Ahead Memory,2.00,1.0,https://web.archive.org/web/20220418055451/https://arxiv.org/pdf/2204.07341.pdf,151000000.00,,79.53,,,0.00,103000000.00,7420000000000000000.00,0.00,103000000.00,,23.77,,,0.0,0,Transformer,Transformer-XL,https://github.com/thu-coai/LaMemo,1,LaMemo,LaMemo
Large regularized LSTM,"Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals",2014-09-08,Recurrent Neural Network Regularization,3224.00,,https://arxiv.org/abs/1409.2329,66000000.00,91000000000000000.00,55.00,,,0.00,929000.00,20200000000000000.00,,929000.00,Penn TreeBank,,,78.40,0.0,0,Recurrent,LSTM,"https://github.com/wojzaremba/lstm., ",1,Large regularized LSTM,Large regularized LSTM
LBL,"Andriy Mnih, Yee Whye Teh",2012-06-27,A Fast and Simple Algorithm for Training Neural Probabilistic Language Models,835.00,,https://arxiv.org/pdf/1206.6426,2000000.00,,45.00,,,0.00,929000.00,502000000000000.00,,929000.00,,,,159.10,0.0,0,Probabilistic,NPLM,,1,LBL,LBL
Linear Transformer (large),"Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",2021-02-22,Linear Transformers Are Secretly Fast Weight Programmers,78.00,,https://arxiv.org/pdf/2102.11174.pdf,90000000.00,,70.00,,,0.00,103000000.00,3890000000000000000.00,,103000000.00,,31.50,,,0.0,0,Transformer,Linear Transformer,https://github.com/ischlag/fast-weight-transformers,1,Linear Transformer (large),Linear Transformer (large)
Linear Transformer (small),"Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",2021-02-22,Linear Transformers Are Secretly Fast Weight Programmers,78.00,,https://arxiv.org/pdf/2102.11174.pdf,40000000.00,,120.00,,,0.00,103000000.00,2970000000000000000.00,,103000000.00,,35.50,,,0.0,0,Transformer,Linear Transformer,https://github.com/ischlag/fast-weight-transformers,1,Linear Transformer (small),Linear Transformer (small)
LLaMA-13B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-27,LLaMA: Open and Efficient Foundation Language Models,963.00,,https://arxiv.org/abs/2302.13971,13000000000.00,,1.09,,,1.00,1000000000000.00,8.5e+22,0.00,1000000000000.00,,,13.99,,0.5,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-13B,LLaMA-13B
LLaMA-13B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-05-23,LLaMA: Open and Efficient Foundation Language Models,0.00,,https://arxiv.org/pdf/2305.14152.pdf,13000000000.00,,1.09,,,0.00,1000000000000.00,8.5e+22,929000.00,1000000000000.00,,,5.54,8.64,0.0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-13B (LoRA finetuned),LLaMA-13B (LoRA finetuned)
LLaMA-30B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-27,LLaMA: Open and Efficient Foundation Language Models,963.00,,https://arxiv.org/abs/2302.13971,30000000000.00,,1.09,,,0.00,1400000000000.00,2.75e+23,0.00,1400000000000.00,,,6.90,,0.5,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,,
LLaMA-33B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-27,LLaMA: Open and Efficient Foundation Language Models,963.00,,https://arxiv.org/abs/2302.13971,33000000000.00,,1.09,,,0.00,1400000000000.00,3.0199999999999995e+23,0.00,1400000000000.00,,,6.35,,0.5,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-33B,LLaMA-33B
LLaMA-33B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-05-23,LLaMA: Open and Efficient Foundation Language Models,0.00,,https://arxiv.org/pdf/2305.14152.pdf,33000000000.00,,1.09,,,0.00,1400000000000.00,3.0199999999999995e+23,929000.00,1400000000000.00,,,,7.68,0.0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-33B (LoRA finetuned),LLaMA-33B (LoRA finetuned)
LLaMA-65B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-27,LLaMA: Open and Efficient Foundation Language Models,963.00,,https://arxiv.org/abs/2302.13971,65200000000.00,,1.09,,,0.00,1400000000000.00,5.970000000000001e+23,0.00,1400000000000.00,,,4.96,,0.5,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-65B,LLaMA-65B
LLaMA-65B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-05-23,LLaMA: Open and Efficient Foundation Language Models,0.00,,https://arxiv.org/pdf/2305.14152.pdf,65200000000.00,,1.09,,,0.00,1400000000000.00,5.970000000000001e+23,2080000.00,1400000000000.00,,,4.27,,0.0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-65B (LoRA finetuned),LLaMA-65B (LoRA finetuned)
LLaMA-7B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-27,LLaMA: Open and Efficient Foundation Language Models,963.00,,https://arxiv.org/abs/2302.13971,7000000000.00,,1.09,,,1.00,1000000000000.00,4.58e+22,0.00,1000000000000.00,,,9.49,,0.5,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-7B,LLaMA-7B
LLaMA-7B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-05-23,LLaMA: Open and Efficient Foundation Language Models,0.00,,https://arxiv.org/pdf/2305.14152.pdf,7000000000.00,,1.09,,,0.00,1000000000000.00,4.58e+22,929000.00,1000000000000.00,,,6.19,9.69,0.0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,1,LLaMA-7B (LoRA finetuned),LLaMA-7B (LoRA finetuned)
Local Transformer,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",2020-03-12,Efficient Content-Based Sparse Attention with Routing Transformers,349.00,,https://arxiv.org/abs/2003.05997,22100000.00,,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,19.80,,,0.0,0,Transformer,Local transformer,https://github.com/google-research/google-research/tree/master/routing_transformer,1,Local Transformer,Local Transformer
LSTM (2018),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-03-04,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,4024.00,,https://arxiv.org/abs/1803.01271,13000000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,78.93,0.0,0,Recurrent,LSTM,http://github.com/locuslab/TCN,1,LSTM (2018),LSTM (2018)
LSTM (PTB),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016-12-13,Improving Neural Language Models with a Continuous Cache,302.00,,https://arxiv.org/abs/1612.04426,32800000.00,,,,,0.00,929000.00,,,929000.00,Penn TreeBank,,,82.30,0.0,0,Recurrent,LSTM,,1,LSTM (PTB),LSTM (PTB)
LSTM (WT2),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016-12-13,Improving Neural Language Models with a Continuous Cache,302.00,,https://arxiv.org/abs/1612.04426,32800000.00,,,,,0.00,2080000.00,,,2080000.00,WikiText-2,,99.30,,0.0,0,Recurrent,LSTM,,1,LSTM (WT2),LSTM (WT2)
LSTM (WT103),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016-12-13,Improving Neural Language Models with a Continuous Cache,302.00,,https://arxiv.org/abs/1612.04426,11000000.00,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,48.70,,,0.0,0,Recurrent,LSTM,,1,LSTM (WT103),LSTM (WT103)
"LSTM (Hebbian, Cache, MbPA)","Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap",2018-03-27,Fast Parametric Learning with Activation Memorization,44.00,,https://arxiv.org/abs/1803.10049,45200000.00,24000000000000000000.00,90.00,,,0.00,103000000.00,2510000000000000000.00,0.00,103000000.00,WikiText-103,29.20,,,0.0,1,Recurrent,LSTM,,1,"""LSTM (Hebbian, Cache, MbPA)""","LSTM (Hebbian, Cache, MbPA)"
LSTM + dynamic eval,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2017-09-21,Dynamic Evaluation of Neural Sequence Models,130.00,,https://arxiv.org/abs/1709.07432,50000000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,59.80,,0.0,0,Recurrent,LSTM,https://github.com/benkrause/dynamic-evaluation,1,LSTM + dynamic eval,LSTM + dynamic eval
LSTM-3-layer+Gadam,"Diego Granziol, Xingchen Wan, Samuel Albanie, Stephen Roberts",2020-03-02,Iterative Averaging in the Quest for Best Test Error,5.00,,https://arxiv.org/pdf/2003.01247,24000000.00,,200.00,,,0.00,929000.00,26800000000000000.00,,929000.00,,,,58.77,0.0,0,Recurrent,LSTM,,1,LSTM-3-layer+Gadam,LSTM-3-layer+Gadam
LSTM-300units,"Martin Sundermeyer, Ralf Schlüter, Hermann Ney",2012-09-01,LSTM Neural Networks for Language Modeling,2503.00,,http://www.quaero.org/media/files/bibliographie/sundermeyer_lstm_neural_interspeech2012.pdf,12000000.00,,,,,0.00,929000.00,,,929000.00,,,,114.50,0.0,0,Recurrent,LSTM,,1,LSTM-300units,LSTM-300units
LSTM-Char-Large,"Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush",2015-08-26,Character-Aware Neural Language Models,2033.00,,https://arxiv.org/abs/1508.06615,19000000.00,,25.00,,,0.00,929000.00,2650000000000000.00,,929000.00,Penn TreeBank,,,78.90,0.0,0,Recurrent,LSTM,https://github.com/yoonkim/lstm-char-cnn,1,LSTM-Char-Large,LSTM-Char-Large
LSTM-Large+Behaviorial-Gating,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",2019-08-31,Behavior Gated Language Models,3.00,0.0,https://arxiv.org/pdf/1909.00107,67000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,75.80,0.0,0,Recurrent,LSTM,,1,LSTM-Large+Behaviorial-Gating,LSTM-Large+Behaviorial-Gating
LSTM-Medium+Behaviorial-Gating,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",2019-08-31,Behavior Gated Language Models,3.00,0.0,https://arxiv.org/pdf/1909.00107,20000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,78.75,0.0,0,Recurrent,LSTM,,1,LSTM-Medium+Behaviorial-Gating,LSTM-Medium+Behaviorial-Gating
LSTM-MemoryAug (PTB),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020-09-29,Neural Language Modeling With Implicit Cache Pointers,4.00,1.0,https://arxiv.org/pdf/2009.13774,13300000.00,,,,,1.00,929000.00,0.00,,929000.00,,,,67.80,0.0,0,Recurrent,LSTM,,1,LSTM-MemoryAug (PTB),LSTM-MemoryAug (PTB)
LSTM-MemoryAug (WT2),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020-09-29,Neural Language Modeling With Implicit Cache Pointers,4.00,1.0,https://arxiv.org/pdf/2009.13774,28500000.00,,,,,1.00,2080000.00,0.00,,2080000.00,,,74.30,,0.0,0,Recurrent,LSTM,,1,LSTM-MemoryAug (WT2),LSTM-MemoryAug (WT2)
LSTM(large)+Sememe+cell,"Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",2019-10-20,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,19.00,,https://arxiv.org/pdf/1910.08910,48000000.00,,40.00,,,0.00,2080000.00,24000000000000000.00,,2080000.00,,,85.76,,0.0,0,Recurrent,LSTM,https://github.com/thunlp/SememeRNN,1,LSTM(large)+Sememe+cell,LSTM(large)+Sememe+cell
LSTM(medium)+Sememe+cell,"Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",2019-10-20,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,19.00,,https://arxiv.org/pdf/1910.08910,10000000.00,,40.00,,,0.00,2080000.00,5000000000000000.00,,2080000.00,,,89.16,,0.0,0,Recurrent,LSTM,https://github.com/thunlp/SememeRNN,1,LSTM(medium)+Sememe+cell,LSTM(medium)+Sememe+cell
LSTM+Adam+Lookahead,"Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba",2019-07-19,"Lookahead Optimizer: k steps forward, 1 step back",612.00,,https://arxiv.org/pdf/1907.08610,7190000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,57.72,0.0,0,Recurrent,LSTM,https://github.com/michaelrzhang/lookahead,1,LSTM+Adam+Lookahead,LSTM+Adam+Lookahead
LSTM+GraB,"Yucheng Lu, Wentao Guo, Christopher De Sa",2022-05-22,GraB: Finding Provably Better Data Permutations than Random Reshuffling,7.00,,https://arxiv.org/pdf/2205.10733,,,50.00,,,0.00,2080000.00,,,2080000.00,,,199.40,,0.0,0,Recurrent,LSTM,https://github.com/EugeneLYC/GraB,0,LSTM+GraB,LSTM+GraB
LSTM+NeuralCache,"Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq",2018-09-24,Information-Weighted Neural Cache Language Models for ASR,3.00,1.0,https://arxiv.org/pdf/1809.08826,2100000.00,,39.00,,,0.00,2080000.00,1020000000000000.00,,2080000.00,,,66.20,,0.0,1,Recurrent,LSTM,,1,LSTM+NeuralCache,LSTM+NeuralCache
LSTM+Noise(Beta),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018-05-03,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.00,,https://arxiv.org/pdf/1805.01500,51000000.00,,200.00,,,0.00,2080000.00,127000000000000000.00,,2080000.00,,,82.90,,0.0,0,Recurrent,LSTM,,1,LSTM+Noise(Beta),LSTM+Noise(Beta)
LTM,"Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel, Hong Xie",2019-04-18,Language Modeling through Long Term Memory Network,19.00,,https://arxiv.org/pdf/1904.08936,,,,,,0.00,929000.00,,,929000.00,,,,83.00,0.0,0,Recurrent,LTM,,0,LTM,LTM
Megatron-LM (2.5B),"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019-09-17,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,905.00,,https://arxiv.org/abs/1909.08053,2500000000.00,3.9999999999999995e+21,4.40,,,0.00,46400000000.00,3.06e+21,,46400000000.00,WikiText-103; CC-Stories; RealNews; OpenWebtext,12.76,,,1.0,0,Transformer,GPT,https://github.com/NVIDIA/Megatron-LM,1,Megatron-LM (2.5B),Megatron-LM (2.5B)
Megatron-LM (355M),"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019-09-17,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,905.00,,https://arxiv.org/abs/1909.08053,355000000.00,750000000000000000000.00,4.40,,,0.00,46400000000.00,435000000000000000000.00,,46400000000.00,WikiText-103; CC-Stories; RealNews; OpenWebtext,19.31,,,1.0,0,Transformer,GPT,https://github.com/NVIDIA/Megatron-LM,1,Megatron-LM (355M),Megatron-LM (355M)
Megatron-LM (8.3B),"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019-09-17,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,905.00,,https://arxiv.org/abs/1909.08053,8300000000.00,9.100000000000001e+21,4.40,,,0.00,46400000000.00,1.02e+22,,46400000000.00,WikiText-103; CC-Stories; RealNews; OpenWebtext,10.81,,,1.0,0,Transformer,GPT,https://github.com/NVIDIA/Megatron-LM,1,Megatron-LM (8.3B),Megatron-LM (8.3B)
Memformer (4 encoder + 16 decoder),"Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, Zhou Yu",2020-10-14,Memformer: A Memory-Augmented Transformer for Sequence Modeling,5.00,,https://arxiv.org/abs/2010.06891,76200000.00,12000000000000000000.00,11.93,,,0.00,103000000.00,562000000000000000.00,0.00,103000000.00,WikiText-103,22.74,,,0.0,0,Transformer,Memformer,,1,Memformer (4 encoder + 16 decoder),Memformer (4 encoder + 16 decoder)
MemSizer,"Yizhe Zhang, Deng Cai",2022-03-23,Linearizing Transformer with Key-Value Memory,0.00,0.0,https://web.archive.org/web/20220327055642/https://arxiv.org/pdf/2203.12644.pdf,357000000.00,7300000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,,20.80,,,0.0,0,Transformer,Transformer-XL,,1,MemSizer,MemSizer
MGK 4 heads (medium),"Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",2021-10-16,Improving Transformers with Probabilistic Attention Keys,12.00,,https://arxiv.org/pdf/2110.08678,90000000.00,,120.00,,,0.00,103000000.00,6670000000000000000.00,0.00,103000000.00,,28.86,,,0.0,0,Transformer,Transformer,https://github.com/minhtannguyen/transformer-mgk,1,MGK 4 heads (medium),MGK 4 heads (medium)
MGK 8 heads (small),"Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",2021-10-16,Improving Transformers with Probabilistic Attention Keys,12.00,,https://arxiv.org/pdf/2110.08678,40000000.00,,120.00,,,0.00,103000000.00,2970000000000000000.00,0.00,103000000.00,,33.93,,,0.0,0,Transformer,Transformer,https://github.com/minhtannguyen/transformer-mgk,1,MGK 8 heads (small),MGK 8 heads (small)
"MicroNet (Adaptive, Cache)","Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han",2020-01-01,MicroNet for Efficient Language Modeling,7.00,,http://proceedings.mlr.press/v123/yan20a.html?ref=https://githubhelp.com,8300000.00,,,,,0.00,103000000.00,0.00,0.00,103000000.00,WikiText-103,35.00,,,0.0,1,Transformer,Transformer-XL,,1,"""MicroNet (Adaptive, Cache)""","MicroNet (Adaptive, Cache)"
mini-GPT-2+Active-AdamW,"Davood Wadi, Marc Fredette, Sylvain Senecal",2023-01-24,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,0.00,0.0,https://arxiv.org/pdf/2301.10133.pdf,2980000.00,,200.00,,,0.00,103000000.00,368000000000000000.00,,103000000.00,,9.50,,,0.0,0,Transformer,GPT,,1,mini-GPT-2+Active-AdamW,mini-GPT-2+Active-AdamW
MMLSTM,"Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",2019-12-05,Major–Minor Long Short-Term Memory for Word-Level Language Model,14.00,,http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf,75000000.00,,50.00,,,0.00,103000000.00,2320000000000000000.00,0.00,103000000.00,WikiText-103,44.69,,,0.0,0,Recurrent,LSTM,,1,MMLSTM,MMLSTM
"Mogrifier (d2, MC) + dynamic eval","Gábor Melis, Tomáš Kočiský, Phil Blunsom",2019-09-04,Mogrifier LSTM,109.00,,https://arxiv.org/abs/1909.01792,24000000.00,,145.00,,,0.00,923000.00,,,923000.00,Penn TreeBank,,,44.80,0.0,0,Recurrent,LSTM,https://github.com/deepmind/lamb,1,"""Mogrifier (d2, MC) + dynamic eval""","Mogrifier (d2, MC) + dynamic eval"
"Mogrifier (d2, MoS2, MC) + dynamic eval","Gábor Melis, Tomáš Kočiský, Phil Blunsom",2019-09-04,Mogrifier LSTM,109.00,,https://arxiv.org/abs/1909.01792,35000000.00,,145.00,,,0.00,2080000.00,,,2080000.00,WikiText-2,,38.60,,0.0,0,Recurrent,LSTM,https://github.com/deepmind/lamb,1,"""Mogrifier (d2, MoS2, MC) + dynamic eval""","Mogrifier (d2, MoS2, MC) + dynamic eval"
Mogrifier RLSTM (PTB),Gábor Melis,2022-11-03,Circling Back to Recurrent Models of Language,0.00,1.0,https://arxiv.org/pdf/2211.01848,24000000.00,,400.00,,,0.00,929000.00,53500000000000000.00,,929000.00,,,,42.90,0.0,0,Recurrent,LSTM,,1,Mogrifier RLSTM (PTB),Mogrifier RLSTM (PTB)
Mogrifier RLSTM (WT2),Gábor Melis,2022-11-03,Circling Back to Recurrent Models of Language,0.00,1.0,https://arxiv.org/pdf/2211.01848,35000000.00,,250.00,,,0.00,2080000.00,109000000000000000.00,,2080000.00,,,38.00,,1.0,0,Recurrent,LSTM,,1,Mogrifier RLSTM (WT2),Mogrifier RLSTM (WT2)
Monarch-GPT-2-Medium,"Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",2022-04-01,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,28.00,,https://arxiv.org/pdf/2204.00595,165000000.00,,110.00,,,0.00,4000000000.00,435999999999999930000.00,,4000000000.00,,20.30,,,0.0,0,Transformer,GPT,https://github.com/HazyResearch/monarch,1,Monarch-GPT-2-Medium,Monarch-GPT-2-Medium
Monarch-GPT-2-Small,"Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",2022-04-01,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,28.00,,https://arxiv.org/pdf/2204.00595,72000000.00,,110.00,,,0.00,4000000000.00,190000000000000030000.00,,4000000000.00,,20.70,,,0.0,0,Transformer,GPT,https://github.com/HazyResearch/monarch,1,Monarch-GPT-2-Small,Monarch-GPT-2-Small
MPT-7B,MosaicML NLP Team,2023-05-05,"Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs",,,https://www.mosaicml.com/blog/mpt-7b,7000000000.00,,1.00,,,0.00,1000000000000.00,4.2e+22,,1000000000000.00,,,9.96,,0.5,0,Transformer,MPT,,1,MPT-7B (base),MPT-7B (base)
Multi-cell LSTM,"Thomas Cherian, Akshay Badola, Vineet Padmanabhan",2018-11-15,Multi-cell LSTM Based Neural Language Model,6.00,,https://arxiv.org/pdf/1811.06477,7200000.00,,50.00,,,0.00,929000.00,2010000000000000.00,,929000.00,,,,77.12,0.0,0,Recurrent,LSTM,,1,Multi-cell LSTM,Multi-cell LSTM
Multipop Adaptive Continuous Stack (PTB),"Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",2018-02-15,Memory Architectures in Recurrent Neural Network Language Models,59.00,,https://openreview.net/forum?id=SkFqf0lAZ,11000000.00,,,,,0.00,929000.00,,,929000.00,,,,63.50,0.0,0,Recurrent,RNN,,1,Multipop Adaptive Continuous Stack (PTB),Multipop Adaptive Continuous Stack (PTB)
Multipop Adaptive Continuous Stack (WT2),"Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",2018-02-15,Memory Architectures in Recurrent Neural Network Language Models,59.00,,https://openreview.net/forum?id=SkFqf0lAZ,26000000.00,,,,,0.00,2080000.00,,,2080000.00,,,72.40,,0.0,0,Recurrent,RNN,,1,Multipop Adaptive Continuous Stack (WT2),Multipop Adaptive Continuous Stack (WT2)
N-gram,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",2014-12-24,Learning Longer Memory in Recurrent Neural Networks,306.00,,https://arxiv.org/abs/1412.7753,,,,,,0.00,929000.00,,,929000.00,,,,141.20,0.0,0,N-gram,N-gram,"http://github.com/facebook/SCRNNs, ",0,N-gram,N-gram
N-gram+Cache,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",2014-12-24,Learning Longer Memory in Recurrent Neural Networks,306.00,,https://arxiv.org/abs/1412.7753,,,,,,0.00,929000.00,,,929000.00,,,,125.00,0.0,1,N-gram,N-gram,"http://github.com/facebook/SCRNNs, ",0,N-gram+Cache,N-gram+Cache
NAS+ESS (23M),"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",2020-05-06,Learning Architectures from an Extended Search Space for Language Modeling,12.00,,https://arxiv.org/pdf/2005.02593,23000000.00,,30.00,,,0.00,103000000.00,426000000000000000.00,0.00,103000000.00,,,,45.60,0.0,0,Recurrent,RNN,,1,NAS+ESS (23M),NAS+ESS (23M)
NAS+ESS (156M),"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",2020-05-06,Learning Architectures from an Extended Search Space for Language Modeling,12.00,,https://arxiv.org/pdf/2005.02593,156000000.00,,30.00,,,0.00,103000000.00,2890000000000000000.00,0.00,103000000.00,,29.20,,,0.0,0,Recurrent,RNN,,1,NAS+ESS (156M),NAS+ESS (156M)
Neural Architecture Search with base 8 and shared embeddings,"Barret Zoph, Quoc V. Le",2016-11-05,Neural Architecture Search with Reinforcement Learning,5473.00,,https://arxiv.org/abs/1611.01578,54000000.00,,35.00,,,0.00,929000.00,10500000000000000.00,,929000.00,Penn TreeBank,,,62.40,0.0,0,Recurrent,RNN,https://github.com/tensorflow/models,1,Neural Architecture Search with base 8 and shared embeddings,Neural Architecture Search with base 8 and shared embeddings
Neural cache model (size=2000),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016-12-13,Improving Neural Language Models with a Continuous Cache,302.00,,https://arxiv.org/abs/1612.04426,,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,,68.90,,0.0,1,Recurrent,LSTM,,0,Neural cache model (size=2000),Neural cache model (size=2000)
Neural cache model (size=2000) (300M),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016-12-13,Improving Neural Language Models with a Continuous Cache,302.00,,https://arxiv.org/abs/1612.04426,300000000.00,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,40.80,,,0.0,1,Recurrent,LSTM,,1,Neural cache model (size=2000) (300M),Neural cache model (size=2000) (300M)
NLM,"Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick",2021-09-09,Efficient Nearest Neighbor Language Models,55.00,,https://arxiv.org/abs/2109.04212,515000000.00,7360000000000000000.00,,,,0.00,103000000.00,0.00,103000000.00,206000000.00,WikiText-103,18.66,,,0.0,0,Transformer,kNN-LM,https://github.com/jxhe/efficient-knnlm,1,NLM,NLM
NMM(LSTM+RNN),"Youssef Oualil, Dietrich Klakow",2017-08-23,A Neural Network Approach for Mixing Language Models,10.00,,https://arxiv.org/pdf/1708.06989,5180000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,102.00,0.0,0,Recurrent,RNN+LSTM,,1,NMM(LSTM+RNN),NMM(LSTM+RNN)
NMST+GPT-2,"Eugene Choi, Cheolhyoung Lee, Kyunghyun Cho",2022-10-03,A Non-monotonic Self-terminating Language Model,0.00,1.0,https://web.archive.org/web/20230220171748/https://arxiv.org/pdf/2210.00660.pdf,124000000.00,120000000000000000000.00,2.98,,,0.00,4000000000.00,8870000000000000000.00,103000000.00,4100000000.00,,20.69,,,0.0,0,Transformer,GPT,https://github.com/nyu-dl/non-monotonic-self-terminating-lm,1,NMST+GPT-2,NMST+GPT-2
NoPos,"Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy",2022-03-30,Transformer Language Models without Positional Encodings Still Learn Positional Information,30.00,,https://arxiv.org/abs/2203.16634,1300000000.00,,199.92,,,0.00,103000000.00,161000000000000000000.00,0.00,103000000.00,The Pile (Subset),20.97,,,0.0,0,Transformer,Transformer-XL,https://github.com/adihaviv/NoPos,1,NoPos,NoPos
ONLSTM-SYD,"Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang",2020-05-12,Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach,15.00,,https://arxiv.org/pdf/2005.05864,25000000.00,,1000.00,,,0.00,929000.00,139000000000000000.00,,929000.00,,,,55.70,0.0,0,Recurrent,LSTM,https://github.com/wenyudu/SDLM,1,ONLSTM-SYD,ONLSTM-SYD
OPT-1.3B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1300000000.00,,1.67,,,0.00,180000000000.00,2.34e+21,,180000000000.00,,,16.41,,1.0,0,Transformer,OPT,,1,OPT-1.3B,OPT-1.3B
OPT-1.3B (finetuned on PTB),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1300000000.00,,1.67,,,0.00,180000000000.00,2.34e+21,929000.00,180000000000.00,,,,12.02,0.0,0,Transformer,OPT,,1,OPT-1.3B (finetuned on PTB),OPT-1.3B (finetuned on PTB)
OPT-1.3B (finetuned),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1300000000.00,,1.67,,,0.00,180000000000.00,2.34e+21,2080000.00,180000000000.00,,,12.22,,0.0,0,Transformer,OPT,,1,OPT-1.3B (finetuned),OPT-1.3B (finetuned)
OPT-125M (finetuned on PTB),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,125000000.00,,1.67,,,0.00,180000000000.00,225000000000000000000.00,929000.00,180000000000.00,,,,16.50,0.0,0,Transformer,OPT,,1,OPT-125M (finetuned on PTB),OPT-125M (finetuned on PTB)
OPT-125M (finetuned),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,125000000.00,,1.67,,,0.00,180000000000.00,225000000000000000000.00,2080000.00,180000000000.00,,,19.85,,0.0,0,Transformer,OPT,,1,OPT-125M (finetuned),OPT-125M (finetuned)
OPT-13B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,13000000000.00,,1.67,,,1.00,180000000000.00,2.34e+22,0.00,180000000000.00,,,10.13,,1.0,0,Transformer,OPT,,1,OPT-13B,OPT-13B
OPT-175B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,175000000000.00,4.16e+23,1.67,,,0.00,180000000000.00,3.15e+23,0.00,180000000000.00,,,8.35,,1.0,0,Transformer,OPT,,1,OPT-175B,OPT-175B
OPT-2.7B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,2700000000.00,,1.67,,,1.00,180000000000.00,4.86e+21,0.00,180000000000.00,,,12.47,,1.0,0,Transformer,OPT,,1,OPT-2.7B,OPT-2.7B
OPT-2.7B (finetuned on PTB),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,2700000000.00,,1.67,,,0.00,180000000000.00,4.86e+21,929000.00,180000000000.00,,,,10.80,0.0,0,Transformer,OPT,,1,OPT-2.7B (finetuned on PTB),OPT-2.7B (finetuned on PTB)
OPT-2.7B (finetuned on WT2),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,2700000000.00,,1.67,,,0.00,180000000000.00,4.86e+21,2080000.00,180000000000.00,,,10.27,,0.0,0,Transformer,OPT,,1,OPT-2.7B (finetuned on WT2),OPT-2.7B (finetuned on WT2)
OPT-30B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,30000000000.00,,1.67,,,0.00,180000000000.00,5.4e+22,0.00,180000000000.00,,,10.67,,1.0,0,Transformer,OPT,,1,OPT-30B,OPT-30B
OPT-350M,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,125000000.00,,1.67,,,0.00,180000000000.00,225000000000000000000.00,,180000000000.00,,,25.42,,1.0,0,Transformer,OPT,,1,OPT-350M,OPT-350M
OPT-6.7B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,6700000000.00,,1.67,,,0.00,180000000000.00,1.21e+22,0.00,180000000000.00,,,10.86,,1.0,0,Transformer,OPT,,1,OPT-6.7B,OPT-6.7B
OPT-66B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,OPT: Open Pre-trained Transformer Language Models,656.00,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,66000000000.00,,1.67,,,1.00,180000000000.00,1.19e+23,0.00,180000000000.00,,,9.34,,1.0,0,Transformer,OPT,,1,OPT-66B,OPT-66B
PAR Transformer Large,"Swetha Mandava, Szymon Migacz, Alex Fit Florea",2020-09-09,Pay Attention when Required,11.00,,https://arxiv.org/abs/2009.04534,,,,,,0.00,103000000.00,0.00,0.00,103000000.00,WikiText-103,18.40,,,0.0,0,Transformer,Transformer-XL,,0,PAR Transformer Large,PAR Transformer Large
PermuteFormer,Peng Chen,2021-09-06,PermuteFormer: Efficient Relative Position Encoding for Long Sequences,12.00,,https://arxiv.org/pdf/2109.02377,33000000.00,3100000000000000000.00,30.00,,,0.00,103000000.00,612000000000000000.00,0.00,103000000.00,,32.49,,,0.0,0,Transformer,Performer,https://github.com/cpcp1998/PermuteFormer,1,PermuteFormer,PermuteFormer
Pointer Sentinel-LSTM,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016-09-26,Pointer Sentinel Mixture Models,1558.00,,https://arxiv.org/abs/1609.07843,20000000.00,,64.00,,,0.00,2080000.00,16000000000000000.00,,2080000.00,WikiText-2,,80.80,,0.0,0,Recurrent,LSTM,,1,Pointer Sentinel-LSTM,Pointer Sentinel-LSTM
Pointer Sentinel-LSTM (medium),"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016-09-26,Pointer Sentinel Mixture Models,1558.00,,https://arxiv.org/abs/1609.07843,21000000.00,,64.00,,,0.00,929000.00,7490000000000000.00,,929000.00,Penn TreeBank,,,70.90,0.0,0,Recurrent,LSTM,,1,Pointer Sentinel-LSTM (medium),Pointer Sentinel-LSTM (medium)
Progressive LRD,"Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, Yang Liu",2022-10-12,Strategies for Applying Low Rank Decomposition to Transformer-Based Models,0.00,1.0,https://web.archive.org/web/20221130215920/https://neurips2022-enlsp.github.io/papers/paper_33.pdf,31000000.00,62000000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,,22.00,,,0.0,0,Transformer,GPT,,1,GPT-2 + Progressive LRD,GPT-2 + Progressive LRD
Pythia-1.4b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,1400000000.00,,1.00,,,0.00,300000000000.00,2.52e+21,,300000000000.00,,,14.72,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-1.4b,Pythia-1.4b
Pythia-12b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,12000000000.00,,1.00,,,0.00,300000000000.00,2.16e+22,,300000000000.00,,,10.54,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-12b,Pythia-12b
Pythia-160m,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,160000000.00,,1.00,,,0.00,300000000000.00,288000000000000000000.00,,300000000000.00,,,33.43,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-160m,Pythia-160m
Pythia-1b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,1000000000.00,,1.00,,,0.00,300000000000.00,1.8e+21,,300000000000.00,,,16.45,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-1b,Pythia-1b
Pythia-2.8b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,2800000000.00,,1.00,,,0.00,300000000000.00,5.04e+21,,300000000000.00,,,12.69,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-2.8b,Pythia-2.8b
Pythia-410m,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,410000000.00,,1.00,,,0.00,300000000000.00,738000000000000000000.00,,300000000000.00,,,20.11,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-410m,Pythia-410m
Pythia-6.9b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,6900000000.00,,1.00,,,0.00,300000000000.00,1.2399999999999998e+22,,300000000000.00,,,11.41,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-6.9b,Pythia-6.9b
Pythia-70m,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023-04-03,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.00,,https://arxiv.org/abs/2304.01373,70000000.00,,1.00,,,0.00,300000000000.00,126000000000000000000.00,,300000000000.00,,,57.04,,0.5,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,1,Pythia-70m,Pythia-70m
QRNN,"Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher",2018-02-01,Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours,4.00,0.0,https://mlsys.org/Conferences/doc/2018/50.pdf,135000000.00,360000000000000000.00,14.00,,,0.00,103000000.00,1170000000000000000.00,0.00,103000000.00,WikiText-103,33.00,,,0.0,0,Recurrent/Convolutional,QRNN,,1,QRNN,QRNN
Quantized ADMM,"Junhao Xu, Xie Chen, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",2021-11-29,Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers,9.00,,https://arxiv.org/pdf/2111.14836,,,50.00,,,0.00,929000.00,0.00,,929000.00,,,,115.90,0.0,0,Recurrent,RNN,,0,Quantized ADMM,Quantized ADMM
R-Transformer,"Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang",2019-07-12,R-Transformer: Recurrent Neural Network Enhanced Transformer,93.00,,https://arxiv.org/abs/1907.05572,15800000.00,,100.00,,,0.00,888000.00,8400000000000000.00,,888000.00,Penn TreeBank,,,84.38,0.0,0,Transformer,R-Transformer,https://github.com/DSE-MSU/R-transformer,1,R-Transformer,R-Transformer
Relational Memory Core,"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",2018-06-05,Relational recurrent neural networks,235.00,,https://arxiv.org/abs/1806.01822,,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,31.60,,,0.0,0,Recurrent,LSTM,,0,Relational Memory Core,Relational Memory Core
retrieval-quality-kNN-LMs,"Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, Mohit Iyyer",2022-10-28,"You can’t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM",7.00,,https://arxiv.org/pdf/2210.15859.pdf,247000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,15.50,,,0.0,0,,,https://stanfordnlp.github.io/stanza/,1,retrieval-quality-kNN-LMs,retrieval-quality-kNN-LMs
RETRO-7B,"Sebastian Borgeaud†, Arthur Mensch†, Jordan Hoffmann†, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae‡, Erich Elsen‡ and Laurent Sifre",2021-12-08,Improving language models by retrieving from trillions of tokens,315.00,,https://arxiv.org/abs/2112.04426,7500000000.00,7.5e+21,,,,0.00,2000000000000.00,0.00,,2000000000000.00,MassiveText,21.53,,,1.0,0,Transformer,GPT,,1,RETRO-7B,RETRO-7B
RFA-GATE-Gaussian-Stateful Big,"Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong",2021-03-03,Random Feature Attention,200.00,,https://arxiv.org/abs/2103.02143,242000000.00,,47.72,,,0.00,103000000.00,7140000000000000000.00,0.00,103000000.00,WikiText-103,23.50,,,0.0,0,Transformer,Transformer-XL,,1,RFA-GATE-Gaussian-Stateful Big,RFA-GATE-Gaussian-Stateful Big
RGC+ASQ (PTB),"Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",2018-08-13,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,28.00,,https://arxiv.org/pdf/1808.04357,69000000.00,,40.00,,,0.00,929000.00,15400000000000000.00,,929000.00,,,,74.69,0.0,0,,,,1,RGC+ASQ (PTB),RGC+ASQ (PTB)
RGC+ASQ (WT2),"Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",2018-08-13,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,28.00,,https://arxiv.org/pdf/1808.04357,209000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,87.84,,0.0,0,,,,1,RGC+ASQ (WT2),RGC+ASQ (WT2)
RHN(depth=40),"Ron Shoham, Haim Permuter",2018-05-23,Highway State Gating for Recurrent Highway Networks: improving information flow through time,0.00,1.0,https://arxiv.org/pdf/1805.09238,,,300.00,,,0.00,929000.00,,,929000.00,,,,63.60,0.0,0,Recurrent,RHN,,0,RHN(depth=40),RHN(depth=40)
RHN+HSG(depth=40),"Ron Shoham, Haim Permuter",2018-05-23,Highway State Gating for Recurrent Highway Networks: improving information flow through time,0.00,1.0,https://arxiv.org/pdf/1805.09238,,,300.00,,,0.00,929000.00,,,929000.00,,,,61.70,0.0,0,Recurrent,RHN,,0,RHN+HSG(depth=40),RHN+HSG(depth=40)
RNN,"Tomas Mikolov, Geoffrey Zweig",2012-12-01,Context dependent recurrent neural network language model,716.00,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,6000000.00,,,,,0.00,929000.00,,,929000.00,Penn TreeBank,,,124.70,0.0,0,Recurrent,RNN,,1,RNN,RNN
RNN (SGD+CLR) (PTB),"Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu",2012-12-04,Advances in Optimizing Recurrent Networks,665.00,,https://arxiv.org/abs/1212.0901,2050000.00,,,,,0.00,929000.00,,,929000.00,,,,128.35,0.0,0,Recurrent,RNN,,1,RNN (SGD+CLR) (PTB),RNN (SGD+CLR) (PTB)
RNN + char2-MS-vec,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019-07-15,Character n-Gram Embeddings to Improve RNN Language Models,26.00,,https://ojs.aaai.org/index.php/AAAI/article/view/4438,158000000.00,,,,,0.00,103000000.00,,,103000000.00,,31.92,,,0.0,0,Recurrent/Convolutional,QRNN,,1,RNN + char2-MS-vec,RNN + char2-MS-vec
RNN + char3-MS-vec,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019-07-16,Character n-Gram Embeddings to Improve RNN Language Models,26.00,,https://ojs.aaai.org/index.php/AAAI/article/view/4439,175000000.00,,,,,0.00,103000000.00,,,103000000.00,,31.81,,,0.0,0,Recurrent/Convolutional,QRNN,,1,RNN + char3-MS-vec,RNN + char3-MS-vec
RNN + char4-MS-vec,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019-07-17,Character n-Gram Embeddings to Improve RNN Language Models,26.00,,https://ojs.aaai.org/index.php/AAAI/article/view/4440,226000000.00,,,,,0.00,103000000.00,,,103000000.00,WikiText-103,32.21,,,0.0,0,Recurrent/Convolutional,QRNN,,1,RNN + char4-MS-vec,RNN + char4-MS-vec
RNN Baseline,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019-07-14,Character n-Gram Embeddings to Improve RNN Language Models,26.00,,https://ojs.aaai.org/index.php/AAAI/article/view/4437,153000000.00,,,,,0.00,103000000.00,,,103000000.00,,32.19,,,0.0,0,Recurrent/Convolutional,QRNN,,1,RNN Baseline,RNN Baseline
RNN+LDA,"Tomas Mikolov, Geoffrey Zweig",2012-12-01,Context dependent recurrent neural network language model,716.00,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,7000000.00,,,,,0.00,929000.00,,,929000.00,Penn TreeBank,,,113.70,0.0,0,Recurrent,RNN,,1,RNN+LDA,RNN+LDA
RNN+LDA+KN5+cache,"Tomas Mikolov, Geoffrey Zweig",2012-12-01,Context dependent recurrent neural network language model,716.00,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,9000000.00,,,,,0.00,929000.00,,,929000.00,Penn TreeBank,,,92.00,0.0,1,Recurrent,RNN,,1,RNN+LDA+KN5+cache,RNN+LDA+KN5+cache
RNN+LSA+KN5+cache (model combination w/ linear extrapolation),"Tomas Mikolov, Geoffrey Zweig",2012-12-01,Context dependent recurrent neural network language model,716.00,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,3140000.00,,,,,0.00,929000.00,,,929000.00,Penn TreeBank,,,72.90,0.0,1,Recurrent,RNN,,1,RNN+LSA+KN5+cache (model combination w/ linear extrapolation),RNN+LSA+KN5+cache (model combination w/ linear extrapolation)
RNN+weight noise+dynamic eval,Alex Graves,2013-08-04,Generating Sequences With Recurrent Neural Networks,4734.00,,https://arxiv.org/abs/1308.0850,54000000.00,,14.00,,,0.00,929000.00,4210000000000000.00,,929000.00,,,,117.00,0.0,0,Recurrent,RNN,,1,RNN+weight noise+dynamic eval,RNN+weight noise+dynamic eval
RNNLM + Dynamic KL Regularization,"Thanapon Noraset, David Demeter, Doug Downey",2018-01-01,Controlling Global Statistics in Recurrent Neural Network Text Generation,6.00,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,27700000.00,,20.00,,,0.00,929000.00,3090000000000000.00,,929000.00,WikiText-103,,,77.80,0.0,0,Recurrent,RNN,,1,RNNLM + Dynamic KL Regularization,RNNLM + Dynamic KL Regularization
RNNLM + Dynamic KL Regularization (WT2),"Thanapon Noraset, David Demeter, Doug Downey",2018-01-01,Controlling Global Statistics in Recurrent Neural Network Text Generation,6.00,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,87600000.00,,20.00,,,0.00,2080000.00,21900000000000000.00,,2080000.00,,,86.80,,0.0,0,Recurrent,RNN,,1,RNNLM + Dynamic KL Regularization (WT2),RNNLM + Dynamic KL Regularization (WT2)
RNS-RNN,"Brian DuSell, David Chiang",2021-09-05,Learning Hierarchical Structures with Differentiable Nondeterministic Stacks,5.00,,https://arxiv.org/pdf/2109.01982,5660000.00,,100.00,,,0.00,929000.00,3150000000000000.00,,929000.00,,,,117.56,0.0,0,Recurrent,RNN,https://github.com/bdusell/nondeterministic-stack-rnn,1,RNS-RNN,RNS-RNN
Routing Transformer,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",2020-03-12,Efficient Content-Based Sparse Attention with Routing Transformers,349.00,,https://arxiv.org/abs/2003.05997,79500000.00,,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,15.80,,,0.0,0,Transformer,Local transformer,https://github.com/google-research/google-research/tree/master/routing_transformer,1,Routing Transformer,Routing Transformer
RSM,"David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo",2019-05-28,Learning distant cause and effect using only local and immediate credit assignment,3.00,1.0,https://arxiv.org/pdf/1905.11589,,,,,,0.00,929000.00,0.00,,929000.00,,,,166.00,0.0,0,,,https://github.com/Cerenaut/rsm,0,RSM,RSM
rTop-k(distributed setting),"Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur",2020-05-21,rTop-k: A Statistical Estimation Approach to Distributed SGD,41.00,,https://arxiv.org/pdf/2005.10761,69000000.00,,38.00,,,0.00,929000.00,14600000000000000.00,,929000.00,,,,82.49,0.0,0,Recurrent,RNN,,1,rTop-k(distributed setting),rTop-k(distributed setting)
S + I-Attention (3),"Artyom Gadetsky, Ilya Yakubovskiy, Dmitry Vetrov",2018-06-26,Conditional Generators of Words Definitions,56.00,,https://arxiv.org/abs/1806.10090,,,35.00,,,0.00,103000000.00,,1080000.00,104000000.00,Oxford Dictionary,43.54,,,1.0,0,Recurrent,LSTM,https://github.com/sbos/AdaGram.jl,0,S + I-Attention (3),S + I-Attention (3)
S4,"Albert Gu, Karan Goel, Christopher Ré",2021-10-31,Efficiently Modeling Long Sequences with Structured State Spaces,171.00,,https://arxiv.org/abs/2111.00396,249000000.00,600000000000000000000.00,509.02,,,0.00,103000000.00,78300000000000000000.00,0.00,103000000.00,WikiText-103,20.95,,,0.0,0,State Space Model,S4,https://github.com/HazyResearch/state-spaces,1,S4,S4
Sandwich Transformer,"Ofir Press, Noah A. Smith, Omer Levy",2019-11-10,Improving Transformer Models by Reordering their Sublayers,57.00,,https://arxiv.org/abs/1911.03864,209000000.00,,180.00,,,0.00,700000000.00,158000000000000000000.00,0.00,700000000.00,Toronto Books Corpus,17.84,,,1.0,0,Transformer,Transformer-XL,https://github.com/ofirpress/sandwich_transformer,1,Sandwich Transformer,Sandwich Transformer
Scatterbrain,"Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré",2021-10-28,Scatterbrain: Unifying Sparse and Low-rank Attention Approximation,8.00,,https://web.archive.org/web/20220808053741/https://arxiv.org/pdf/2110.15343.pdf,,,30.00,,,0.00,103000000.00,,0.00,103000000.00,,26.72,,,0.0,0,Transformer,Transformer,https://github.com/HazyResearch/scatterbrain,0,Scatterbrain,Scatterbrain
SCRN(Structurally Constrained Recurrent Network),"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",2014-12-24,Learning Longer Memory in Recurrent Neural Networks,306.00,,https://arxiv.org/abs/1412.7753,26500000.00,,,,,1.00,929000.00,,,929000.00,,,,115.00,0.0,0,Recurrent,RNN,https://github.com/facebookarchive/SCRNNs,1,SCRN(Structurally Constrained Recurrent Network),SCRN(Structurally Constrained Recurrent Network)
Search-Proven Best LSTM,"R. Józefowicz, Wojciech Zaremba, Ilya Sutskever",2015-07-06,An Empirical Exploration of Recurrent Network Architectures,2207.00,,https://proceedings.mlr.press/v37/jozefowicz15.pdf,20000000.00,,30.00,,,0.00,929000.00,3340000000000000.00,,929000.00,,,,79.83,0.0,0,Recurrent,LSTM,,1,Search-Proven Best LSTM,Search-Proven Best LSTM
"Segatron -XL base, M=150 + HCP","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022-03-21,Better Language Model with Hypernym Class Prediction,3.00,1.0,https://arxiv.org/abs/2203.10692,151000000.00,,18.64,,,0.00,103000000.00,1740000000000000000.00,,103000000.00,WikiText-103,22.10,,,0.0,0,Transformer,Transformer-XL,https://github.com/richardbaihe/robustLM,1,"""Segatron -XL base, M=150 + HCP""","Segatron -XL base, M=150 + HCP"
"Segatron XL base, M=384","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",2020-04-30,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,13.00,,https://arxiv.org/abs/2004.14996,151000000.00,,18.64,,,0.00,103000000.00,1740000000000000000.00,,103000000.00,WikiText-103,22.50,,,0.0,0,Transformer,Transformer-XL,https://github.com/rsvp-ai/segatron_aaai,1,"""Segatron XL base, M=384""","Segatron XL base, M=384"
"Segatron XL large, M=384","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",2020-04-30,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,13.00,,https://arxiv.org/abs/2004.14996,257000000.00,,167.02,,,0.00,103000000.00,26500000000000000000.00,,103000000.00,WikiText-103,17.10,,,0.0,0,Transformer,Transformer-XL,https://github.com/rsvp-ai/segatron_aaai,1,"""Segatron XL large, M=384""","Segatron XL large, M=384"
"Segatron-XL large, M=384 + HCP","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022-03-21,Better Language Model with Hypernym Class Prediction,3.00,1.0,https://arxiv.org/abs/2203.10692,257000000.00,,167.02,,,0.00,103000000.00,26500000000000000000.00,,103000000.00,WikiText-103,17.00,,,0.0,0,Transformer,Transformer-XL,https://github.com/richardbaihe/robustLM,1,"""Segatron-XL large, M=384 + HCP""","Segatron-XL large, M=384 + HCP"
Selfish-RNN (AWD-LSTM-MoS),"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021-01-22,Selfish Sparse RNN Training,31.00,,https://arxiv.org/pdf/2101.09048,15600000.00,,1000.00,,,0.00,2080000.00,195000000000000000.00,,2080000.00,,,63.05,,0.0,0,Recurrent,LSTM,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,1,Selfish-RNN (AWD-LSTM-MoS),Selfish-RNN (AWD-LSTM-MoS)
Selfish-RNN (ON-LSTM),"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021-01-22,Selfish Sparse RNN Training,31.00,,https://arxiv.org/pdf/2101.09048,11300000.00,,1000.00,,,0.00,929000.00,63000000000000000.00,,929000.00,,,,55.82,0.0,0,Recurrent,LSTM,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,1,Selfish-RNN (ON-LSTM),Selfish-RNN (ON-LSTM)
Selfish-RNN (SNT-ASGD) Stacked LSTMs,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021-01-22,Selfish Sparse RNN Training,31.00,,https://arxiv.org/pdf/2101.09048,25200000.00,,100.00,,,0.00,929000.00,14000000000000000.00,,929000.00,,,,71.42,0.0,0,Recurrent,LSTM,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,1,Selfish-RNN (SNT-ASGD) Stacked LSTMs,Selfish-RNN (SNT-ASGD) Stacked LSTMs
Selfish-RNN (SNT-ASGD)RHNs,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021-01-22,Selfish Sparse RNN Training,31.00,,https://arxiv.org/pdf/2101.09048,7600000.00,,500.00,,,0.00,929000.00,21200000000000000.00,,929000.00,,,,64.03,0.0,0,Recurrent,RHN,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,1,Selfish-RNN (SNT-ASGD)RHNs,Selfish-RNN (SNT-ASGD)RHNs
Shortformer,"Ofir Press, Noah A. Smith, Mike Lewis",2020-12-31,Shortformer: Better Language Modeling using Shorter Inputs,43.00,,https://arxiv.org/abs/2012.15832,24000000.00,,205.00,,,0.00,103000000.00,3040000000000000000.00,0.00,103000000.00,WikiText-103,18.15,,,0.0,0,Transformer,Shortformer,https://github.com/ofirpress/shortformer,1,Shortformer,Shortformer
SPALM + kNN,"Dani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong",2021-04-26,Adaptive Semiparametric Language Models,70.00,,https://web.archive.org/web/20230210050534/https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00371/100688/Adaptive-Semiparametric-Language-Models,,,,,,0.00,110000000.00,,,110000000.00,,17.66,,,0.0,0,Transformer,Transformer,,0,SPALM + kNN,SPALM + kNN
SPALM + RelationLM,"Qi Liu, Dani Yogatama, Phil Blunsom",2022-01-24,Relational Memory-Augmented Language Models,21.00,,https://arxiv.org/pdf/2201.09680,124000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,18.60,,,0.0,0,Transformer,Transformer-XL,,1,SPALM + RelationLM,SPALM + RelationLM
Sparse Wide GPT-3 Small,"Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie",2023-03-21,Sparse Iso-FLOP Transformations for Maximizing Training Efficiency,0.00,0.0,https://arxiv.org/pdf/2303.11525.pdf,1300000000.00,,110.00,,,0.00,103000000.00,88400000000000000000.00,,103000000.00,,20.40,,,0.0,0,Transformer,GPT,https://github.com/CerebrasResearch/Sparse-IFT,1,Sparse Wide GPT-3 Small,Sparse Wide GPT-3 Small
SparseOPT-13B,"Elias Frantar, Dan Alistarh",2023-01-02,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.00,,https://arxiv.org/abs/2301.00774,6500000000.00,,1.67,,,1.00,180000000000.00,1.17e+22,0.00,180000000000.00,,11.17,,,1.0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,1,SparseOPT-13B,SparseOPT-13B
SparseOPT-175B,"Elias Frantar, Dan Alistarh",2023-01-02,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.00,,https://arxiv.org/abs/2301.00774,87500000000.00,,1.67,,,1.00,180000000000.00,1.58e+23,0.00,180000000000.00,,8.21,,,1.0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,1,SparseOPT-175B,SparseOPT-175B
SparseOPT-30B,"Elias Frantar, Dan Alistarh",2023-01-02,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.00,,https://arxiv.org/abs/2301.00774,15000000000.00,,1.67,,,1.00,180000000000.00,2.7e+22,0.00,180000000000.00,,9.79,,,1.0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,1,SparseOPT-30B,SparseOPT-30B
SparseOPT-66B,"Elias Frantar, Dan Alistarh",2023-01-02,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.00,,https://arxiv.org/abs/2301.00774,33000000000.00,,1.67,,,1.00,180000000000.00,5.94e+22,0.00,180000000000.00,,9.32,,,1.0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,1,SparseOPT-66B,SparseOPT-66B
SPN-4,"W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",2014-01-01,Language modeling with sum-product networks,102.00,,https://spn.cs.washington.edu/papers/is14.pdf,5000000.00,,,,,0.00,1010000.00,,,1010000.00,Penn TreeBank,,,100.00,0.0,0,,,https://github.com/stakok/lmspn,1,SPN-4,SPN-4
SPN-4+KN5,"W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",2014-01-01,Language modeling with sum-product networks,102.00,,https://spn.cs.washington.edu/papers/is14.pdf,5000000.00,44000000000000000.00,,,,0.00,1010000.00,,,1010000.00,Penn TreeBank,,,80.60,0.0,0,,,https://github.com/stakok/lmspn,1,SPN-4+KN5,SPN-4+KN5
SRU++ Base,Tao Lei,2021-02-24,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,35.00,,https://arxiv.org/abs/2102.12459,108000000.00,5800000000000000000.00,25.56,,,0.00,103000000.00,1710000000000000000.00,0.00,103000000.00,WikiText-103,18.30,,,0.0,0,Recurrent,SRU,https://github.com/asappresearch/sru,1,SRU++ Base,SRU++ Base
SRU++ Large,Tao Lei,2021-02-24,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,35.00,,https://arxiv.org/abs/2102.12459,234000000.00,11000000000000000000.00,34.08,,,0.00,103000000.00,4930000000000000000.00,0.00,103000000.00,WikiText-103,17.10,,,0.0,0,Recurrent,SRU,https://github.com/asappresearch/sru,1,SRU++ Large,SRU++ Large
SRU++ Large only 2 attention layers (k=5),Tao Lei,2021-02-24,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,35.00,,https://arxiv.org/abs/2102.12459,225000000.00,8000000000000000000.00,34.08,,,0.00,103000000.00,4740000000000000000.00,0.00,103000000.00,WikiText-103,17.30,,,0.0,0,Recurrent,SRU,https://github.com/asappresearch/sru,1,SRU++ Large only 2 attention layers (k=5),SRU++ Large only 2 attention layers (k=5)
Stack RNN,"Armand Joulin, Tomas Mikolov",2015-03-03,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,440.00,,https://arxiv.org/abs/1503.01007,2010000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,118.00,0.0,0,Recurrent,RNN,"https://github.com/facebook/Stack-RNN, ",1,Stack RNN,Stack RNN
Stacked-LSTM+Pruning,"Liangjian Wen, Xuanyang Zhang, Haoli Bai, Zenglin Xu",2019-06-17,Structured Pruning of Recurrent Neural Networks through Neuron Selection,34.00,,https://arxiv.org/pdf/1906.06847,6160000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,78.08,0.0,0,Recurrent,LSTM,,1,Stacked-LSTM+PruningNo,Stacked-LSTM+PruningNo
Subformer (122M),"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021-01-01,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,12.00,,https://arxiv.org/abs/2101.00234,122000000.00,,70.29,,,0.00,103000000.00,5300000000000000000.00,0.00,103000000.00,WikiText-103,19.90,,,0.0,0,Transformer,Subformer,https://github.com/machelreid/subformer,1,Subformer (122M),Subformer (122M)
Subformer (83M),"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021-01-01,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,12.00,,https://arxiv.org/abs/2101.00234,83000000.00,,70.29,,,0.00,103000000.00,3610000000000000000.00,0.00,103000000.00,WikiText-103,20.88,,,0.0,0,Transformer,Subformer,https://github.com/machelreid/subformer,1,Subformer (83M),Subformer (83M)
Subformer (96M),"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021-01-01,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,12.00,,https://arxiv.org/abs/2101.00234,96000000.00,,70.29,,,0.00,103000000.00,4170000000000000000.00,0.00,103000000.00,WikiText-103,20.39,,,0.0,0,Transformer,Subformer,https://github.com/machelreid/subformer,1,Subformer (96M),Subformer (96M)
T2R + Pretrain,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021-03-24,Finetuning Pretrained Transformers into RNNs,30.00,,https://arxiv.org/abs/2103.13076,450000000.00,20300000000000000000.00,34.47,,,1.00,103000000.00,9590000000000000000.00,103000000.00,206000000.00,WikiText-103,19.60,,,0.0,0,Transformer,ELU,https://github.com/jungokasai/T2R/,1,T2R + Pretrain,T2R + Pretrain
T2R + Random Init,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021-03-24,Finetuning Pretrained Transformers into RNNs,30.00,,https://arxiv.org/abs/2103.13076,450000000.00,61000000000000000000.00,205.48,,,1.00,103000000.00,57100000000000010000.00,,103000000.00,,20.80,,,0.0,0,Transformer,ELU,https://github.com/jungokasai/T2R/,1,T2R + Random Init,T2R + Random Init
T2R 75% + Pretrain,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021-03-24,Finetuning Pretrained Transformers into RNNs,30.00,,https://arxiv.org/abs/2103.13076,450000000.00,19300000000000000000.00,34.47,,,1.00,103000000.00,9590000000000000000.00,103000000.00,206000000.00,WikiText-103,18.50,,,0.0,0,Transformer,ELU,https://github.com/jungokasai/T2R/,1,T2R 75% + Pretrain,T2R 75% + Pretrain
TaLK Convolution,"Vasileios Lioutas, Yuhong Guo",2020-02-08,Time-aware Large Kernel Convolutions,24.00,,https://arxiv.org/abs/2002.03184,240000000.00,,187.43,,,0.00,103000000.00,27800000000000000000.00,0.00,103000000.00,WikiText-103,23.30,,,0.0,0,Convolutional,Transformer,https://github.com/lioutasb/TaLKConvolutions,1,TaLK Convolution,TaLK Convolution
TCN (13M),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-02-15,Convolutional Sequence Modeling Revisited,64.00,,https://openreview.net/forum?id=rk8wKk-R-,13000000.00,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,,,90.17,0.0,0,Convolutional,TCN,,1,TCN (13M),TCN (13M)
TCN (148M),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-02-15,Convolutional Sequence Modeling Revisited,64.00,,https://openreview.net/forum?id=rk8wKk-R-,148000000.00,,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,45.19,,,0.0,0,Convolutional,TCN,,1,TCN (148M),TCN (148M)
Temporal Convolutional Attention-based Network(TCAN) (PTB),"Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",2020-02-28,Temporal Convolutional Attention-based Network For Sequence Modeling,33.00,,https://arxiv.org/pdf/2002.12530,13000000.00,,,,,0.00,103000000.00,0.00,,103000000.00,,,,26.92,0.0,0,,,https://github.com/haohy/TCAN,1,Temporal Convolutional Attention-based Network(TCAN) (PTB),Temporal Convolutional Attention-based Network(TCAN) (PTB)
Temporal Convolutional Attention-based Network(TCAN) (WT2),"Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",2020-02-28,Temporal Convolutional Attention-based Network For Sequence Modeling,33.00,,https://arxiv.org/pdf/2002.12530,33000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,6.66,,0.0,0,,,https://github.com/haohy/TCAN,1,Temporal Convolutional Attention-based Network(TCAN) (WT2),Temporal Convolutional Attention-based Network(TCAN) (WT2)
Tensor-Transformer(1core)+PN (PTB),"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",2020-03-17,PowerNorm: Rethinking Batch Normalization in Transformers,60.00,,https://arxiv.org/pdf/2003.07845,12000000.00,,30.00,,,0.00,929000.00,2010000000000000.00,,929000.00,,,,47.60,0.0,0,Transformer,Tensorized Transformer,https://github.com/sIncerass/powernorm,1,Tensor-Transformer(1core)+PN (PTB),Tensor-Transformer(1core)+PN (PTB)
Tensor-Transformer(1core)+PN (WT103),"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",2020-03-17,PowerNorm: Rethinking Batch Normalization in Transformers,60.00,,https://arxiv.org/pdf/2003.07845,85300000.00,,30.00,,,0.00,103000000.00,1580000000000000000.00,,103000000.00,,17.90,,,0.0,0,Transformer,Tensorized Transformer,https://github.com/sIncerass/powernorm,1,Tensor-Transformer(1core)+PN (WT103),Tensor-Transformer(1core)+PN (WT103)
Tensorized Transformer (151M),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019-06-24,A Tensorized Transformer for Language Modeling,126.00,,https://arxiv.org/abs/1906.09777,151000000.00,,30.00,,,0.00,103000000.00,2800000000000000000.00,,103000000.00,WikiText-103,18.80,,,0.0,0,Transformer,Tensorized Transformer,,1,Tensorized Transformer (151M),Tensorized Transformer (151M)
Tensorized Transformer (257M),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019-06-24,A Tensorized Transformer for Language Modeling,126.00,,https://arxiv.org/abs/1906.09777,257000000.00,,30.00,,,0.00,103000000.00,4760000000000000000.00,,103000000.00,WikiText-103,21.20,,,0.0,0,Transformer,Tensorized Transformer,,1,Tensorized Transformer (257M),Tensorized Transformer (257M)
Tensorized Transformer (core-2),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019-06-24,A Tensorized Transformer for Language Modeling,126.00,,https://arxiv.org/abs/1906.09777,85300000.00,,30.00,,,0.00,103000000.00,1580000000000000000.00,,103000000.00,WikiText-103,18.90,,,0.0,0,Transformer,Tensorized Transformer,,1,Tensorized Transformer (core-2),Tensorized Transformer (core-2)
Tensorized Transformer (large PTB),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019-06-24,A Tensorized Transformer for Language Modeling,126.00,,https://arxiv.org/abs/1906.09777,24000000.00,,30.00,,,0.00,929000.00,4010000000000000.00,,929000.00,WikiText-103,,,52.70,0.0,0,Transformer,Tensorized Transformer,,1,Tensorized Transformer (large PTB),Tensorized Transformer (large PTB)
Tensorized Transformer (small),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019-06-24,A Tensorized Transformer for Language Modeling,126.00,,https://arxiv.org/abs/1906.09777,12000000.00,,30.00,,,0.00,929000.00,2010000000000000.00,,929000.00,WikiText-103,,,57.90,0.0,0,Transformer,Tensorized Transformer,,1,Tensorized Transformer (small),Tensorized Transformer (small)
TF-LM-discourse LSTM (PTB),"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",2018-05-01,TF-LM: TensorFlow-based Language Modeling Toolkit,7.00,,https://aclanthology.org/L18-1470.pdf,,,39.00,,,0.00,929000.00,,,929000.00,,,,84.10,0.0,0,Recurrent,LSTM,https://github.com/lverwimp/tf-lm,0,TF-LM-discourse LSTM (PTB),TF-LM-discourse LSTM (PTB)
TF-LM-discourse LSTM (WT2),"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",2018-05-01,TF-LM: TensorFlow-based Language Modeling Toolkit,7.00,,https://aclanthology.org/L18-1470.pdf,,,39.00,,,0.00,2080000.00,,,2080000.00,,,98.20,,0.0,0,Recurrent,LSTM,https://github.com/lverwimp/tf-lm,0,TF-LM-discourse LSTM (WT2),TF-LM-discourse LSTM (WT2)
top-down frozen classifier,"Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals",2021-02-09,Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers,2.00,0.0,https://arxiv.org/pdf/2102.04697,,,,,,0.00,,,,0.00,,,65.20,,0.0,0,,,,0,top-down frozen classifier,top-down frozen classifier
TPM-LVD,"Anji Liu, Honghua Zhang, Guy Van den Broeck",2022-10-10,Scaling up Probabilistic Circuits by Latent Variable Distillation,7.00,,https://arxiv.org/pdf/2210.04398.pdf,1120000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,197.50,,0.0,0,,,,1,TPM-LVD,TPM-LVD
Transformer (Adaptive Input Embeddings),"Alexei Baevski, Michael Auli",2018-09-28,Adaptive Input Representations for Neural Language Modeling,337.00,,https://arxiv.org/abs/1809.10853,247000000.00,73000000000000000000.00,180.00,,,0.00,103000000.00,27500000000000000000.00,0.00,103000000.00,WikiText-103,18.70,,,0.0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/fairseq,1,Transformer (Adaptive Input Embeddings) WT103,Transformer (Adaptive Input Embeddings) WT103
Transformer + Average Attention Network,"Jian Guo Zhang, Jian Ping Li, Huang Li",2019-01-01,Language Modeling with Transformer,126.00,,https://ieeexplore.ieee.org/abstract/document/9067534,,,,,,1.00,103000000.00,,0.00,103000000.00,WikiText-103,22.13,,,0.0,0,Transformer,Transformer,,0,Transformer + Average Attention Network,Transformer + Average Attention Network
Transformer + GFM,"Hao Yu, Jianxin Wu",2022-12-01,"Compressing Transformers: Features Are Low-Rank, but Weights Are Not",0.00,1.0,https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf,185000000.00,8040000000000000000.00,,,,0.00,103000000.00,,,103000000.00,,20.05,,,0.0,0,Transformer,Transformer-XL,,1,Transformer + GFM,Transformer + GFM
Transformer Large + HCP,"He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022-03-21,Better Language Model with Hypernym Class Prediction,3.00,1.0,https://arxiv.org/abs/2203.10692,257000000.00,,38.18,,,0.00,103000000.00,6060000000000000000.00,,103000000.00,WikiText-103,25.30,,,0.0,0,Transformer,Transformer-XL,https://github.com/richardbaihe/robustLM,1,Transformer Large + HCP,Transformer Large + HCP
Transformer LM + MinSen,"Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",2021-11-29,Mixed Precision of Quantization of Transformer Language Models for Speech Recognition,9.00,,https://arxiv.org/pdf/2112.11540,,,,,,0.00,929000.00,0.00,,929000.00,,,,56.82,0.0,0,,,,0,Transformer LM + MinSen,Transformer LM + MinSen
Transformer-C,"Simeng Sun, Mohit Iyyer",2021-04-08,Revisiting Simple Neural Probabilistic Language Models,10.00,,https://arxiv.org/abs/2104.03474,148000000.00,21000000000000000.00,19.88,,,0.00,103000000.00,1820000000000000000.00,0.00,103000000.00,WikiText-103,25.10,,,0.0,0,Transformer,Transformer,https://github.com/SimengSun/revisit-nplm,1,Transformer-C,Transformer-C
Transformer-XL + AutoDropout (PTB),"Hieu Pham, Quoc V. Le",2021-01-05,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,45.00,,https://arxiv.org/abs/2101.01761,24000000.00,,,,,0.00,929000.00,,,929000.00,Penn TreeBank,,,54.90,0.0,0,Transformer,Transformer-XL,https://github.com/google-research/googleresearch/tree/master/auto_dropout,1,Transformer-XL + AutoDropout (PTB),Transformer-XL + AutoDropout (PTB)
Transformer-XL + AutoDropout (WT2),"Hieu Pham, Quoc V. Le",2021-01-05,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,45.00,,https://arxiv.org/abs/2101.01761,35000000.00,,,,,0.00,2080000.00,,,2080000.00,WikiText-2,,59.90,,0.0,0,Transformer,Transformer-XL,https://github.com/google-research/googleresearch/tree/master/auto_dropout,1,Transformer-XL + AutoDropout (WT2),Transformer-XL + AutoDropout (WT2)
Transformer-XL + RMS dynamic eval,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2019-04-17,Dynamic Evaluation of Transformer Language Models,40.00,,https://arxiv.org/abs/1904.08378,257000000.00,,,,,0.00,103000000.00,0.00,103000000.00,206000000.00,WikiText-103,16.40,,,0.0,0,Transformer,Transformer-XL,https://github.com/benkrause/dynamiceval-transformer,1,Transformer-XL + RMS dynamic eval,Transformer-XL + RMS dynamic eval
Transformer-XL + RMT,"Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev",2022-07-14,Recurrent Memory Transformer,19.00,,https://web.archive.org/web/20220715153256/https://arxiv.org/pdf/2207.06881.pdf,247000000.00,,,,,0.00,103000000.00,,103000000.00,206000000.00,,23.99,,,0.0,0,Recurrent/Transformer,Transformer-XL,"https://github.com/booydar/transformer-xl, https://github.com/kimiyoung/transformer-xl, https://github.com/GokuMohandas/fast-weights/blob/539fb10e3c384d5f782af2560bf28631cd0eaa61/, https://github.com/kimiyoung/transformer-xl, ",1,Transformer-XL + RMT,Transformer-XL + RMT
Transformer-XL + SIS,"Sagar Verma, Jean-Christophe Pesquet",2021-05-03,Sparsifying Networks via Subdifferential Inclusion,9.00,,https://web.archive.org/web/20220122141508/http://proceedings.mlr.press/v139/verma21b/verma21b.pdf,246000000.00,10400000000000000000.00,,,,0.00,103000000.00,,,103000000.00,,21.10,,,0.0,0,Transformer,Transformer-XL,https://sagarverma.github.io/compression,1,Transformer-XL + SIS,Transformer-XL + SIS
Transformer-XL DeFINE (107M),"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019-11-27,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.00,,https://arxiv.org/abs/1911.12385,107000000.00,5200000000000000000.00,20.00,,,0.00,103000000.00,1330000000000000000.00,,103000000.00,WikiText-103; Penn Treebank,25.72,,,0.0,0,Recurrent,LSTM,,1,Transformer-XL DeFINE (107M),Transformer-XL DeFINE (107M)
Transformer-XL DeFINE (141M),"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019-11-27,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.00,,https://arxiv.org/abs/1911.12385,141000000.00,6200000000000000000.00,20.00,,,0.00,103000000.00,1750000000000000000.00,,103000000.00,WikiText-103; Penn Treebank,24.17,,,0.0,0,Recurrent,LSTM,,1,Transformer-XL DeFINE (141M),Transformer-XL DeFINE (141M)
Transformer-XL Large,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",2019-01-09,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,3155.00,,https://arxiv.org/abs/1901.02860,257000000.00,10900000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,WikiText-103,18.30,,,0.0,0,Transformer,Transformer-XL,https://github.com/kimiyoung/,1,Transformer-XL (257M),Transformer-XL (257M)
Transformer-XL Large + Phrase Induction,"Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",2019-06-04,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",12.00,,https://arxiv.org/abs/1906.01702,257000000.00,7300000000000000000.00,1.00,,,0.00,103000000.00,159000000000000000.00,103000000.00,206000000.00,WikiText-103,17.40,,,0.0,0,Transformer,Transformer-XL,https://github.com/luohongyin/PILM,1,Transformer-XL Large + Phrase Induction,Transformer-XL Large + Phrase Induction
Transformer-XL-ptb,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",2019-01-09,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,3155.00,,https://arxiv.org/abs/1901.02860,257000000.00,10900000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,Penn TreeBank,,,54.52,1.0,0,Transformer,Transformer-XL,https://github.com/kimiyoung/,1,Transformer-XL-ptb,Transformer-XL-ptb
Transformer-XL+AdamGapAware(GA),"Saar Barkai, Ido Hakimi, Assaf Schuster",2019-09-24,Gap Aware Mitigation of Gradient Staleness,12.00,,https://arxiv.org/pdf/1909.10802,257000000.00,,,,,0.00,103000000.00,0.00,103000000.00,206000000.00,,26.48,,,0.0,0,Transformer,Transformer-XL,,1,Transformer-XL+AdamGapAware(GA),Transformer-XL+AdamGapAware(GA)
Transformer-XL+AdamP,"Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",2020-06-15,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,114.00,,https://arxiv.org/pdf/2006.08217,257000000.00,,,,,0.00,103000000.00,,103000000.00,206000000.00,,23.26,,,0.0,0,Transformer,Transformer-XL,https://github.com/clovaai/adamp,1,Transformer-XL+AdamP,Transformer-XL+AdamP
Transformer-XL+WN+AdamP,"Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",2020-06-15,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,114.00,,https://arxiv.org/pdf/2006.08217,257000000.00,,,,,0.00,103000000.00,,,103000000.00,,22.77,,,0.0,0,Transformer,Transformer-XL,https://github.com/clovaai/adamp,1,Transformer-XL+WN+AdamP,Transformer-XL+WN+AdamP
Transformer+Recurrent Windows of Context,"Davis Yoshida, Allyson Ettinger, Kevin Gimpel",2020-08-16,Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size,4.00,0.5,https://arxiv.org/pdf/2008.07027,124000000.00,116999999999999980000.00,2.00,,,0.00,4000000000.00,5950000000000000000.00,103000000.00,4100000000.00,,26.73,,,0.0,0,Recurrent/Transformer,GPT,,1,Transformer+Recurrent Windows of Context,Transformer+Recurrent Windows of Context
TransformerXL + FWL,"Kevin Clark, Kelvin Guu, Ming-Wei Chang, Panupong Pasupat, Geoffrey Hinton, Mohammad Norouzi",2022-12-05,Meta-Learning Fast Weight Language Models,2.00,1.0,https://web.archive.org/web/20221207113900/https://arxiv.org/pdf/2212.02475.pdf,257000000.00,,,,,0.00,103000000.00,,103000000.00,206000000.00,,16.60,,,0.0,0,Transformer,Transformer-XL,,1,Transformer-XL + FWL,Transformer-XL + FWL
Transformer-XL + PowerSGD + L-Greco,"Mohammadreza Alimohammadi, Ilia Markov, Elias Frantar, Dan Alistarh",2022-10-31,L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression,3.00,0.0,https://web.archive.org/web/20221101102609/https://arxiv.org/pdf/2210.17357.pdf,,414000000000000000.00,,,,0.00,103000000.00,,0.00,103000000.00,,24.08,,,0.0,0,Transformer,Transformer-XL,https://github.com/LGrCo/L-GreCo,0,Transformer-XL + PowerSGD + L-Greco,Transformer-XL + PowerSGD + L-Greco
TransformerXL + spectrum control,"Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",2020-03-11,Improving Neural Language Generation with Spectrum Control,55.00,,https://openreview.net/forum?id=ByxY8CNtvr,151000000.00,460000000000000000.00,250.00,,,0.00,103000000.00,23300000000000000000.00,0.00,103000000.00,WikiText-103,23.20,,,0.0,0,Transformer,Transformer-XL,,1,TransformerXL + spectrum control,TransformerXL + spectrum control
TransformerXL-LayerFusion-CA,"James O' Neill, Greg Ver Steeg, Aram Galstyan",2020-07-29,Compressing Deep Neural Networks via Layer Fusion,5.00,,https://arxiv.org/pdf/2007.14917,,,,,,0.00,103000000.00,,2080000.00,105000000.00,,,11.13,,0.0,0,Transformer,Transformer-XL,,0,TransformerXL-LayerFusion-CA,TransformerXL-LayerFusion-CA
TransformerXL+RelationLM,"Qi Liu, Dani Yogatama, Phil Blunsom",2022-01-24,Relational Memory-Augmented Language Models,21.00,,https://arxiv.org/pdf/2201.09680,124000000.00,3.2e+21,,,,0.00,103000000.00,0.00,,103000000.00,WikiText-103,18.60,,,0.0,0,Transformer,Transformer-XL,,1,TransformerXL+RelationLM,TransformerXL+RelationLM
TransfoRNN(d=1024)(2-layer) (PTB),"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",2021-04-04,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,0.00,0.0,https://arxiv.org/pdf/2104.01572,49900000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,82.10,0.0,0,Transformer/RNN,TransfoRNN,,1,TransfoRNN(d=1024)(2-layer) (PTB),TransfoRNN(d=1024)(2-layer) (PTB)
TransfoRNN(d=1024)(2-layer) (WT2),"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",2021-04-04,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,0.00,0.0,https://arxiv.org/pdf/2104.01572,97600000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,94.80,,0.0,0,Transformer/RNN,TransfoRNN,,1,TransfoRNN(d=1024)(2-layer) (WT2),TransfoRNN(d=1024)(2-layer) (WT2)
TrellisNet,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-10-15,Trellis Networks for Sequence Modeling,132.00,,https://arxiv.org/abs/1810.06682,180000000.00,,25.00,,,0.00,103000000.00,2780000000000000000.00,0.00,103000000.00,WikiText-103,29.19,,,0.0,0,Recurrent/Convolutional,TrellisNet,https://github.com/locuslab/trellisnet,1,TrellisNet,TrellisNet
TrellisNet-MoS (1.4x larger),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018-10-15,Trellis Networks for Sequence Modeling,132.00,,https://arxiv.org/abs/1810.06682,180000000.00,,25.00,,,0.00,,,0.00,929000.00,Penn TreeBank,0.00,,54.19,0.0,0,Recurrent/Convolutional,TrellisNet,https://github.com/locuslab/trellisnet,1,TrellisNet-MoS (1.4x larger) PTB,TrellisNet-MoS (1.4x larger) PTB
TRIMELMext (247M),"Zexuan Zhong, Tao Lei, Danqi Chen",2022-05-25,Training Language Models with Memory Augmentation,35.00,,https://arxiv.org/abs/2205.12674,247000000.00,,204.72,,,0.00,103000000.00,31200000000000000000.00,103000000.00,206000000.00,WikiText-103,15.37,,,0.0,0,Transformer,Transformer-XL,https://github.com/princeton-nlp/TRIME,1,TRIMELMext (247M),TRIMELMext (247M)
TRIMELMext (7M),"Zexuan Zhong, Tao Lei, Danqi Chen",2022-05-25,Training Language Models with Memory Augmentation,35.00,,https://arxiv.org/abs/2205.12674,7000000.00,,47.72,,,0.00,103000000.00,206000000000000000.00,103000000.00,206000000.00,WikiText-103,42.36,,,0.0,0,Transformer,Transformer-XL,https://github.com/princeton-nlp/TRIME,1,TRIMELMext (7M),TRIMELMext (7M)
TRIMELMlong (150M),"Zexuan Zhong, Tao Lei, Danqi Chen",2022-05-25,Training Language Models with Memory Augmentation,35.00,,https://arxiv.org/abs/2205.12674,150000000.00,,139.81,,,0.00,103000000.00,13000000000000000000.00,103000000.00,206000000.00,WikiText-103,22.66,,,0.0,0,Transformer,Transformer-XL,https://github.com/princeton-nlp/TRIME,1,TRIMELMlong (150M),TRIMELMlong (150M)
True-Regularization+Finetune,"Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",2019-04-08,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,24.00,,https://arxiv.org/pdf/1904.04163,7000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,54.00,0.0,0,Recurrent,RNN,,1,True-Regularization+Finetune,True-Regularization+Finetune
True-Regularization+Finetune+Dynamic-Eval,"Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",2019-04-08,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,24.00,,https://arxiv.org/pdf/1904.04163,7000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,47.60,0.0,0,Recurrent,RNN,,1,True-Regularization+Finetune+Dynamic-Eval,True-Regularization+Finetune+Dynamic-Eval
TSLM+MoS (PTB),"Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",2019-01-31,A Generalized Language Model in Tensor Space,21.00,,https://arxiv.org/pdf/1901.11167,2630000.00,,,,,0.00,929000.00,0.00,,929000.00,Penn TreeBank,,,83.60,0.0,0,,TSLM,,1,TSLM+MoS (PTB),TSLM+MoS (PTB)
TSLM+MoS (WT2),"Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",2019-01-31,A Generalized Language Model in Tensor Space,21.00,,https://arxiv.org/pdf/1901.11167,9120000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,81.00,,0.0,0,,TSLM,,1,TSLM+MoS (WT2),TSLM+MoS (WT2)
Turing-NLG,Corby Rosset,2020-02-13,Turing-NLG: A 17-billion-parameter language model by Microsoft,,,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,17000000000.00,,3.39,,,0.00,46400000000.00,1.5999999999999998e+22,,46400000000.00,,10.21,,,0.5,0,Transformer,NLG,,1,Turing-NLG,Turing-NLG
"Variational (untied weights, MC) LSTM (Large)","Yarin Gal, Zoubin Ghahramani",2015-12-16,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,1838.00,,https://arxiv.org/abs/1512.05287?context=stat,66000000.00,,16.00,,,0.00,888000.00,5620000000000000.00,,888000.00,Penn TreeBank,,,73.40,0.0,0,Recurrent,RNN,https://github.com/yaringal/BayesianRNN,1,"""Variational (untied weights, MC) LSTM (Large)""","Variational (untied weights, MC) LSTM (Large)"
Variational RHN + WT,"Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",2016-07-12,Recurrent Highway Networks,493.00,,https://arxiv.org/abs/1607.03474,23000000.00,,20.00,,,0.00,929000.00,2560000000000000.00,,929000.00,Penn TreeBank,,,65.40,0.0,0,Recurrent,RHN,https://github.com/julian121266/RecurrentHighwayNetworks,1,Variational RHN + WT,Variational RHN + WT
VD-LSTM+REAL Large,"Hakan Inan, Khashayar Khosravi, Richard Socher",2016-11-04,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,397.00,,https://arxiv.org/abs/1611.01462,51000000.00,,75.00,,,0.00,929000.00,21300000000000000.00,,929000.00,Penn TreeBank,,,68.50,0.0,0,Recurrent,LSTM,,1,VD-LSTM+REAL Large,VD-LSTM+REAL Large
VD-LSTM+REAL Medium,"Hakan Inan, Khashayar Khosravi, Richard Socher",2016-11-04,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,397.00,,https://arxiv.org/abs/1611.01462,20400000.00,,,,,1.00,2080000.00,0.00,,2080000.00,WikiText-2,,87.00,,0.0,0,Recurrent,LSTM,,1,VD-LSTM+REAL Medium,VD-LSTM+REAL Medium
VD-LSTM+REAL Small,"Hakan Inan, Khashayar Khosravi, Richard Socher",2016-11-04,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,397.00,,https://arxiv.org/abs/1611.01462,6080000.00,,60.00,,,1.00,2080000.00,4550000000000000.00,,2080000.00,WikiText-2,,98.90,,0.0,0,Recurrent,LSTM,,1,VD-LSTM+REAL Small,VD-LSTM+REAL Small
VD-RHN,"Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",2016-07-12,Recurrent Highway Networks,493.00,,https://arxiv.org/abs/1607.03474,32000000.00,,20.00,,,0.00,929000.00,3570000000000000.00,,929000.00,Penn TreeBank,,,68.50,0.0,0,Recurrent,RHN,https://github.com/julian121266/RecurrentHighwayNetworks,1,VD-RHN,VD-RHN
VRNS-RNN-3-3-5,"Brian DuSell, David Chiang",2022-10-04,The Surprising Computational Power of Nondeterministic Stack RNNs,1.00,1.0,https://arxiv.org/pdf/2210.01343,1500000.00,,,,,1.00,929000.00,0.00,,929000.00,,,,120.12,0.0,0,Recurrent,RNN,,1,VRNS-RNN-3-3-5,VRNS-RNN-3-3-5
WD+LR+M,"Ross M. Clarke, Elre T. Oldewage, José Miguel Hernández-Lobato",2021-10-20,Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation,5.00,,https://arxiv.org/pdf/2110.10461,,,72.00,,,1.00,929000.00,,,929000.00,,,,100.00,0.0,0,Recurrent,LSTM,https://github.com/rmclarke/OptimisingWeightUpdateHyperparameters,0,WD+LR+M,WD+LR+M
WeNet (PTB),"Zhiheng Huang, Bing Xiang",2019-04-08,WeNet: Weighted Networks for Recurrent Network Architecture Search,5.00,,https://arxiv.org/pdf/1904.03819,23000000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,54.80,0.0,0,NAS,WeNet,,1,WeNet (PTB),WeNet (PTB)
WeNet (WT2),"Zhiheng Huang, Bing Xiang",2019-04-08,WeNet: Weighted Networks for Recurrent Network Architecture Search,5.00,,https://arxiv.org/pdf/1904.03819,33000000.00,,,,,0.00,2080000.00,0.00,,2080000.00,,,66.60,,0.0,0,NAS,WeNet,,1,"WeNet (WT2),WeNet (Penn Treebank)","WeNet (WT2), WeNet (Penn Treebank)"
Word-Independent-SRNN+KN5,"Youssef Oualil, Clayton Greenberg, Mittul Singh, Dietrich Klakow",2017-03-23,Sequential Recurrent Neural Networks for Language Modeling,7.00,,https://arxiv.org/pdf/1703.08068,5320000.00,,,,,0.00,929000.00,0.00,,929000.00,,,,94.00,0.0,0,Recurrent,RNN,,1,Word-Independent-SRNN+KN5,Word-Independent-SRNN+KN5
Zoneout + Variational LSTM (PTB),"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016-09-26,Pointer Sentinel Mixture Models,1558.00,,https://arxiv.org/abs/1609.07843,21000000.00,,64.00,,,0.00,929000.00,7490000000000000.00,,929000.00,Penn TreeBank,,,80.60,0.0,0,Recurrent,LSTM,,1,Zoneout + Variational LSTM (PTB),Zoneout + Variational LSTM (PTB)
Zoneout + Variational LSTM (WT2),"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016-09-26,Pointer Sentinel Mixture Models,1558.00,,https://arxiv.org/abs/1609.07843,21000000.00,,64.00,,,0.00,2080000.00,16800000000000000.00,,2080000.00,WikiText-2,,100.90,,0.0,0,Recurrent,LSTM,,1,Zoneout + Variational LSTM (WT2),Zoneout + Variational LSTM (WT2)