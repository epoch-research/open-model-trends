{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from data import *\n",
    "from plotting import *\n",
    "from regression import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_dir = 'results/benchmark/initial/model/'\n",
    "code_results_dir = 'results/benchmark/initial/code/'\n",
    "os.makedirs(model_results_dir, exist_ok=True)\n",
    "os.makedirs(code_results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pcd_df = load_pcd_df()\n",
    "benchmark_df = load_benchmark_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Author(s)</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Peer reviewed?</th>\n",
       "      <th>Link</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Training Compute</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>...</th>\n",
       "      <th>Perplexity (WT2)</th>\n",
       "      <th>Perplexity (PTB)</th>\n",
       "      <th>Zero-shot?</th>\n",
       "      <th>Uses Cache</th>\n",
       "      <th>Architecture</th>\n",
       "      <th>Base Model</th>\n",
       "      <th>GitHub</th>\n",
       "      <th>Complete row</th>\n",
       "      <th>All ML Systems</th>\n",
       "      <th>System (from All ML Systems)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)</td>\n",
       "      <td>Sho Takase, Jun Suzuki, Masaaki Nagata</td>\n",
       "      <td>2018-08-30</td>\n",
       "      <td>Direct Output Connection for a High-Rank Langu...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1808.10143</td>\n",
       "      <td>114000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>https://github.com/nttcslab-nlp/doc_lm</td>\n",
       "      <td>1</td>\n",
       "      <td>(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)</td>\n",
       "      <td>(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)</td>\n",
       "      <td>Sho Takase, Jun Suzuki, Masaaki Nagata</td>\n",
       "      <td>2018-08-30</td>\n",
       "      <td>Direct Output Connection for a High-Rank Langu...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1808.10143</td>\n",
       "      <td>185000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>https://github.com/nttcslab-nlp/doc_lm</td>\n",
       "      <td>1</td>\n",
       "      <td>(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)</td>\n",
       "      <td>(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$\\infty$-former (SM)</td>\n",
       "      <td>Pedro Henrique Martins, Zita Marinho, André F....</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>$\\infty$-former: Infinite Memory Transformer</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2109.00301</td>\n",
       "      <td>117000000.0</td>\n",
       "      <td>1.200000e+22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>GPT</td>\n",
       "      <td>https://github.com/deep-spin/infinite-former</td>\n",
       "      <td>1</td>\n",
       "      <td>$\\infty$-former (SM)</td>\n",
       "      <td>$\\infty$-former (SM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-layer-LSTM</td>\n",
       "      <td>H. T. Kung, Bradley McDanel, Sai Qian Zhang</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>Term Revealing: Furthering Quantization at Run...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2007.06389</td>\n",
       "      <td>86500000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>86.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1-layer-LSTM</td>\n",
       "      <td>1-layer-LSTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-layer skip-LSTM + dropout tuning (PTB)</td>\n",
       "      <td>Gábor Melis, Charles Blundell, Tomáš Kočiský, ...</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>Pushing the bounds of dropout</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1805.09208</td>\n",
       "      <td>5400000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2-layer skip-LSTM + dropout tuning (PTB)</td>\n",
       "      <td>2-layer skip-LSTM + dropout tuning (PTB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>WeNet (PTB)</td>\n",
       "      <td>Zhiheng Huang, Bing Xiang</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>WeNet: Weighted Networks for Recurrent Network...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/1904.03819</td>\n",
       "      <td>23000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NAS</td>\n",
       "      <td>WeNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WeNet (PTB)</td>\n",
       "      <td>WeNet (PTB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>WeNet (WT2)</td>\n",
       "      <td>Zhiheng Huang, Bing Xiang</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>WeNet: Weighted Networks for Recurrent Network...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/1904.03819</td>\n",
       "      <td>33000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>66.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NAS</td>\n",
       "      <td>WeNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WeNet (WT2),WeNet (Penn Treebank)</td>\n",
       "      <td>WeNet (WT2), WeNet (Penn Treebank)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Word-Independent-SRNN+KN5</td>\n",
       "      <td>Youssef Oualil, Clayton Greenberg, Mittul Sing...</td>\n",
       "      <td>2017-03-23</td>\n",
       "      <td>Sequential Recurrent Neural Networks for Langu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/1703.08068</td>\n",
       "      <td>5320000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>RNN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Word-Independent-SRNN+KN5</td>\n",
       "      <td>Word-Independent-SRNN+KN5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Zoneout + Variational LSTM (PTB)</td>\n",
       "      <td>Stephen Merity, Caiming Xiong, James Bradbury,...</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>Pointer Sentinel Mixture Models</td>\n",
       "      <td>1558.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1609.07843</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Zoneout + Variational LSTM (PTB)</td>\n",
       "      <td>Zoneout + Variational LSTM (PTB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Zoneout + Variational LSTM (WT2)</td>\n",
       "      <td>Stephen Merity, Caiming Xiong, James Bradbury,...</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>Pointer Sentinel Mixture Models</td>\n",
       "      <td>1558.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1609.07843</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Zoneout + Variational LSTM (WT2)</td>\n",
       "      <td>Zoneout + Variational LSTM (WT2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       System  \\\n",
       "0    (ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)   \n",
       "1    (ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)   \n",
       "2                        $\\infty$-former (SM)   \n",
       "3                                1-layer-LSTM   \n",
       "4    2-layer skip-LSTM + dropout tuning (PTB)   \n",
       "..                                        ...   \n",
       "403                               WeNet (PTB)   \n",
       "404                               WeNet (WT2)   \n",
       "405                 Word-Independent-SRNN+KN5   \n",
       "406          Zoneout + Variational LSTM (PTB)   \n",
       "407          Zoneout + Variational LSTM (WT2)   \n",
       "\n",
       "                                             Author(s) Publication date  \\\n",
       "0               Sho Takase, Jun Suzuki, Masaaki Nagata       2018-08-30   \n",
       "1               Sho Takase, Jun Suzuki, Masaaki Nagata       2018-08-30   \n",
       "2    Pedro Henrique Martins, Zita Marinho, André F....       2021-09-01   \n",
       "3          H. T. Kung, Bradley McDanel, Sai Qian Zhang       2020-07-13   \n",
       "4    Gábor Melis, Charles Blundell, Tomáš Kočiský, ...       2018-05-23   \n",
       "..                                                 ...              ...   \n",
       "403                          Zhiheng Huang, Bing Xiang       2019-04-08   \n",
       "404                          Zhiheng Huang, Bing Xiang       2019-04-08   \n",
       "405  Youssef Oualil, Clayton Greenberg, Mittul Sing...       2017-03-23   \n",
       "406  Stephen Merity, Caiming Xiong, James Bradbury,...       2016-09-26   \n",
       "407  Stephen Merity, Caiming Xiong, James Bradbury,...       2016-09-26   \n",
       "\n",
       "                                             Reference  Citations  \\\n",
       "0    Direct Output Connection for a High-Rank Langu...       36.0   \n",
       "1    Direct Output Connection for a High-Rank Langu...       36.0   \n",
       "2         $\\infty$-former: Infinite Memory Transformer       31.0   \n",
       "3    Term Revealing: Furthering Quantization at Run...        9.0   \n",
       "4                        Pushing the bounds of dropout       14.0   \n",
       "..                                                 ...        ...   \n",
       "403  WeNet: Weighted Networks for Recurrent Network...        5.0   \n",
       "404  WeNet: Weighted Networks for Recurrent Network...        5.0   \n",
       "405  Sequential Recurrent Neural Networks for Langu...        7.0   \n",
       "406                    Pointer Sentinel Mixture Models     1558.0   \n",
       "407                    Pointer Sentinel Mixture Models     1558.0   \n",
       "\n",
       "     Peer reviewed?                              Link   Parameters  \\\n",
       "0               NaN  https://arxiv.org/abs/1808.10143  114000000.0   \n",
       "1               NaN  https://arxiv.org/abs/1808.10143  185000000.0   \n",
       "2               NaN  https://arxiv.org/abs/2109.00301  117000000.0   \n",
       "3               NaN  https://arxiv.org/pdf/2007.06389   86500000.0   \n",
       "4               NaN  https://arxiv.org/abs/1805.09208    5400000.0   \n",
       "..              ...                               ...          ...   \n",
       "403             NaN  https://arxiv.org/pdf/1904.03819   23000000.0   \n",
       "404             NaN  https://arxiv.org/pdf/1904.03819   33000000.0   \n",
       "405             NaN  https://arxiv.org/pdf/1703.08068    5320000.0   \n",
       "406             NaN  https://arxiv.org/abs/1609.07843   21000000.0   \n",
       "407             NaN  https://arxiv.org/abs/1609.07843   21000000.0   \n",
       "\n",
       "     Training Compute  Epoch  ...  Perplexity (WT2)  Perplexity (PTB)  \\\n",
       "0                 NaN  300.0  ...               NaN             47.17   \n",
       "1                 NaN  300.0  ...             53.09               NaN   \n",
       "2        1.200000e+22    1.0  ...               NaN               NaN   \n",
       "3                 NaN    NaN  ...             86.85               NaN   \n",
       "4                 NaN    NaN  ...               NaN             55.30   \n",
       "..                ...    ...  ...               ...               ...   \n",
       "403               NaN    NaN  ...               NaN             54.80   \n",
       "404               NaN    NaN  ...             66.60               NaN   \n",
       "405               NaN    NaN  ...               NaN             94.00   \n",
       "406               NaN   64.0  ...               NaN             80.60   \n",
       "407               NaN   64.0  ...            100.90               NaN   \n",
       "\n",
       "     Zero-shot?  Uses Cache Architecture  Base Model  \\\n",
       "0           0.0           0    Recurrent        LSTM   \n",
       "1           0.0           0    Recurrent        LSTM   \n",
       "2           1.0           0  Transformer         GPT   \n",
       "3           0.0           0    Recurrent        LSTM   \n",
       "4           0.0           0    Recurrent        LSTM   \n",
       "..          ...         ...          ...         ...   \n",
       "403         0.0           0          NAS       WeNet   \n",
       "404         0.0           0          NAS       WeNet   \n",
       "405         0.0           0    Recurrent         RNN   \n",
       "406         0.0           0    Recurrent        LSTM   \n",
       "407         0.0           0    Recurrent        LSTM   \n",
       "\n",
       "                                           GitHub Complete row  \\\n",
       "0          https://github.com/nttcslab-nlp/doc_lm            1   \n",
       "1          https://github.com/nttcslab-nlp/doc_lm            1   \n",
       "2    https://github.com/deep-spin/infinite-former            1   \n",
       "3                                             NaN            1   \n",
       "4                                             NaN            1   \n",
       "..                                            ...          ...   \n",
       "403                                           NaN            1   \n",
       "404                                           NaN            1   \n",
       "405                                           NaN            1   \n",
       "406                                           NaN            1   \n",
       "407                                           NaN            1   \n",
       "\n",
       "                               All ML Systems  \\\n",
       "0    (ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)   \n",
       "1    (ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)   \n",
       "2                        $\\infty$-former (SM)   \n",
       "3                                1-layer-LSTM   \n",
       "4    2-layer skip-LSTM + dropout tuning (PTB)   \n",
       "..                                        ...   \n",
       "403                               WeNet (PTB)   \n",
       "404         WeNet (WT2),WeNet (Penn Treebank)   \n",
       "405                 Word-Independent-SRNN+KN5   \n",
       "406          Zoneout + Variational LSTM (PTB)   \n",
       "407          Zoneout + Variational LSTM (WT2)   \n",
       "\n",
       "                 System (from All ML Systems)  \n",
       "0    (ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)  \n",
       "1    (ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)  \n",
       "2                        $\\infty$-former (SM)  \n",
       "3                                1-layer-LSTM  \n",
       "4    2-layer skip-LSTM + dropout tuning (PTB)  \n",
       "..                                        ...  \n",
       "403                               WeNet (PTB)  \n",
       "404        WeNet (WT2), WeNet (Penn Treebank)  \n",
       "405                 Word-Independent-SRNN+KN5  \n",
       "406          Zoneout + Variational LSTM (PTB)  \n",
       "407          Zoneout + Variational LSTM (WT2)  \n",
       "\n",
       "[408 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set System as index\n",
    "benchmark_df.set_index('System', inplace=True)\n",
    "pcd_df.set_index('System', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$\\\\infty$-former (SM)',\n",
       " '(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB)',\n",
       " '(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)',\n",
       " '1-layer-LSTM',\n",
       " '2-layer skip-LSTM + dropout tuning (PTB)',\n",
       " '2-layer skip-LSTM + dropout tuning (WT2)',\n",
       " '2-layer-LSTM+Deep-Gradient-Compression',\n",
       " '2nd order FOFE-FNNLM',\n",
       " '3-Layer-Tensor-Transformer+AdaHessian',\n",
       " '4 layer Densely Connected LSTM',\n",
       " '4 layer QRNN (h=2500)',\n",
       " '4 layer QRNN + dynamic evaluation',\n",
       " '4-gram + 8 DENN',\n",
       " '6-Layer-Tensor-Transformer+AdaHessian',\n",
       " 'ADP-FAIRSEQ+NGRAMRES',\n",
       " 'AFP+FPI (PTB)',\n",
       " 'AFP+FPI (WT2)',\n",
       " 'ALiBi (L=3072, Lvalid = 3072)',\n",
       " 'AWD-FWM (PTB)',\n",
       " 'AWD-FWM (WT2)',\n",
       " 'AWD-LSTM',\n",
       " 'AWD-LSTM + DeFINE',\n",
       " 'AWD-LSTM + MoS + Partial Shuffled',\n",
       " 'AWD-LSTM + Phrase Induction + finetuning',\n",
       " 'AWD-LSTM + dynamic eval (PTB)',\n",
       " 'AWD-LSTM + dynamic eval (WT2)',\n",
       " 'AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB)',\n",
       " 'AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)',\n",
       " 'AWD-LSTM+Behaviorial-Gating',\n",
       " 'AWD-LSTM+WT+Cache+IOG (PTB)',\n",
       " 'AWD-LSTM+WT+Cache+IOG (WT2)',\n",
       " 'AWD-LSTM-DOC (fin) (23M)',\n",
       " 'AWD-LSTM-DOC (fin) (37M)',\n",
       " 'AWD-LSTM-DRILL + dynamic evaluation† (PTB)',\n",
       " 'AWD-LSTM-DRILL + dynamic evaluation† (WT2)',\n",
       " 'AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)',\n",
       " 'AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)',\n",
       " 'AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)',\n",
       " 'AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)',\n",
       " 'AWD-LSTM-MoS+PDR + dynamic evaluation (PTB)',\n",
       " 'AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)',\n",
       " 'Adaptive Input Transformer + RD',\n",
       " 'Adaptive Inputs + LayerDrop',\n",
       " 'Adaptive LSTM + DeFINE',\n",
       " 'AdvSoft + 4 layer QRNN + dynamic evaluation',\n",
       " 'Adversarial + AWD-LSTM-MoS + partial shuffled',\n",
       " 'All-attention network + adaptive span',\n",
       " 'Alleviated TOI 10 (PTB)',\n",
       " 'Alleviated TOI 10 (WT103)',\n",
       " 'Alleviated TOI 10 (WT2)',\n",
       " 'Amended-DARTS',\n",
       " 'B2T connection (16L)',\n",
       " 'BERT-Large-CAS (PTB+WT2+WT103)',\n",
       " 'BERT-Large-CAS (WT103)',\n",
       " 'BERT-Large-CAS (WT2)',\n",
       " 'BLOOM-1.7B',\n",
       " 'BLOOM-1B',\n",
       " 'BLOOM-3B',\n",
       " 'BLOOM-560M',\n",
       " 'BLOOM-7.1B',\n",
       " 'Base LM + kNN LM + Continuous Cache',\n",
       " 'Byte-mLSTM+emb+WN+VD',\n",
       " 'CD-GraB (WT103)',\n",
       " 'CD-GraB (WT2)',\n",
       " 'CODA',\n",
       " 'CT-MoS (PTB)',\n",
       " 'CT-MoS (WT2)',\n",
       " 'CT-MoS + DynamicEval (PTB)',\n",
       " 'CT-MoS + DynamicEval (WT2)',\n",
       " 'Char-CNN-BiLSTM',\n",
       " 'Chinchilla',\n",
       " 'Compress-LSTM (4.6M)',\n",
       " 'Compress-LSTM (66M)',\n",
       " 'Compressive Transformers for Long-Range Sequence Modelling',\n",
       " 'CryptoGRU',\n",
       " 'D-LSRC(100)+KN5',\n",
       " 'D-LSRC(200)+KN5',\n",
       " 'DARTS',\n",
       " 'DARTS (second order)',\n",
       " 'DEQ-Transformer (Medium, Adaptive Embedding)',\n",
       " 'DEQ-Transformer (Post-LN) + Jacobian Regularisation',\n",
       " 'DEQ-TrellisNet',\n",
       " 'DITTO',\n",
       " 'DOC + Finetune∗ + Partial Shuffle (PTB)',\n",
       " 'DOC + Finetune∗ + Partial Shuffle (WT2)',\n",
       " 'DOT(S)-RNN',\n",
       " 'DeLight',\n",
       " 'Decay RNN',\n",
       " 'Deep RNN',\n",
       " 'Delta RNN (+ full context)',\n",
       " 'Densely Connected LSTM + Var. Dropout',\n",
       " 'DiffQ Transformer (16L)',\n",
       " 'DiffStk-MRNN',\n",
       " 'Dropout-LSTM+Noise(Bernoulli) (PTB)',\n",
       " 'Dropout-LSTM+Noise(Bernoulli) (WT2)',\n",
       " 'Dropout-LSTM+Noise(Laplace)',\n",
       " 'E-SPA',\n",
       " 'EGRU (PTB)',\n",
       " 'EGRU (WT2)',\n",
       " 'EI-REHN-1000D',\n",
       " 'EI-REHN-1200D',\n",
       " 'ENAS',\n",
       " 'EN^2AS with performance reward',\n",
       " 'ERNIE-Doc (151M)',\n",
       " 'ERNIE-Doc (247M)',\n",
       " 'Engine-Base (NE)',\n",
       " 'Engine-XL(NE)',\n",
       " 'FAIRSEQ Adaptive Inputs',\n",
       " 'FMMformer (2-kernel fast weight + Band20)',\n",
       " 'FNetAR Medium',\n",
       " 'Fairseq + UID: variance',\n",
       " 'Feedback Transformer',\n",
       " 'Fine-tuned-AWD-LSTM-DOC(fin)',\n",
       " 'Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB)',\n",
       " 'Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2)',\n",
       " 'Fraternal dropout + AWD-LSTM 3-layer (PTB)',\n",
       " 'Fraternal dropout + AWD-LSTM 3-layer (WT2)',\n",
       " 'GCNN-14',\n",
       " 'GCRN-M1, dropout',\n",
       " 'GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB)',\n",
       " 'GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)',\n",
       " 'GLM-10B',\n",
       " 'GLM-10B-bidirectional',\n",
       " 'GLM-10B-unidirectional',\n",
       " 'GLM-130B',\n",
       " 'GLM-2B',\n",
       " 'GPT-2 (1.5B, Curriculum Learning 45K)',\n",
       " 'GPT-2 (117M)',\n",
       " 'GPT-2 (117M, SLW 110K)',\n",
       " 'GPT-2 (345M)',\n",
       " 'GPT-2 (762M)',\n",
       " 'GPT-2 (fine-tuned with HYDRA)',\n",
       " 'GPT-2-Medium+Pixelfly',\n",
       " 'GPT-2-Small+Pixelfly',\n",
       " 'GPT-3 175B (davinci)',\n",
       " 'GPT-J-6B',\n",
       " 'GPT-Neo-1.3B',\n",
       " 'GPT-Neo-1.3B (finetuned)',\n",
       " 'GPT-Neo-125M',\n",
       " 'GPT-Neo-2.7B',\n",
       " 'GPT-Neo-2.7B (finetuned on PTB)',\n",
       " 'GPT-Neo-2.7B (finetuned)',\n",
       " 'GPT-NeoX-20B',\n",
       " 'GPT2+CoreLM+Fine-Tuning',\n",
       " 'GPT2-Large+LHOPT',\n",
       " 'GPT2-LayerFusion-WS',\n",
       " 'GPT3-6.7B (rerun of original)',\n",
       " 'GPT3-6.7B + muP',\n",
       " 'GRU + p-tHSM (pretrain via Brown) (PTB)',\n",
       " 'GRU + p-tHSM (pretrain via Brown) (WT103)',\n",
       " 'GRU + p-tHSM (pretrain via Brown) (WT2)',\n",
       " 'Gated HORNN (3rd order)',\n",
       " 'Gopher (280B)',\n",
       " 'Gopher (7.1B)',\n",
       " 'Grown to Prune Two-layer stacked LSTM',\n",
       " 'H-LSTM+wg+rcp+rcg+wp',\n",
       " 'HSO',\n",
       " 'Hybrid H3-1.3B',\n",
       " 'Hybrid H3-125M',\n",
       " 'Hybrid H3-2.7B',\n",
       " 'Hybrid H3-355M',\n",
       " 'Hyena-3-slim',\n",
       " 'ISS',\n",
       " 'Integer Transformer',\n",
       " 'KnGPT2',\n",
       " 'LBL',\n",
       " 'LLaMA-13B',\n",
       " 'LLaMA-13B (LoRA finetuned)',\n",
       " 'LLaMA-33B',\n",
       " 'LLaMA-33B (LoRA finetuned)',\n",
       " 'LLaMA-65B',\n",
       " 'LLaMA-65B (LoRA finetuned)',\n",
       " 'LLaMA-7B',\n",
       " 'LLaMA-7B (LoRA finetuned)',\n",
       " 'LSTM (2018)',\n",
       " 'LSTM (Hebbian, Cache, MbPA)',\n",
       " 'LSTM (PTB)',\n",
       " 'LSTM (WT103)',\n",
       " 'LSTM (WT2)',\n",
       " 'LSTM + dynamic eval',\n",
       " 'LSTM(large)+Sememe+cell',\n",
       " 'LSTM(medium)+Sememe+cell',\n",
       " 'LSTM+Adam+Lookahead',\n",
       " 'LSTM+GraB',\n",
       " 'LSTM+NeuralCache',\n",
       " 'LSTM+Noise(Beta)',\n",
       " 'LSTM-3-layer+Gadam',\n",
       " 'LSTM-300units',\n",
       " 'LSTM-Char-Large',\n",
       " 'LSTM-Large+Behaviorial-Gating',\n",
       " 'LSTM-Medium+Behaviorial-Gating',\n",
       " 'LSTM-MemoryAug (PTB)',\n",
       " 'LSTM-MemoryAug (WT2)',\n",
       " 'LTM',\n",
       " 'L_UL-seq',\n",
       " 'LaMemo',\n",
       " 'Large regularized LSTM',\n",
       " 'Linear Transformer (large)',\n",
       " 'Linear Transformer (small)',\n",
       " 'Local Transformer',\n",
       " 'MGK 4 heads (medium)',\n",
       " 'MGK 8 heads (small)',\n",
       " 'MMLSTM',\n",
       " 'Megatron-LM (2.5B)',\n",
       " 'Megatron-LM (355M)',\n",
       " 'Megatron-LM (8.3B)',\n",
       " 'MemSizer',\n",
       " 'Memformer (4 encoder + 16 decoder)',\n",
       " 'MicroNet (Adaptive, Cache)',\n",
       " 'Mogrifier (d2, MC) + dynamic eval',\n",
       " 'Mogrifier (d2, MoS2, MC) + dynamic eval',\n",
       " 'Mogrifier RLSTM (PTB)',\n",
       " 'Mogrifier RLSTM (WT2)',\n",
       " 'Monarch-GPT-2-Medium',\n",
       " 'Monarch-GPT-2-Small',\n",
       " 'Multi-cell LSTM',\n",
       " 'Multipop Adaptive Continuous Stack (PTB)',\n",
       " 'Multipop Adaptive Continuous Stack (WT2)',\n",
       " 'N-gram',\n",
       " 'N-gram+Cache',\n",
       " 'NAS+ESS (156M)',\n",
       " 'NAS+ESS (23M)',\n",
       " 'NLM',\n",
       " 'NMM(LSTM+RNN)',\n",
       " 'NMST+GPT-2',\n",
       " 'Neural Architecture Search with base 8 and shared embeddings',\n",
       " 'Neural cache model (size=2000)',\n",
       " 'Neural cache model (size=2000) (300M)',\n",
       " 'NoPos',\n",
       " 'ONLSTM-SYD',\n",
       " 'OPT-1.3B',\n",
       " 'OPT-1.3B (finetuned on PTB)',\n",
       " 'OPT-1.3B (finetuned)',\n",
       " 'OPT-125M (finetuned on PTB)',\n",
       " 'OPT-125M (finetuned)',\n",
       " 'OPT-13B',\n",
       " 'OPT-175B',\n",
       " 'OPT-2.7B',\n",
       " 'OPT-2.7B (finetuned on PTB)',\n",
       " 'OPT-2.7B (finetuned on WT2)',\n",
       " 'OPT-30B',\n",
       " 'OPT-350M',\n",
       " 'OPT-6.7B',\n",
       " 'OPT-66B',\n",
       " 'PAR Transformer Large',\n",
       " 'PermuteFormer',\n",
       " 'Pointer Sentinel-LSTM',\n",
       " 'Pointer Sentinel-LSTM (medium)',\n",
       " 'Pythia-1.4b',\n",
       " 'Pythia-12b',\n",
       " 'Pythia-160m',\n",
       " 'Pythia-1b',\n",
       " 'Pythia-2.8b',\n",
       " 'Pythia-410m',\n",
       " 'Pythia-6.9b',\n",
       " 'Pythia-70m',\n",
       " 'QRNN',\n",
       " 'Quantized ADMM',\n",
       " 'R-Transformer',\n",
       " 'RETRO-7B',\n",
       " 'RFA-GATE-Gaussian-Stateful Big',\n",
       " 'RGC+ASQ (PTB)',\n",
       " 'RGC+ASQ (WT2)',\n",
       " 'RHN(depth=40)',\n",
       " 'RHN+HSG(depth=40)',\n",
       " 'RNN',\n",
       " 'RNN (SGD+CLR) (PTB)',\n",
       " 'RNN + char2-MS-vec',\n",
       " 'RNN + char3-MS-vec',\n",
       " 'RNN + char4-MS-vec',\n",
       " 'RNN Baseline',\n",
       " 'RNN+LDA',\n",
       " 'RNN+LDA+KN5+cache',\n",
       " 'RNN+LSA+KN5+cache (model combination w/ linear extrapolation)',\n",
       " 'RNN+weight noise+dynamic eval',\n",
       " 'RNNLM + Dynamic KL Regularization',\n",
       " 'RNNLM + Dynamic KL Regularization (WT2)',\n",
       " 'RNS-RNN',\n",
       " 'RSM',\n",
       " 'Relational Memory Core',\n",
       " 'Routing Transformer',\n",
       " 'S + I-Attention (3)',\n",
       " 'S4',\n",
       " 'SCRN(Structurally Constrained Recurrent Network)',\n",
       " 'SPALM + RelationLM',\n",
       " 'SPALM + kNN',\n",
       " 'SPN-4',\n",
       " 'SPN-4+KN5',\n",
       " 'SRU++ Base',\n",
       " 'SRU++ Large',\n",
       " 'SRU++ Large only 2 attention layers (k=5)',\n",
       " 'Sandwich Transformer',\n",
       " 'Scatterbrain',\n",
       " 'Search-Proven Best LSTM',\n",
       " 'Segatron -XL base, M=150 + HCP',\n",
       " 'Segatron XL base, M=384',\n",
       " 'Segatron XL large, M=384',\n",
       " 'Segatron-XL large, M=384 + HCP',\n",
       " 'Selfish-RNN (AWD-LSTM-MoS)',\n",
       " 'Selfish-RNN (ON-LSTM)',\n",
       " 'Selfish-RNN (SNT-ASGD) Stacked LSTMs',\n",
       " 'Selfish-RNN (SNT-ASGD)RHNs',\n",
       " 'Shortformer',\n",
       " 'Sparse Wide GPT-3 Small',\n",
       " 'SparseOPT-13B',\n",
       " 'SparseOPT-175B',\n",
       " 'SparseOPT-30B',\n",
       " 'SparseOPT-66B',\n",
       " 'Stack RNN',\n",
       " 'Subformer (122M)',\n",
       " 'Subformer (83M)',\n",
       " 'Subformer (96M)',\n",
       " 'T2R + Pretrain',\n",
       " 'T2R + Random Init',\n",
       " 'T2R 75% + Pretrain',\n",
       " 'TCN (13M)',\n",
       " 'TCN (148M)',\n",
       " 'TF-LM-discourse LSTM (PTB)',\n",
       " 'TF-LM-discourse LSTM (WT2)',\n",
       " 'TPM-LVD',\n",
       " 'TRIMELMext (247M)',\n",
       " 'TRIMELMext (7M)',\n",
       " 'TRIMELMlong (150M)',\n",
       " 'TSLM+MoS (PTB)',\n",
       " 'TSLM+MoS (WT2)',\n",
       " 'TaLK Convolution',\n",
       " 'Temporal Convolutional Attention-based Network(TCAN) (PTB)',\n",
       " 'Temporal Convolutional Attention-based Network(TCAN) (WT2)',\n",
       " 'Tensor-Transformer(1core)+PN (PTB)',\n",
       " 'Tensor-Transformer(1core)+PN (WT103)',\n",
       " 'Tensorized Transformer (151M)',\n",
       " 'Tensorized Transformer (257M)',\n",
       " 'Tensorized Transformer (core-2)',\n",
       " 'Tensorized Transformer (large PTB)',\n",
       " 'Tensorized Transformer (small)',\n",
       " 'TransfoRNN(d=1024)(2-layer) (PTB)',\n",
       " 'TransfoRNN(d=1024)(2-layer) (WT2)',\n",
       " 'Transformer + Average Attention Network',\n",
       " 'Transformer + GFM',\n",
       " 'Transformer LM + MinSen',\n",
       " 'Transformer Large + HCP',\n",
       " 'Transformer+Recurrent Windows of Context',\n",
       " 'Transformer-C',\n",
       " 'Transformer-XL + AutoDropout (PTB)',\n",
       " 'Transformer-XL + AutoDropout (WT2)',\n",
       " 'Transformer-XL + PowerSGD + L-Greco',\n",
       " 'Transformer-XL + RMS dynamic eval',\n",
       " 'Transformer-XL + RMT',\n",
       " 'Transformer-XL + SIS',\n",
       " 'Transformer-XL DeFINE (107M)',\n",
       " 'Transformer-XL DeFINE (141M)',\n",
       " 'Transformer-XL Large + Phrase Induction',\n",
       " 'Transformer-XL+AdamGapAware(GA)',\n",
       " 'Transformer-XL+AdamP',\n",
       " 'Transformer-XL+WN+AdamP',\n",
       " 'Transformer-XL-ptb',\n",
       " 'TransformerXL + spectrum control',\n",
       " 'TransformerXL+RelationLM',\n",
       " 'TransformerXL-LayerFusion-CA',\n",
       " 'TrellisNet',\n",
       " 'True-Regularization+Finetune',\n",
       " 'True-Regularization+Finetune+Dynamic-Eval',\n",
       " 'Turing-NLG',\n",
       " 'VD-LSTM+REAL Large',\n",
       " 'VD-LSTM+REAL Medium',\n",
       " 'VD-LSTM+REAL Small',\n",
       " 'VD-RHN',\n",
       " 'VRNS-RNN-3-3-5',\n",
       " 'Variational (untied weights, MC) LSTM (Large)',\n",
       " 'Variational RHN + WT',\n",
       " 'WD+LR+M',\n",
       " 'WeNet (PTB)',\n",
       " 'WeNet (WT2)',\n",
       " 'Word-Independent-SRNN+KN5',\n",
       " 'Zoneout + Variational LSTM (PTB)',\n",
       " 'Zoneout + Variational LSTM (WT2)',\n",
       " 'aLSTM(depth-2)+RecurrentPolicy (PTB)',\n",
       " 'aLSTM(depth-2)+RecurrentPolicy (WT2)',\n",
       " 'bRSM + cache',\n",
       " 'base LM+GNN',\n",
       " 'base LM+GNN+kNN',\n",
       " 'dense-IndRNN+dynamic eval',\n",
       " 'genCNN + dyn eval',\n",
       " 'mini-GPT-2+Active-AdamW',\n",
       " 'rTop-k(distributed setting)',\n",
       " 'retrieval-quality-kNN-LMs',\n",
       " 'top-down frozen classifier'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find overlapping System between the two dataframes\n",
    "systems = set(pcd_df.index) & set(benchmark_df.index)\n",
    "systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter systems that have non-empty value for 'Model accessibility' in pcd_df\n",
    "systems_with_model_accessibility = list(systems & set(pcd_df[pcd_df['Model accessibility'].notnull()].index))\n",
    "len(systems_with_model_accessibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter systems that have non-empty value for 'Code accessibility' in pcd_df\n",
    "systems_with_code_accessibility = list(systems & set(pcd_df[pcd_df['Code accessibility'].notnull()].index))\n",
    "len(systems_with_code_accessibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "153\n",
      "111\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "print(sum(benchmark_df.loc[:, 'Perplexity (WT103)'].notna()))\n",
    "print(sum(benchmark_df.loc[systems_with_model_accessibility, 'Perplexity (WT103)'].notna()))\n",
    "print(sum(benchmark_df.loc[:, 'Perplexity (WT2)'].notna()))\n",
    "print(sum(benchmark_df.loc[systems_with_model_accessibility, 'Perplexity (WT2)'].notna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "153\n",
      "111\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "print(sum(benchmark_df.loc[:, 'Perplexity (WT103)'].notna()))\n",
    "print(sum(benchmark_df.loc[systems_with_code_accessibility, 'Perplexity (WT103)'].notna()))\n",
    "print(sum(benchmark_df.loc[:, 'Perplexity (WT2)'].notna()))\n",
    "print(sum(benchmark_df.loc[systems_with_code_accessibility, 'Perplexity (WT2)'].notna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign model and code accessibility values to benchmark_df\n",
    "benchmark_df.loc[systems_with_model_accessibility, 'Model accessibility'] = pcd_df.loc[\n",
    "    systems_with_model_accessibility, 'Model accessibility'\n",
    "]\n",
    "\n",
    "benchmark_df.loc[systems_with_code_accessibility, 'Code accessibility'] = pcd_df.loc[\n",
    "    systems_with_code_accessibility, 'Code accessibility'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df.loc[systems_with_model_accessibility, 'Training compute (FLOP)'] = pcd_df.loc[\n",
    "    systems_with_model_accessibility, 'Training compute (FLOP)'\n",
    "]\n",
    "benchmark_df.loc[systems_with_code_accessibility, 'Training compute (FLOP)'] = pcd_df.loc[\n",
    "    systems_with_code_accessibility, 'Training compute (FLOP)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filtered_benchmark_df = benchmark_df.loc[systems_with_model_accessibility]\n",
    "code_filtered_benchmark_df = benchmark_df.loc[systems_with_code_accessibility]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model accessibility plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wikitext perplexity of systems with model accessibility\n",
    "fig = px.scatter(\n",
    "    model_filtered_benchmark_df,\n",
    "    x='Publication date',\n",
    "    y='Perplexity (WT103)',\n",
    "    color='Model accessibility',\n",
    "    # text=filtered_benchmark_df.index\n",
    ")\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='top center')\n",
    "save_plot(fig, model_results_dir, 'WT103_vs_model_accessibility')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    model_filtered_benchmark_df,\n",
    "    x='Training compute (FLOP)',\n",
    "    y='Perplexity (WT103)',\n",
    "    color='Model accessibility'\n",
    ")\n",
    "\n",
    "# Log x-axis\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='middle left')\n",
    "\n",
    "# Zoom in\n",
    "fig.update_layout(yaxis_range=[0, 60])\n",
    "\n",
    "save_plot(fig, model_results_dir, 'WT103_vs_compute')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    model_filtered_benchmark_df,\n",
    "    x='Training compute (FLOP)',\n",
    "    y='Perplexity (WT2)',\n",
    "    color='Model accessibility',\n",
    "    # text=filtered_benchmark_df.index\n",
    ")\n",
    "\n",
    "# Log x-axis\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='middle left')\n",
    "\n",
    "save_plot(fig, model_results_dir, 'WT2_vs_compute')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    model_filtered_benchmark_df,\n",
    "    x='Training compute (FLOP)',\n",
    "    y='Perplexity (PTB)',\n",
    "    color='Model accessibility',\n",
    "    # text=filtered_benchmark_df.index\n",
    ")\n",
    "\n",
    "# Log x-axis\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='middle left')\n",
    "\n",
    "save_plot(fig, code_results_dir, 'PTB_vs_compute')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code accessibility plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wikitext perplexity of systems with code accessibility\n",
    "fig = px.scatter(\n",
    "    code_filtered_benchmark_df,\n",
    "    x='Publication date',\n",
    "    y='Perplexity (WT103)',\n",
    "    color='Code accessibility',\n",
    "    # text=filtered_benchmark_df.index\n",
    ")\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "save_plot(fig, code_results_dir, 'WT103_vs_code_accessibility')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    code_filtered_benchmark_df,\n",
    "    x='Training compute (FLOP)',\n",
    "    y='Perplexity (WT103)',\n",
    "    color='Code accessibility'\n",
    ")\n",
    "\n",
    "# Log x-axis\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='middle left')\n",
    "\n",
    "# Zoom in\n",
    "fig.update_layout(yaxis_range=[0, 60])\n",
    "\n",
    "save_plot(fig, code_results_dir, 'WT103_vs_compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    code_filtered_benchmark_df,\n",
    "    x='Training compute (FLOP)',\n",
    "    y='Perplexity (WT2)',\n",
    "    color='Code accessibility',\n",
    "    # text=filtered_benchmark_df.index\n",
    ")\n",
    "\n",
    "# Log x-axis\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='middle left')\n",
    "\n",
    "save_plot(fig, code_results_dir, 'WT2_vs_compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    code_filtered_benchmark_df,\n",
    "    x='Training compute (FLOP)',\n",
    "    y='Perplexity (PTB)',\n",
    "    color='Code accessibility',\n",
    "    # text=filtered_benchmark_df.index\n",
    ")\n",
    "\n",
    "# Log x-axis\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Move text to top\n",
    "fig.update_traces(textposition='middle left')\n",
    "\n",
    "save_plot(fig, code_results_dir, 'PTB_vs_compute')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
